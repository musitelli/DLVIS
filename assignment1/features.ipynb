{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060c71d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Google Colab\n",
      "/Users/mateo/Library/CloudStorage/OneDrive-MusitelliFilm&Digital/04 - FING/DLVIS/Entregables/assignment1/cs231n/datasets\n",
      "/Users/mateo/Library/CloudStorage/OneDrive-MusitelliFilm&Digital/04 - FING/DLVIS/Entregables/assignment1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Check if running on Google Colab\n",
    "# If so, mount Google Drive and download CIFAR-10 dataset\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # This mounts your Google Drive to the Colab VM.\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "    # assignment folder, e.g. 'cs231n/assignments/assignment1/'\n",
    "    FOLDERNAME = None\n",
    "    assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "    # Now that we've mounted your Drive, this ensures that\n",
    "    # the Python interpreter of the Colab VM can load\n",
    "    # python files from within it.\n",
    "    sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "    # This downloads the CIFAR-10 dataset to your Drive\n",
    "    # if it doesn't already exist.\n",
    "    %cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "    !bash get_datasets.sh\n",
    "    %cd /content/drive/My\\ Drive/$FOLDERNAME\n",
    "# END OF COLAB SETUP\n",
    "\n",
    "# If not running on Google Colab, download CIFAR-10 dataset locally\n",
    "else:\n",
    "    print(\"Not running on Google Colab\")\n",
    "    %cd ./cs231n/datasets/\n",
    "    !bash get_datasets.sh\n",
    "    %cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56528d9",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Image features exercise\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](https://eva.fing.edu.uy/mod/assign/view.php?id=194303) on the course website.*\n",
    "\n",
    "We have seen that we can achieve reasonable performance on an image classification task by training a linear classifier on the pixels of the input image. In this exercise we will show that we can improve our classification performance by training linear classifiers not on raw pixels but on features that are computed from the raw pixels.\n",
    "\n",
    "All of your work for this exercise will be done in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed3d6f7",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9de2e",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Load data\n",
    "Similar to previous exercises, we will load CIFAR-10 data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "498a936a",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "from cs231n.features import color_histogram_hsv, hog_feature\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "\n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b699165d",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Extract Features\n",
    "For each image we will compute a Histogram of Oriented\n",
    "Gradients (HOG) as well as a color histogram using the hue channel in HSV\n",
    "color space. We form our final feature vector for each image by concatenating\n",
    "the HOG and color histogram feature vectors.\n",
    "\n",
    "Roughly speaking, HOG should capture the texture of the image while ignoring\n",
    "color information, and the color histogram represents the color of the input\n",
    "image while ignoring texture. As a result, we expect that using both together\n",
    "ought to work better than using either alone. Verifying this assumption would\n",
    "be a good thing to try for your own interest.\n",
    "\n",
    "The `hog_feature` and `color_histogram_hsv` functions both operate on a single\n",
    "image and return a feature vector for that image. The `extract_features`\n",
    "function takes a set of images and a list of feature functions and evaluates\n",
    "each feature function on each image, storing the results in a matrix where\n",
    "each column is the concatenation of all feature vectors for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1877e14",
   "metadata": {
    "scrolled": true,
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting features for 1000 / 49000 images\n",
      "Done extracting features for 2000 / 49000 images\n",
      "Done extracting features for 3000 / 49000 images\n",
      "Done extracting features for 4000 / 49000 images\n",
      "Done extracting features for 5000 / 49000 images\n",
      "Done extracting features for 6000 / 49000 images\n",
      "Done extracting features for 7000 / 49000 images\n",
      "Done extracting features for 8000 / 49000 images\n",
      "Done extracting features for 9000 / 49000 images\n",
      "Done extracting features for 10000 / 49000 images\n",
      "Done extracting features for 11000 / 49000 images\n",
      "Done extracting features for 12000 / 49000 images\n",
      "Done extracting features for 13000 / 49000 images\n",
      "Done extracting features for 14000 / 49000 images\n",
      "Done extracting features for 15000 / 49000 images\n",
      "Done extracting features for 16000 / 49000 images\n",
      "Done extracting features for 17000 / 49000 images\n",
      "Done extracting features for 18000 / 49000 images\n",
      "Done extracting features for 19000 / 49000 images\n",
      "Done extracting features for 20000 / 49000 images\n",
      "Done extracting features for 21000 / 49000 images\n",
      "Done extracting features for 22000 / 49000 images\n",
      "Done extracting features for 23000 / 49000 images\n",
      "Done extracting features for 24000 / 49000 images\n",
      "Done extracting features for 25000 / 49000 images\n",
      "Done extracting features for 26000 / 49000 images\n",
      "Done extracting features for 27000 / 49000 images\n",
      "Done extracting features for 28000 / 49000 images\n",
      "Done extracting features for 29000 / 49000 images\n",
      "Done extracting features for 30000 / 49000 images\n",
      "Done extracting features for 31000 / 49000 images\n",
      "Done extracting features for 32000 / 49000 images\n",
      "Done extracting features for 33000 / 49000 images\n",
      "Done extracting features for 34000 / 49000 images\n",
      "Done extracting features for 35000 / 49000 images\n",
      "Done extracting features for 36000 / 49000 images\n",
      "Done extracting features for 37000 / 49000 images\n",
      "Done extracting features for 38000 / 49000 images\n",
      "Done extracting features for 39000 / 49000 images\n",
      "Done extracting features for 40000 / 49000 images\n",
      "Done extracting features for 41000 / 49000 images\n",
      "Done extracting features for 42000 / 49000 images\n",
      "Done extracting features for 43000 / 49000 images\n",
      "Done extracting features for 44000 / 49000 images\n",
      "Done extracting features for 45000 / 49000 images\n",
      "Done extracting features for 46000 / 49000 images\n",
      "Done extracting features for 47000 / 49000 images\n",
      "Done extracting features for 48000 / 49000 images\n",
      "Done extracting features for 49000 / 49000 images\n"
     ]
    }
   ],
   "source": [
    "from cs231n.features import *\n",
    "\n",
    "num_color_bins = 10 # Number of bins in the color histogram\n",
    "feature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n",
    "X_train_feats = extract_features(X_train, feature_fns, verbose=True)\n",
    "X_val_feats = extract_features(X_val, feature_fns)\n",
    "X_test_feats = extract_features(X_test, feature_fns)\n",
    "\n",
    "# Preprocessing: Subtract the mean feature\n",
    "mean_feat = np.mean(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats -= mean_feat\n",
    "X_val_feats -= mean_feat\n",
    "X_test_feats -= mean_feat\n",
    "\n",
    "# Preprocessing: Divide by standard deviation. This ensures that each feature\n",
    "# has roughly the same scale.\n",
    "std_feat = np.std(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats /= std_feat\n",
    "X_val_feats /= std_feat\n",
    "X_test_feats /= std_feat\n",
    "\n",
    "# Preprocessing: Add a bias dimension\n",
    "X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\n",
    "X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])\n",
    "X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a07aac",
   "metadata": {},
   "source": [
    "## Train Softmax on features\n",
    "Using the multiclass Softmax code developed earlier in the assignment, train Softmax on top of the features extracted above; this should achieve better results than training Softmax directly on top of raw pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f29a59",
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateo/Library/CloudStorage/OneDrive-MusitelliFilm&Digital/04 - FING/DLVIS/Entregables/assignment1/cs231n/classifiers/softmax.py:94: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.mean(np.log(np.e**s_y / np.sum(np.e**s))) + reg * np.sum(W**2)\n",
      "/Users/mateo/Library/CloudStorage/OneDrive-MusitelliFilm&Digital/04 - FING/DLVIS/Entregables/assignment1/cs231n/classifiers/softmax.py:99: RuntimeWarning: invalid value encountered in divide\n",
      "  P = np.e ** s / np.sum(np.e ** s, axis = 1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.000000e-12 reg 1.000000e+01 train accuracy: 0.093143 val accuracy: 0.091000\n",
      "lr 1.000000e-12 reg 5.000000e+01 train accuracy: 0.085694 val accuracy: 0.088000\n",
      "lr 1.000000e-12 reg 1.000000e+02 train accuracy: 0.098327 val accuracy: 0.089000\n",
      "lr 1.000000e-12 reg 2.000000e+02 train accuracy: 0.089224 val accuracy: 0.091000\n",
      "lr 1.000000e-12 reg 1.000000e+03 train accuracy: 0.099633 val accuracy: 0.110000\n",
      "lr 1.000000e-12 reg 5.000000e+03 train accuracy: 0.093510 val accuracy: 0.091000\n",
      "lr 1.000000e-12 reg 2.500000e+04 train accuracy: 0.098286 val accuracy: 0.085000\n",
      "lr 1.000000e-12 reg 5.000000e+04 train accuracy: 0.113469 val accuracy: 0.109000\n",
      "lr 1.000000e-12 reg 1.000000e+06 train accuracy: 0.092918 val accuracy: 0.101000\n",
      "lr 1.000000e-12 reg 2.000000e+06 train accuracy: 0.100429 val accuracy: 0.105000\n",
      "lr 5.000000e-12 reg 1.000000e+01 train accuracy: 0.113490 val accuracy: 0.107000\n",
      "lr 5.000000e-12 reg 5.000000e+01 train accuracy: 0.117714 val accuracy: 0.140000\n",
      "lr 5.000000e-12 reg 1.000000e+02 train accuracy: 0.098122 val accuracy: 0.102000\n",
      "lr 5.000000e-12 reg 2.000000e+02 train accuracy: 0.105204 val accuracy: 0.102000\n",
      "lr 5.000000e-12 reg 1.000000e+03 train accuracy: 0.113735 val accuracy: 0.108000\n",
      "lr 5.000000e-12 reg 5.000000e+03 train accuracy: 0.095102 val accuracy: 0.102000\n",
      "lr 5.000000e-12 reg 2.500000e+04 train accuracy: 0.104531 val accuracy: 0.099000\n",
      "lr 5.000000e-12 reg 5.000000e+04 train accuracy: 0.095878 val accuracy: 0.090000\n",
      "lr 5.000000e-12 reg 1.000000e+06 train accuracy: 0.093388 val accuracy: 0.091000\n",
      "lr 5.000000e-12 reg 2.000000e+06 train accuracy: 0.117531 val accuracy: 0.110000\n",
      "lr 1.000000e-10 reg 1.000000e+01 train accuracy: 0.095571 val accuracy: 0.088000\n",
      "lr 1.000000e-10 reg 5.000000e+01 train accuracy: 0.089163 val accuracy: 0.100000\n",
      "lr 1.000000e-10 reg 1.000000e+02 train accuracy: 0.079755 val accuracy: 0.073000\n",
      "lr 1.000000e-10 reg 2.000000e+02 train accuracy: 0.100143 val accuracy: 0.098000\n",
      "lr 1.000000e-10 reg 1.000000e+03 train accuracy: 0.109020 val accuracy: 0.114000\n",
      "lr 1.000000e-10 reg 5.000000e+03 train accuracy: 0.102633 val accuracy: 0.098000\n",
      "lr 1.000000e-10 reg 2.500000e+04 train accuracy: 0.098469 val accuracy: 0.096000\n",
      "lr 1.000000e-10 reg 5.000000e+04 train accuracy: 0.113694 val accuracy: 0.116000\n",
      "lr 1.000000e-10 reg 1.000000e+06 train accuracy: 0.083286 val accuracy: 0.080000\n",
      "lr 1.000000e-10 reg 2.000000e+06 train accuracy: 0.115286 val accuracy: 0.123000\n",
      "lr 5.000000e-10 reg 1.000000e+01 train accuracy: 0.094898 val accuracy: 0.116000\n",
      "lr 5.000000e-10 reg 5.000000e+01 train accuracy: 0.102837 val accuracy: 0.099000\n",
      "lr 5.000000e-10 reg 1.000000e+02 train accuracy: 0.106184 val accuracy: 0.102000\n",
      "lr 5.000000e-10 reg 2.000000e+02 train accuracy: 0.098224 val accuracy: 0.107000\n",
      "lr 5.000000e-10 reg 1.000000e+03 train accuracy: 0.105184 val accuracy: 0.095000\n",
      "lr 5.000000e-10 reg 5.000000e+03 train accuracy: 0.105633 val accuracy: 0.097000\n",
      "lr 5.000000e-10 reg 2.500000e+04 train accuracy: 0.083327 val accuracy: 0.069000\n",
      "lr 5.000000e-10 reg 5.000000e+04 train accuracy: 0.084898 val accuracy: 0.100000\n",
      "lr 5.000000e-10 reg 1.000000e+06 train accuracy: 0.101408 val accuracy: 0.104000\n",
      "lr 5.000000e-10 reg 2.000000e+06 train accuracy: 0.101388 val accuracy: 0.096000\n",
      "lr 1.000000e-09 reg 1.000000e+01 train accuracy: 0.100653 val accuracy: 0.101000\n",
      "lr 1.000000e-09 reg 5.000000e+01 train accuracy: 0.075714 val accuracy: 0.068000\n",
      "lr 1.000000e-09 reg 1.000000e+02 train accuracy: 0.118122 val accuracy: 0.115000\n",
      "lr 1.000000e-09 reg 2.000000e+02 train accuracy: 0.132796 val accuracy: 0.139000\n",
      "lr 1.000000e-09 reg 1.000000e+03 train accuracy: 0.080286 val accuracy: 0.076000\n",
      "lr 1.000000e-09 reg 5.000000e+03 train accuracy: 0.098000 val accuracy: 0.106000\n",
      "lr 1.000000e-09 reg 2.500000e+04 train accuracy: 0.123694 val accuracy: 0.122000\n",
      "lr 1.000000e-09 reg 5.000000e+04 train accuracy: 0.102041 val accuracy: 0.116000\n",
      "lr 1.000000e-09 reg 1.000000e+06 train accuracy: 0.100204 val accuracy: 0.106000\n",
      "lr 1.000000e-09 reg 2.000000e+06 train accuracy: 0.084102 val accuracy: 0.075000\n",
      "lr 5.000000e-09 reg 1.000000e+01 train accuracy: 0.109163 val accuracy: 0.096000\n",
      "lr 5.000000e-09 reg 5.000000e+01 train accuracy: 0.097163 val accuracy: 0.106000\n",
      "lr 5.000000e-09 reg 1.000000e+02 train accuracy: 0.092592 val accuracy: 0.081000\n",
      "lr 5.000000e-09 reg 2.000000e+02 train accuracy: 0.101816 val accuracy: 0.113000\n",
      "lr 5.000000e-09 reg 1.000000e+03 train accuracy: 0.106204 val accuracy: 0.090000\n",
      "lr 5.000000e-09 reg 5.000000e+03 train accuracy: 0.105531 val accuracy: 0.092000\n",
      "lr 5.000000e-09 reg 2.500000e+04 train accuracy: 0.102592 val accuracy: 0.086000\n",
      "lr 5.000000e-09 reg 5.000000e+04 train accuracy: 0.105020 val accuracy: 0.083000\n",
      "lr 5.000000e-09 reg 1.000000e+06 train accuracy: 0.415306 val accuracy: 0.417000\n",
      "lr 5.000000e-09 reg 2.000000e+06 train accuracy: 0.412898 val accuracy: 0.409000\n",
      "lr 1.000000e-07 reg 1.000000e+01 train accuracy: 0.086735 val accuracy: 0.079000\n",
      "lr 1.000000e-07 reg 5.000000e+01 train accuracy: 0.101673 val accuracy: 0.108000\n",
      "lr 1.000000e-07 reg 1.000000e+02 train accuracy: 0.107816 val accuracy: 0.114000\n",
      "lr 1.000000e-07 reg 2.000000e+02 train accuracy: 0.113082 val accuracy: 0.132000\n",
      "lr 1.000000e-07 reg 1.000000e+03 train accuracy: 0.130061 val accuracy: 0.112000\n",
      "lr 1.000000e-07 reg 5.000000e+03 train accuracy: 0.104408 val accuracy: 0.106000\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.356837 val accuracy: 0.346000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.413265 val accuracy: 0.407000\n",
      "lr 1.000000e-07 reg 1.000000e+06 train accuracy: 0.401061 val accuracy: 0.403000\n",
      "lr 1.000000e-07 reg 2.000000e+06 train accuracy: 0.397714 val accuracy: 0.389000\n",
      "lr 5.000000e-07 reg 1.000000e+01 train accuracy: 0.100592 val accuracy: 0.097000\n",
      "lr 5.000000e-07 reg 5.000000e+01 train accuracy: 0.106306 val accuracy: 0.109000\n",
      "lr 5.000000e-07 reg 1.000000e+02 train accuracy: 0.111490 val accuracy: 0.117000\n",
      "lr 5.000000e-07 reg 2.000000e+02 train accuracy: 0.113673 val accuracy: 0.136000\n",
      "lr 5.000000e-07 reg 1.000000e+03 train accuracy: 0.117061 val accuracy: 0.104000\n",
      "lr 5.000000e-07 reg 5.000000e+03 train accuracy: 0.414082 val accuracy: 0.412000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.412735 val accuracy: 0.408000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.411286 val accuracy: 0.408000\n",
      "lr 5.000000e-07 reg 1.000000e+06 train accuracy: 0.332163 val accuracy: 0.306000\n",
      "lr 5.000000e-07 reg 2.000000e+06 train accuracy: 0.091735 val accuracy: 0.075000\n",
      "lr 1.000000e-05 reg 1.000000e+01 train accuracy: 0.246918 val accuracy: 0.249000\n",
      "lr 1.000000e-05 reg 5.000000e+01 train accuracy: 0.348020 val accuracy: 0.324000\n",
      "lr 1.000000e-05 reg 1.000000e+02 train accuracy: 0.401510 val accuracy: 0.386000\n",
      "lr 1.000000e-05 reg 2.000000e+02 train accuracy: 0.416714 val accuracy: 0.419000\n",
      "lr 1.000000e-05 reg 1.000000e+03 train accuracy: 0.409163 val accuracy: 0.400000\n",
      "lr 1.000000e-05 reg 5.000000e+03 train accuracy: 0.415102 val accuracy: 0.403000\n",
      "lr 1.000000e-05 reg 2.500000e+04 train accuracy: 0.356286 val accuracy: 0.367000\n",
      "lr 1.000000e-05 reg 5.000000e+04 train accuracy: 0.310776 val accuracy: 0.302000\n",
      "lr 1.000000e-05 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 2.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 1.000000e+01 train accuracy: 0.412306 val accuracy: 0.417000\n",
      "lr 5.000000e-05 reg 5.000000e+01 train accuracy: 0.414429 val accuracy: 0.417000\n",
      "lr 5.000000e-05 reg 1.000000e+02 train accuracy: 0.414224 val accuracy: 0.416000\n",
      "lr 5.000000e-05 reg 2.000000e+02 train accuracy: 0.410388 val accuracy: 0.397000\n",
      "lr 5.000000e-05 reg 1.000000e+03 train accuracy: 0.396980 val accuracy: 0.391000\n",
      "lr 5.000000e-05 reg 5.000000e+03 train accuracy: 0.361041 val accuracy: 0.388000\n",
      "lr 5.000000e-05 reg 2.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 2.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 1.000000e+01 train accuracy: 0.415204 val accuracy: 0.412000\n",
      "lr 1.000000e-03 reg 5.000000e+01 train accuracy: 0.406510 val accuracy: 0.412000\n",
      "lr 1.000000e-03 reg 1.000000e+02 train accuracy: 0.397388 val accuracy: 0.407000\n",
      "lr 1.000000e-03 reg 2.000000e+02 train accuracy: 0.381776 val accuracy: 0.372000\n",
      "lr 1.000000e-03 reg 1.000000e+03 train accuracy: 0.111510 val accuracy: 0.110000\n",
      "lr 1.000000e-03 reg 5.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 2.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 2.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 1.000000e+01 train accuracy: 0.410347 val accuracy: 0.415000\n",
      "lr 5.000000e-03 reg 5.000000e+01 train accuracy: 0.372694 val accuracy: 0.377000\n",
      "lr 5.000000e-03 reg 1.000000e+02 train accuracy: 0.299122 val accuracy: 0.297000\n",
      "lr 5.000000e-03 reg 2.000000e+02 train accuracy: 0.122286 val accuracy: 0.105000\n",
      "lr 5.000000e-03 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 5.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 2.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 2.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved: 0.419000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune the learning rate and regularization strength\n",
    "\n",
    "from cs231n.classifiers.linear_classifier import Softmax\n",
    "\n",
    "learning_rates = [1e-3, 5e-3, 1e-5, 5e-5, 1e-7, 5e-7, 1e-9, 5e-9, 1e-10, 5e-10, 1e-12, 5e-12]\n",
    "regularization_strengths = [10, 50, 1e2, 2e2, 1e3, 5e3, 2.5e4, 5e4, 1e6, 2e6]\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the Softmax;     #\n",
    "# save the best trained classifer in best_softmax. You might also want to play #\n",
    "# with different numbers of bins in the color histogram. If you are careful    #\n",
    "# you should be able to get accuracy of near 0.42 on the validation set.       #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# All-for-all GridSearch\n",
    "for lr in learning_rates:\n",
    "    for rs in regularization_strengths:\n",
    "\n",
    "        # Initialize classifier\n",
    "        softmax = Softmax()\n",
    "        # Train\n",
    "        loss_hist = softmax.train(X_train_feats, y_train, learning_rate=lr, reg=rs, num_iters=1500, verbose=False)\n",
    "        # Evaluate on train\n",
    "        y_train_pred = softmax.predict(X_train_feats)\n",
    "        # Evaluate on validation\n",
    "        y_val_pred = softmax.predict(X_val_feats)\n",
    "        \n",
    "        # Accuracies\n",
    "        train_acc = np.mean(y_train == y_train_pred)\n",
    "        val_acc = np.mean(y_val == y_val_pred)\n",
    "\n",
    "        # Update dictionary (train, val)\n",
    "        results[(lr,rs)] = (train_acc, val_acc)\n",
    "\n",
    "        # Compare with previous parameters\n",
    "        if val_acc > best_val:\n",
    "            # If accuracy increases, update the best result and the classifier\n",
    "            best_val = val_acc\n",
    "            best_softmax = softmax\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfb068d2",
   "metadata": {
    "test": "svm_test_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.425\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your trained Softmax on the test set: you should be able to get at least 0.40\n",
    "y_test_pred = best_softmax.predict(X_test_feats)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ab4f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 80 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# An important way to gain intuition about how an algorithm works is to\n",
    "# visualize the mistakes that it makes. In this visualization, we show examples\n",
    "# of images that are misclassified by our current system. The first column\n",
    "# shows images that our system labeled as \"plane\" but whose true label is\n",
    "# something other than \"plane\".\n",
    "\n",
    "examples_per_class = 8\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for cls, cls_name in enumerate(classes):\n",
    "    idxs = np.where((y_test != cls) & (y_test_pred == cls))[0]\n",
    "    idxs = np.random.choice(idxs, examples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)\n",
    "        plt.imshow(X_test[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cde5e7",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "### Inline question 1:\n",
    "Describe the misclassification results that you see. Do they make sense?\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "Errors make a lot of sense!\n",
    "\n",
    "The model pays special attention to pixel values and images showing a vast majority of pixel values that match, will be classified as belonging to the same class.\n",
    "\n",
    "For instance, fifth image on the ship column (starting from 0), displays a plane surrounding by blue... Blue is the color of the sea, which makes this image to be classified as a ship given its blue background."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59b590",
   "metadata": {},
   "source": [
    "## Neural Network on image features\n",
    "Earlier in this assigment we saw that training a two-layer neural network on raw pixels achieved better classification performance than linear classifiers on raw pixels. In this notebook we have seen that linear classifiers on image features outperform linear classifiers on raw pixels. \n",
    "\n",
    "For completeness, we should also try training a neural network on image features. This approach should outperform all previous approaches: you should easily be able to achieve over 55% classification accuracy on the test set; our best model achieves about 60% classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b1ebd7d",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 155)\n",
      "(49000, 154)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: Remove the bias dimension\n",
    "# Make sure to run this cell only ONCE\n",
    "print(X_train_feats.shape)\n",
    "X_train_feats = X_train_feats[:, :-1]\n",
    "X_val_feats = X_val_feats[:, :-1]\n",
    "X_test_feats = X_test_feats[:, :-1]\n",
    "\n",
    "print(X_train_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fbe3367",
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.001, 'num_epochs': 20, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 15300) loss: 2.305829\n",
      "(Epoch 0 / 20) train acc: 0.081000; val_acc: 0.111000\n",
      "(Iteration 101 / 15300) loss: 2.305859\n",
      "(Iteration 201 / 15300) loss: 2.305734\n",
      "(Iteration 301 / 15300) loss: 2.305643\n",
      "(Iteration 401 / 15300) loss: 2.305559\n",
      "(Iteration 501 / 15300) loss: 2.305439\n",
      "(Iteration 601 / 15300) loss: 2.305449\n",
      "(Iteration 701 / 15300) loss: 2.305137\n",
      "(Epoch 1 / 20) train acc: 0.202000; val_acc: 0.168000\n",
      "(Iteration 801 / 15300) loss: 2.305342\n",
      "(Iteration 901 / 15300) loss: 2.305419\n",
      "(Iteration 1001 / 15300) loss: 2.305173\n",
      "(Iteration 1101 / 15300) loss: 2.304792\n",
      "(Iteration 1201 / 15300) loss: 2.304856\n",
      "(Iteration 1301 / 15300) loss: 2.305256\n",
      "(Iteration 1401 / 15300) loss: 2.304795\n",
      "(Iteration 1501 / 15300) loss: 2.304830\n",
      "(Epoch 2 / 20) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 1601 / 15300) loss: 2.304808\n",
      "(Iteration 1701 / 15300) loss: 2.304780\n",
      "(Iteration 1801 / 15300) loss: 2.304674\n",
      "(Iteration 1901 / 15300) loss: 2.304547\n",
      "(Iteration 2001 / 15300) loss: 2.305017\n",
      "(Iteration 2101 / 15300) loss: 2.304745\n",
      "(Iteration 2201 / 15300) loss: 2.304568\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.083000\n",
      "(Iteration 2301 / 15300) loss: 2.304423\n",
      "(Iteration 2401 / 15300) loss: 2.304498\n",
      "(Iteration 2501 / 15300) loss: 2.304202\n",
      "(Iteration 2601 / 15300) loss: 2.304530\n",
      "(Iteration 2701 / 15300) loss: 2.304379\n",
      "(Iteration 2801 / 15300) loss: 2.304366\n",
      "(Iteration 2901 / 15300) loss: 2.304178\n",
      "(Iteration 3001 / 15300) loss: 2.303972\n",
      "(Epoch 4 / 20) train acc: 0.121000; val_acc: 0.083000\n",
      "(Iteration 3101 / 15300) loss: 2.304275\n",
      "(Iteration 3201 / 15300) loss: 2.304317\n",
      "(Iteration 3301 / 15300) loss: 2.304232\n",
      "(Iteration 3401 / 15300) loss: 2.304451\n",
      "(Iteration 3501 / 15300) loss: 2.303826\n",
      "(Iteration 3601 / 15300) loss: 2.304396\n",
      "(Iteration 3701 / 15300) loss: 2.304080\n",
      "(Iteration 3801 / 15300) loss: 2.303880\n",
      "(Epoch 5 / 20) train acc: 0.078000; val_acc: 0.079000\n",
      "(Iteration 3901 / 15300) loss: 2.303570\n",
      "(Iteration 4001 / 15300) loss: 2.304363\n",
      "(Iteration 4101 / 15300) loss: 2.304329\n",
      "(Iteration 4201 / 15300) loss: 2.303827\n",
      "(Iteration 4301 / 15300) loss: 2.303257\n",
      "(Iteration 4401 / 15300) loss: 2.303864\n",
      "(Iteration 4501 / 15300) loss: 2.303404\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 4601 / 15300) loss: 2.304081\n",
      "(Iteration 4701 / 15300) loss: 2.303302\n",
      "(Iteration 4801 / 15300) loss: 2.303391\n",
      "(Iteration 4901 / 15300) loss: 2.303504\n",
      "(Iteration 5001 / 15300) loss: 2.304019\n",
      "(Iteration 5101 / 15300) loss: 2.303526\n",
      "(Iteration 5201 / 15300) loss: 2.303199\n",
      "(Iteration 5301 / 15300) loss: 2.303424\n",
      "(Epoch 7 / 20) train acc: 0.113000; val_acc: 0.079000\n",
      "(Iteration 5401 / 15300) loss: 2.303738\n",
      "(Iteration 5501 / 15300) loss: 2.303241\n",
      "(Iteration 5601 / 15300) loss: 2.303363\n",
      "(Iteration 5701 / 15300) loss: 2.303423\n",
      "(Iteration 5801 / 15300) loss: 2.303614\n",
      "(Iteration 5901 / 15300) loss: 2.303952\n",
      "(Iteration 6001 / 15300) loss: 2.303517\n",
      "(Iteration 6101 / 15300) loss: 2.303043\n",
      "(Epoch 8 / 20) train acc: 0.090000; val_acc: 0.079000\n",
      "(Iteration 6201 / 15300) loss: 2.303637\n",
      "(Iteration 6301 / 15300) loss: 2.302944\n",
      "(Iteration 6401 / 15300) loss: 2.303111\n",
      "(Iteration 6501 / 15300) loss: 2.303182\n",
      "(Iteration 6601 / 15300) loss: 2.303345\n",
      "(Iteration 6701 / 15300) loss: 2.303425\n",
      "(Iteration 6801 / 15300) loss: 2.303452\n",
      "(Epoch 9 / 20) train acc: 0.103000; val_acc: 0.080000\n",
      "(Iteration 6901 / 15300) loss: 2.303533\n",
      "(Iteration 7001 / 15300) loss: 2.303185\n",
      "(Iteration 7101 / 15300) loss: 2.303181\n",
      "(Iteration 7201 / 15300) loss: 2.302985\n",
      "(Iteration 7301 / 15300) loss: 2.303218\n",
      "(Iteration 7401 / 15300) loss: 2.303163\n",
      "(Iteration 7501 / 15300) loss: 2.302883\n",
      "(Iteration 7601 / 15300) loss: 2.303186\n",
      "(Epoch 10 / 20) train acc: 0.107000; val_acc: 0.092000\n",
      "(Iteration 7701 / 15300) loss: 2.302815\n",
      "(Iteration 7801 / 15300) loss: 2.303297\n",
      "(Iteration 7901 / 15300) loss: 2.303015\n",
      "(Iteration 8001 / 15300) loss: 2.303136\n",
      "(Iteration 8101 / 15300) loss: 2.303136\n",
      "(Iteration 8201 / 15300) loss: 2.303315\n",
      "(Iteration 8301 / 15300) loss: 2.302877\n",
      "(Iteration 8401 / 15300) loss: 2.303246\n",
      "(Epoch 11 / 20) train acc: 0.135000; val_acc: 0.106000\n",
      "(Iteration 8501 / 15300) loss: 2.303071\n",
      "(Iteration 8601 / 15300) loss: 2.302880\n",
      "(Iteration 8701 / 15300) loss: 2.302697\n",
      "(Iteration 8801 / 15300) loss: 2.302779\n",
      "(Iteration 8901 / 15300) loss: 2.303198\n",
      "(Iteration 9001 / 15300) loss: 2.302494\n",
      "(Iteration 9101 / 15300) loss: 2.303280\n",
      "(Epoch 12 / 20) train acc: 0.156000; val_acc: 0.115000\n",
      "(Iteration 9201 / 15300) loss: 2.302587\n",
      "(Iteration 9301 / 15300) loss: 2.302851\n",
      "(Iteration 9401 / 15300) loss: 2.302625\n",
      "(Iteration 9501 / 15300) loss: 2.302955\n",
      "(Iteration 9601 / 15300) loss: 2.302542\n",
      "(Iteration 9701 / 15300) loss: 2.302968\n",
      "(Iteration 9801 / 15300) loss: 2.302410\n",
      "(Iteration 9901 / 15300) loss: 2.302810\n",
      "(Epoch 13 / 20) train acc: 0.171000; val_acc: 0.148000\n",
      "(Iteration 10001 / 15300) loss: 2.302966\n",
      "(Iteration 10101 / 15300) loss: 2.303049\n",
      "(Iteration 10201 / 15300) loss: 2.302662\n",
      "(Iteration 10301 / 15300) loss: 2.302600\n",
      "(Iteration 10401 / 15300) loss: 2.302843\n",
      "(Iteration 10501 / 15300) loss: 2.302334\n",
      "(Iteration 10601 / 15300) loss: 2.302303\n",
      "(Iteration 10701 / 15300) loss: 2.302484\n",
      "(Epoch 14 / 20) train acc: 0.188000; val_acc: 0.156000\n",
      "(Iteration 10801 / 15300) loss: 2.302865\n",
      "(Iteration 10901 / 15300) loss: 2.302280\n",
      "(Iteration 11001 / 15300) loss: 2.302538\n",
      "(Iteration 11101 / 15300) loss: 2.302366\n",
      "(Iteration 11201 / 15300) loss: 2.302397\n",
      "(Iteration 11301 / 15300) loss: 2.302052\n",
      "(Iteration 11401 / 15300) loss: 2.302277\n",
      "(Epoch 15 / 20) train acc: 0.132000; val_acc: 0.132000\n",
      "(Iteration 11501 / 15300) loss: 2.302384\n",
      "(Iteration 11601 / 15300) loss: 2.302366\n",
      "(Iteration 11701 / 15300) loss: 2.302809\n",
      "(Iteration 11801 / 15300) loss: 2.302148\n",
      "(Iteration 11901 / 15300) loss: 2.302295\n",
      "(Iteration 12001 / 15300) loss: 2.302250\n",
      "(Iteration 12101 / 15300) loss: 2.302619\n",
      "(Iteration 12201 / 15300) loss: 2.302777\n",
      "(Epoch 16 / 20) train acc: 0.153000; val_acc: 0.139000\n",
      "(Iteration 12301 / 15300) loss: 2.301574\n",
      "(Iteration 12401 / 15300) loss: 2.302152\n",
      "(Iteration 12501 / 15300) loss: 2.301926\n",
      "(Iteration 12601 / 15300) loss: 2.301873\n",
      "(Iteration 12701 / 15300) loss: 2.302571\n",
      "(Iteration 12801 / 15300) loss: 2.302657\n",
      "(Iteration 12901 / 15300) loss: 2.302224\n",
      "(Iteration 13001 / 15300) loss: 2.302414\n",
      "(Epoch 17 / 20) train acc: 0.167000; val_acc: 0.142000\n",
      "(Iteration 13101 / 15300) loss: 2.302437\n",
      "(Iteration 13201 / 15300) loss: 2.301671\n",
      "(Iteration 13301 / 15300) loss: 2.302072\n",
      "(Iteration 13401 / 15300) loss: 2.302227\n",
      "(Iteration 13501 / 15300) loss: 2.302751\n",
      "(Iteration 13601 / 15300) loss: 2.302032\n",
      "(Iteration 13701 / 15300) loss: 2.302304\n",
      "(Epoch 18 / 20) train acc: 0.152000; val_acc: 0.147000\n",
      "(Iteration 13801 / 15300) loss: 2.302114\n",
      "(Iteration 13901 / 15300) loss: 2.302025\n",
      "(Iteration 14001 / 15300) loss: 2.301374\n",
      "(Iteration 14101 / 15300) loss: 2.302360\n",
      "(Iteration 14201 / 15300) loss: 2.302008\n",
      "(Iteration 14301 / 15300) loss: 2.302011\n",
      "(Iteration 14401 / 15300) loss: 2.301921\n",
      "(Iteration 14501 / 15300) loss: 2.301954\n",
      "(Epoch 19 / 20) train acc: 0.204000; val_acc: 0.156000\n",
      "(Iteration 14601 / 15300) loss: 2.301661\n",
      "(Iteration 14701 / 15300) loss: 2.301596\n",
      "(Iteration 14801 / 15300) loss: 2.302049\n",
      "(Iteration 14901 / 15300) loss: 2.301596\n",
      "(Iteration 15001 / 15300) loss: 2.302540\n",
      "(Iteration 15101 / 15300) loss: 2.301886\n",
      "(Iteration 15201 / 15300) loss: 2.302076\n",
      "(Epoch 20 / 20) train acc: 0.209000; val_acc: 0.159000\n",
      "New best model found with validation accuracy: 0.1149\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.001, 'num_epochs': 20, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 30620) loss: 2.305852\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.088000\n",
      "(Iteration 101 / 30620) loss: 2.305698\n",
      "(Iteration 201 / 30620) loss: 2.305792\n",
      "(Iteration 301 / 30620) loss: 2.305599\n",
      "(Iteration 401 / 30620) loss: 2.305491\n",
      "(Iteration 501 / 30620) loss: 2.305693\n",
      "(Iteration 601 / 30620) loss: 2.305382\n",
      "(Iteration 701 / 30620) loss: 2.305009\n",
      "(Iteration 801 / 30620) loss: 2.305429\n",
      "(Iteration 901 / 30620) loss: 2.305393\n",
      "(Iteration 1001 / 30620) loss: 2.304931\n",
      "(Iteration 1101 / 30620) loss: 2.304984\n",
      "(Iteration 1201 / 30620) loss: 2.304886\n",
      "(Iteration 1301 / 30620) loss: 2.304922\n",
      "(Iteration 1401 / 30620) loss: 2.304995\n",
      "(Iteration 1501 / 30620) loss: 2.305129\n",
      "(Epoch 1 / 20) train acc: 0.155000; val_acc: 0.098000\n",
      "(Iteration 1601 / 30620) loss: 2.305025\n",
      "(Iteration 1701 / 30620) loss: 2.304582\n",
      "(Iteration 1801 / 30620) loss: 2.304589\n",
      "(Iteration 1901 / 30620) loss: 2.305086\n",
      "(Iteration 2001 / 30620) loss: 2.304991\n",
      "(Iteration 2101 / 30620) loss: 2.305241\n",
      "(Iteration 2201 / 30620) loss: 2.304543\n",
      "(Iteration 2301 / 30620) loss: 2.305548\n",
      "(Iteration 2401 / 30620) loss: 2.305646\n",
      "(Iteration 2501 / 30620) loss: 2.304649\n",
      "(Iteration 2601 / 30620) loss: 2.305048\n",
      "(Iteration 2701 / 30620) loss: 2.303807\n",
      "(Iteration 2801 / 30620) loss: 2.304069\n",
      "(Iteration 2901 / 30620) loss: 2.304418\n",
      "(Iteration 3001 / 30620) loss: 2.304318\n",
      "(Epoch 2 / 20) train acc: 0.115000; val_acc: 0.090000\n",
      "(Iteration 3101 / 30620) loss: 2.304672\n",
      "(Iteration 3201 / 30620) loss: 2.303952\n",
      "(Iteration 3301 / 30620) loss: 2.304383\n",
      "(Iteration 3401 / 30620) loss: 2.303853\n",
      "(Iteration 3501 / 30620) loss: 2.304811\n",
      "(Iteration 3601 / 30620) loss: 2.303331\n",
      "(Iteration 3701 / 30620) loss: 2.304187\n",
      "(Iteration 3801 / 30620) loss: 2.303335\n",
      "(Iteration 3901 / 30620) loss: 2.303670\n",
      "(Iteration 4001 / 30620) loss: 2.303803\n",
      "(Iteration 4101 / 30620) loss: 2.303802\n",
      "(Iteration 4201 / 30620) loss: 2.304093\n",
      "(Iteration 4301 / 30620) loss: 2.302931\n",
      "(Iteration 4401 / 30620) loss: 2.303471\n",
      "(Iteration 4501 / 30620) loss: 2.303050\n",
      "(Epoch 3 / 20) train acc: 0.144000; val_acc: 0.160000\n",
      "(Iteration 4601 / 30620) loss: 2.304127\n",
      "(Iteration 4701 / 30620) loss: 2.304248\n",
      "(Iteration 4801 / 30620) loss: 2.303035\n",
      "(Iteration 4901 / 30620) loss: 2.304224\n",
      "(Iteration 5001 / 30620) loss: 2.304145\n",
      "(Iteration 5101 / 30620) loss: 2.303382\n",
      "(Iteration 5201 / 30620) loss: 2.303546\n",
      "(Iteration 5301 / 30620) loss: 2.303610\n",
      "(Iteration 5401 / 30620) loss: 2.302936\n",
      "(Iteration 5501 / 30620) loss: 2.303524\n",
      "(Iteration 5601 / 30620) loss: 2.303693\n",
      "(Iteration 5701 / 30620) loss: 2.303852\n",
      "(Iteration 5801 / 30620) loss: 2.302868\n",
      "(Iteration 5901 / 30620) loss: 2.303932\n",
      "(Iteration 6001 / 30620) loss: 2.303325\n",
      "(Iteration 6101 / 30620) loss: 2.303110\n",
      "(Epoch 4 / 20) train acc: 0.165000; val_acc: 0.159000\n",
      "(Iteration 6201 / 30620) loss: 2.302605\n",
      "(Iteration 6301 / 30620) loss: 2.303185\n",
      "(Iteration 6401 / 30620) loss: 2.304129\n",
      "(Iteration 6501 / 30620) loss: 2.303044\n",
      "(Iteration 6601 / 30620) loss: 2.302778\n",
      "(Iteration 6701 / 30620) loss: 2.302858\n",
      "(Iteration 6801 / 30620) loss: 2.302179\n",
      "(Iteration 6901 / 30620) loss: 2.302927\n",
      "(Iteration 7001 / 30620) loss: 2.303139\n",
      "(Iteration 7101 / 30620) loss: 2.301794\n",
      "(Iteration 7201 / 30620) loss: 2.303403\n",
      "(Iteration 7301 / 30620) loss: 2.302225\n",
      "(Iteration 7401 / 30620) loss: 2.302023\n",
      "(Iteration 7501 / 30620) loss: 2.301781\n",
      "(Iteration 7601 / 30620) loss: 2.303486\n",
      "(Epoch 5 / 20) train acc: 0.165000; val_acc: 0.145000\n",
      "(Iteration 7701 / 30620) loss: 2.303023\n",
      "(Iteration 7801 / 30620) loss: 2.302284\n",
      "(Iteration 7901 / 30620) loss: 2.301708\n",
      "(Iteration 8001 / 30620) loss: 2.304825\n",
      "(Iteration 8101 / 30620) loss: 2.302253\n",
      "(Iteration 8201 / 30620) loss: 2.302285\n",
      "(Iteration 8301 / 30620) loss: 2.300997\n",
      "(Iteration 8401 / 30620) loss: 2.303414\n",
      "(Iteration 8501 / 30620) loss: 2.300105\n",
      "(Iteration 8601 / 30620) loss: 2.302410\n",
      "(Iteration 8701 / 30620) loss: 2.300405\n",
      "(Iteration 8801 / 30620) loss: 2.302243\n",
      "(Iteration 8901 / 30620) loss: 2.302463\n",
      "(Iteration 9001 / 30620) loss: 2.301717\n",
      "(Iteration 9101 / 30620) loss: 2.301941\n",
      "(Epoch 6 / 20) train acc: 0.148000; val_acc: 0.150000\n",
      "(Iteration 9201 / 30620) loss: 2.303919\n",
      "(Iteration 9301 / 30620) loss: 2.300899\n",
      "(Iteration 9401 / 30620) loss: 2.301727\n",
      "(Iteration 9501 / 30620) loss: 2.300124\n",
      "(Iteration 9601 / 30620) loss: 2.302136\n",
      "(Iteration 9701 / 30620) loss: 2.301270\n",
      "(Iteration 9801 / 30620) loss: 2.302029\n",
      "(Iteration 9901 / 30620) loss: 2.301682\n",
      "(Iteration 10001 / 30620) loss: 2.300472\n",
      "(Iteration 10101 / 30620) loss: 2.301939\n",
      "(Iteration 10201 / 30620) loss: 2.301186\n",
      "(Iteration 10301 / 30620) loss: 2.300374\n",
      "(Iteration 10401 / 30620) loss: 2.300512\n",
      "(Iteration 10501 / 30620) loss: 2.300328\n",
      "(Iteration 10601 / 30620) loss: 2.300243\n",
      "(Iteration 10701 / 30620) loss: 2.299278\n",
      "(Epoch 7 / 20) train acc: 0.159000; val_acc: 0.166000\n",
      "(Iteration 10801 / 30620) loss: 2.298704\n",
      "(Iteration 10901 / 30620) loss: 2.301741\n",
      "(Iteration 11001 / 30620) loss: 2.302796\n",
      "(Iteration 11101 / 30620) loss: 2.301963\n",
      "(Iteration 11201 / 30620) loss: 2.298951\n",
      "(Iteration 11301 / 30620) loss: 2.300206\n",
      "(Iteration 11401 / 30620) loss: 2.298339\n",
      "(Iteration 11501 / 30620) loss: 2.301851\n",
      "(Iteration 11601 / 30620) loss: 2.300958\n",
      "(Iteration 11701 / 30620) loss: 2.302955\n",
      "(Iteration 11801 / 30620) loss: 2.299291\n",
      "(Iteration 11901 / 30620) loss: 2.300108\n",
      "(Iteration 12001 / 30620) loss: 2.300834\n",
      "(Iteration 12101 / 30620) loss: 2.296466\n",
      "(Iteration 12201 / 30620) loss: 2.299942\n",
      "(Epoch 8 / 20) train acc: 0.158000; val_acc: 0.166000\n",
      "(Iteration 12301 / 30620) loss: 2.300113\n",
      "(Iteration 12401 / 30620) loss: 2.299380\n",
      "(Iteration 12501 / 30620) loss: 2.297778\n",
      "(Iteration 12601 / 30620) loss: 2.299734\n",
      "(Iteration 12701 / 30620) loss: 2.296302\n",
      "(Iteration 12801 / 30620) loss: 2.295562\n",
      "(Iteration 12901 / 30620) loss: 2.298644\n",
      "(Iteration 13001 / 30620) loss: 2.301077\n",
      "(Iteration 13101 / 30620) loss: 2.299016\n",
      "(Iteration 13201 / 30620) loss: 2.298192\n",
      "(Iteration 13301 / 30620) loss: 2.295139\n",
      "(Iteration 13401 / 30620) loss: 2.294478\n",
      "(Iteration 13501 / 30620) loss: 2.301224\n",
      "(Iteration 13601 / 30620) loss: 2.299009\n",
      "(Iteration 13701 / 30620) loss: 2.299675\n",
      "(Epoch 9 / 20) train acc: 0.179000; val_acc: 0.184000\n",
      "(Iteration 13801 / 30620) loss: 2.296833\n",
      "(Iteration 13901 / 30620) loss: 2.303013\n",
      "(Iteration 14001 / 30620) loss: 2.298730\n",
      "(Iteration 14101 / 30620) loss: 2.294502\n",
      "(Iteration 14201 / 30620) loss: 2.296313\n",
      "(Iteration 14301 / 30620) loss: 2.298690\n",
      "(Iteration 14401 / 30620) loss: 2.293012\n",
      "(Iteration 14501 / 30620) loss: 2.294313\n",
      "(Iteration 14601 / 30620) loss: 2.298529\n",
      "(Iteration 14701 / 30620) loss: 2.299870\n",
      "(Iteration 14801 / 30620) loss: 2.298124\n",
      "(Iteration 14901 / 30620) loss: 2.293576\n",
      "(Iteration 15001 / 30620) loss: 2.298277\n",
      "(Iteration 15101 / 30620) loss: 2.297318\n",
      "(Iteration 15201 / 30620) loss: 2.297881\n",
      "(Iteration 15301 / 30620) loss: 2.297490\n",
      "(Epoch 10 / 20) train acc: 0.190000; val_acc: 0.185000\n",
      "(Iteration 15401 / 30620) loss: 2.290552\n",
      "(Iteration 15501 / 30620) loss: 2.292427\n",
      "(Iteration 15601 / 30620) loss: 2.300721\n",
      "(Iteration 15701 / 30620) loss: 2.288025\n",
      "(Iteration 15801 / 30620) loss: 2.297755\n",
      "(Iteration 15901 / 30620) loss: 2.294344\n",
      "(Iteration 16001 / 30620) loss: 2.297314\n",
      "(Iteration 16101 / 30620) loss: 2.299626\n",
      "(Iteration 16201 / 30620) loss: 2.294848\n",
      "(Iteration 16301 / 30620) loss: 2.296905\n",
      "(Iteration 16401 / 30620) loss: 2.289427\n",
      "(Iteration 16501 / 30620) loss: 2.294461\n",
      "(Iteration 16601 / 30620) loss: 2.298309\n",
      "(Iteration 16701 / 30620) loss: 2.299973\n",
      "(Iteration 16801 / 30620) loss: 2.296481\n",
      "(Epoch 11 / 20) train acc: 0.190000; val_acc: 0.187000\n",
      "(Iteration 16901 / 30620) loss: 2.292416\n",
      "(Iteration 17001 / 30620) loss: 2.288716\n",
      "(Iteration 17101 / 30620) loss: 2.281206\n",
      "(Iteration 17201 / 30620) loss: 2.294005\n",
      "(Iteration 17301 / 30620) loss: 2.289814\n",
      "(Iteration 17401 / 30620) loss: 2.290765\n",
      "(Iteration 17501 / 30620) loss: 2.289317\n",
      "(Iteration 17601 / 30620) loss: 2.293001\n",
      "(Iteration 17701 / 30620) loss: 2.297198\n",
      "(Iteration 17801 / 30620) loss: 2.294681\n",
      "(Iteration 17901 / 30620) loss: 2.293820\n",
      "(Iteration 18001 / 30620) loss: 2.305453\n",
      "(Iteration 18101 / 30620) loss: 2.293651\n",
      "(Iteration 18201 / 30620) loss: 2.289867\n",
      "(Iteration 18301 / 30620) loss: 2.289501\n",
      "(Epoch 12 / 20) train acc: 0.192000; val_acc: 0.197000\n",
      "(Iteration 18401 / 30620) loss: 2.288187\n",
      "(Iteration 18501 / 30620) loss: 2.289503\n",
      "(Iteration 18601 / 30620) loss: 2.277790\n",
      "(Iteration 18701 / 30620) loss: 2.283720\n",
      "(Iteration 18801 / 30620) loss: 2.287366\n",
      "(Iteration 18901 / 30620) loss: 2.298479\n",
      "(Iteration 19001 / 30620) loss: 2.290272\n",
      "(Iteration 19101 / 30620) loss: 2.289359\n",
      "(Iteration 19201 / 30620) loss: 2.288711\n",
      "(Iteration 19301 / 30620) loss: 2.290873\n",
      "(Iteration 19401 / 30620) loss: 2.279353\n",
      "(Iteration 19501 / 30620) loss: 2.291188\n",
      "(Iteration 19601 / 30620) loss: 2.284192\n",
      "(Iteration 19701 / 30620) loss: 2.297461\n",
      "(Iteration 19801 / 30620) loss: 2.288956\n",
      "(Iteration 19901 / 30620) loss: 2.298002\n",
      "(Epoch 13 / 20) train acc: 0.202000; val_acc: 0.196000\n",
      "(Iteration 20001 / 30620) loss: 2.284455\n",
      "(Iteration 20101 / 30620) loss: 2.285890\n",
      "(Iteration 20201 / 30620) loss: 2.281026\n",
      "(Iteration 20301 / 30620) loss: 2.293344\n",
      "(Iteration 20401 / 30620) loss: 2.289162\n",
      "(Iteration 20501 / 30620) loss: 2.286054\n",
      "(Iteration 20601 / 30620) loss: 2.268974\n",
      "(Iteration 20701 / 30620) loss: 2.283215\n",
      "(Iteration 20801 / 30620) loss: 2.285018\n",
      "(Iteration 20901 / 30620) loss: 2.291227\n",
      "(Iteration 21001 / 30620) loss: 2.273236\n",
      "(Iteration 21101 / 30620) loss: 2.284661\n",
      "(Iteration 21201 / 30620) loss: 2.265996\n",
      "(Iteration 21301 / 30620) loss: 2.283054\n",
      "(Iteration 21401 / 30620) loss: 2.292681\n",
      "(Epoch 14 / 20) train acc: 0.220000; val_acc: 0.197000\n",
      "(Iteration 21501 / 30620) loss: 2.274869\n",
      "(Iteration 21601 / 30620) loss: 2.289891\n",
      "(Iteration 21701 / 30620) loss: 2.283345\n",
      "(Iteration 21801 / 30620) loss: 2.285773\n",
      "(Iteration 21901 / 30620) loss: 2.279002\n",
      "(Iteration 22001 / 30620) loss: 2.290943\n",
      "(Iteration 22101 / 30620) loss: 2.281063\n",
      "(Iteration 22201 / 30620) loss: 2.271085\n",
      "(Iteration 22301 / 30620) loss: 2.277542\n",
      "(Iteration 22401 / 30620) loss: 2.290504\n",
      "(Iteration 22501 / 30620) loss: 2.285473\n",
      "(Iteration 22601 / 30620) loss: 2.276562\n",
      "(Iteration 22701 / 30620) loss: 2.268067\n",
      "(Iteration 22801 / 30620) loss: 2.299913\n",
      "(Iteration 22901 / 30620) loss: 2.291014\n",
      "(Epoch 15 / 20) train acc: 0.224000; val_acc: 0.215000\n",
      "(Iteration 23001 / 30620) loss: 2.285290\n",
      "(Iteration 23101 / 30620) loss: 2.291216\n",
      "(Iteration 23201 / 30620) loss: 2.271644\n",
      "(Iteration 23301 / 30620) loss: 2.278080\n",
      "(Iteration 23401 / 30620) loss: 2.280480\n",
      "(Iteration 23501 / 30620) loss: 2.282612\n",
      "(Iteration 23601 / 30620) loss: 2.269582\n",
      "(Iteration 23701 / 30620) loss: 2.279460\n",
      "(Iteration 23801 / 30620) loss: 2.273685\n",
      "(Iteration 23901 / 30620) loss: 2.284269\n",
      "(Iteration 24001 / 30620) loss: 2.291548\n",
      "(Iteration 24101 / 30620) loss: 2.282462\n",
      "(Iteration 24201 / 30620) loss: 2.285610\n",
      "(Iteration 24301 / 30620) loss: 2.286716\n",
      "(Iteration 24401 / 30620) loss: 2.277633\n",
      "(Epoch 16 / 20) train acc: 0.212000; val_acc: 0.211000\n",
      "(Iteration 24501 / 30620) loss: 2.270515\n",
      "(Iteration 24601 / 30620) loss: 2.283780\n",
      "(Iteration 24701 / 30620) loss: 2.293839\n",
      "(Iteration 24801 / 30620) loss: 2.273758\n",
      "(Iteration 24901 / 30620) loss: 2.256397\n",
      "(Iteration 25001 / 30620) loss: 2.287159\n",
      "(Iteration 25101 / 30620) loss: 2.272585\n",
      "(Iteration 25201 / 30620) loss: 2.279848\n",
      "(Iteration 25301 / 30620) loss: 2.279373\n",
      "(Iteration 25401 / 30620) loss: 2.294491\n",
      "(Iteration 25501 / 30620) loss: 2.282106\n",
      "(Iteration 25601 / 30620) loss: 2.256068\n",
      "(Iteration 25701 / 30620) loss: 2.260206\n",
      "(Iteration 25801 / 30620) loss: 2.292804\n",
      "(Iteration 25901 / 30620) loss: 2.295957\n",
      "(Iteration 26001 / 30620) loss: 2.255662\n",
      "(Epoch 17 / 20) train acc: 0.253000; val_acc: 0.218000\n",
      "(Iteration 26101 / 30620) loss: 2.283926\n",
      "(Iteration 26201 / 30620) loss: 2.271678\n",
      "(Iteration 26301 / 30620) loss: 2.291383\n",
      "(Iteration 26401 / 30620) loss: 2.258205\n",
      "(Iteration 26501 / 30620) loss: 2.257129\n",
      "(Iteration 26601 / 30620) loss: 2.287940\n",
      "(Iteration 26701 / 30620) loss: 2.271973\n",
      "(Iteration 26801 / 30620) loss: 2.266586\n",
      "(Iteration 26901 / 30620) loss: 2.263991\n",
      "(Iteration 27001 / 30620) loss: 2.282720\n",
      "(Iteration 27101 / 30620) loss: 2.252725\n",
      "(Iteration 27201 / 30620) loss: 2.254816\n",
      "(Iteration 27301 / 30620) loss: 2.257425\n",
      "(Iteration 27401 / 30620) loss: 2.287320\n",
      "(Iteration 27501 / 30620) loss: 2.276089\n",
      "(Epoch 18 / 20) train acc: 0.230000; val_acc: 0.216000\n",
      "(Iteration 27601 / 30620) loss: 2.273796\n",
      "(Iteration 27701 / 30620) loss: 2.272222\n",
      "(Iteration 27801 / 30620) loss: 2.278037\n",
      "(Iteration 27901 / 30620) loss: 2.260336\n",
      "(Iteration 28001 / 30620) loss: 2.280025\n",
      "(Iteration 28101 / 30620) loss: 2.260699\n",
      "(Iteration 28201 / 30620) loss: 2.267272\n",
      "(Iteration 28301 / 30620) loss: 2.263553\n",
      "(Iteration 28401 / 30620) loss: 2.281565\n",
      "(Iteration 28501 / 30620) loss: 2.246593\n",
      "(Iteration 28601 / 30620) loss: 2.242492\n",
      "(Iteration 28701 / 30620) loss: 2.273635\n",
      "(Iteration 28801 / 30620) loss: 2.295306\n",
      "(Iteration 28901 / 30620) loss: 2.266601\n",
      "(Iteration 29001 / 30620) loss: 2.277943\n",
      "(Epoch 19 / 20) train acc: 0.216000; val_acc: 0.226000\n",
      "(Iteration 29101 / 30620) loss: 2.246005\n",
      "(Iteration 29201 / 30620) loss: 2.297955\n",
      "(Iteration 29301 / 30620) loss: 2.275701\n",
      "(Iteration 29401 / 30620) loss: 2.265756\n",
      "(Iteration 29501 / 30620) loss: 2.274948\n",
      "(Iteration 29601 / 30620) loss: 2.268449\n",
      "(Iteration 29701 / 30620) loss: 2.258224\n",
      "(Iteration 29801 / 30620) loss: 2.273345\n",
      "(Iteration 29901 / 30620) loss: 2.286715\n",
      "(Iteration 30001 / 30620) loss: 2.282963\n",
      "(Iteration 30101 / 30620) loss: 2.277485\n",
      "(Iteration 30201 / 30620) loss: 2.245623\n",
      "(Iteration 30301 / 30620) loss: 2.257634\n",
      "(Iteration 30401 / 30620) loss: 2.297460\n",
      "(Iteration 30501 / 30620) loss: 2.262544\n",
      "(Iteration 30601 / 30620) loss: 2.265134\n",
      "(Epoch 20 / 20) train acc: 0.234000; val_acc: 0.219000\n",
      "New best model found with validation accuracy: 0.1749\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.001, 'num_epochs': 20, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 15300) loss: 2.302910\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.103000\n",
      "(Iteration 101 / 15300) loss: 2.302868\n",
      "(Iteration 201 / 15300) loss: 2.302791\n",
      "(Iteration 301 / 15300) loss: 2.302768\n",
      "(Iteration 401 / 15300) loss: 2.302849\n",
      "(Iteration 501 / 15300) loss: 2.302828\n",
      "(Iteration 601 / 15300) loss: 2.302887\n",
      "(Iteration 701 / 15300) loss: 2.302923\n",
      "(Epoch 1 / 20) train acc: 0.092000; val_acc: 0.112000\n",
      "(Iteration 801 / 15300) loss: 2.302898\n",
      "(Iteration 901 / 15300) loss: 2.302586\n",
      "(Iteration 1001 / 15300) loss: 2.302576\n",
      "(Iteration 1101 / 15300) loss: 2.302870\n",
      "(Iteration 1201 / 15300) loss: 2.302662\n",
      "(Iteration 1301 / 15300) loss: 2.302548\n",
      "(Iteration 1401 / 15300) loss: 2.302605\n",
      "(Iteration 1501 / 15300) loss: 2.302520\n",
      "(Epoch 2 / 20) train acc: 0.093000; val_acc: 0.120000\n",
      "(Iteration 1601 / 15300) loss: 2.302631\n",
      "(Iteration 1701 / 15300) loss: 2.302637\n",
      "(Iteration 1801 / 15300) loss: 2.302438\n",
      "(Iteration 1901 / 15300) loss: 2.302452\n",
      "(Iteration 2001 / 15300) loss: 2.302621\n",
      "(Iteration 2101 / 15300) loss: 2.302688\n",
      "(Iteration 2201 / 15300) loss: 2.302419\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.131000\n",
      "(Iteration 2301 / 15300) loss: 2.302343\n",
      "(Iteration 2401 / 15300) loss: 2.302636\n",
      "(Iteration 2501 / 15300) loss: 2.302471\n",
      "(Iteration 2601 / 15300) loss: 2.302419\n",
      "(Iteration 2701 / 15300) loss: 2.302376\n",
      "(Iteration 2801 / 15300) loss: 2.302365\n",
      "(Iteration 2901 / 15300) loss: 2.302632\n",
      "(Iteration 3001 / 15300) loss: 2.302384\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.087000\n",
      "(Iteration 3101 / 15300) loss: 2.302094\n",
      "(Iteration 3201 / 15300) loss: 2.302434\n",
      "(Iteration 3301 / 15300) loss: 2.302639\n",
      "(Iteration 3401 / 15300) loss: 2.302250\n",
      "(Iteration 3501 / 15300) loss: 2.302177\n",
      "(Iteration 3601 / 15300) loss: 2.302246\n",
      "(Iteration 3701 / 15300) loss: 2.302307\n",
      "(Iteration 3801 / 15300) loss: 2.302284\n",
      "(Epoch 5 / 20) train acc: 0.118000; val_acc: 0.092000\n",
      "(Iteration 3901 / 15300) loss: 2.302071\n",
      "(Iteration 4001 / 15300) loss: 2.302403\n",
      "(Iteration 4101 / 15300) loss: 2.302155\n",
      "(Iteration 4201 / 15300) loss: 2.301894\n",
      "(Iteration 4301 / 15300) loss: 2.301946\n",
      "(Iteration 4401 / 15300) loss: 2.302266\n",
      "(Iteration 4501 / 15300) loss: 2.301915\n",
      "(Epoch 6 / 20) train acc: 0.124000; val_acc: 0.097000\n",
      "(Iteration 4601 / 15300) loss: 2.302410\n",
      "(Iteration 4701 / 15300) loss: 2.301798\n",
      "(Iteration 4801 / 15300) loss: 2.302103\n",
      "(Iteration 4901 / 15300) loss: 2.302191\n",
      "(Iteration 5001 / 15300) loss: 2.301501\n",
      "(Iteration 5101 / 15300) loss: 2.301525\n",
      "(Iteration 5201 / 15300) loss: 2.301440\n",
      "(Iteration 5301 / 15300) loss: 2.301585\n",
      "(Epoch 7 / 20) train acc: 0.134000; val_acc: 0.143000\n",
      "(Iteration 5401 / 15300) loss: 2.301994\n",
      "(Iteration 5501 / 15300) loss: 2.301068\n",
      "(Iteration 5601 / 15300) loss: 2.300643\n",
      "(Iteration 5701 / 15300) loss: 2.300697\n",
      "(Iteration 5801 / 15300) loss: 2.301452\n",
      "(Iteration 5901 / 15300) loss: 2.301011\n",
      "(Iteration 6001 / 15300) loss: 2.301332\n",
      "(Iteration 6101 / 15300) loss: 2.301941\n",
      "(Epoch 8 / 20) train acc: 0.143000; val_acc: 0.130000\n",
      "(Iteration 6201 / 15300) loss: 2.301519\n",
      "(Iteration 6301 / 15300) loss: 2.301591\n",
      "(Iteration 6401 / 15300) loss: 2.301341\n",
      "(Iteration 6501 / 15300) loss: 2.301592\n",
      "(Iteration 6601 / 15300) loss: 2.300778\n",
      "(Iteration 6701 / 15300) loss: 2.301433\n",
      "(Iteration 6801 / 15300) loss: 2.301553\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.159000\n",
      "(Iteration 6901 / 15300) loss: 2.300635\n",
      "(Iteration 7001 / 15300) loss: 2.301104\n",
      "(Iteration 7101 / 15300) loss: 2.300492\n",
      "(Iteration 7201 / 15300) loss: 2.300840\n",
      "(Iteration 7301 / 15300) loss: 2.300714\n",
      "(Iteration 7401 / 15300) loss: 2.300921\n",
      "(Iteration 7501 / 15300) loss: 2.301109\n",
      "(Iteration 7601 / 15300) loss: 2.301249\n",
      "(Epoch 10 / 20) train acc: 0.183000; val_acc: 0.161000\n",
      "(Iteration 7701 / 15300) loss: 2.299616\n",
      "(Iteration 7801 / 15300) loss: 2.301049\n",
      "(Iteration 7901 / 15300) loss: 2.301348\n",
      "(Iteration 8001 / 15300) loss: 2.300096\n",
      "(Iteration 8101 / 15300) loss: 2.300492\n",
      "(Iteration 8201 / 15300) loss: 2.300753\n",
      "(Iteration 8301 / 15300) loss: 2.300621\n",
      "(Iteration 8401 / 15300) loss: 2.300838\n",
      "(Epoch 11 / 20) train acc: 0.201000; val_acc: 0.175000\n",
      "(Iteration 8501 / 15300) loss: 2.300046\n",
      "(Iteration 8601 / 15300) loss: 2.299927\n",
      "(Iteration 8701 / 15300) loss: 2.301693\n",
      "(Iteration 8801 / 15300) loss: 2.298816\n",
      "(Iteration 8901 / 15300) loss: 2.300067\n",
      "(Iteration 9001 / 15300) loss: 2.298555\n",
      "(Iteration 9101 / 15300) loss: 2.298996\n",
      "(Epoch 12 / 20) train acc: 0.203000; val_acc: 0.179000\n",
      "(Iteration 9201 / 15300) loss: 2.301494\n",
      "(Iteration 9301 / 15300) loss: 2.300291\n",
      "(Iteration 9401 / 15300) loss: 2.298719\n",
      "(Iteration 9501 / 15300) loss: 2.299064\n",
      "(Iteration 9601 / 15300) loss: 2.300068\n",
      "(Iteration 9701 / 15300) loss: 2.299464\n",
      "(Iteration 9801 / 15300) loss: 2.300445\n",
      "(Iteration 9901 / 15300) loss: 2.298647\n",
      "(Epoch 13 / 20) train acc: 0.242000; val_acc: 0.206000\n",
      "(Iteration 10001 / 15300) loss: 2.299879\n",
      "(Iteration 10101 / 15300) loss: 2.299588\n",
      "(Iteration 10201 / 15300) loss: 2.300592\n",
      "(Iteration 10301 / 15300) loss: 2.299715\n",
      "(Iteration 10401 / 15300) loss: 2.298778\n",
      "(Iteration 10501 / 15300) loss: 2.298758\n",
      "(Iteration 10601 / 15300) loss: 2.298886\n",
      "(Iteration 10701 / 15300) loss: 2.299664\n",
      "(Epoch 14 / 20) train acc: 0.238000; val_acc: 0.227000\n",
      "(Iteration 10801 / 15300) loss: 2.299761\n",
      "(Iteration 10901 / 15300) loss: 2.299526\n",
      "(Iteration 11001 / 15300) loss: 2.298952\n",
      "(Iteration 11101 / 15300) loss: 2.298759\n",
      "(Iteration 11201 / 15300) loss: 2.298519\n",
      "(Iteration 11301 / 15300) loss: 2.297796\n",
      "(Iteration 11401 / 15300) loss: 2.299523\n",
      "(Epoch 15 / 20) train acc: 0.226000; val_acc: 0.239000\n",
      "(Iteration 11501 / 15300) loss: 2.298900\n",
      "(Iteration 11601 / 15300) loss: 2.298269\n",
      "(Iteration 11701 / 15300) loss: 2.298046\n",
      "(Iteration 11801 / 15300) loss: 2.298291\n",
      "(Iteration 11901 / 15300) loss: 2.296536\n",
      "(Iteration 12001 / 15300) loss: 2.299703\n",
      "(Iteration 12101 / 15300) loss: 2.299609\n",
      "(Iteration 12201 / 15300) loss: 2.298657\n",
      "(Epoch 16 / 20) train acc: 0.283000; val_acc: 0.244000\n",
      "(Iteration 12301 / 15300) loss: 2.298876\n",
      "(Iteration 12401 / 15300) loss: 2.298424\n",
      "(Iteration 12501 / 15300) loss: 2.297709\n",
      "(Iteration 12601 / 15300) loss: 2.297765\n",
      "(Iteration 12701 / 15300) loss: 2.298892\n",
      "(Iteration 12801 / 15300) loss: 2.300212\n",
      "(Iteration 12901 / 15300) loss: 2.296911\n",
      "(Iteration 13001 / 15300) loss: 2.299226\n",
      "(Epoch 17 / 20) train acc: 0.272000; val_acc: 0.244000\n",
      "(Iteration 13101 / 15300) loss: 2.298936\n",
      "(Iteration 13201 / 15300) loss: 2.298395\n",
      "(Iteration 13301 / 15300) loss: 2.297062\n",
      "(Iteration 13401 / 15300) loss: 2.296540\n",
      "(Iteration 13501 / 15300) loss: 2.296640\n",
      "(Iteration 13601 / 15300) loss: 2.295463\n",
      "(Iteration 13701 / 15300) loss: 2.295532\n",
      "(Epoch 18 / 20) train acc: 0.277000; val_acc: 0.255000\n",
      "(Iteration 13801 / 15300) loss: 2.298371\n",
      "(Iteration 13901 / 15300) loss: 2.296485\n",
      "(Iteration 14001 / 15300) loss: 2.296863\n",
      "(Iteration 14101 / 15300) loss: 2.294869\n",
      "(Iteration 14201 / 15300) loss: 2.296095\n",
      "(Iteration 14301 / 15300) loss: 2.296525\n",
      "(Iteration 14401 / 15300) loss: 2.296527\n",
      "(Iteration 14501 / 15300) loss: 2.296299\n",
      "(Epoch 19 / 20) train acc: 0.288000; val_acc: 0.245000\n",
      "(Iteration 14601 / 15300) loss: 2.298400\n",
      "(Iteration 14701 / 15300) loss: 2.293844\n",
      "(Iteration 14801 / 15300) loss: 2.297616\n",
      "(Iteration 14901 / 15300) loss: 2.296265\n",
      "(Iteration 15001 / 15300) loss: 2.295925\n",
      "(Iteration 15101 / 15300) loss: 2.295456\n",
      "(Iteration 15201 / 15300) loss: 2.296147\n",
      "(Epoch 20 / 20) train acc: 0.242000; val_acc: 0.258000\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.001, 'num_epochs': 20, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 30620) loss: 2.302868\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.112000\n",
      "(Iteration 101 / 30620) loss: 2.302780\n",
      "(Iteration 201 / 30620) loss: 2.302885\n",
      "(Iteration 301 / 30620) loss: 2.302722\n",
      "(Iteration 401 / 30620) loss: 2.303083\n",
      "(Iteration 501 / 30620) loss: 2.302795\n",
      "(Iteration 601 / 30620) loss: 2.303653\n",
      "(Iteration 701 / 30620) loss: 2.302626\n",
      "(Iteration 801 / 30620) loss: 2.302760\n",
      "(Iteration 901 / 30620) loss: 2.302591\n",
      "(Iteration 1001 / 30620) loss: 2.302989\n",
      "(Iteration 1101 / 30620) loss: 2.302550\n",
      "(Iteration 1201 / 30620) loss: 2.302703\n",
      "(Iteration 1301 / 30620) loss: 2.303372\n",
      "(Iteration 1401 / 30620) loss: 2.302350\n",
      "(Iteration 1501 / 30620) loss: 2.302704\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 1601 / 30620) loss: 2.303063\n",
      "(Iteration 1701 / 30620) loss: 2.302753\n",
      "(Iteration 1801 / 30620) loss: 2.302516\n",
      "(Iteration 1901 / 30620) loss: 2.302519\n",
      "(Iteration 2001 / 30620) loss: 2.302584\n",
      "(Iteration 2101 / 30620) loss: 2.302970\n",
      "(Iteration 2201 / 30620) loss: 2.303180\n",
      "(Iteration 2301 / 30620) loss: 2.302978\n",
      "(Iteration 2401 / 30620) loss: 2.302166\n",
      "(Iteration 2501 / 30620) loss: 2.303183\n",
      "(Iteration 2601 / 30620) loss: 2.303528\n",
      "(Iteration 2701 / 30620) loss: 2.302140\n",
      "(Iteration 2801 / 30620) loss: 2.302396\n",
      "(Iteration 2901 / 30620) loss: 2.301425\n",
      "(Iteration 3001 / 30620) loss: 2.302803\n",
      "(Epoch 2 / 20) train acc: 0.130000; val_acc: 0.114000\n",
      "(Iteration 3101 / 30620) loss: 2.300949\n",
      "(Iteration 3201 / 30620) loss: 2.302368\n",
      "(Iteration 3301 / 30620) loss: 2.301839\n",
      "(Iteration 3401 / 30620) loss: 2.302670\n",
      "(Iteration 3501 / 30620) loss: 2.301011\n",
      "(Iteration 3601 / 30620) loss: 2.302364\n",
      "(Iteration 3701 / 30620) loss: 2.300767\n",
      "(Iteration 3801 / 30620) loss: 2.302597\n",
      "(Iteration 3901 / 30620) loss: 2.301127\n",
      "(Iteration 4001 / 30620) loss: 2.302453\n",
      "(Iteration 4101 / 30620) loss: 2.303158\n",
      "(Iteration 4201 / 30620) loss: 2.302462\n",
      "(Iteration 4301 / 30620) loss: 2.301824\n",
      "(Iteration 4401 / 30620) loss: 2.302366\n",
      "(Iteration 4501 / 30620) loss: 2.302055\n",
      "(Epoch 3 / 20) train acc: 0.103000; val_acc: 0.116000\n",
      "(Iteration 4601 / 30620) loss: 2.301821\n",
      "(Iteration 4701 / 30620) loss: 2.302172\n",
      "(Iteration 4801 / 30620) loss: 2.302012\n",
      "(Iteration 4901 / 30620) loss: 2.301521\n",
      "(Iteration 5001 / 30620) loss: 2.302806\n",
      "(Iteration 5101 / 30620) loss: 2.301845\n",
      "(Iteration 5201 / 30620) loss: 2.300953\n",
      "(Iteration 5301 / 30620) loss: 2.301083\n",
      "(Iteration 5401 / 30620) loss: 2.302130\n",
      "(Iteration 5501 / 30620) loss: 2.301933\n",
      "(Iteration 5601 / 30620) loss: 2.300398\n",
      "(Iteration 5701 / 30620) loss: 2.301482\n",
      "(Iteration 5801 / 30620) loss: 2.299666\n",
      "(Iteration 5901 / 30620) loss: 2.300023\n",
      "(Iteration 6001 / 30620) loss: 2.300554\n",
      "(Iteration 6101 / 30620) loss: 2.301558\n",
      "(Epoch 4 / 20) train acc: 0.185000; val_acc: 0.137000\n",
      "(Iteration 6201 / 30620) loss: 2.300814\n",
      "(Iteration 6301 / 30620) loss: 2.299905\n",
      "(Iteration 6401 / 30620) loss: 2.299914\n",
      "(Iteration 6501 / 30620) loss: 2.300033\n",
      "(Iteration 6601 / 30620) loss: 2.300441\n",
      "(Iteration 6701 / 30620) loss: 2.301665\n",
      "(Iteration 6801 / 30620) loss: 2.301240\n",
      "(Iteration 6901 / 30620) loss: 2.299387\n",
      "(Iteration 7001 / 30620) loss: 2.299242\n",
      "(Iteration 7101 / 30620) loss: 2.298441\n",
      "(Iteration 7201 / 30620) loss: 2.298734\n",
      "(Iteration 7301 / 30620) loss: 2.300888\n",
      "(Iteration 7401 / 30620) loss: 2.297638\n",
      "(Iteration 7501 / 30620) loss: 2.297861\n",
      "(Iteration 7601 / 30620) loss: 2.302364\n",
      "(Epoch 5 / 20) train acc: 0.190000; val_acc: 0.170000\n",
      "(Iteration 7701 / 30620) loss: 2.297932\n",
      "(Iteration 7801 / 30620) loss: 2.300180\n",
      "(Iteration 7901 / 30620) loss: 2.295496\n",
      "(Iteration 8001 / 30620) loss: 2.298718\n",
      "(Iteration 8101 / 30620) loss: 2.298668\n",
      "(Iteration 8201 / 30620) loss: 2.298709\n",
      "(Iteration 8301 / 30620) loss: 2.294645\n",
      "(Iteration 8401 / 30620) loss: 2.297059\n",
      "(Iteration 8501 / 30620) loss: 2.299333\n",
      "(Iteration 8601 / 30620) loss: 2.300158\n",
      "(Iteration 8701 / 30620) loss: 2.298357\n",
      "(Iteration 8801 / 30620) loss: 2.293990\n",
      "(Iteration 8901 / 30620) loss: 2.295562\n",
      "(Iteration 9001 / 30620) loss: 2.296291\n",
      "(Iteration 9101 / 30620) loss: 2.295657\n",
      "(Epoch 6 / 20) train acc: 0.253000; val_acc: 0.234000\n",
      "(Iteration 9201 / 30620) loss: 2.297495\n",
      "(Iteration 9301 / 30620) loss: 2.292379\n",
      "(Iteration 9401 / 30620) loss: 2.296211\n",
      "(Iteration 9501 / 30620) loss: 2.296511\n",
      "(Iteration 9601 / 30620) loss: 2.297792\n",
      "(Iteration 9701 / 30620) loss: 2.298617\n",
      "(Iteration 9801 / 30620) loss: 2.296857\n",
      "(Iteration 9901 / 30620) loss: 2.291353\n",
      "(Iteration 10001 / 30620) loss: 2.290897\n",
      "(Iteration 10101 / 30620) loss: 2.294965\n",
      "(Iteration 10201 / 30620) loss: 2.290873\n",
      "(Iteration 10301 / 30620) loss: 2.285669\n",
      "(Iteration 10401 / 30620) loss: 2.288414\n",
      "(Iteration 10501 / 30620) loss: 2.289802\n",
      "(Iteration 10601 / 30620) loss: 2.290144\n",
      "(Iteration 10701 / 30620) loss: 2.284344\n",
      "(Epoch 7 / 20) train acc: 0.257000; val_acc: 0.273000\n",
      "(Iteration 10801 / 30620) loss: 2.293557\n",
      "(Iteration 10901 / 30620) loss: 2.291667\n",
      "(Iteration 11001 / 30620) loss: 2.289365\n",
      "(Iteration 11101 / 30620) loss: 2.284773\n",
      "(Iteration 11201 / 30620) loss: 2.293704\n",
      "(Iteration 11301 / 30620) loss: 2.291799\n",
      "(Iteration 11401 / 30620) loss: 2.289908\n",
      "(Iteration 11501 / 30620) loss: 2.287137\n",
      "(Iteration 11601 / 30620) loss: 2.290988\n",
      "(Iteration 11701 / 30620) loss: 2.290314\n",
      "(Iteration 11801 / 30620) loss: 2.281628\n",
      "(Iteration 11901 / 30620) loss: 2.281814\n",
      "(Iteration 12001 / 30620) loss: 2.293490\n",
      "(Iteration 12101 / 30620) loss: 2.291824\n",
      "(Iteration 12201 / 30620) loss: 2.284568\n",
      "(Epoch 8 / 20) train acc: 0.253000; val_acc: 0.263000\n",
      "(Iteration 12301 / 30620) loss: 2.283726\n",
      "(Iteration 12401 / 30620) loss: 2.286465\n",
      "(Iteration 12501 / 30620) loss: 2.294143\n",
      "(Iteration 12601 / 30620) loss: 2.275360\n",
      "(Iteration 12701 / 30620) loss: 2.281661\n",
      "(Iteration 12801 / 30620) loss: 2.281930\n",
      "(Iteration 12901 / 30620) loss: 2.294085\n",
      "(Iteration 13001 / 30620) loss: 2.293749\n",
      "(Iteration 13101 / 30620) loss: 2.278546\n",
      "(Iteration 13201 / 30620) loss: 2.278222\n",
      "(Iteration 13301 / 30620) loss: 2.280428\n",
      "(Iteration 13401 / 30620) loss: 2.273750\n",
      "(Iteration 13501 / 30620) loss: 2.274081\n",
      "(Iteration 13601 / 30620) loss: 2.275988\n",
      "(Iteration 13701 / 30620) loss: 2.266530\n",
      "(Epoch 9 / 20) train acc: 0.291000; val_acc: 0.278000\n",
      "(Iteration 13801 / 30620) loss: 2.264707\n",
      "(Iteration 13901 / 30620) loss: 2.272225\n",
      "(Iteration 14001 / 30620) loss: 2.278161\n",
      "(Iteration 14101 / 30620) loss: 2.276481\n",
      "(Iteration 14201 / 30620) loss: 2.283348\n",
      "(Iteration 14301 / 30620) loss: 2.292103\n",
      "(Iteration 14401 / 30620) loss: 2.281550\n",
      "(Iteration 14501 / 30620) loss: 2.277330\n",
      "(Iteration 14601 / 30620) loss: 2.272638\n",
      "(Iteration 14701 / 30620) loss: 2.266652\n",
      "(Iteration 14801 / 30620) loss: 2.284771\n",
      "(Iteration 14901 / 30620) loss: 2.262619\n",
      "(Iteration 15001 / 30620) loss: 2.283305\n",
      "(Iteration 15101 / 30620) loss: 2.274982\n",
      "(Iteration 15201 / 30620) loss: 2.270036\n",
      "(Iteration 15301 / 30620) loss: 2.256890\n",
      "(Epoch 10 / 20) train acc: 0.235000; val_acc: 0.257000\n",
      "(Iteration 15401 / 30620) loss: 2.276260\n",
      "(Iteration 15501 / 30620) loss: 2.244921\n",
      "(Iteration 15601 / 30620) loss: 2.278902\n",
      "(Iteration 15701 / 30620) loss: 2.281858\n",
      "(Iteration 15801 / 30620) loss: 2.276496\n",
      "(Iteration 15901 / 30620) loss: 2.269002\n",
      "(Iteration 16001 / 30620) loss: 2.285947\n",
      "(Iteration 16101 / 30620) loss: 2.233141\n",
      "(Iteration 16201 / 30620) loss: 2.250406\n",
      "(Iteration 16301 / 30620) loss: 2.231924\n",
      "(Iteration 16401 / 30620) loss: 2.275931\n",
      "(Iteration 16501 / 30620) loss: 2.264072\n",
      "(Iteration 16601 / 30620) loss: 2.241833\n",
      "(Iteration 16701 / 30620) loss: 2.233204\n",
      "(Iteration 16801 / 30620) loss: 2.272451\n",
      "(Epoch 11 / 20) train acc: 0.270000; val_acc: 0.262000\n",
      "(Iteration 16901 / 30620) loss: 2.283537\n",
      "(Iteration 17001 / 30620) loss: 2.254163\n",
      "(Iteration 17101 / 30620) loss: 2.241041\n",
      "(Iteration 17201 / 30620) loss: 2.241986\n",
      "(Iteration 17301 / 30620) loss: 2.216517\n",
      "(Iteration 17401 / 30620) loss: 2.192712\n",
      "(Iteration 17501 / 30620) loss: 2.256588\n",
      "(Iteration 17601 / 30620) loss: 2.241243\n",
      "(Iteration 17701 / 30620) loss: 2.217418\n",
      "(Iteration 17801 / 30620) loss: 2.246134\n",
      "(Iteration 17901 / 30620) loss: 2.232109\n",
      "(Iteration 18001 / 30620) loss: 2.261079\n",
      "(Iteration 18101 / 30620) loss: 2.231267\n",
      "(Iteration 18201 / 30620) loss: 2.245278\n",
      "(Iteration 18301 / 30620) loss: 2.215080\n",
      "(Epoch 12 / 20) train acc: 0.260000; val_acc: 0.261000\n",
      "(Iteration 18401 / 30620) loss: 2.204992\n",
      "(Iteration 18501 / 30620) loss: 2.234457\n",
      "(Iteration 18601 / 30620) loss: 2.267365\n",
      "(Iteration 18701 / 30620) loss: 2.278523\n",
      "(Iteration 18801 / 30620) loss: 2.223868\n",
      "(Iteration 18901 / 30620) loss: 2.244348\n",
      "(Iteration 19001 / 30620) loss: 2.201546\n",
      "(Iteration 19101 / 30620) loss: 2.231205\n",
      "(Iteration 19201 / 30620) loss: 2.203927\n",
      "(Iteration 19301 / 30620) loss: 2.203671\n",
      "(Iteration 19401 / 30620) loss: 2.199817\n",
      "(Iteration 19501 / 30620) loss: 2.204836\n",
      "(Iteration 19601 / 30620) loss: 2.190011\n",
      "(Iteration 19701 / 30620) loss: 2.224587\n",
      "(Iteration 19801 / 30620) loss: 2.251509\n",
      "(Iteration 19901 / 30620) loss: 2.206872\n",
      "(Epoch 13 / 20) train acc: 0.266000; val_acc: 0.269000\n",
      "(Iteration 20001 / 30620) loss: 2.189889\n",
      "(Iteration 20101 / 30620) loss: 2.184475\n",
      "(Iteration 20201 / 30620) loss: 2.211919\n",
      "(Iteration 20301 / 30620) loss: 2.205277\n",
      "(Iteration 20401 / 30620) loss: 2.280534\n",
      "(Iteration 20501 / 30620) loss: 2.206680\n",
      "(Iteration 20601 / 30620) loss: 2.207874\n",
      "(Iteration 20701 / 30620) loss: 2.186622\n",
      "(Iteration 20801 / 30620) loss: 2.224639\n",
      "(Iteration 20901 / 30620) loss: 2.199725\n",
      "(Iteration 21001 / 30620) loss: 2.226745\n",
      "(Iteration 21101 / 30620) loss: 2.224132\n",
      "(Iteration 21201 / 30620) loss: 2.162012\n",
      "(Iteration 21301 / 30620) loss: 2.223309\n",
      "(Iteration 21401 / 30620) loss: 2.243897\n",
      "(Epoch 14 / 20) train acc: 0.261000; val_acc: 0.268000\n",
      "(Iteration 21501 / 30620) loss: 2.151859\n",
      "(Iteration 21601 / 30620) loss: 2.212945\n",
      "(Iteration 21701 / 30620) loss: 2.164824\n",
      "(Iteration 21801 / 30620) loss: 2.162627\n",
      "(Iteration 21901 / 30620) loss: 2.179343\n",
      "(Iteration 22001 / 30620) loss: 2.190019\n",
      "(Iteration 22101 / 30620) loss: 2.224886\n",
      "(Iteration 22201 / 30620) loss: 2.202839\n",
      "(Iteration 22301 / 30620) loss: 2.221803\n",
      "(Iteration 22401 / 30620) loss: 2.173843\n",
      "(Iteration 22501 / 30620) loss: 2.218849\n",
      "(Iteration 22601 / 30620) loss: 2.159301\n",
      "(Iteration 22701 / 30620) loss: 2.166428\n",
      "(Iteration 22801 / 30620) loss: 2.186071\n",
      "(Iteration 22901 / 30620) loss: 2.241946\n",
      "(Epoch 15 / 20) train acc: 0.248000; val_acc: 0.271000\n",
      "(Iteration 23001 / 30620) loss: 2.170065\n",
      "(Iteration 23101 / 30620) loss: 2.186401\n",
      "(Iteration 23201 / 30620) loss: 2.238874\n",
      "(Iteration 23301 / 30620) loss: 2.206770\n",
      "(Iteration 23401 / 30620) loss: 2.211542\n",
      "(Iteration 23501 / 30620) loss: 2.159331\n",
      "(Iteration 23601 / 30620) loss: 2.157504\n",
      "(Iteration 23701 / 30620) loss: 2.138048\n",
      "(Iteration 23801 / 30620) loss: 2.225288\n",
      "(Iteration 23901 / 30620) loss: 2.203152\n",
      "(Iteration 24001 / 30620) loss: 2.225907\n",
      "(Iteration 24101 / 30620) loss: 2.184981\n",
      "(Iteration 24201 / 30620) loss: 2.242965\n",
      "(Iteration 24301 / 30620) loss: 2.150938\n",
      "(Iteration 24401 / 30620) loss: 2.124968\n",
      "(Epoch 16 / 20) train acc: 0.252000; val_acc: 0.269000\n",
      "(Iteration 24501 / 30620) loss: 2.119133\n",
      "(Iteration 24601 / 30620) loss: 2.180023\n",
      "(Iteration 24701 / 30620) loss: 2.231791\n",
      "(Iteration 24801 / 30620) loss: 2.111123\n",
      "(Iteration 24901 / 30620) loss: 2.207800\n",
      "(Iteration 25001 / 30620) loss: 2.151549\n",
      "(Iteration 25101 / 30620) loss: 2.210753\n",
      "(Iteration 25201 / 30620) loss: 2.149702\n",
      "(Iteration 25301 / 30620) loss: 2.180965\n",
      "(Iteration 25401 / 30620) loss: 2.135799\n",
      "(Iteration 25501 / 30620) loss: 2.040059\n",
      "(Iteration 25601 / 30620) loss: 2.214446\n",
      "(Iteration 25701 / 30620) loss: 2.101972\n",
      "(Iteration 25801 / 30620) loss: 2.198656\n",
      "(Iteration 25901 / 30620) loss: 2.177830\n",
      "(Iteration 26001 / 30620) loss: 2.147190\n",
      "(Epoch 17 / 20) train acc: 0.251000; val_acc: 0.266000\n",
      "(Iteration 26101 / 30620) loss: 2.109058\n",
      "(Iteration 26201 / 30620) loss: 2.142432\n",
      "(Iteration 26301 / 30620) loss: 2.122902\n",
      "(Iteration 26401 / 30620) loss: 2.087703\n",
      "(Iteration 26501 / 30620) loss: 2.089944\n",
      "(Iteration 26601 / 30620) loss: 2.104149\n",
      "(Iteration 26701 / 30620) loss: 2.154388\n",
      "(Iteration 26801 / 30620) loss: 2.030186\n",
      "(Iteration 26901 / 30620) loss: 2.163619\n",
      "(Iteration 27001 / 30620) loss: 2.155830\n",
      "(Iteration 27101 / 30620) loss: 2.074549\n",
      "(Iteration 27201 / 30620) loss: 2.143442\n",
      "(Iteration 27301 / 30620) loss: 2.082047\n",
      "(Iteration 27401 / 30620) loss: 2.197364\n",
      "(Iteration 27501 / 30620) loss: 2.110539\n",
      "(Epoch 18 / 20) train acc: 0.245000; val_acc: 0.272000\n",
      "(Iteration 27601 / 30620) loss: 2.138910\n",
      "(Iteration 27701 / 30620) loss: 2.127941\n",
      "(Iteration 27801 / 30620) loss: 2.133334\n",
      "(Iteration 27901 / 30620) loss: 2.183967\n",
      "(Iteration 28001 / 30620) loss: 2.086581\n",
      "(Iteration 28101 / 30620) loss: 2.065968\n",
      "(Iteration 28201 / 30620) loss: 2.135198\n",
      "(Iteration 28301 / 30620) loss: 2.159355\n",
      "(Iteration 28401 / 30620) loss: 2.144067\n",
      "(Iteration 28501 / 30620) loss: 2.104967\n",
      "(Iteration 28601 / 30620) loss: 2.126098\n",
      "(Iteration 28701 / 30620) loss: 2.137173\n",
      "(Iteration 28801 / 30620) loss: 2.092787\n",
      "(Iteration 28901 / 30620) loss: 2.104393\n",
      "(Iteration 29001 / 30620) loss: 2.108023\n",
      "(Epoch 19 / 20) train acc: 0.276000; val_acc: 0.272000\n",
      "(Iteration 29101 / 30620) loss: 2.044828\n",
      "(Iteration 29201 / 30620) loss: 2.053270\n",
      "(Iteration 29301 / 30620) loss: 2.091053\n",
      "(Iteration 29401 / 30620) loss: 2.182324\n",
      "(Iteration 29501 / 30620) loss: 2.116584\n",
      "(Iteration 29601 / 30620) loss: 2.043466\n",
      "(Iteration 29701 / 30620) loss: 2.195097\n",
      "(Iteration 29801 / 30620) loss: 2.108584\n",
      "(Iteration 29901 / 30620) loss: 2.190789\n",
      "(Iteration 30001 / 30620) loss: 2.073515\n",
      "(Iteration 30101 / 30620) loss: 2.145489\n",
      "(Iteration 30201 / 30620) loss: 2.206851\n",
      "(Iteration 30301 / 30620) loss: 2.223854\n",
      "(Iteration 30401 / 30620) loss: 2.198314\n",
      "(Iteration 30501 / 30620) loss: 1.963179\n",
      "(Iteration 30601 / 30620) loss: 2.072615\n",
      "(Epoch 20 / 20) train acc: 0.259000; val_acc: 0.265000\n",
      "New best model found with validation accuracy: 0.2246\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.001, 'num_epochs': 30, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 22950) loss: 2.305911\n",
      "(Epoch 0 / 30) train acc: 0.104000; val_acc: 0.115000\n",
      "(Iteration 101 / 22950) loss: 2.305806\n",
      "(Iteration 201 / 22950) loss: 2.305706\n",
      "(Iteration 301 / 22950) loss: 2.305643\n",
      "(Iteration 401 / 22950) loss: 2.305532\n",
      "(Iteration 501 / 22950) loss: 2.305575\n",
      "(Iteration 601 / 22950) loss: 2.305399\n",
      "(Iteration 701 / 22950) loss: 2.305413\n",
      "(Epoch 1 / 30) train acc: 0.133000; val_acc: 0.128000\n",
      "(Iteration 801 / 22950) loss: 2.305239\n",
      "(Iteration 901 / 22950) loss: 2.305298\n",
      "(Iteration 1001 / 22950) loss: 2.305213\n",
      "(Iteration 1101 / 22950) loss: 2.305034\n",
      "(Iteration 1201 / 22950) loss: 2.305179\n",
      "(Iteration 1301 / 22950) loss: 2.305017\n",
      "(Iteration 1401 / 22950) loss: 2.305098\n",
      "(Iteration 1501 / 22950) loss: 2.304755\n",
      "(Epoch 2 / 30) train acc: 0.183000; val_acc: 0.153000\n",
      "(Iteration 1601 / 22950) loss: 2.304671\n",
      "(Iteration 1701 / 22950) loss: 2.305395\n",
      "(Iteration 1801 / 22950) loss: 2.304887\n",
      "(Iteration 1901 / 22950) loss: 2.304538\n",
      "(Iteration 2001 / 22950) loss: 2.304399\n",
      "(Iteration 2101 / 22950) loss: 2.304808\n",
      "(Iteration 2201 / 22950) loss: 2.304604\n",
      "(Epoch 3 / 30) train acc: 0.106000; val_acc: 0.120000\n",
      "(Iteration 2301 / 22950) loss: 2.304629\n",
      "(Iteration 2401 / 22950) loss: 2.304245\n",
      "(Iteration 2501 / 22950) loss: 2.304557\n",
      "(Iteration 2601 / 22950) loss: 2.304579\n",
      "(Iteration 2701 / 22950) loss: 2.304356\n",
      "(Iteration 2801 / 22950) loss: 2.304295\n",
      "(Iteration 2901 / 22950) loss: 2.304366\n",
      "(Iteration 3001 / 22950) loss: 2.304045\n",
      "(Epoch 4 / 30) train acc: 0.218000; val_acc: 0.216000\n",
      "(Iteration 3101 / 22950) loss: 2.304555\n",
      "(Iteration 3201 / 22950) loss: 2.304030\n",
      "(Iteration 3301 / 22950) loss: 2.304108\n",
      "(Iteration 3401 / 22950) loss: 2.304176\n",
      "(Iteration 3501 / 22950) loss: 2.304110\n",
      "(Iteration 3601 / 22950) loss: 2.304144\n",
      "(Iteration 3701 / 22950) loss: 2.304025\n",
      "(Iteration 3801 / 22950) loss: 2.303929\n",
      "(Epoch 5 / 30) train acc: 0.196000; val_acc: 0.164000\n",
      "(Iteration 3901 / 22950) loss: 2.304003\n",
      "(Iteration 4001 / 22950) loss: 2.304112\n",
      "(Iteration 4101 / 22950) loss: 2.303800\n",
      "(Iteration 4201 / 22950) loss: 2.303956\n",
      "(Iteration 4301 / 22950) loss: 2.304134\n",
      "(Iteration 4401 / 22950) loss: 2.303908\n",
      "(Iteration 4501 / 22950) loss: 2.304035\n",
      "(Epoch 6 / 30) train acc: 0.134000; val_acc: 0.104000\n",
      "(Iteration 4601 / 22950) loss: 2.304188\n",
      "(Iteration 4701 / 22950) loss: 2.303970\n",
      "(Iteration 4801 / 22950) loss: 2.303637\n",
      "(Iteration 4901 / 22950) loss: 2.303633\n",
      "(Iteration 5001 / 22950) loss: 2.303985\n",
      "(Iteration 5101 / 22950) loss: 2.303874\n",
      "(Iteration 5201 / 22950) loss: 2.303343\n",
      "(Iteration 5301 / 22950) loss: 2.304023\n",
      "(Epoch 7 / 30) train acc: 0.122000; val_acc: 0.130000\n",
      "(Iteration 5401 / 22950) loss: 2.303622\n",
      "(Iteration 5501 / 22950) loss: 2.303692\n",
      "(Iteration 5601 / 22950) loss: 2.303363\n",
      "(Iteration 5701 / 22950) loss: 2.303630\n",
      "(Iteration 5801 / 22950) loss: 2.303672\n",
      "(Iteration 5901 / 22950) loss: 2.303497\n",
      "(Iteration 6001 / 22950) loss: 2.303410\n",
      "(Iteration 6101 / 22950) loss: 2.303282\n",
      "(Epoch 8 / 30) train acc: 0.200000; val_acc: 0.168000\n",
      "(Iteration 6201 / 22950) loss: 2.303311\n",
      "(Iteration 6301 / 22950) loss: 2.303598\n",
      "(Iteration 6401 / 22950) loss: 2.303711\n",
      "(Iteration 6501 / 22950) loss: 2.303671\n",
      "(Iteration 6601 / 22950) loss: 2.302849\n",
      "(Iteration 6701 / 22950) loss: 2.303577\n",
      "(Iteration 6801 / 22950) loss: 2.303472\n",
      "(Epoch 9 / 30) train acc: 0.224000; val_acc: 0.201000\n",
      "(Iteration 6901 / 22950) loss: 2.302993\n",
      "(Iteration 7001 / 22950) loss: 2.303270\n",
      "(Iteration 7101 / 22950) loss: 2.303489\n",
      "(Iteration 7201 / 22950) loss: 2.302914\n",
      "(Iteration 7301 / 22950) loss: 2.303082\n",
      "(Iteration 7401 / 22950) loss: 2.303285\n",
      "(Iteration 7501 / 22950) loss: 2.303495\n",
      "(Iteration 7601 / 22950) loss: 2.302967\n",
      "(Epoch 10 / 30) train acc: 0.211000; val_acc: 0.198000\n",
      "(Iteration 7701 / 22950) loss: 2.303165\n",
      "(Iteration 7801 / 22950) loss: 2.303281\n",
      "(Iteration 7901 / 22950) loss: 2.303403\n",
      "(Iteration 8001 / 22950) loss: 2.303066\n",
      "(Iteration 8101 / 22950) loss: 2.303221\n",
      "(Iteration 8201 / 22950) loss: 2.303546\n",
      "(Iteration 8301 / 22950) loss: 2.303049\n",
      "(Iteration 8401 / 22950) loss: 2.302935\n",
      "(Epoch 11 / 30) train acc: 0.201000; val_acc: 0.207000\n",
      "(Iteration 8501 / 22950) loss: 2.303007\n",
      "(Iteration 8601 / 22950) loss: 2.303217\n",
      "(Iteration 8701 / 22950) loss: 2.302631\n",
      "(Iteration 8801 / 22950) loss: 2.303443\n",
      "(Iteration 8901 / 22950) loss: 2.302925\n",
      "(Iteration 9001 / 22950) loss: 2.302779\n",
      "(Iteration 9101 / 22950) loss: 2.302932\n",
      "(Epoch 12 / 30) train acc: 0.225000; val_acc: 0.202000\n",
      "(Iteration 9201 / 22950) loss: 2.303088\n",
      "(Iteration 9301 / 22950) loss: 2.302858\n",
      "(Iteration 9401 / 22950) loss: 2.303012\n",
      "(Iteration 9501 / 22950) loss: 2.303215\n",
      "(Iteration 9601 / 22950) loss: 2.302805\n",
      "(Iteration 9701 / 22950) loss: 2.303006\n",
      "(Iteration 9801 / 22950) loss: 2.303143\n",
      "(Iteration 9901 / 22950) loss: 2.302650\n",
      "(Epoch 13 / 30) train acc: 0.231000; val_acc: 0.210000\n",
      "(Iteration 10001 / 22950) loss: 2.302869\n",
      "(Iteration 10101 / 22950) loss: 2.302351\n",
      "(Iteration 10201 / 22950) loss: 2.302706\n",
      "(Iteration 10301 / 22950) loss: 2.302765\n",
      "(Iteration 10401 / 22950) loss: 2.302779\n",
      "(Iteration 10501 / 22950) loss: 2.302666\n",
      "(Iteration 10601 / 22950) loss: 2.302876\n",
      "(Iteration 10701 / 22950) loss: 2.302233\n",
      "(Epoch 14 / 30) train acc: 0.220000; val_acc: 0.205000\n",
      "(Iteration 10801 / 22950) loss: 2.302924\n",
      "(Iteration 10901 / 22950) loss: 2.302113\n",
      "(Iteration 11001 / 22950) loss: 2.302756\n",
      "(Iteration 11101 / 22950) loss: 2.302649\n",
      "(Iteration 11201 / 22950) loss: 2.303021\n",
      "(Iteration 11301 / 22950) loss: 2.303068\n",
      "(Iteration 11401 / 22950) loss: 2.303111\n",
      "(Epoch 15 / 30) train acc: 0.219000; val_acc: 0.207000\n",
      "(Iteration 11501 / 22950) loss: 2.302751\n",
      "(Iteration 11601 / 22950) loss: 2.302946\n",
      "(Iteration 11701 / 22950) loss: 2.302405\n",
      "(Iteration 11801 / 22950) loss: 2.302924\n",
      "(Iteration 11901 / 22950) loss: 2.302550\n",
      "(Iteration 12001 / 22950) loss: 2.301977\n",
      "(Iteration 12101 / 22950) loss: 2.302458\n",
      "(Iteration 12201 / 22950) loss: 2.302364\n",
      "(Epoch 16 / 30) train acc: 0.193000; val_acc: 0.202000\n",
      "(Iteration 12301 / 22950) loss: 2.302094\n",
      "(Iteration 12401 / 22950) loss: 2.302630\n",
      "(Iteration 12501 / 22950) loss: 2.302262\n",
      "(Iteration 12601 / 22950) loss: 2.302467\n",
      "(Iteration 12701 / 22950) loss: 2.302261\n",
      "(Iteration 12801 / 22950) loss: 2.302477\n",
      "(Iteration 12901 / 22950) loss: 2.302710\n",
      "(Iteration 13001 / 22950) loss: 2.302334\n",
      "(Epoch 17 / 30) train acc: 0.203000; val_acc: 0.188000\n",
      "(Iteration 13101 / 22950) loss: 2.302632\n",
      "(Iteration 13201 / 22950) loss: 2.301915\n",
      "(Iteration 13301 / 22950) loss: 2.302578\n",
      "(Iteration 13401 / 22950) loss: 2.301748\n",
      "(Iteration 13501 / 22950) loss: 2.301717\n",
      "(Iteration 13601 / 22950) loss: 2.302028\n",
      "(Iteration 13701 / 22950) loss: 2.302753\n",
      "(Epoch 18 / 30) train acc: 0.243000; val_acc: 0.190000\n",
      "(Iteration 13801 / 22950) loss: 2.302619\n",
      "(Iteration 13901 / 22950) loss: 2.302374\n",
      "(Iteration 14001 / 22950) loss: 2.302932\n",
      "(Iteration 14101 / 22950) loss: 2.301951\n",
      "(Iteration 14201 / 22950) loss: 2.302446\n",
      "(Iteration 14301 / 22950) loss: 2.302029\n",
      "(Iteration 14401 / 22950) loss: 2.302206\n",
      "(Iteration 14501 / 22950) loss: 2.302549\n",
      "(Epoch 19 / 30) train acc: 0.223000; val_acc: 0.186000\n",
      "(Iteration 14601 / 22950) loss: 2.302444\n",
      "(Iteration 14701 / 22950) loss: 2.301755\n",
      "(Iteration 14801 / 22950) loss: 2.302169\n",
      "(Iteration 14901 / 22950) loss: 2.302372\n",
      "(Iteration 15001 / 22950) loss: 2.301646\n",
      "(Iteration 15101 / 22950) loss: 2.302198\n",
      "(Iteration 15201 / 22950) loss: 2.302501\n",
      "(Epoch 20 / 30) train acc: 0.216000; val_acc: 0.189000\n",
      "(Iteration 15301 / 22950) loss: 2.302036\n",
      "(Iteration 15401 / 22950) loss: 2.302226\n",
      "(Iteration 15501 / 22950) loss: 2.302891\n",
      "(Iteration 15601 / 22950) loss: 2.301983\n",
      "(Iteration 15701 / 22950) loss: 2.301955\n",
      "(Iteration 15801 / 22950) loss: 2.302025\n",
      "(Iteration 15901 / 22950) loss: 2.302013\n",
      "(Iteration 16001 / 22950) loss: 2.302025\n",
      "(Epoch 21 / 30) train acc: 0.208000; val_acc: 0.185000\n",
      "(Iteration 16101 / 22950) loss: 2.301655\n",
      "(Iteration 16201 / 22950) loss: 2.302338\n",
      "(Iteration 16301 / 22950) loss: 2.301883\n",
      "(Iteration 16401 / 22950) loss: 2.302195\n",
      "(Iteration 16501 / 22950) loss: 2.302646\n",
      "(Iteration 16601 / 22950) loss: 2.301879\n",
      "(Iteration 16701 / 22950) loss: 2.302086\n",
      "(Iteration 16801 / 22950) loss: 2.301897\n",
      "(Epoch 22 / 30) train acc: 0.191000; val_acc: 0.185000\n",
      "(Iteration 16901 / 22950) loss: 2.302032\n",
      "(Iteration 17001 / 22950) loss: 2.302055\n",
      "(Iteration 17101 / 22950) loss: 2.302299\n",
      "(Iteration 17201 / 22950) loss: 2.302193\n",
      "(Iteration 17301 / 22950) loss: 2.301841\n",
      "(Iteration 17401 / 22950) loss: 2.302380\n",
      "(Iteration 17501 / 22950) loss: 2.301755\n",
      "(Epoch 23 / 30) train acc: 0.197000; val_acc: 0.186000\n",
      "(Iteration 17601 / 22950) loss: 2.302371\n",
      "(Iteration 17701 / 22950) loss: 2.301851\n",
      "(Iteration 17801 / 22950) loss: 2.301849\n",
      "(Iteration 17901 / 22950) loss: 2.302156\n",
      "(Iteration 18001 / 22950) loss: 2.302071\n",
      "(Iteration 18101 / 22950) loss: 2.301931\n",
      "(Iteration 18201 / 22950) loss: 2.301843\n",
      "(Iteration 18301 / 22950) loss: 2.301845\n",
      "(Epoch 24 / 30) train acc: 0.226000; val_acc: 0.186000\n",
      "(Iteration 18401 / 22950) loss: 2.301660\n",
      "(Iteration 18501 / 22950) loss: 2.301530\n",
      "(Iteration 18601 / 22950) loss: 2.302279\n",
      "(Iteration 18701 / 22950) loss: 2.301305\n",
      "(Iteration 18801 / 22950) loss: 2.302618\n",
      "(Iteration 18901 / 22950) loss: 2.302099\n",
      "(Iteration 19001 / 22950) loss: 2.302429\n",
      "(Iteration 19101 / 22950) loss: 2.301921\n",
      "(Epoch 25 / 30) train acc: 0.205000; val_acc: 0.185000\n",
      "(Iteration 19201 / 22950) loss: 2.301413\n",
      "(Iteration 19301 / 22950) loss: 2.301891\n",
      "(Iteration 19401 / 22950) loss: 2.301872\n",
      "(Iteration 19501 / 22950) loss: 2.301487\n",
      "(Iteration 19601 / 22950) loss: 2.301396\n",
      "(Iteration 19701 / 22950) loss: 2.301046\n",
      "(Iteration 19801 / 22950) loss: 2.301422\n",
      "(Epoch 26 / 30) train acc: 0.213000; val_acc: 0.183000\n",
      "(Iteration 19901 / 22950) loss: 2.301840\n",
      "(Iteration 20001 / 22950) loss: 2.301470\n",
      "(Iteration 20101 / 22950) loss: 2.301678\n",
      "(Iteration 20201 / 22950) loss: 2.302119\n",
      "(Iteration 20301 / 22950) loss: 2.302577\n",
      "(Iteration 20401 / 22950) loss: 2.301820\n",
      "(Iteration 20501 / 22950) loss: 2.302220\n",
      "(Iteration 20601 / 22950) loss: 2.302013\n",
      "(Epoch 27 / 30) train acc: 0.218000; val_acc: 0.184000\n",
      "(Iteration 20701 / 22950) loss: 2.301657\n",
      "(Iteration 20801 / 22950) loss: 2.301455\n",
      "(Iteration 20901 / 22950) loss: 2.301849\n",
      "(Iteration 21001 / 22950) loss: 2.302070\n",
      "(Iteration 21101 / 22950) loss: 2.301942\n",
      "(Iteration 21201 / 22950) loss: 2.302131\n",
      "(Iteration 21301 / 22950) loss: 2.301757\n",
      "(Iteration 21401 / 22950) loss: 2.302418\n",
      "(Epoch 28 / 30) train acc: 0.203000; val_acc: 0.184000\n",
      "(Iteration 21501 / 22950) loss: 2.301620\n",
      "(Iteration 21601 / 22950) loss: 2.302202\n",
      "(Iteration 21701 / 22950) loss: 2.301533\n",
      "(Iteration 21801 / 22950) loss: 2.301614\n",
      "(Iteration 21901 / 22950) loss: 2.301373\n",
      "(Iteration 22001 / 22950) loss: 2.301853\n",
      "(Iteration 22101 / 22950) loss: 2.302099\n",
      "(Epoch 29 / 30) train acc: 0.208000; val_acc: 0.185000\n",
      "(Iteration 22201 / 22950) loss: 2.302042\n",
      "(Iteration 22301 / 22950) loss: 2.301236\n",
      "(Iteration 22401 / 22950) loss: 2.301145\n",
      "(Iteration 22501 / 22950) loss: 2.301993\n",
      "(Iteration 22601 / 22950) loss: 2.301758\n",
      "(Iteration 22701 / 22950) loss: 2.302101\n",
      "(Iteration 22801 / 22950) loss: 2.301914\n",
      "(Iteration 22901 / 22950) loss: 2.301140\n",
      "(Epoch 30 / 30) train acc: 0.216000; val_acc: 0.187000\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.001, 'num_epochs': 30, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 45930) loss: 2.305866\n",
      "(Epoch 0 / 30) train acc: 0.103000; val_acc: 0.094000\n",
      "(Iteration 101 / 45930) loss: 2.305985\n",
      "(Iteration 201 / 45930) loss: 2.305625\n",
      "(Iteration 301 / 45930) loss: 2.305443\n",
      "(Iteration 401 / 45930) loss: 2.305758\n",
      "(Iteration 501 / 45930) loss: 2.305718\n",
      "(Iteration 601 / 45930) loss: 2.305380\n",
      "(Iteration 701 / 45930) loss: 2.305788\n",
      "(Iteration 801 / 45930) loss: 2.305858\n",
      "(Iteration 901 / 45930) loss: 2.305656\n",
      "(Iteration 1001 / 45930) loss: 2.305662\n",
      "(Iteration 1101 / 45930) loss: 2.304905\n",
      "(Iteration 1201 / 45930) loss: 2.305179\n",
      "(Iteration 1301 / 45930) loss: 2.305237\n",
      "(Iteration 1401 / 45930) loss: 2.304770\n",
      "(Iteration 1501 / 45930) loss: 2.304942\n",
      "(Epoch 1 / 30) train acc: 0.085000; val_acc: 0.078000\n",
      "(Iteration 1601 / 45930) loss: 2.304655\n",
      "(Iteration 1701 / 45930) loss: 2.304617\n",
      "(Iteration 1801 / 45930) loss: 2.304057\n",
      "(Iteration 1901 / 45930) loss: 2.305460\n",
      "(Iteration 2001 / 45930) loss: 2.304323\n",
      "(Iteration 2101 / 45930) loss: 2.303928\n",
      "(Iteration 2201 / 45930) loss: 2.304458\n",
      "(Iteration 2301 / 45930) loss: 2.304548\n",
      "(Iteration 2401 / 45930) loss: 2.303776\n",
      "(Iteration 2501 / 45930) loss: 2.304856\n",
      "(Iteration 2601 / 45930) loss: 2.304579\n",
      "(Iteration 2701 / 45930) loss: 2.304914\n",
      "(Iteration 2801 / 45930) loss: 2.303149\n",
      "(Iteration 2901 / 45930) loss: 2.303875\n",
      "(Iteration 3001 / 45930) loss: 2.303231\n",
      "(Epoch 2 / 30) train acc: 0.100000; val_acc: 0.080000\n",
      "(Iteration 3101 / 45930) loss: 2.304158\n",
      "(Iteration 3201 / 45930) loss: 2.304712\n",
      "(Iteration 3301 / 45930) loss: 2.302838\n",
      "(Iteration 3401 / 45930) loss: 2.302795\n",
      "(Iteration 3501 / 45930) loss: 2.303496\n",
      "(Iteration 3601 / 45930) loss: 2.303743\n",
      "(Iteration 3701 / 45930) loss: 2.303978\n",
      "(Iteration 3801 / 45930) loss: 2.303945\n",
      "(Iteration 3901 / 45930) loss: 2.304089\n",
      "(Iteration 4001 / 45930) loss: 2.302607\n",
      "(Iteration 4101 / 45930) loss: 2.303629\n",
      "(Iteration 4201 / 45930) loss: 2.302886\n",
      "(Iteration 4301 / 45930) loss: 2.303237\n",
      "(Iteration 4401 / 45930) loss: 2.304548\n",
      "(Iteration 4501 / 45930) loss: 2.303374\n",
      "(Epoch 3 / 30) train acc: 0.116000; val_acc: 0.115000\n",
      "(Iteration 4601 / 45930) loss: 2.303632\n",
      "(Iteration 4701 / 45930) loss: 2.303479\n",
      "(Iteration 4801 / 45930) loss: 2.304110\n",
      "(Iteration 4901 / 45930) loss: 2.303468\n",
      "(Iteration 5001 / 45930) loss: 2.302987\n",
      "(Iteration 5101 / 45930) loss: 2.303697\n",
      "(Iteration 5201 / 45930) loss: 2.304609\n",
      "(Iteration 5301 / 45930) loss: 2.303222\n",
      "(Iteration 5401 / 45930) loss: 2.303482\n",
      "(Iteration 5501 / 45930) loss: 2.302996\n",
      "(Iteration 5601 / 45930) loss: 2.302954\n",
      "(Iteration 5701 / 45930) loss: 2.303874\n",
      "(Iteration 5801 / 45930) loss: 2.303619\n",
      "(Iteration 5901 / 45930) loss: 2.302735\n",
      "(Iteration 6001 / 45930) loss: 2.303048\n",
      "(Iteration 6101 / 45930) loss: 2.303948\n",
      "(Epoch 4 / 30) train acc: 0.104000; val_acc: 0.120000\n",
      "(Iteration 6201 / 45930) loss: 2.301518\n",
      "(Iteration 6301 / 45930) loss: 2.302426\n",
      "(Iteration 6401 / 45930) loss: 2.303780\n",
      "(Iteration 6501 / 45930) loss: 2.302698\n",
      "(Iteration 6601 / 45930) loss: 2.302955\n",
      "(Iteration 6701 / 45930) loss: 2.303540\n",
      "(Iteration 6801 / 45930) loss: 2.302590\n",
      "(Iteration 6901 / 45930) loss: 2.302911\n",
      "(Iteration 7001 / 45930) loss: 2.303150\n",
      "(Iteration 7101 / 45930) loss: 2.302955\n",
      "(Iteration 7201 / 45930) loss: 2.302177\n",
      "(Iteration 7301 / 45930) loss: 2.303780\n",
      "(Iteration 7401 / 45930) loss: 2.304112\n",
      "(Iteration 7501 / 45930) loss: 2.302443\n",
      "(Iteration 7601 / 45930) loss: 2.302323\n",
      "(Epoch 5 / 30) train acc: 0.138000; val_acc: 0.138000\n",
      "(Iteration 7701 / 45930) loss: 2.301546\n",
      "(Iteration 7801 / 45930) loss: 2.301460\n",
      "(Iteration 7901 / 45930) loss: 2.301844\n",
      "(Iteration 8001 / 45930) loss: 2.302346\n",
      "(Iteration 8101 / 45930) loss: 2.304125\n",
      "(Iteration 8201 / 45930) loss: 2.303021\n",
      "(Iteration 8301 / 45930) loss: 2.302358\n",
      "(Iteration 8401 / 45930) loss: 2.302346\n",
      "(Iteration 8501 / 45930) loss: 2.302609\n",
      "(Iteration 8601 / 45930) loss: 2.301645\n",
      "(Iteration 8701 / 45930) loss: 2.302225\n",
      "(Iteration 8801 / 45930) loss: 2.303234\n",
      "(Iteration 8901 / 45930) loss: 2.301304\n",
      "(Iteration 9001 / 45930) loss: 2.302009\n",
      "(Iteration 9101 / 45930) loss: 2.301674\n",
      "(Epoch 6 / 30) train acc: 0.200000; val_acc: 0.185000\n",
      "(Iteration 9201 / 45930) loss: 2.301092\n",
      "(Iteration 9301 / 45930) loss: 2.301978\n",
      "(Iteration 9401 / 45930) loss: 2.302589\n",
      "(Iteration 9501 / 45930) loss: 2.303418\n",
      "(Iteration 9601 / 45930) loss: 2.300853\n",
      "(Iteration 9701 / 45930) loss: 2.301207\n",
      "(Iteration 9801 / 45930) loss: 2.301859\n",
      "(Iteration 9901 / 45930) loss: 2.301991\n",
      "(Iteration 10001 / 45930) loss: 2.301422\n",
      "(Iteration 10101 / 45930) loss: 2.301546\n",
      "(Iteration 10201 / 45930) loss: 2.300832\n",
      "(Iteration 10301 / 45930) loss: 2.301202\n",
      "(Iteration 10401 / 45930) loss: 2.301025\n",
      "(Iteration 10501 / 45930) loss: 2.300967\n",
      "(Iteration 10601 / 45930) loss: 2.301457\n",
      "(Iteration 10701 / 45930) loss: 2.301678\n",
      "(Epoch 7 / 30) train acc: 0.212000; val_acc: 0.232000\n",
      "(Iteration 10801 / 45930) loss: 2.300527\n",
      "(Iteration 10901 / 45930) loss: 2.302383\n",
      "(Iteration 11001 / 45930) loss: 2.302662\n",
      "(Iteration 11101 / 45930) loss: 2.300775\n",
      "(Iteration 11201 / 45930) loss: 2.303213\n",
      "(Iteration 11301 / 45930) loss: 2.301980\n",
      "(Iteration 11401 / 45930) loss: 2.302124\n",
      "(Iteration 11501 / 45930) loss: 2.299933\n",
      "(Iteration 11601 / 45930) loss: 2.301136\n",
      "(Iteration 11701 / 45930) loss: 2.300927\n",
      "(Iteration 11801 / 45930) loss: 2.299695\n",
      "(Iteration 11901 / 45930) loss: 2.300757\n",
      "(Iteration 12001 / 45930) loss: 2.299576\n",
      "(Iteration 12101 / 45930) loss: 2.301554\n",
      "(Iteration 12201 / 45930) loss: 2.301285\n",
      "(Epoch 8 / 30) train acc: 0.211000; val_acc: 0.238000\n",
      "(Iteration 12301 / 45930) loss: 2.301477\n",
      "(Iteration 12401 / 45930) loss: 2.299416\n",
      "(Iteration 12501 / 45930) loss: 2.301902\n",
      "(Iteration 12601 / 45930) loss: 2.301744\n",
      "(Iteration 12701 / 45930) loss: 2.300230\n",
      "(Iteration 12801 / 45930) loss: 2.299538\n",
      "(Iteration 12901 / 45930) loss: 2.300920\n",
      "(Iteration 13001 / 45930) loss: 2.301191\n",
      "(Iteration 13101 / 45930) loss: 2.300763\n",
      "(Iteration 13201 / 45930) loss: 2.298218\n",
      "(Iteration 13301 / 45930) loss: 2.296387\n",
      "(Iteration 13401 / 45930) loss: 2.299465\n",
      "(Iteration 13501 / 45930) loss: 2.298761\n",
      "(Iteration 13601 / 45930) loss: 2.296325\n",
      "(Iteration 13701 / 45930) loss: 2.300371\n",
      "(Epoch 9 / 30) train acc: 0.231000; val_acc: 0.233000\n",
      "(Iteration 13801 / 45930) loss: 2.297433\n",
      "(Iteration 13901 / 45930) loss: 2.300466\n",
      "(Iteration 14001 / 45930) loss: 2.298760\n",
      "(Iteration 14101 / 45930) loss: 2.297304\n",
      "(Iteration 14201 / 45930) loss: 2.299178\n",
      "(Iteration 14301 / 45930) loss: 2.300113\n",
      "(Iteration 14401 / 45930) loss: 2.300505\n",
      "(Iteration 14501 / 45930) loss: 2.300877\n",
      "(Iteration 14601 / 45930) loss: 2.300477\n",
      "(Iteration 14701 / 45930) loss: 2.298855\n",
      "(Iteration 14801 / 45930) loss: 2.300717\n",
      "(Iteration 14901 / 45930) loss: 2.299699\n",
      "(Iteration 15001 / 45930) loss: 2.298744\n",
      "(Iteration 15101 / 45930) loss: 2.299475\n",
      "(Iteration 15201 / 45930) loss: 2.298445\n",
      "(Iteration 15301 / 45930) loss: 2.299077\n",
      "(Epoch 10 / 30) train acc: 0.225000; val_acc: 0.227000\n",
      "(Iteration 15401 / 45930) loss: 2.298110\n",
      "(Iteration 15501 / 45930) loss: 2.297412\n",
      "(Iteration 15601 / 45930) loss: 2.298992\n",
      "(Iteration 15701 / 45930) loss: 2.299150\n",
      "(Iteration 15801 / 45930) loss: 2.297259\n",
      "(Iteration 15901 / 45930) loss: 2.298671\n",
      "(Iteration 16001 / 45930) loss: 2.297779\n",
      "(Iteration 16101 / 45930) loss: 2.296353\n",
      "(Iteration 16201 / 45930) loss: 2.293987\n",
      "(Iteration 16301 / 45930) loss: 2.296378\n",
      "(Iteration 16401 / 45930) loss: 2.297045\n",
      "(Iteration 16501 / 45930) loss: 2.296314\n",
      "(Iteration 16601 / 45930) loss: 2.295701\n",
      "(Iteration 16701 / 45930) loss: 2.301111\n",
      "(Iteration 16801 / 45930) loss: 2.298655\n",
      "(Epoch 11 / 30) train acc: 0.224000; val_acc: 0.227000\n",
      "(Iteration 16901 / 45930) loss: 2.293977\n",
      "(Iteration 17001 / 45930) loss: 2.296029\n",
      "(Iteration 17101 / 45930) loss: 2.294262\n",
      "(Iteration 17201 / 45930) loss: 2.293472\n",
      "(Iteration 17301 / 45930) loss: 2.291765\n",
      "(Iteration 17401 / 45930) loss: 2.298076\n",
      "(Iteration 17501 / 45930) loss: 2.295622\n",
      "(Iteration 17601 / 45930) loss: 2.292495\n",
      "(Iteration 17701 / 45930) loss: 2.296117\n",
      "(Iteration 17801 / 45930) loss: 2.295412\n",
      "(Iteration 17901 / 45930) loss: 2.295887\n",
      "(Iteration 18001 / 45930) loss: 2.299401\n",
      "(Iteration 18101 / 45930) loss: 2.296400\n",
      "(Iteration 18201 / 45930) loss: 2.298536\n",
      "(Iteration 18301 / 45930) loss: 2.297248\n",
      "(Epoch 12 / 30) train acc: 0.223000; val_acc: 0.229000\n",
      "(Iteration 18401 / 45930) loss: 2.298178\n",
      "(Iteration 18501 / 45930) loss: 2.299254\n",
      "(Iteration 18601 / 45930) loss: 2.294588\n",
      "(Iteration 18701 / 45930) loss: 2.293905\n",
      "(Iteration 18801 / 45930) loss: 2.298068\n",
      "(Iteration 18901 / 45930) loss: 2.291319\n",
      "(Iteration 19001 / 45930) loss: 2.296673\n",
      "(Iteration 19101 / 45930) loss: 2.299922\n",
      "(Iteration 19201 / 45930) loss: 2.292964\n",
      "(Iteration 19301 / 45930) loss: 2.296404\n",
      "(Iteration 19401 / 45930) loss: 2.297213\n",
      "(Iteration 19501 / 45930) loss: 2.295169\n",
      "(Iteration 19601 / 45930) loss: 2.285335\n",
      "(Iteration 19701 / 45930) loss: 2.292946\n",
      "(Iteration 19801 / 45930) loss: 2.296857\n",
      "(Iteration 19901 / 45930) loss: 2.287347\n",
      "(Epoch 13 / 30) train acc: 0.250000; val_acc: 0.235000\n",
      "(Iteration 20001 / 45930) loss: 2.294623\n",
      "(Iteration 20101 / 45930) loss: 2.292837\n",
      "(Iteration 20201 / 45930) loss: 2.291637\n",
      "(Iteration 20301 / 45930) loss: 2.287180\n",
      "(Iteration 20401 / 45930) loss: 2.289746\n",
      "(Iteration 20501 / 45930) loss: 2.287544\n",
      "(Iteration 20601 / 45930) loss: 2.289300\n",
      "(Iteration 20701 / 45930) loss: 2.290965\n",
      "(Iteration 20801 / 45930) loss: 2.294328\n",
      "(Iteration 20901 / 45930) loss: 2.290412\n",
      "(Iteration 21001 / 45930) loss: 2.295717\n",
      "(Iteration 21101 / 45930) loss: 2.289211\n",
      "(Iteration 21201 / 45930) loss: 2.300467\n",
      "(Iteration 21301 / 45930) loss: 2.295164\n",
      "(Iteration 21401 / 45930) loss: 2.296050\n",
      "(Epoch 14 / 30) train acc: 0.266000; val_acc: 0.240000\n",
      "(Iteration 21501 / 45930) loss: 2.290851\n",
      "(Iteration 21601 / 45930) loss: 2.292413\n",
      "(Iteration 21701 / 45930) loss: 2.280261\n",
      "(Iteration 21801 / 45930) loss: 2.296118\n",
      "(Iteration 21901 / 45930) loss: 2.285245\n",
      "(Iteration 22001 / 45930) loss: 2.298878\n",
      "(Iteration 22101 / 45930) loss: 2.295899\n",
      "(Iteration 22201 / 45930) loss: 2.291527\n",
      "(Iteration 22301 / 45930) loss: 2.290537\n",
      "(Iteration 22401 / 45930) loss: 2.296468\n",
      "(Iteration 22501 / 45930) loss: 2.300561\n",
      "(Iteration 22601 / 45930) loss: 2.286623\n",
      "(Iteration 22701 / 45930) loss: 2.283834\n",
      "(Iteration 22801 / 45930) loss: 2.288232\n",
      "(Iteration 22901 / 45930) loss: 2.295432\n",
      "(Epoch 15 / 30) train acc: 0.255000; val_acc: 0.255000\n",
      "(Iteration 23001 / 45930) loss: 2.291797\n",
      "(Iteration 23101 / 45930) loss: 2.289993\n",
      "(Iteration 23201 / 45930) loss: 2.283721\n",
      "(Iteration 23301 / 45930) loss: 2.286629\n",
      "(Iteration 23401 / 45930) loss: 2.289823\n",
      "(Iteration 23501 / 45930) loss: 2.282858\n",
      "(Iteration 23601 / 45930) loss: 2.280917\n",
      "(Iteration 23701 / 45930) loss: 2.279714\n",
      "(Iteration 23801 / 45930) loss: 2.286014\n",
      "(Iteration 23901 / 45930) loss: 2.294644\n",
      "(Iteration 24001 / 45930) loss: 2.277731\n",
      "(Iteration 24101 / 45930) loss: 2.288740\n",
      "(Iteration 24201 / 45930) loss: 2.294713\n",
      "(Iteration 24301 / 45930) loss: 2.282989\n",
      "(Iteration 24401 / 45930) loss: 2.290729\n",
      "(Epoch 16 / 30) train acc: 0.229000; val_acc: 0.249000\n",
      "(Iteration 24501 / 45930) loss: 2.291216\n",
      "(Iteration 24601 / 45930) loss: 2.286188\n",
      "(Iteration 24701 / 45930) loss: 2.300688\n",
      "(Iteration 24801 / 45930) loss: 2.287916\n",
      "(Iteration 24901 / 45930) loss: 2.286866\n",
      "(Iteration 25001 / 45930) loss: 2.286985\n",
      "(Iteration 25101 / 45930) loss: 2.286907\n",
      "(Iteration 25201 / 45930) loss: 2.293007\n",
      "(Iteration 25301 / 45930) loss: 2.283768\n",
      "(Iteration 25401 / 45930) loss: 2.286735\n",
      "(Iteration 25501 / 45930) loss: 2.283483\n",
      "(Iteration 25601 / 45930) loss: 2.290234\n",
      "(Iteration 25701 / 45930) loss: 2.282306\n",
      "(Iteration 25801 / 45930) loss: 2.287752\n",
      "(Iteration 25901 / 45930) loss: 2.291127\n",
      "(Iteration 26001 / 45930) loss: 2.294783\n",
      "(Epoch 17 / 30) train acc: 0.233000; val_acc: 0.246000\n",
      "(Iteration 26101 / 45930) loss: 2.288437\n",
      "(Iteration 26201 / 45930) loss: 2.292408\n",
      "(Iteration 26301 / 45930) loss: 2.282854\n",
      "(Iteration 26401 / 45930) loss: 2.287503\n",
      "(Iteration 26501 / 45930) loss: 2.282416\n",
      "(Iteration 26601 / 45930) loss: 2.289581\n",
      "(Iteration 26701 / 45930) loss: 2.291799\n",
      "(Iteration 26801 / 45930) loss: 2.282139\n",
      "(Iteration 26901 / 45930) loss: 2.271625\n",
      "(Iteration 27001 / 45930) loss: 2.272357\n",
      "(Iteration 27101 / 45930) loss: 2.285992\n",
      "(Iteration 27201 / 45930) loss: 2.285860\n",
      "(Iteration 27301 / 45930) loss: 2.291517\n",
      "(Iteration 27401 / 45930) loss: 2.288750\n",
      "(Iteration 27501 / 45930) loss: 2.273350\n",
      "(Epoch 18 / 30) train acc: 0.230000; val_acc: 0.241000\n",
      "(Iteration 27601 / 45930) loss: 2.282502\n",
      "(Iteration 27701 / 45930) loss: 2.289215\n",
      "(Iteration 27801 / 45930) loss: 2.282468\n",
      "(Iteration 27901 / 45930) loss: 2.283146\n",
      "(Iteration 28001 / 45930) loss: 2.287884\n",
      "(Iteration 28101 / 45930) loss: 2.279235\n",
      "(Iteration 28201 / 45930) loss: 2.284143\n",
      "(Iteration 28301 / 45930) loss: 2.267357\n",
      "(Iteration 28401 / 45930) loss: 2.289848\n",
      "(Iteration 28501 / 45930) loss: 2.287370\n",
      "(Iteration 28601 / 45930) loss: 2.288724\n",
      "(Iteration 28701 / 45930) loss: 2.286220\n",
      "(Iteration 28801 / 45930) loss: 2.270957\n",
      "(Iteration 28901 / 45930) loss: 2.272247\n",
      "(Iteration 29001 / 45930) loss: 2.255019\n",
      "(Epoch 19 / 30) train acc: 0.245000; val_acc: 0.237000\n",
      "(Iteration 29101 / 45930) loss: 2.277768\n",
      "(Iteration 29201 / 45930) loss: 2.282702\n",
      "(Iteration 29301 / 45930) loss: 2.292809\n",
      "(Iteration 29401 / 45930) loss: 2.290694\n",
      "(Iteration 29501 / 45930) loss: 2.288083\n",
      "(Iteration 29601 / 45930) loss: 2.263509\n",
      "(Iteration 29701 / 45930) loss: 2.270681\n",
      "(Iteration 29801 / 45930) loss: 2.281294\n",
      "(Iteration 29901 / 45930) loss: 2.258346\n",
      "(Iteration 30001 / 45930) loss: 2.277316\n",
      "(Iteration 30101 / 45930) loss: 2.280654\n",
      "(Iteration 30201 / 45930) loss: 2.277935\n",
      "(Iteration 30301 / 45930) loss: 2.285031\n",
      "(Iteration 30401 / 45930) loss: 2.280854\n",
      "(Iteration 30501 / 45930) loss: 2.267871\n",
      "(Iteration 30601 / 45930) loss: 2.283555\n",
      "(Epoch 20 / 30) train acc: 0.232000; val_acc: 0.234000\n",
      "(Iteration 30701 / 45930) loss: 2.294992\n",
      "(Iteration 30801 / 45930) loss: 2.263626\n",
      "(Iteration 30901 / 45930) loss: 2.294388\n",
      "(Iteration 31001 / 45930) loss: 2.272136\n",
      "(Iteration 31101 / 45930) loss: 2.284928\n",
      "(Iteration 31201 / 45930) loss: 2.284483\n",
      "(Iteration 31301 / 45930) loss: 2.276569\n",
      "(Iteration 31401 / 45930) loss: 2.254041\n",
      "(Iteration 31501 / 45930) loss: 2.280954\n",
      "(Iteration 31601 / 45930) loss: 2.274004\n",
      "(Iteration 31701 / 45930) loss: 2.286485\n",
      "(Iteration 31801 / 45930) loss: 2.257331\n",
      "(Iteration 31901 / 45930) loss: 2.277429\n",
      "(Iteration 32001 / 45930) loss: 2.285100\n",
      "(Iteration 32101 / 45930) loss: 2.279589\n",
      "(Epoch 21 / 30) train acc: 0.221000; val_acc: 0.230000\n",
      "(Iteration 32201 / 45930) loss: 2.272847\n",
      "(Iteration 32301 / 45930) loss: 2.267688\n",
      "(Iteration 32401 / 45930) loss: 2.265606\n",
      "(Iteration 32501 / 45930) loss: 2.257104\n",
      "(Iteration 32601 / 45930) loss: 2.259541\n",
      "(Iteration 32701 / 45930) loss: 2.260175\n",
      "(Iteration 32801 / 45930) loss: 2.270506\n",
      "(Iteration 32901 / 45930) loss: 2.253036\n",
      "(Iteration 33001 / 45930) loss: 2.297251\n",
      "(Iteration 33101 / 45930) loss: 2.275039\n",
      "(Iteration 33201 / 45930) loss: 2.260210\n",
      "(Iteration 33301 / 45930) loss: 2.284232\n",
      "(Iteration 33401 / 45930) loss: 2.278773\n",
      "(Iteration 33501 / 45930) loss: 2.267863\n",
      "(Iteration 33601 / 45930) loss: 2.271097\n",
      "(Epoch 22 / 30) train acc: 0.225000; val_acc: 0.230000\n",
      "(Iteration 33701 / 45930) loss: 2.273950\n",
      "(Iteration 33801 / 45930) loss: 2.269786\n",
      "(Iteration 33901 / 45930) loss: 2.294566\n",
      "(Iteration 34001 / 45930) loss: 2.271966\n",
      "(Iteration 34101 / 45930) loss: 2.278162\n",
      "(Iteration 34201 / 45930) loss: 2.259730\n",
      "(Iteration 34301 / 45930) loss: 2.247064\n",
      "(Iteration 34401 / 45930) loss: 2.277510\n",
      "(Iteration 34501 / 45930) loss: 2.265076\n",
      "(Iteration 34601 / 45930) loss: 2.253192\n",
      "(Iteration 34701 / 45930) loss: 2.245239\n",
      "(Iteration 34801 / 45930) loss: 2.269417\n",
      "(Iteration 34901 / 45930) loss: 2.283510\n",
      "(Iteration 35001 / 45930) loss: 2.284648\n",
      "(Iteration 35101 / 45930) loss: 2.273823\n",
      "(Iteration 35201 / 45930) loss: 2.262085\n",
      "(Epoch 23 / 30) train acc: 0.229000; val_acc: 0.229000\n",
      "(Iteration 35301 / 45930) loss: 2.293831\n",
      "(Iteration 35401 / 45930) loss: 2.245406\n",
      "(Iteration 35501 / 45930) loss: 2.294871\n",
      "(Iteration 35601 / 45930) loss: 2.260168\n",
      "(Iteration 35701 / 45930) loss: 2.274743\n",
      "(Iteration 35801 / 45930) loss: 2.279605\n",
      "(Iteration 35901 / 45930) loss: 2.288686\n",
      "(Iteration 36001 / 45930) loss: 2.292309\n",
      "(Iteration 36101 / 45930) loss: 2.268549\n",
      "(Iteration 36201 / 45930) loss: 2.263395\n",
      "(Iteration 36301 / 45930) loss: 2.264795\n",
      "(Iteration 36401 / 45930) loss: 2.249432\n",
      "(Iteration 36501 / 45930) loss: 2.271219\n",
      "(Iteration 36601 / 45930) loss: 2.253166\n",
      "(Iteration 36701 / 45930) loss: 2.267089\n",
      "(Epoch 24 / 30) train acc: 0.228000; val_acc: 0.228000\n",
      "(Iteration 36801 / 45930) loss: 2.253779\n",
      "(Iteration 36901 / 45930) loss: 2.252905\n",
      "(Iteration 37001 / 45930) loss: 2.272885\n",
      "(Iteration 37101 / 45930) loss: 2.264683\n",
      "(Iteration 37201 / 45930) loss: 2.271473\n",
      "(Iteration 37301 / 45930) loss: 2.258849\n",
      "(Iteration 37401 / 45930) loss: 2.266502\n",
      "(Iteration 37501 / 45930) loss: 2.270099\n",
      "(Iteration 37601 / 45930) loss: 2.264553\n",
      "(Iteration 37701 / 45930) loss: 2.263529\n",
      "(Iteration 37801 / 45930) loss: 2.244343\n",
      "(Iteration 37901 / 45930) loss: 2.289808\n",
      "(Iteration 38001 / 45930) loss: 2.275084\n",
      "(Iteration 38101 / 45930) loss: 2.269387\n",
      "(Iteration 38201 / 45930) loss: 2.250207\n",
      "(Epoch 25 / 30) train acc: 0.241000; val_acc: 0.228000\n",
      "(Iteration 38301 / 45930) loss: 2.268637\n",
      "(Iteration 38401 / 45930) loss: 2.287035\n",
      "(Iteration 38501 / 45930) loss: 2.288063\n",
      "(Iteration 38601 / 45930) loss: 2.286775\n",
      "(Iteration 38701 / 45930) loss: 2.252898\n",
      "(Iteration 38801 / 45930) loss: 2.290996\n",
      "(Iteration 38901 / 45930) loss: 2.266269\n",
      "(Iteration 39001 / 45930) loss: 2.267787\n",
      "(Iteration 39101 / 45930) loss: 2.287257\n",
      "(Iteration 39201 / 45930) loss: 2.276990\n",
      "(Iteration 39301 / 45930) loss: 2.248988\n",
      "(Iteration 39401 / 45930) loss: 2.289294\n",
      "(Iteration 39501 / 45930) loss: 2.253349\n",
      "(Iteration 39601 / 45930) loss: 2.260732\n",
      "(Iteration 39701 / 45930) loss: 2.284593\n",
      "(Iteration 39801 / 45930) loss: 2.264865\n",
      "(Epoch 26 / 30) train acc: 0.227000; val_acc: 0.228000\n",
      "(Iteration 39901 / 45930) loss: 2.278968\n",
      "(Iteration 40001 / 45930) loss: 2.270215\n",
      "(Iteration 40101 / 45930) loss: 2.252253\n",
      "(Iteration 40201 / 45930) loss: 2.248531\n",
      "(Iteration 40301 / 45930) loss: 2.255522\n",
      "(Iteration 40401 / 45930) loss: 2.280464\n",
      "(Iteration 40501 / 45930) loss: 2.297134\n",
      "(Iteration 40601 / 45930) loss: 2.254086\n",
      "(Iteration 40701 / 45930) loss: 2.273567\n",
      "(Iteration 40801 / 45930) loss: 2.274005\n",
      "(Iteration 40901 / 45930) loss: 2.290941\n",
      "(Iteration 41001 / 45930) loss: 2.271694\n",
      "(Iteration 41101 / 45930) loss: 2.282548\n",
      "(Iteration 41201 / 45930) loss: 2.258646\n",
      "(Iteration 41301 / 45930) loss: 2.208389\n",
      "(Epoch 27 / 30) train acc: 0.215000; val_acc: 0.226000\n",
      "(Iteration 41401 / 45930) loss: 2.238029\n",
      "(Iteration 41501 / 45930) loss: 2.241622\n",
      "(Iteration 41601 / 45930) loss: 2.272815\n",
      "(Iteration 41701 / 45930) loss: 2.262703\n",
      "(Iteration 41801 / 45930) loss: 2.277189\n",
      "(Iteration 41901 / 45930) loss: 2.282841\n",
      "(Iteration 42001 / 45930) loss: 2.261732\n",
      "(Iteration 42101 / 45930) loss: 2.275882\n",
      "(Iteration 42201 / 45930) loss: 2.242164\n",
      "(Iteration 42301 / 45930) loss: 2.269620\n",
      "(Iteration 42401 / 45930) loss: 2.286531\n",
      "(Iteration 42501 / 45930) loss: 2.267434\n",
      "(Iteration 42601 / 45930) loss: 2.279418\n",
      "(Iteration 42701 / 45930) loss: 2.266287\n",
      "(Iteration 42801 / 45930) loss: 2.247864\n",
      "(Epoch 28 / 30) train acc: 0.222000; val_acc: 0.226000\n",
      "(Iteration 42901 / 45930) loss: 2.263258\n",
      "(Iteration 43001 / 45930) loss: 2.286821\n",
      "(Iteration 43101 / 45930) loss: 2.255181\n",
      "(Iteration 43201 / 45930) loss: 2.289321\n",
      "(Iteration 43301 / 45930) loss: 2.216476\n",
      "(Iteration 43401 / 45930) loss: 2.251429\n",
      "(Iteration 43501 / 45930) loss: 2.264484\n",
      "(Iteration 43601 / 45930) loss: 2.274003\n",
      "(Iteration 43701 / 45930) loss: 2.224833\n",
      "(Iteration 43801 / 45930) loss: 2.254956\n",
      "(Iteration 43901 / 45930) loss: 2.227511\n",
      "(Iteration 44001 / 45930) loss: 2.252487\n",
      "(Iteration 44101 / 45930) loss: 2.262038\n",
      "(Iteration 44201 / 45930) loss: 2.266492\n",
      "(Iteration 44301 / 45930) loss: 2.278160\n",
      "(Epoch 29 / 30) train acc: 0.207000; val_acc: 0.226000\n",
      "(Iteration 44401 / 45930) loss: 2.273468\n",
      "(Iteration 44501 / 45930) loss: 2.240385\n",
      "(Iteration 44601 / 45930) loss: 2.285964\n",
      "(Iteration 44701 / 45930) loss: 2.263917\n",
      "(Iteration 44801 / 45930) loss: 2.281366\n",
      "(Iteration 44901 / 45930) loss: 2.277658\n",
      "(Iteration 45001 / 45930) loss: 2.277317\n",
      "(Iteration 45101 / 45930) loss: 2.244903\n",
      "(Iteration 45201 / 45930) loss: 2.273777\n",
      "(Iteration 45301 / 45930) loss: 2.238152\n",
      "(Iteration 45401 / 45930) loss: 2.269741\n",
      "(Iteration 45501 / 45930) loss: 2.283844\n",
      "(Iteration 45601 / 45930) loss: 2.287837\n",
      "(Iteration 45701 / 45930) loss: 2.273627\n",
      "(Iteration 45801 / 45930) loss: 2.265357\n",
      "(Iteration 45901 / 45930) loss: 2.281880\n",
      "(Epoch 30 / 30) train acc: 0.233000; val_acc: 0.225000\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.001, 'num_epochs': 30, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 22950) loss: 2.302894\n",
      "(Epoch 0 / 30) train acc: 0.117000; val_acc: 0.122000\n",
      "(Iteration 101 / 22950) loss: 2.302801\n",
      "(Iteration 201 / 22950) loss: 2.302844\n",
      "(Iteration 301 / 22950) loss: 2.302678\n",
      "(Iteration 401 / 22950) loss: 2.302678\n",
      "(Iteration 501 / 22950) loss: 2.302772\n",
      "(Iteration 601 / 22950) loss: 2.302457\n",
      "(Iteration 701 / 22950) loss: 2.302935\n",
      "(Epoch 1 / 30) train acc: 0.118000; val_acc: 0.087000\n",
      "(Iteration 801 / 22950) loss: 2.302573\n",
      "(Iteration 901 / 22950) loss: 2.302692\n",
      "(Iteration 1001 / 22950) loss: 2.302735\n",
      "(Iteration 1101 / 22950) loss: 2.302800\n",
      "(Iteration 1201 / 22950) loss: 2.302694\n",
      "(Iteration 1301 / 22950) loss: 2.302817\n",
      "(Iteration 1401 / 22950) loss: 2.302726\n",
      "(Iteration 1501 / 22950) loss: 2.302793\n",
      "(Epoch 2 / 30) train acc: 0.134000; val_acc: 0.125000\n",
      "(Iteration 1601 / 22950) loss: 2.302658\n",
      "(Iteration 1701 / 22950) loss: 2.302487\n",
      "(Iteration 1801 / 22950) loss: 2.302751\n",
      "(Iteration 1901 / 22950) loss: 2.302684\n",
      "(Iteration 2001 / 22950) loss: 2.302502\n",
      "(Iteration 2101 / 22950) loss: 2.302662\n",
      "(Iteration 2201 / 22950) loss: 2.302624\n",
      "(Epoch 3 / 30) train acc: 0.152000; val_acc: 0.134000\n",
      "(Iteration 2301 / 22950) loss: 2.302371\n",
      "(Iteration 2401 / 22950) loss: 2.302712\n",
      "(Iteration 2501 / 22950) loss: 2.302376\n",
      "(Iteration 2601 / 22950) loss: 2.302503\n",
      "(Iteration 2701 / 22950) loss: 2.302535\n",
      "(Iteration 2801 / 22950) loss: 2.302514\n",
      "(Iteration 2901 / 22950) loss: 2.302394\n",
      "(Iteration 3001 / 22950) loss: 2.302138\n",
      "(Epoch 4 / 30) train acc: 0.092000; val_acc: 0.083000\n",
      "(Iteration 3101 / 22950) loss: 2.302465\n",
      "(Iteration 3201 / 22950) loss: 2.302096\n",
      "(Iteration 3301 / 22950) loss: 2.302182\n",
      "(Iteration 3401 / 22950) loss: 2.302430\n",
      "(Iteration 3501 / 22950) loss: 2.302208\n",
      "(Iteration 3601 / 22950) loss: 2.302412\n",
      "(Iteration 3701 / 22950) loss: 2.302110\n",
      "(Iteration 3801 / 22950) loss: 2.302170\n",
      "(Epoch 5 / 30) train acc: 0.162000; val_acc: 0.148000\n",
      "(Iteration 3901 / 22950) loss: 2.302300\n",
      "(Iteration 4001 / 22950) loss: 2.301919\n",
      "(Iteration 4101 / 22950) loss: 2.302130\n",
      "(Iteration 4201 / 22950) loss: 2.301947\n",
      "(Iteration 4301 / 22950) loss: 2.302217\n",
      "(Iteration 4401 / 22950) loss: 2.302096\n",
      "(Iteration 4501 / 22950) loss: 2.301928\n",
      "(Epoch 6 / 30) train acc: 0.148000; val_acc: 0.124000\n",
      "(Iteration 4601 / 22950) loss: 2.301848\n",
      "(Iteration 4701 / 22950) loss: 2.301832\n",
      "(Iteration 4801 / 22950) loss: 2.301862\n",
      "(Iteration 4901 / 22950) loss: 2.301808\n",
      "(Iteration 5001 / 22950) loss: 2.302130\n",
      "(Iteration 5101 / 22950) loss: 2.301470\n",
      "(Iteration 5201 / 22950) loss: 2.301413\n",
      "(Iteration 5301 / 22950) loss: 2.301552\n",
      "(Epoch 7 / 30) train acc: 0.188000; val_acc: 0.162000\n",
      "(Iteration 5401 / 22950) loss: 2.301749\n",
      "(Iteration 5501 / 22950) loss: 2.301510\n",
      "(Iteration 5601 / 22950) loss: 2.301964\n",
      "(Iteration 5701 / 22950) loss: 2.301529\n",
      "(Iteration 5801 / 22950) loss: 2.301570\n",
      "(Iteration 5901 / 22950) loss: 2.301953\n",
      "(Iteration 6001 / 22950) loss: 2.301026\n",
      "(Iteration 6101 / 22950) loss: 2.301814\n",
      "(Epoch 8 / 30) train acc: 0.209000; val_acc: 0.219000\n",
      "(Iteration 6201 / 22950) loss: 2.301258\n",
      "(Iteration 6301 / 22950) loss: 2.301230\n",
      "(Iteration 6401 / 22950) loss: 2.301162\n",
      "(Iteration 6501 / 22950) loss: 2.301391\n",
      "(Iteration 6601 / 22950) loss: 2.300839\n",
      "(Iteration 6701 / 22950) loss: 2.301503\n",
      "(Iteration 6801 / 22950) loss: 2.300998\n",
      "(Epoch 9 / 30) train acc: 0.214000; val_acc: 0.204000\n",
      "(Iteration 6901 / 22950) loss: 2.301565\n",
      "(Iteration 7001 / 22950) loss: 2.301413\n",
      "(Iteration 7101 / 22950) loss: 2.300765\n",
      "(Iteration 7201 / 22950) loss: 2.301110\n",
      "(Iteration 7301 / 22950) loss: 2.300480\n",
      "(Iteration 7401 / 22950) loss: 2.301158\n",
      "(Iteration 7501 / 22950) loss: 2.301458\n",
      "(Iteration 7601 / 22950) loss: 2.300616\n",
      "(Epoch 10 / 30) train acc: 0.239000; val_acc: 0.229000\n",
      "(Iteration 7701 / 22950) loss: 2.300379\n",
      "(Iteration 7801 / 22950) loss: 2.301291\n",
      "(Iteration 7901 / 22950) loss: 2.300271\n",
      "(Iteration 8001 / 22950) loss: 2.300656\n",
      "(Iteration 8101 / 22950) loss: 2.300532\n",
      "(Iteration 8201 / 22950) loss: 2.300454\n",
      "(Iteration 8301 / 22950) loss: 2.300259\n",
      "(Iteration 8401 / 22950) loss: 2.300192\n",
      "(Epoch 11 / 30) train acc: 0.282000; val_acc: 0.234000\n",
      "(Iteration 8501 / 22950) loss: 2.300493\n",
      "(Iteration 8601 / 22950) loss: 2.300367\n",
      "(Iteration 8701 / 22950) loss: 2.300079\n",
      "(Iteration 8801 / 22950) loss: 2.300694\n",
      "(Iteration 8901 / 22950) loss: 2.300323\n",
      "(Iteration 9001 / 22950) loss: 2.299345\n",
      "(Iteration 9101 / 22950) loss: 2.299439\n",
      "(Epoch 12 / 30) train acc: 0.215000; val_acc: 0.236000\n",
      "(Iteration 9201 / 22950) loss: 2.299489\n",
      "(Iteration 9301 / 22950) loss: 2.300386\n",
      "(Iteration 9401 / 22950) loss: 2.299451\n",
      "(Iteration 9501 / 22950) loss: 2.300084\n",
      "(Iteration 9601 / 22950) loss: 2.300414\n",
      "(Iteration 9701 / 22950) loss: 2.299233\n",
      "(Iteration 9801 / 22950) loss: 2.299384\n",
      "(Iteration 9901 / 22950) loss: 2.299944\n",
      "(Epoch 13 / 30) train acc: 0.246000; val_acc: 0.239000\n",
      "(Iteration 10001 / 22950) loss: 2.298798\n",
      "(Iteration 10101 / 22950) loss: 2.298788\n",
      "(Iteration 10201 / 22950) loss: 2.297873\n",
      "(Iteration 10301 / 22950) loss: 2.299193\n",
      "(Iteration 10401 / 22950) loss: 2.298970\n",
      "(Iteration 10501 / 22950) loss: 2.299898\n",
      "(Iteration 10601 / 22950) loss: 2.299632\n",
      "(Iteration 10701 / 22950) loss: 2.300171\n",
      "(Epoch 14 / 30) train acc: 0.281000; val_acc: 0.246000\n",
      "(Iteration 10801 / 22950) loss: 2.297228\n",
      "(Iteration 10901 / 22950) loss: 2.299081\n",
      "(Iteration 11001 / 22950) loss: 2.299699\n",
      "(Iteration 11101 / 22950) loss: 2.298527\n",
      "(Iteration 11201 / 22950) loss: 2.298627\n",
      "(Iteration 11301 / 22950) loss: 2.299043\n",
      "(Iteration 11401 / 22950) loss: 2.298351\n",
      "(Epoch 15 / 30) train acc: 0.239000; val_acc: 0.246000\n",
      "(Iteration 11501 / 22950) loss: 2.299211\n",
      "(Iteration 11601 / 22950) loss: 2.298588\n",
      "(Iteration 11701 / 22950) loss: 2.298684\n",
      "(Iteration 11801 / 22950) loss: 2.297850\n",
      "(Iteration 11901 / 22950) loss: 2.298463\n",
      "(Iteration 12001 / 22950) loss: 2.298932\n",
      "(Iteration 12101 / 22950) loss: 2.299634\n",
      "(Iteration 12201 / 22950) loss: 2.299157\n",
      "(Epoch 16 / 30) train acc: 0.275000; val_acc: 0.249000\n",
      "(Iteration 12301 / 22950) loss: 2.299751\n",
      "(Iteration 12401 / 22950) loss: 2.299338\n",
      "(Iteration 12501 / 22950) loss: 2.298905\n",
      "(Iteration 12601 / 22950) loss: 2.297648\n",
      "(Iteration 12701 / 22950) loss: 2.299790\n",
      "(Iteration 12801 / 22950) loss: 2.297991\n",
      "(Iteration 12901 / 22950) loss: 2.296980\n",
      "(Iteration 13001 / 22950) loss: 2.298687\n",
      "(Epoch 17 / 30) train acc: 0.260000; val_acc: 0.257000\n",
      "(Iteration 13101 / 22950) loss: 2.298154\n",
      "(Iteration 13201 / 22950) loss: 2.298468\n",
      "(Iteration 13301 / 22950) loss: 2.297878\n",
      "(Iteration 13401 / 22950) loss: 2.296978\n",
      "(Iteration 13501 / 22950) loss: 2.296667\n",
      "(Iteration 13601 / 22950) loss: 2.297347\n",
      "(Iteration 13701 / 22950) loss: 2.296736\n",
      "(Epoch 18 / 30) train acc: 0.284000; val_acc: 0.264000\n",
      "(Iteration 13801 / 22950) loss: 2.296742\n",
      "(Iteration 13901 / 22950) loss: 2.297568\n",
      "(Iteration 14001 / 22950) loss: 2.299635\n",
      "(Iteration 14101 / 22950) loss: 2.297904\n",
      "(Iteration 14201 / 22950) loss: 2.298376\n",
      "(Iteration 14301 / 22950) loss: 2.296608\n",
      "(Iteration 14401 / 22950) loss: 2.297344\n",
      "(Iteration 14501 / 22950) loss: 2.295941\n",
      "(Epoch 19 / 30) train acc: 0.285000; val_acc: 0.277000\n",
      "(Iteration 14601 / 22950) loss: 2.298886\n",
      "(Iteration 14701 / 22950) loss: 2.298805\n",
      "(Iteration 14801 / 22950) loss: 2.296830\n",
      "(Iteration 14901 / 22950) loss: 2.297181\n",
      "(Iteration 15001 / 22950) loss: 2.296396\n",
      "(Iteration 15101 / 22950) loss: 2.297403\n",
      "(Iteration 15201 / 22950) loss: 2.295931\n",
      "(Epoch 20 / 30) train acc: 0.300000; val_acc: 0.275000\n",
      "(Iteration 15301 / 22950) loss: 2.296367\n",
      "(Iteration 15401 / 22950) loss: 2.297841\n",
      "(Iteration 15501 / 22950) loss: 2.296477\n",
      "(Iteration 15601 / 22950) loss: 2.296329\n",
      "(Iteration 15701 / 22950) loss: 2.295446\n",
      "(Iteration 15801 / 22950) loss: 2.296429\n",
      "(Iteration 15901 / 22950) loss: 2.296148\n",
      "(Iteration 16001 / 22950) loss: 2.297829\n",
      "(Epoch 21 / 30) train acc: 0.288000; val_acc: 0.269000\n",
      "(Iteration 16101 / 22950) loss: 2.297785\n",
      "(Iteration 16201 / 22950) loss: 2.297719\n",
      "(Iteration 16301 / 22950) loss: 2.294590\n",
      "(Iteration 16401 / 22950) loss: 2.298038\n",
      "(Iteration 16501 / 22950) loss: 2.295733\n",
      "(Iteration 16601 / 22950) loss: 2.295257\n",
      "(Iteration 16701 / 22950) loss: 2.296085\n",
      "(Iteration 16801 / 22950) loss: 2.294886\n",
      "(Epoch 22 / 30) train acc: 0.277000; val_acc: 0.267000\n",
      "(Iteration 16901 / 22950) loss: 2.298113\n",
      "(Iteration 17001 / 22950) loss: 2.296499\n",
      "(Iteration 17101 / 22950) loss: 2.297537\n",
      "(Iteration 17201 / 22950) loss: 2.293991\n",
      "(Iteration 17301 / 22950) loss: 2.295833\n",
      "(Iteration 17401 / 22950) loss: 2.296565\n",
      "(Iteration 17501 / 22950) loss: 2.295015\n",
      "(Epoch 23 / 30) train acc: 0.306000; val_acc: 0.268000\n",
      "(Iteration 17601 / 22950) loss: 2.294770\n",
      "(Iteration 17701 / 22950) loss: 2.296062\n",
      "(Iteration 17801 / 22950) loss: 2.298009\n",
      "(Iteration 17901 / 22950) loss: 2.298669\n",
      "(Iteration 18001 / 22950) loss: 2.295584\n",
      "(Iteration 18101 / 22950) loss: 2.296594\n",
      "(Iteration 18201 / 22950) loss: 2.296312\n",
      "(Iteration 18301 / 22950) loss: 2.295127\n",
      "(Epoch 24 / 30) train acc: 0.286000; val_acc: 0.271000\n",
      "(Iteration 18401 / 22950) loss: 2.295384\n",
      "(Iteration 18501 / 22950) loss: 2.296266\n",
      "(Iteration 18601 / 22950) loss: 2.294050\n",
      "(Iteration 18701 / 22950) loss: 2.296448\n",
      "(Iteration 18801 / 22950) loss: 2.295631\n",
      "(Iteration 18901 / 22950) loss: 2.296320\n",
      "(Iteration 19001 / 22950) loss: 2.293771\n",
      "(Iteration 19101 / 22950) loss: 2.298660\n",
      "(Epoch 25 / 30) train acc: 0.297000; val_acc: 0.269000\n",
      "(Iteration 19201 / 22950) loss: 2.294457\n",
      "(Iteration 19301 / 22950) loss: 2.294287\n",
      "(Iteration 19401 / 22950) loss: 2.294677\n",
      "(Iteration 19501 / 22950) loss: 2.293158\n",
      "(Iteration 19601 / 22950) loss: 2.296994\n",
      "(Iteration 19701 / 22950) loss: 2.297130\n",
      "(Iteration 19801 / 22950) loss: 2.296548\n",
      "(Epoch 26 / 30) train acc: 0.304000; val_acc: 0.271000\n",
      "(Iteration 19901 / 22950) loss: 2.295261\n",
      "(Iteration 20001 / 22950) loss: 2.293129\n",
      "(Iteration 20101 / 22950) loss: 2.295467\n",
      "(Iteration 20201 / 22950) loss: 2.295012\n",
      "(Iteration 20301 / 22950) loss: 2.294610\n",
      "(Iteration 20401 / 22950) loss: 2.296333\n",
      "(Iteration 20501 / 22950) loss: 2.295547\n",
      "(Iteration 20601 / 22950) loss: 2.294334\n",
      "(Epoch 27 / 30) train acc: 0.305000; val_acc: 0.279000\n",
      "(Iteration 20701 / 22950) loss: 2.295398\n",
      "(Iteration 20801 / 22950) loss: 2.296685\n",
      "(Iteration 20901 / 22950) loss: 2.293612\n",
      "(Iteration 21001 / 22950) loss: 2.296245\n",
      "(Iteration 21101 / 22950) loss: 2.291277\n",
      "(Iteration 21201 / 22950) loss: 2.292885\n",
      "(Iteration 21301 / 22950) loss: 2.292921\n",
      "(Iteration 21401 / 22950) loss: 2.293443\n",
      "(Epoch 28 / 30) train acc: 0.284000; val_acc: 0.282000\n",
      "(Iteration 21501 / 22950) loss: 2.292821\n",
      "(Iteration 21601 / 22950) loss: 2.293532\n",
      "(Iteration 21701 / 22950) loss: 2.297270\n",
      "(Iteration 21801 / 22950) loss: 2.295971\n",
      "(Iteration 21901 / 22950) loss: 2.291971\n",
      "(Iteration 22001 / 22950) loss: 2.297194\n",
      "(Iteration 22101 / 22950) loss: 2.291061\n",
      "(Epoch 29 / 30) train acc: 0.296000; val_acc: 0.285000\n",
      "(Iteration 22201 / 22950) loss: 2.293593\n",
      "(Iteration 22301 / 22950) loss: 2.296440\n",
      "(Iteration 22401 / 22950) loss: 2.294537\n",
      "(Iteration 22501 / 22950) loss: 2.294251\n",
      "(Iteration 22601 / 22950) loss: 2.290057\n",
      "(Iteration 22701 / 22950) loss: 2.293269\n",
      "(Iteration 22801 / 22950) loss: 2.292724\n",
      "(Iteration 22901 / 22950) loss: 2.295920\n",
      "(Epoch 30 / 30) train acc: 0.297000; val_acc: 0.286000\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.001, 'num_epochs': 30, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 45930) loss: 2.302906\n",
      "(Epoch 0 / 30) train acc: 0.103000; val_acc: 0.096000\n",
      "(Iteration 101 / 45930) loss: 2.302958\n",
      "(Iteration 201 / 45930) loss: 2.302914\n",
      "(Iteration 301 / 45930) loss: 2.302833\n",
      "(Iteration 401 / 45930) loss: 2.302871\n",
      "(Iteration 501 / 45930) loss: 2.302895\n",
      "(Iteration 601 / 45930) loss: 2.302880\n",
      "(Iteration 701 / 45930) loss: 2.302957\n",
      "(Iteration 801 / 45930) loss: 2.303053\n",
      "(Iteration 901 / 45930) loss: 2.302695\n",
      "(Iteration 1001 / 45930) loss: 2.302659\n",
      "(Iteration 1101 / 45930) loss: 2.302343\n",
      "(Iteration 1201 / 45930) loss: 2.302377\n",
      "(Iteration 1301 / 45930) loss: 2.302493\n",
      "(Iteration 1401 / 45930) loss: 2.303111\n",
      "(Iteration 1501 / 45930) loss: 2.302696\n",
      "(Epoch 1 / 30) train acc: 0.153000; val_acc: 0.138000\n",
      "(Iteration 1601 / 45930) loss: 2.302548\n",
      "(Iteration 1701 / 45930) loss: 2.302669\n",
      "(Iteration 1801 / 45930) loss: 2.302601\n",
      "(Iteration 1901 / 45930) loss: 2.301654\n",
      "(Iteration 2001 / 45930) loss: 2.302773\n",
      "(Iteration 2101 / 45930) loss: 2.302654\n",
      "(Iteration 2201 / 45930) loss: 2.302819\n",
      "(Iteration 2301 / 45930) loss: 2.302757\n",
      "(Iteration 2401 / 45930) loss: 2.302731\n",
      "(Iteration 2501 / 45930) loss: 2.302185\n",
      "(Iteration 2601 / 45930) loss: 2.302389\n",
      "(Iteration 2701 / 45930) loss: 2.302688\n",
      "(Iteration 2801 / 45930) loss: 2.302540\n",
      "(Iteration 2901 / 45930) loss: 2.302169\n",
      "(Iteration 3001 / 45930) loss: 2.302042\n",
      "(Epoch 2 / 30) train acc: 0.213000; val_acc: 0.196000\n",
      "(Iteration 3101 / 45930) loss: 2.301974\n",
      "(Iteration 3201 / 45930) loss: 2.302516\n",
      "(Iteration 3301 / 45930) loss: 2.301876\n",
      "(Iteration 3401 / 45930) loss: 2.301939\n",
      "(Iteration 3501 / 45930) loss: 2.303251\n",
      "(Iteration 3601 / 45930) loss: 2.302056\n",
      "(Iteration 3701 / 45930) loss: 2.302499\n",
      "(Iteration 3801 / 45930) loss: 2.301495\n",
      "(Iteration 3901 / 45930) loss: 2.302510\n",
      "(Iteration 4001 / 45930) loss: 2.301512\n",
      "(Iteration 4101 / 45930) loss: 2.302618\n",
      "(Iteration 4201 / 45930) loss: 2.301223\n",
      "(Iteration 4301 / 45930) loss: 2.302461\n",
      "(Iteration 4401 / 45930) loss: 2.301742\n",
      "(Iteration 4501 / 45930) loss: 2.301208\n",
      "(Epoch 3 / 30) train acc: 0.135000; val_acc: 0.116000\n",
      "(Iteration 4601 / 45930) loss: 2.301827\n",
      "(Iteration 4701 / 45930) loss: 2.301218\n",
      "(Iteration 4801 / 45930) loss: 2.301323\n",
      "(Iteration 4901 / 45930) loss: 2.301250\n",
      "(Iteration 5001 / 45930) loss: 2.300863\n",
      "(Iteration 5101 / 45930) loss: 2.301497\n",
      "(Iteration 5201 / 45930) loss: 2.301678\n",
      "(Iteration 5301 / 45930) loss: 2.301735\n",
      "(Iteration 5401 / 45930) loss: 2.301422\n",
      "(Iteration 5501 / 45930) loss: 2.300991\n",
      "(Iteration 5601 / 45930) loss: 2.301221\n",
      "(Iteration 5701 / 45930) loss: 2.300636\n",
      "(Iteration 5801 / 45930) loss: 2.300173\n",
      "(Iteration 5901 / 45930) loss: 2.301125\n",
      "(Iteration 6001 / 45930) loss: 2.300017\n",
      "(Iteration 6101 / 45930) loss: 2.300237\n",
      "(Epoch 4 / 30) train acc: 0.214000; val_acc: 0.204000\n",
      "(Iteration 6201 / 45930) loss: 2.302119\n",
      "(Iteration 6301 / 45930) loss: 2.302064\n",
      "(Iteration 6401 / 45930) loss: 2.300186\n",
      "(Iteration 6501 / 45930) loss: 2.299449\n",
      "(Iteration 6601 / 45930) loss: 2.298697\n",
      "(Iteration 6701 / 45930) loss: 2.299371\n",
      "(Iteration 6801 / 45930) loss: 2.298441\n",
      "(Iteration 6901 / 45930) loss: 2.300560\n",
      "(Iteration 7001 / 45930) loss: 2.299735\n",
      "(Iteration 7101 / 45930) loss: 2.300754\n",
      "(Iteration 7201 / 45930) loss: 2.300510\n",
      "(Iteration 7301 / 45930) loss: 2.300152\n",
      "(Iteration 7401 / 45930) loss: 2.299712\n",
      "(Iteration 7501 / 45930) loss: 2.300615\n",
      "(Iteration 7601 / 45930) loss: 2.299055\n",
      "(Epoch 5 / 30) train acc: 0.231000; val_acc: 0.218000\n",
      "(Iteration 7701 / 45930) loss: 2.298407\n",
      "(Iteration 7801 / 45930) loss: 2.299253\n",
      "(Iteration 7901 / 45930) loss: 2.300104\n",
      "(Iteration 8001 / 45930) loss: 2.297398\n",
      "(Iteration 8101 / 45930) loss: 2.298073\n",
      "(Iteration 8201 / 45930) loss: 2.298315\n",
      "(Iteration 8301 / 45930) loss: 2.296952\n",
      "(Iteration 8401 / 45930) loss: 2.298118\n",
      "(Iteration 8501 / 45930) loss: 2.298462\n",
      "(Iteration 8601 / 45930) loss: 2.295750\n",
      "(Iteration 8701 / 45930) loss: 2.298675\n",
      "(Iteration 8801 / 45930) loss: 2.296513\n",
      "(Iteration 8901 / 45930) loss: 2.296984\n",
      "(Iteration 9001 / 45930) loss: 2.297862\n",
      "(Iteration 9101 / 45930) loss: 2.299722\n",
      "(Epoch 6 / 30) train acc: 0.228000; val_acc: 0.230000\n",
      "(Iteration 9201 / 45930) loss: 2.296777\n",
      "(Iteration 9301 / 45930) loss: 2.297692\n",
      "(Iteration 9401 / 45930) loss: 2.297217\n",
      "(Iteration 9501 / 45930) loss: 2.294965\n",
      "(Iteration 9601 / 45930) loss: 2.295360\n",
      "(Iteration 9701 / 45930) loss: 2.297894\n",
      "(Iteration 9801 / 45930) loss: 2.294700\n",
      "(Iteration 9901 / 45930) loss: 2.295380\n",
      "(Iteration 10001 / 45930) loss: 2.297617\n",
      "(Iteration 10101 / 45930) loss: 2.294915\n",
      "(Iteration 10201 / 45930) loss: 2.293390\n",
      "(Iteration 10301 / 45930) loss: 2.290540\n",
      "(Iteration 10401 / 45930) loss: 2.289247\n",
      "(Iteration 10501 / 45930) loss: 2.292555\n",
      "(Iteration 10601 / 45930) loss: 2.297514\n",
      "(Iteration 10701 / 45930) loss: 2.291068\n",
      "(Epoch 7 / 30) train acc: 0.261000; val_acc: 0.251000\n",
      "(Iteration 10801 / 45930) loss: 2.293304\n",
      "(Iteration 10901 / 45930) loss: 2.291586\n",
      "(Iteration 11001 / 45930) loss: 2.295054\n",
      "(Iteration 11101 / 45930) loss: 2.290201\n",
      "(Iteration 11201 / 45930) loss: 2.285615\n",
      "(Iteration 11301 / 45930) loss: 2.288918\n",
      "(Iteration 11401 / 45930) loss: 2.286777\n",
      "(Iteration 11501 / 45930) loss: 2.282334\n",
      "(Iteration 11601 / 45930) loss: 2.291174\n",
      "(Iteration 11701 / 45930) loss: 2.287846\n",
      "(Iteration 11801 / 45930) loss: 2.279816\n",
      "(Iteration 11901 / 45930) loss: 2.282143\n",
      "(Iteration 12001 / 45930) loss: 2.283055\n",
      "(Iteration 12101 / 45930) loss: 2.289612\n",
      "(Iteration 12201 / 45930) loss: 2.286921\n",
      "(Epoch 8 / 30) train acc: 0.291000; val_acc: 0.279000\n",
      "(Iteration 12301 / 45930) loss: 2.287945\n",
      "(Iteration 12401 / 45930) loss: 2.298749\n",
      "(Iteration 12501 / 45930) loss: 2.290893\n",
      "(Iteration 12601 / 45930) loss: 2.287930\n",
      "(Iteration 12701 / 45930) loss: 2.284106\n",
      "(Iteration 12801 / 45930) loss: 2.281873\n",
      "(Iteration 12901 / 45930) loss: 2.289794\n",
      "(Iteration 13001 / 45930) loss: 2.282659\n",
      "(Iteration 13101 / 45930) loss: 2.279802\n",
      "(Iteration 13201 / 45930) loss: 2.290841\n",
      "(Iteration 13301 / 45930) loss: 2.275051\n",
      "(Iteration 13401 / 45930) loss: 2.277499\n",
      "(Iteration 13501 / 45930) loss: 2.279232\n",
      "(Iteration 13601 / 45930) loss: 2.282091\n",
      "(Iteration 13701 / 45930) loss: 2.283395\n",
      "(Epoch 9 / 30) train acc: 0.304000; val_acc: 0.287000\n",
      "(Iteration 13801 / 45930) loss: 2.280089\n",
      "(Iteration 13901 / 45930) loss: 2.280718\n",
      "(Iteration 14001 / 45930) loss: 2.272243\n",
      "(Iteration 14101 / 45930) loss: 2.277794\n",
      "(Iteration 14201 / 45930) loss: 2.266638\n",
      "(Iteration 14301 / 45930) loss: 2.285154\n",
      "(Iteration 14401 / 45930) loss: 2.268736\n",
      "(Iteration 14501 / 45930) loss: 2.281697\n",
      "(Iteration 14601 / 45930) loss: 2.280845\n",
      "(Iteration 14701 / 45930) loss: 2.274107\n",
      "(Iteration 14801 / 45930) loss: 2.283081\n",
      "(Iteration 14901 / 45930) loss: 2.264743\n",
      "(Iteration 15001 / 45930) loss: 2.271756\n",
      "(Iteration 15101 / 45930) loss: 2.265304\n",
      "(Iteration 15201 / 45930) loss: 2.260551\n",
      "(Iteration 15301 / 45930) loss: 2.263849\n",
      "(Epoch 10 / 30) train acc: 0.249000; val_acc: 0.275000\n",
      "(Iteration 15401 / 45930) loss: 2.273356\n",
      "(Iteration 15501 / 45930) loss: 2.264765\n",
      "(Iteration 15601 / 45930) loss: 2.267720\n",
      "(Iteration 15701 / 45930) loss: 2.260205\n",
      "(Iteration 15801 / 45930) loss: 2.275937\n",
      "(Iteration 15901 / 45930) loss: 2.261791\n",
      "(Iteration 16001 / 45930) loss: 2.241342\n",
      "(Iteration 16101 / 45930) loss: 2.262897\n",
      "(Iteration 16201 / 45930) loss: 2.268441\n",
      "(Iteration 16301 / 45930) loss: 2.255413\n",
      "(Iteration 16401 / 45930) loss: 2.258412\n",
      "(Iteration 16501 / 45930) loss: 2.260192\n",
      "(Iteration 16601 / 45930) loss: 2.265415\n",
      "(Iteration 16701 / 45930) loss: 2.264189\n",
      "(Iteration 16801 / 45930) loss: 2.255507\n",
      "(Epoch 11 / 30) train acc: 0.247000; val_acc: 0.265000\n",
      "(Iteration 16901 / 45930) loss: 2.270825\n",
      "(Iteration 17001 / 45930) loss: 2.242061\n",
      "(Iteration 17101 / 45930) loss: 2.251740\n",
      "(Iteration 17201 / 45930) loss: 2.244933\n",
      "(Iteration 17301 / 45930) loss: 2.251451\n",
      "(Iteration 17401 / 45930) loss: 2.231620\n",
      "(Iteration 17501 / 45930) loss: 2.249884\n",
      "(Iteration 17601 / 45930) loss: 2.258284\n",
      "(Iteration 17701 / 45930) loss: 2.234884\n",
      "(Iteration 17801 / 45930) loss: 2.232967\n",
      "(Iteration 17901 / 45930) loss: 2.247814\n",
      "(Iteration 18001 / 45930) loss: 2.266656\n",
      "(Iteration 18101 / 45930) loss: 2.236975\n",
      "(Iteration 18201 / 45930) loss: 2.253233\n",
      "(Iteration 18301 / 45930) loss: 2.230766\n",
      "(Epoch 12 / 30) train acc: 0.268000; val_acc: 0.267000\n",
      "(Iteration 18401 / 45930) loss: 2.250963\n",
      "(Iteration 18501 / 45930) loss: 2.267353\n",
      "(Iteration 18601 / 45930) loss: 2.231237\n",
      "(Iteration 18701 / 45930) loss: 2.235037\n",
      "(Iteration 18801 / 45930) loss: 2.253658\n",
      "(Iteration 18901 / 45930) loss: 2.244455\n",
      "(Iteration 19001 / 45930) loss: 2.242443\n",
      "(Iteration 19101 / 45930) loss: 2.251126\n",
      "(Iteration 19201 / 45930) loss: 2.211099\n",
      "(Iteration 19301 / 45930) loss: 2.212891\n",
      "(Iteration 19401 / 45930) loss: 2.197901\n",
      "(Iteration 19501 / 45930) loss: 2.197172\n",
      "(Iteration 19601 / 45930) loss: 2.184554\n",
      "(Iteration 19701 / 45930) loss: 2.212753\n",
      "(Iteration 19801 / 45930) loss: 2.213112\n",
      "(Iteration 19901 / 45930) loss: 2.232741\n",
      "(Epoch 13 / 30) train acc: 0.285000; val_acc: 0.260000\n",
      "(Iteration 20001 / 45930) loss: 2.217598\n",
      "(Iteration 20101 / 45930) loss: 2.224169\n",
      "(Iteration 20201 / 45930) loss: 2.231741\n",
      "(Iteration 20301 / 45930) loss: 2.259965\n",
      "(Iteration 20401 / 45930) loss: 2.207180\n",
      "(Iteration 20501 / 45930) loss: 2.149638\n",
      "(Iteration 20601 / 45930) loss: 2.219450\n",
      "(Iteration 20701 / 45930) loss: 2.246334\n",
      "(Iteration 20801 / 45930) loss: 2.211795\n",
      "(Iteration 20901 / 45930) loss: 2.230379\n",
      "(Iteration 21001 / 45930) loss: 2.180421\n",
      "(Iteration 21101 / 45930) loss: 2.188357\n",
      "(Iteration 21201 / 45930) loss: 2.200371\n",
      "(Iteration 21301 / 45930) loss: 2.216036\n",
      "(Iteration 21401 / 45930) loss: 2.179423\n",
      "(Epoch 14 / 30) train acc: 0.243000; val_acc: 0.255000\n",
      "(Iteration 21501 / 45930) loss: 2.193417\n",
      "(Iteration 21601 / 45930) loss: 2.232548\n",
      "(Iteration 21701 / 45930) loss: 2.226592\n",
      "(Iteration 21801 / 45930) loss: 2.197436\n",
      "(Iteration 21901 / 45930) loss: 2.211239\n",
      "(Iteration 22001 / 45930) loss: 2.193713\n",
      "(Iteration 22101 / 45930) loss: 2.209906\n",
      "(Iteration 22201 / 45930) loss: 2.200247\n",
      "(Iteration 22301 / 45930) loss: 2.193535\n",
      "(Iteration 22401 / 45930) loss: 2.209903\n",
      "(Iteration 22501 / 45930) loss: 2.167833\n",
      "(Iteration 22601 / 45930) loss: 2.222997\n",
      "(Iteration 22701 / 45930) loss: 2.200584\n",
      "(Iteration 22801 / 45930) loss: 2.169673\n",
      "(Iteration 22901 / 45930) loss: 2.196127\n",
      "(Epoch 15 / 30) train acc: 0.227000; val_acc: 0.255000\n",
      "(Iteration 23001 / 45930) loss: 2.208297\n",
      "(Iteration 23101 / 45930) loss: 2.179258\n",
      "(Iteration 23201 / 45930) loss: 2.191687\n",
      "(Iteration 23301 / 45930) loss: 2.227920\n",
      "(Iteration 23401 / 45930) loss: 2.165465\n",
      "(Iteration 23501 / 45930) loss: 2.148357\n",
      "(Iteration 23601 / 45930) loss: 2.179535\n",
      "(Iteration 23701 / 45930) loss: 2.138750\n",
      "(Iteration 23801 / 45930) loss: 2.204697\n",
      "(Iteration 23901 / 45930) loss: 2.162273\n",
      "(Iteration 24001 / 45930) loss: 2.204480\n",
      "(Iteration 24101 / 45930) loss: 2.216925\n",
      "(Iteration 24201 / 45930) loss: 2.228408\n",
      "(Iteration 24301 / 45930) loss: 2.146013\n",
      "(Iteration 24401 / 45930) loss: 2.195724\n",
      "(Epoch 16 / 30) train acc: 0.239000; val_acc: 0.264000\n",
      "(Iteration 24501 / 45930) loss: 2.196347\n",
      "(Iteration 24601 / 45930) loss: 2.174806\n",
      "(Iteration 24701 / 45930) loss: 2.214351\n",
      "(Iteration 24801 / 45930) loss: 2.188230\n",
      "(Iteration 24901 / 45930) loss: 2.254077\n",
      "(Iteration 25001 / 45930) loss: 2.213413\n",
      "(Iteration 25101 / 45930) loss: 2.117247\n",
      "(Iteration 25201 / 45930) loss: 2.143500\n",
      "(Iteration 25301 / 45930) loss: 2.143912\n",
      "(Iteration 25401 / 45930) loss: 2.083450\n",
      "(Iteration 25501 / 45930) loss: 2.186136\n",
      "(Iteration 25601 / 45930) loss: 2.198212\n",
      "(Iteration 25701 / 45930) loss: 2.155799\n",
      "(Iteration 25801 / 45930) loss: 2.056425\n",
      "(Iteration 25901 / 45930) loss: 2.070124\n",
      "(Iteration 26001 / 45930) loss: 2.215528\n",
      "(Epoch 17 / 30) train acc: 0.252000; val_acc: 0.261000\n",
      "(Iteration 26101 / 45930) loss: 2.222917\n",
      "(Iteration 26201 / 45930) loss: 2.127707\n",
      "(Iteration 26301 / 45930) loss: 2.203610\n",
      "(Iteration 26401 / 45930) loss: 2.111159\n",
      "(Iteration 26501 / 45930) loss: 2.106955\n",
      "(Iteration 26601 / 45930) loss: 2.136604\n",
      "(Iteration 26701 / 45930) loss: 2.077405\n",
      "(Iteration 26801 / 45930) loss: 2.169190\n",
      "(Iteration 26901 / 45930) loss: 2.165962\n",
      "(Iteration 27001 / 45930) loss: 2.215124\n",
      "(Iteration 27101 / 45930) loss: 2.197403\n",
      "(Iteration 27201 / 45930) loss: 2.170454\n",
      "(Iteration 27301 / 45930) loss: 2.205999\n",
      "(Iteration 27401 / 45930) loss: 2.186370\n",
      "(Iteration 27501 / 45930) loss: 2.115208\n",
      "(Epoch 18 / 30) train acc: 0.241000; val_acc: 0.265000\n",
      "(Iteration 27601 / 45930) loss: 2.162164\n",
      "(Iteration 27701 / 45930) loss: 2.120855\n",
      "(Iteration 27801 / 45930) loss: 2.120261\n",
      "(Iteration 27901 / 45930) loss: 2.121398\n",
      "(Iteration 28001 / 45930) loss: 2.125990\n",
      "(Iteration 28101 / 45930) loss: 2.181052\n",
      "(Iteration 28201 / 45930) loss: 2.163960\n",
      "(Iteration 28301 / 45930) loss: 2.082948\n",
      "(Iteration 28401 / 45930) loss: 2.165813\n",
      "(Iteration 28501 / 45930) loss: 2.145105\n",
      "(Iteration 28601 / 45930) loss: 2.117103\n",
      "(Iteration 28701 / 45930) loss: 2.134027\n",
      "(Iteration 28801 / 45930) loss: 2.119784\n",
      "(Iteration 28901 / 45930) loss: 2.106963\n",
      "(Iteration 29001 / 45930) loss: 2.128373\n",
      "(Epoch 19 / 30) train acc: 0.251000; val_acc: 0.266000\n",
      "(Iteration 29101 / 45930) loss: 2.131139\n",
      "(Iteration 29201 / 45930) loss: 2.097075\n",
      "(Iteration 29301 / 45930) loss: 2.066696\n",
      "(Iteration 29401 / 45930) loss: 2.010830\n",
      "(Iteration 29501 / 45930) loss: 2.122500\n",
      "(Iteration 29601 / 45930) loss: 2.136260\n",
      "(Iteration 29701 / 45930) loss: 2.174385\n",
      "(Iteration 29801 / 45930) loss: 2.128271\n",
      "(Iteration 29901 / 45930) loss: 2.094707\n",
      "(Iteration 30001 / 45930) loss: 2.113070\n",
      "(Iteration 30101 / 45930) loss: 2.061188\n",
      "(Iteration 30201 / 45930) loss: 2.132390\n",
      "(Iteration 30301 / 45930) loss: 2.151198\n",
      "(Iteration 30401 / 45930) loss: 2.105752\n",
      "(Iteration 30501 / 45930) loss: 2.130190\n",
      "(Iteration 30601 / 45930) loss: 2.145270\n",
      "(Epoch 20 / 30) train acc: 0.241000; val_acc: 0.266000\n",
      "(Iteration 30701 / 45930) loss: 2.093880\n",
      "(Iteration 30801 / 45930) loss: 2.126945\n",
      "(Iteration 30901 / 45930) loss: 2.233292\n",
      "(Iteration 31001 / 45930) loss: 2.123737\n",
      "(Iteration 31101 / 45930) loss: 2.088234\n",
      "(Iteration 31201 / 45930) loss: 2.108748\n",
      "(Iteration 31301 / 45930) loss: 2.135214\n",
      "(Iteration 31401 / 45930) loss: 2.168438\n",
      "(Iteration 31501 / 45930) loss: 2.071265\n",
      "(Iteration 31601 / 45930) loss: 2.127869\n",
      "(Iteration 31701 / 45930) loss: 2.174787\n",
      "(Iteration 31801 / 45930) loss: 2.134804\n",
      "(Iteration 31901 / 45930) loss: 2.090778\n",
      "(Iteration 32001 / 45930) loss: 2.100433\n",
      "(Iteration 32101 / 45930) loss: 2.167327\n",
      "(Epoch 21 / 30) train acc: 0.267000; val_acc: 0.266000\n",
      "(Iteration 32201 / 45930) loss: 2.067164\n",
      "(Iteration 32301 / 45930) loss: 2.122702\n",
      "(Iteration 32401 / 45930) loss: 2.145982\n",
      "(Iteration 32501 / 45930) loss: 2.106819\n",
      "(Iteration 32601 / 45930) loss: 2.129636\n",
      "(Iteration 32701 / 45930) loss: 2.128847\n",
      "(Iteration 32801 / 45930) loss: 2.166797\n",
      "(Iteration 32901 / 45930) loss: 2.116494\n",
      "(Iteration 33001 / 45930) loss: 2.049469\n",
      "(Iteration 33101 / 45930) loss: 2.125042\n",
      "(Iteration 33201 / 45930) loss: 2.210728\n",
      "(Iteration 33301 / 45930) loss: 2.061464\n",
      "(Iteration 33401 / 45930) loss: 2.058218\n",
      "(Iteration 33501 / 45930) loss: 2.081375\n",
      "(Iteration 33601 / 45930) loss: 2.095760\n",
      "(Epoch 22 / 30) train acc: 0.258000; val_acc: 0.267000\n",
      "(Iteration 33701 / 45930) loss: 2.145056\n",
      "(Iteration 33801 / 45930) loss: 2.005103\n",
      "(Iteration 33901 / 45930) loss: 2.178679\n",
      "(Iteration 34001 / 45930) loss: 1.993909\n",
      "(Iteration 34101 / 45930) loss: 2.048098\n",
      "(Iteration 34201 / 45930) loss: 2.141953\n",
      "(Iteration 34301 / 45930) loss: 2.122922\n",
      "(Iteration 34401 / 45930) loss: 2.025276\n",
      "(Iteration 34501 / 45930) loss: 2.006242\n",
      "(Iteration 34601 / 45930) loss: 2.047169\n",
      "(Iteration 34701 / 45930) loss: 2.033700\n",
      "(Iteration 34801 / 45930) loss: 2.149267\n",
      "(Iteration 34901 / 45930) loss: 2.098501\n",
      "(Iteration 35001 / 45930) loss: 2.012362\n",
      "(Iteration 35101 / 45930) loss: 1.993726\n",
      "(Iteration 35201 / 45930) loss: 2.143645\n",
      "(Epoch 23 / 30) train acc: 0.272000; val_acc: 0.268000\n",
      "(Iteration 35301 / 45930) loss: 2.121169\n",
      "(Iteration 35401 / 45930) loss: 2.103403\n",
      "(Iteration 35501 / 45930) loss: 2.134312\n",
      "(Iteration 35601 / 45930) loss: 2.055764\n",
      "(Iteration 35701 / 45930) loss: 2.086844\n",
      "(Iteration 35801 / 45930) loss: 2.096664\n",
      "(Iteration 35901 / 45930) loss: 1.999421\n",
      "(Iteration 36001 / 45930) loss: 2.132729\n",
      "(Iteration 36101 / 45930) loss: 2.132681\n",
      "(Iteration 36201 / 45930) loss: 2.101320\n",
      "(Iteration 36301 / 45930) loss: 2.081329\n",
      "(Iteration 36401 / 45930) loss: 1.992287\n",
      "(Iteration 36501 / 45930) loss: 2.175262\n",
      "(Iteration 36601 / 45930) loss: 2.085043\n",
      "(Iteration 36701 / 45930) loss: 2.122085\n",
      "(Epoch 24 / 30) train acc: 0.251000; val_acc: 0.268000\n",
      "(Iteration 36801 / 45930) loss: 2.064256\n",
      "(Iteration 36901 / 45930) loss: 1.990861\n",
      "(Iteration 37001 / 45930) loss: 2.176443\n",
      "(Iteration 37101 / 45930) loss: 2.096693\n",
      "(Iteration 37201 / 45930) loss: 2.061170\n",
      "(Iteration 37301 / 45930) loss: 2.150513\n",
      "(Iteration 37401 / 45930) loss: 2.037775\n",
      "(Iteration 37501 / 45930) loss: 2.048200\n",
      "(Iteration 37601 / 45930) loss: 2.069874\n",
      "(Iteration 37701 / 45930) loss: 2.141173\n",
      "(Iteration 37801 / 45930) loss: 2.179360\n",
      "(Iteration 37901 / 45930) loss: 2.108526\n",
      "(Iteration 38001 / 45930) loss: 2.021973\n",
      "(Iteration 38101 / 45930) loss: 2.048942\n",
      "(Iteration 38201 / 45930) loss: 2.097389\n",
      "(Epoch 25 / 30) train acc: 0.265000; val_acc: 0.268000\n",
      "(Iteration 38301 / 45930) loss: 2.123577\n",
      "(Iteration 38401 / 45930) loss: 2.038127\n",
      "(Iteration 38501 / 45930) loss: 2.062012\n",
      "(Iteration 38601 / 45930) loss: 2.072587\n",
      "(Iteration 38701 / 45930) loss: 2.039018\n",
      "(Iteration 38801 / 45930) loss: 2.108614\n",
      "(Iteration 38901 / 45930) loss: 2.011630\n",
      "(Iteration 39001 / 45930) loss: 2.114710\n",
      "(Iteration 39101 / 45930) loss: 2.136625\n",
      "(Iteration 39201 / 45930) loss: 2.241249\n",
      "(Iteration 39301 / 45930) loss: 1.982020\n",
      "(Iteration 39401 / 45930) loss: 1.951278\n",
      "(Iteration 39501 / 45930) loss: 2.100999\n",
      "(Iteration 39601 / 45930) loss: 2.188174\n",
      "(Iteration 39701 / 45930) loss: 2.029027\n",
      "(Iteration 39801 / 45930) loss: 2.127639\n",
      "(Epoch 26 / 30) train acc: 0.268000; val_acc: 0.270000\n",
      "(Iteration 39901 / 45930) loss: 2.118500\n",
      "(Iteration 40001 / 45930) loss: 2.179657\n",
      "(Iteration 40101 / 45930) loss: 2.176838\n",
      "(Iteration 40201 / 45930) loss: 2.120521\n",
      "(Iteration 40301 / 45930) loss: 1.982851\n",
      "(Iteration 40401 / 45930) loss: 2.099609\n",
      "(Iteration 40501 / 45930) loss: 2.119617\n",
      "(Iteration 40601 / 45930) loss: 2.039234\n",
      "(Iteration 40701 / 45930) loss: 2.005419\n",
      "(Iteration 40801 / 45930) loss: 2.033144\n",
      "(Iteration 40901 / 45930) loss: 2.032493\n",
      "(Iteration 41001 / 45930) loss: 1.964006\n",
      "(Iteration 41101 / 45930) loss: 2.165641\n",
      "(Iteration 41201 / 45930) loss: 2.000505\n",
      "(Iteration 41301 / 45930) loss: 1.991453\n",
      "(Epoch 27 / 30) train acc: 0.273000; val_acc: 0.272000\n",
      "(Iteration 41401 / 45930) loss: 2.045164\n",
      "(Iteration 41501 / 45930) loss: 2.016946\n",
      "(Iteration 41601 / 45930) loss: 2.069064\n",
      "(Iteration 41701 / 45930) loss: 2.164832\n",
      "(Iteration 41801 / 45930) loss: 2.157097\n",
      "(Iteration 41901 / 45930) loss: 2.167468\n",
      "(Iteration 42001 / 45930) loss: 2.103033\n",
      "(Iteration 42101 / 45930) loss: 1.943530\n",
      "(Iteration 42201 / 45930) loss: 2.095572\n",
      "(Iteration 42301 / 45930) loss: 2.082905\n",
      "(Iteration 42401 / 45930) loss: 2.000688\n",
      "(Iteration 42501 / 45930) loss: 2.058846\n",
      "(Iteration 42601 / 45930) loss: 1.973275\n",
      "(Iteration 42701 / 45930) loss: 2.044079\n",
      "(Iteration 42801 / 45930) loss: 2.091997\n",
      "(Epoch 28 / 30) train acc: 0.251000; val_acc: 0.272000\n",
      "(Iteration 42901 / 45930) loss: 2.071200\n",
      "(Iteration 43001 / 45930) loss: 2.024523\n",
      "(Iteration 43101 / 45930) loss: 2.114732\n",
      "(Iteration 43201 / 45930) loss: 1.956064\n",
      "(Iteration 43301 / 45930) loss: 1.987616\n",
      "(Iteration 43401 / 45930) loss: 2.056533\n",
      "(Iteration 43501 / 45930) loss: 2.074234\n",
      "(Iteration 43601 / 45930) loss: 2.137498\n",
      "(Iteration 43701 / 45930) loss: 2.045654\n",
      "(Iteration 43801 / 45930) loss: 1.985419\n",
      "(Iteration 43901 / 45930) loss: 2.113319\n",
      "(Iteration 44001 / 45930) loss: 2.274215\n",
      "(Iteration 44101 / 45930) loss: 2.160570\n",
      "(Iteration 44201 / 45930) loss: 2.088775\n",
      "(Iteration 44301 / 45930) loss: 2.037696\n",
      "(Epoch 29 / 30) train acc: 0.288000; val_acc: 0.271000\n",
      "(Iteration 44401 / 45930) loss: 2.023609\n",
      "(Iteration 44501 / 45930) loss: 2.163306\n",
      "(Iteration 44601 / 45930) loss: 2.189546\n",
      "(Iteration 44701 / 45930) loss: 2.028757\n",
      "(Iteration 44801 / 45930) loss: 2.094881\n",
      "(Iteration 44901 / 45930) loss: 2.188087\n",
      "(Iteration 45001 / 45930) loss: 2.048448\n",
      "(Iteration 45101 / 45930) loss: 2.007841\n",
      "(Iteration 45201 / 45930) loss: 2.095157\n",
      "(Iteration 45301 / 45930) loss: 2.137812\n",
      "(Iteration 45401 / 45930) loss: 2.117420\n",
      "(Iteration 45501 / 45930) loss: 2.130147\n",
      "(Iteration 45601 / 45930) loss: 2.139294\n",
      "(Iteration 45701 / 45930) loss: 2.068656\n",
      "(Iteration 45801 / 45930) loss: 2.093026\n",
      "(Iteration 45901 / 45930) loss: 2.085883\n",
      "(Epoch 30 / 30) train acc: 0.263000; val_acc: 0.277000\n",
      "New best model found with validation accuracy: 0.2456\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.001, 'num_epochs': 40, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 30600) loss: 2.305912\n",
      "(Epoch 0 / 40) train acc: 0.076000; val_acc: 0.090000\n",
      "(Iteration 101 / 30600) loss: 2.305745\n",
      "(Iteration 201 / 30600) loss: 2.305701\n",
      "(Iteration 301 / 30600) loss: 2.305751\n",
      "(Iteration 401 / 30600) loss: 2.305458\n",
      "(Iteration 501 / 30600) loss: 2.305240\n",
      "(Iteration 601 / 30600) loss: 2.305394\n",
      "(Iteration 701 / 30600) loss: 2.305305\n",
      "(Epoch 1 / 40) train acc: 0.111000; val_acc: 0.087000\n",
      "(Iteration 801 / 30600) loss: 2.305118\n",
      "(Iteration 901 / 30600) loss: 2.305093\n",
      "(Iteration 1001 / 30600) loss: 2.305149\n",
      "(Iteration 1101 / 30600) loss: 2.305043\n",
      "(Iteration 1201 / 30600) loss: 2.305013\n",
      "(Iteration 1301 / 30600) loss: 2.305188\n",
      "(Iteration 1401 / 30600) loss: 2.304700\n",
      "(Iteration 1501 / 30600) loss: 2.305059\n",
      "(Epoch 2 / 40) train acc: 0.085000; val_acc: 0.087000\n",
      "(Iteration 1601 / 30600) loss: 2.304937\n",
      "(Iteration 1701 / 30600) loss: 2.304853\n",
      "(Iteration 1801 / 30600) loss: 2.304761\n",
      "(Iteration 1901 / 30600) loss: 2.304964\n",
      "(Iteration 2001 / 30600) loss: 2.304892\n",
      "(Iteration 2101 / 30600) loss: 2.304826\n",
      "(Iteration 2201 / 30600) loss: 2.304648\n",
      "(Epoch 3 / 40) train acc: 0.156000; val_acc: 0.112000\n",
      "(Iteration 2301 / 30600) loss: 2.304595\n",
      "(Iteration 2401 / 30600) loss: 2.304796\n",
      "(Iteration 2501 / 30600) loss: 2.304202\n",
      "(Iteration 2601 / 30600) loss: 2.304524\n",
      "(Iteration 2701 / 30600) loss: 2.304824\n",
      "(Iteration 2801 / 30600) loss: 2.304178\n",
      "(Iteration 2901 / 30600) loss: 2.304080\n",
      "(Iteration 3001 / 30600) loss: 2.304161\n",
      "(Epoch 4 / 40) train acc: 0.103000; val_acc: 0.095000\n",
      "(Iteration 3101 / 30600) loss: 2.304331\n",
      "(Iteration 3201 / 30600) loss: 2.304200\n",
      "(Iteration 3301 / 30600) loss: 2.304072\n",
      "(Iteration 3401 / 30600) loss: 2.303712\n",
      "(Iteration 3501 / 30600) loss: 2.303771\n",
      "(Iteration 3601 / 30600) loss: 2.303839\n",
      "(Iteration 3701 / 30600) loss: 2.304142\n",
      "(Iteration 3801 / 30600) loss: 2.303825\n",
      "(Epoch 5 / 40) train acc: 0.200000; val_acc: 0.182000\n",
      "(Iteration 3901 / 30600) loss: 2.304180\n",
      "(Iteration 4001 / 30600) loss: 2.303651\n",
      "(Iteration 4101 / 30600) loss: 2.304106\n",
      "(Iteration 4201 / 30600) loss: 2.304114\n",
      "(Iteration 4301 / 30600) loss: 2.304251\n",
      "(Iteration 4401 / 30600) loss: 2.303923\n",
      "(Iteration 4501 / 30600) loss: 2.304008\n",
      "(Epoch 6 / 40) train acc: 0.160000; val_acc: 0.107000\n",
      "(Iteration 4601 / 30600) loss: 2.303346\n",
      "(Iteration 4701 / 30600) loss: 2.304019\n",
      "(Iteration 4801 / 30600) loss: 2.303540\n",
      "(Iteration 4901 / 30600) loss: 2.302816\n",
      "(Iteration 5001 / 30600) loss: 2.303348\n",
      "(Iteration 5101 / 30600) loss: 2.303753\n",
      "(Iteration 5201 / 30600) loss: 2.303378\n",
      "(Iteration 5301 / 30600) loss: 2.303877\n",
      "(Epoch 7 / 40) train acc: 0.110000; val_acc: 0.087000\n",
      "(Iteration 5401 / 30600) loss: 2.303541\n",
      "(Iteration 5501 / 30600) loss: 2.303653\n",
      "(Iteration 5601 / 30600) loss: 2.303492\n",
      "(Iteration 5701 / 30600) loss: 2.304389\n",
      "(Iteration 5801 / 30600) loss: 2.303529\n",
      "(Iteration 5901 / 30600) loss: 2.303814\n",
      "(Iteration 6001 / 30600) loss: 2.303278\n",
      "(Iteration 6101 / 30600) loss: 2.303523\n",
      "(Epoch 8 / 40) train acc: 0.106000; val_acc: 0.085000\n",
      "(Iteration 6201 / 30600) loss: 2.303688\n",
      "(Iteration 6301 / 30600) loss: 2.303553\n",
      "(Iteration 6401 / 30600) loss: 2.303700\n",
      "(Iteration 6501 / 30600) loss: 2.303213\n",
      "(Iteration 6601 / 30600) loss: 2.303379\n",
      "(Iteration 6701 / 30600) loss: 2.303794\n",
      "(Iteration 6801 / 30600) loss: 2.304236\n",
      "(Epoch 9 / 40) train acc: 0.143000; val_acc: 0.104000\n",
      "(Iteration 6901 / 30600) loss: 2.302988\n",
      "(Iteration 7001 / 30600) loss: 2.303501\n",
      "(Iteration 7101 / 30600) loss: 2.303788\n",
      "(Iteration 7201 / 30600) loss: 2.303353\n",
      "(Iteration 7301 / 30600) loss: 2.303995\n",
      "(Iteration 7401 / 30600) loss: 2.303291\n",
      "(Iteration 7501 / 30600) loss: 2.304033\n",
      "(Iteration 7601 / 30600) loss: 2.302847\n",
      "(Epoch 10 / 40) train acc: 0.138000; val_acc: 0.119000\n",
      "(Iteration 7701 / 30600) loss: 2.303292\n",
      "(Iteration 7801 / 30600) loss: 2.302834\n",
      "(Iteration 7901 / 30600) loss: 2.303537\n",
      "(Iteration 8001 / 30600) loss: 2.302879\n",
      "(Iteration 8101 / 30600) loss: 2.303304\n",
      "(Iteration 8201 / 30600) loss: 2.303808\n",
      "(Iteration 8301 / 30600) loss: 2.303349\n",
      "(Iteration 8401 / 30600) loss: 2.303372\n",
      "(Epoch 11 / 40) train acc: 0.151000; val_acc: 0.129000\n",
      "(Iteration 8501 / 30600) loss: 2.302763\n",
      "(Iteration 8601 / 30600) loss: 2.303123\n",
      "(Iteration 8701 / 30600) loss: 2.302357\n",
      "(Iteration 8801 / 30600) loss: 2.303282\n",
      "(Iteration 8901 / 30600) loss: 2.302578\n",
      "(Iteration 9001 / 30600) loss: 2.302790\n",
      "(Iteration 9101 / 30600) loss: 2.303152\n",
      "(Epoch 12 / 40) train acc: 0.155000; val_acc: 0.131000\n",
      "(Iteration 9201 / 30600) loss: 2.302990\n",
      "(Iteration 9301 / 30600) loss: 2.303003\n",
      "(Iteration 9401 / 30600) loss: 2.303912\n",
      "(Iteration 9501 / 30600) loss: 2.303454\n",
      "(Iteration 9601 / 30600) loss: 2.302438\n",
      "(Iteration 9701 / 30600) loss: 2.302779\n",
      "(Iteration 9801 / 30600) loss: 2.302896\n",
      "(Iteration 9901 / 30600) loss: 2.303164\n",
      "(Epoch 13 / 40) train acc: 0.158000; val_acc: 0.122000\n",
      "(Iteration 10001 / 30600) loss: 2.303249\n",
      "(Iteration 10101 / 30600) loss: 2.302340\n",
      "(Iteration 10201 / 30600) loss: 2.302293\n",
      "(Iteration 10301 / 30600) loss: 2.302913\n",
      "(Iteration 10401 / 30600) loss: 2.302025\n",
      "(Iteration 10501 / 30600) loss: 2.302434\n",
      "(Iteration 10601 / 30600) loss: 2.303531\n",
      "(Iteration 10701 / 30600) loss: 2.303217\n",
      "(Epoch 14 / 40) train acc: 0.172000; val_acc: 0.131000\n",
      "(Iteration 10801 / 30600) loss: 2.302514\n",
      "(Iteration 10901 / 30600) loss: 2.303030\n",
      "(Iteration 11001 / 30600) loss: 2.303304\n",
      "(Iteration 11101 / 30600) loss: 2.302924\n",
      "(Iteration 11201 / 30600) loss: 2.301863\n",
      "(Iteration 11301 / 30600) loss: 2.302749\n",
      "(Iteration 11401 / 30600) loss: 2.301879\n",
      "(Epoch 15 / 40) train acc: 0.155000; val_acc: 0.129000\n",
      "(Iteration 11501 / 30600) loss: 2.302451\n",
      "(Iteration 11601 / 30600) loss: 2.302246\n",
      "(Iteration 11701 / 30600) loss: 2.302312\n",
      "(Iteration 11801 / 30600) loss: 2.302332\n",
      "(Iteration 11901 / 30600) loss: 2.302363\n",
      "(Iteration 12001 / 30600) loss: 2.302461\n",
      "(Iteration 12101 / 30600) loss: 2.302741\n",
      "(Iteration 12201 / 30600) loss: 2.301762\n",
      "(Epoch 16 / 40) train acc: 0.150000; val_acc: 0.129000\n",
      "(Iteration 12301 / 30600) loss: 2.303416\n",
      "(Iteration 12401 / 30600) loss: 2.302696\n",
      "(Iteration 12501 / 30600) loss: 2.303046\n",
      "(Iteration 12601 / 30600) loss: 2.302874\n",
      "(Iteration 12701 / 30600) loss: 2.302055\n",
      "(Iteration 12801 / 30600) loss: 2.302656\n",
      "(Iteration 12901 / 30600) loss: 2.302865\n",
      "(Iteration 13001 / 30600) loss: 2.303229\n",
      "(Epoch 17 / 40) train acc: 0.173000; val_acc: 0.129000\n",
      "(Iteration 13101 / 30600) loss: 2.302567\n",
      "(Iteration 13201 / 30600) loss: 2.303071\n",
      "(Iteration 13301 / 30600) loss: 2.302370\n",
      "(Iteration 13401 / 30600) loss: 2.302697\n",
      "(Iteration 13501 / 30600) loss: 2.302100\n",
      "(Iteration 13601 / 30600) loss: 2.301990\n",
      "(Iteration 13701 / 30600) loss: 2.301604\n",
      "(Epoch 18 / 40) train acc: 0.168000; val_acc: 0.130000\n",
      "(Iteration 13801 / 30600) loss: 2.303634\n",
      "(Iteration 13901 / 30600) loss: 2.301653\n",
      "(Iteration 14001 / 30600) loss: 2.302183\n",
      "(Iteration 14101 / 30600) loss: 2.302324\n",
      "(Iteration 14201 / 30600) loss: 2.301598\n",
      "(Iteration 14301 / 30600) loss: 2.301522\n",
      "(Iteration 14401 / 30600) loss: 2.302096\n",
      "(Iteration 14501 / 30600) loss: 2.302692\n",
      "(Epoch 19 / 40) train acc: 0.176000; val_acc: 0.128000\n",
      "(Iteration 14601 / 30600) loss: 2.303197\n",
      "(Iteration 14701 / 30600) loss: 2.302253\n",
      "(Iteration 14801 / 30600) loss: 2.302034\n",
      "(Iteration 14901 / 30600) loss: 2.302491\n",
      "(Iteration 15001 / 30600) loss: 2.302536\n",
      "(Iteration 15101 / 30600) loss: 2.302352\n",
      "(Iteration 15201 / 30600) loss: 2.302891\n",
      "(Epoch 20 / 40) train acc: 0.166000; val_acc: 0.129000\n",
      "(Iteration 15301 / 30600) loss: 2.302232\n",
      "(Iteration 15401 / 30600) loss: 2.301607\n",
      "(Iteration 15501 / 30600) loss: 2.301635\n",
      "(Iteration 15601 / 30600) loss: 2.303085\n",
      "(Iteration 15701 / 30600) loss: 2.302674\n",
      "(Iteration 15801 / 30600) loss: 2.302510\n",
      "(Iteration 15901 / 30600) loss: 2.302697\n",
      "(Iteration 16001 / 30600) loss: 2.301360\n",
      "(Epoch 21 / 40) train acc: 0.148000; val_acc: 0.128000\n",
      "(Iteration 16101 / 30600) loss: 2.302166\n",
      "(Iteration 16201 / 30600) loss: 2.303153\n",
      "(Iteration 16301 / 30600) loss: 2.302748\n",
      "(Iteration 16401 / 30600) loss: 2.302010\n",
      "(Iteration 16501 / 30600) loss: 2.302323\n",
      "(Iteration 16601 / 30600) loss: 2.301816\n",
      "(Iteration 16701 / 30600) loss: 2.302036\n",
      "(Iteration 16801 / 30600) loss: 2.301654\n",
      "(Epoch 22 / 40) train acc: 0.171000; val_acc: 0.130000\n",
      "(Iteration 16901 / 30600) loss: 2.302047\n",
      "(Iteration 17001 / 30600) loss: 2.302725\n",
      "(Iteration 17101 / 30600) loss: 2.302792\n",
      "(Iteration 17201 / 30600) loss: 2.302299\n",
      "(Iteration 17301 / 30600) loss: 2.301227\n",
      "(Iteration 17401 / 30600) loss: 2.302835\n",
      "(Iteration 17501 / 30600) loss: 2.302668\n",
      "(Epoch 23 / 40) train acc: 0.171000; val_acc: 0.130000\n",
      "(Iteration 17601 / 30600) loss: 2.302274\n",
      "(Iteration 17701 / 30600) loss: 2.302863\n",
      "(Iteration 17801 / 30600) loss: 2.301632\n",
      "(Iteration 17901 / 30600) loss: 2.302193\n",
      "(Iteration 18001 / 30600) loss: 2.301869\n",
      "(Iteration 18101 / 30600) loss: 2.301951\n",
      "(Iteration 18201 / 30600) loss: 2.301970\n",
      "(Iteration 18301 / 30600) loss: 2.302194\n",
      "(Epoch 24 / 40) train acc: 0.165000; val_acc: 0.131000\n",
      "(Iteration 18401 / 30600) loss: 2.302510\n",
      "(Iteration 18501 / 30600) loss: 2.302123\n",
      "(Iteration 18601 / 30600) loss: 2.302197\n",
      "(Iteration 18701 / 30600) loss: 2.301554\n",
      "(Iteration 18801 / 30600) loss: 2.301062\n",
      "(Iteration 18901 / 30600) loss: 2.301938\n",
      "(Iteration 19001 / 30600) loss: 2.301797\n",
      "(Iteration 19101 / 30600) loss: 2.301766\n",
      "(Epoch 25 / 40) train acc: 0.153000; val_acc: 0.132000\n",
      "(Iteration 19201 / 30600) loss: 2.302152\n",
      "(Iteration 19301 / 30600) loss: 2.302285\n",
      "(Iteration 19401 / 30600) loss: 2.301018\n",
      "(Iteration 19501 / 30600) loss: 2.300935\n",
      "(Iteration 19601 / 30600) loss: 2.301782\n",
      "(Iteration 19701 / 30600) loss: 2.301091\n",
      "(Iteration 19801 / 30600) loss: 2.301423\n",
      "(Epoch 26 / 40) train acc: 0.160000; val_acc: 0.131000\n",
      "(Iteration 19901 / 30600) loss: 2.302250\n",
      "(Iteration 20001 / 30600) loss: 2.302127\n",
      "(Iteration 20101 / 30600) loss: 2.301506\n",
      "(Iteration 20201 / 30600) loss: 2.301948\n",
      "(Iteration 20301 / 30600) loss: 2.302880\n",
      "(Iteration 20401 / 30600) loss: 2.301836\n",
      "(Iteration 20501 / 30600) loss: 2.300897\n",
      "(Iteration 20601 / 30600) loss: 2.301997\n",
      "(Epoch 27 / 40) train acc: 0.181000; val_acc: 0.133000\n",
      "(Iteration 20701 / 30600) loss: 2.301686\n",
      "(Iteration 20801 / 30600) loss: 2.302025\n",
      "(Iteration 20901 / 30600) loss: 2.301742\n",
      "(Iteration 21001 / 30600) loss: 2.301298\n",
      "(Iteration 21101 / 30600) loss: 2.301358\n",
      "(Iteration 21201 / 30600) loss: 2.301884\n",
      "(Iteration 21301 / 30600) loss: 2.301200\n",
      "(Iteration 21401 / 30600) loss: 2.300766\n",
      "(Epoch 28 / 40) train acc: 0.171000; val_acc: 0.132000\n",
      "(Iteration 21501 / 30600) loss: 2.301195\n",
      "(Iteration 21601 / 30600) loss: 2.301436\n",
      "(Iteration 21701 / 30600) loss: 2.301149\n",
      "(Iteration 21801 / 30600) loss: 2.301569\n",
      "(Iteration 21901 / 30600) loss: 2.302652\n",
      "(Iteration 22001 / 30600) loss: 2.301938\n",
      "(Iteration 22101 / 30600) loss: 2.302093\n",
      "(Epoch 29 / 40) train acc: 0.161000; val_acc: 0.132000\n",
      "(Iteration 22201 / 30600) loss: 2.302010\n",
      "(Iteration 22301 / 30600) loss: 2.301181\n",
      "(Iteration 22401 / 30600) loss: 2.300867\n",
      "(Iteration 22501 / 30600) loss: 2.302144\n",
      "(Iteration 22601 / 30600) loss: 2.301516\n",
      "(Iteration 22701 / 30600) loss: 2.302123\n",
      "(Iteration 22801 / 30600) loss: 2.301584\n",
      "(Iteration 22901 / 30600) loss: 2.301693\n",
      "(Epoch 30 / 40) train acc: 0.167000; val_acc: 0.133000\n",
      "(Iteration 23001 / 30600) loss: 2.302371\n",
      "(Iteration 23101 / 30600) loss: 2.301729\n",
      "(Iteration 23201 / 30600) loss: 2.301750\n",
      "(Iteration 23301 / 30600) loss: 2.301348\n",
      "(Iteration 23401 / 30600) loss: 2.301049\n",
      "(Iteration 23501 / 30600) loss: 2.302519\n",
      "(Iteration 23601 / 30600) loss: 2.302045\n",
      "(Iteration 23701 / 30600) loss: 2.302270\n",
      "(Epoch 31 / 40) train acc: 0.170000; val_acc: 0.133000\n",
      "(Iteration 23801 / 30600) loss: 2.301866\n",
      "(Iteration 23901 / 30600) loss: 2.301725\n",
      "(Iteration 24001 / 30600) loss: 2.302052\n",
      "(Iteration 24101 / 30600) loss: 2.301726\n",
      "(Iteration 24201 / 30600) loss: 2.302815\n",
      "(Iteration 24301 / 30600) loss: 2.301730\n",
      "(Iteration 24401 / 30600) loss: 2.301186\n",
      "(Epoch 32 / 40) train acc: 0.170000; val_acc: 0.133000\n",
      "(Iteration 24501 / 30600) loss: 2.303368\n",
      "(Iteration 24601 / 30600) loss: 2.301868\n",
      "(Iteration 24701 / 30600) loss: 2.301779\n",
      "(Iteration 24801 / 30600) loss: 2.301797\n",
      "(Iteration 24901 / 30600) loss: 2.302460\n",
      "(Iteration 25001 / 30600) loss: 2.302279\n",
      "(Iteration 25101 / 30600) loss: 2.301159\n",
      "(Iteration 25201 / 30600) loss: 2.301566\n",
      "(Epoch 33 / 40) train acc: 0.169000; val_acc: 0.133000\n",
      "(Iteration 25301 / 30600) loss: 2.302246\n",
      "(Iteration 25401 / 30600) loss: 2.302922\n",
      "(Iteration 25501 / 30600) loss: 2.301197\n",
      "(Iteration 25601 / 30600) loss: 2.302446\n",
      "(Iteration 25701 / 30600) loss: 2.302400\n",
      "(Iteration 25801 / 30600) loss: 2.301063\n",
      "(Iteration 25901 / 30600) loss: 2.302471\n",
      "(Iteration 26001 / 30600) loss: 2.300747\n",
      "(Epoch 34 / 40) train acc: 0.157000; val_acc: 0.133000\n",
      "(Iteration 26101 / 30600) loss: 2.301180\n",
      "(Iteration 26201 / 30600) loss: 2.300860\n",
      "(Iteration 26301 / 30600) loss: 2.301051\n",
      "(Iteration 26401 / 30600) loss: 2.302214\n",
      "(Iteration 26501 / 30600) loss: 2.301568\n",
      "(Iteration 26601 / 30600) loss: 2.301288\n",
      "(Iteration 26701 / 30600) loss: 2.301462\n",
      "(Epoch 35 / 40) train acc: 0.158000; val_acc: 0.133000\n",
      "(Iteration 26801 / 30600) loss: 2.301261\n",
      "(Iteration 26901 / 30600) loss: 2.302113\n",
      "(Iteration 27001 / 30600) loss: 2.301574\n",
      "(Iteration 27101 / 30600) loss: 2.301341\n",
      "(Iteration 27201 / 30600) loss: 2.301113\n",
      "(Iteration 27301 / 30600) loss: 2.301760\n",
      "(Iteration 27401 / 30600) loss: 2.302683\n",
      "(Iteration 27501 / 30600) loss: 2.301568\n",
      "(Epoch 36 / 40) train acc: 0.154000; val_acc: 0.133000\n",
      "(Iteration 27601 / 30600) loss: 2.301763\n",
      "(Iteration 27701 / 30600) loss: 2.301435\n",
      "(Iteration 27801 / 30600) loss: 2.300585\n",
      "(Iteration 27901 / 30600) loss: 2.301847\n",
      "(Iteration 28001 / 30600) loss: 2.301003\n",
      "(Iteration 28101 / 30600) loss: 2.301395\n",
      "(Iteration 28201 / 30600) loss: 2.302837\n",
      "(Iteration 28301 / 30600) loss: 2.302177\n",
      "(Epoch 37 / 40) train acc: 0.171000; val_acc: 0.134000\n",
      "(Iteration 28401 / 30600) loss: 2.301309\n",
      "(Iteration 28501 / 30600) loss: 2.301831\n",
      "(Iteration 28601 / 30600) loss: 2.301708\n",
      "(Iteration 28701 / 30600) loss: 2.301156\n",
      "(Iteration 28801 / 30600) loss: 2.301556\n",
      "(Iteration 28901 / 30600) loss: 2.301252\n",
      "(Iteration 29001 / 30600) loss: 2.301100\n",
      "(Epoch 38 / 40) train acc: 0.152000; val_acc: 0.134000\n",
      "(Iteration 29101 / 30600) loss: 2.300643\n",
      "(Iteration 29201 / 30600) loss: 2.301530\n",
      "(Iteration 29301 / 30600) loss: 2.301583\n",
      "(Iteration 29401 / 30600) loss: 2.300589\n",
      "(Iteration 29501 / 30600) loss: 2.301351\n",
      "(Iteration 29601 / 30600) loss: 2.301982\n",
      "(Iteration 29701 / 30600) loss: 2.302395\n",
      "(Iteration 29801 / 30600) loss: 2.301037\n",
      "(Epoch 39 / 40) train acc: 0.144000; val_acc: 0.134000\n",
      "(Iteration 29901 / 30600) loss: 2.301587\n",
      "(Iteration 30001 / 30600) loss: 2.302649\n",
      "(Iteration 30101 / 30600) loss: 2.301689\n",
      "(Iteration 30201 / 30600) loss: 2.301596\n",
      "(Iteration 30301 / 30600) loss: 2.302802\n",
      "(Iteration 30401 / 30600) loss: 2.301024\n",
      "(Iteration 30501 / 30600) loss: 2.301467\n",
      "(Epoch 40 / 40) train acc: 0.155000; val_acc: 0.134000\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.001, 'num_epochs': 40, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 61240) loss: 2.305873\n",
      "(Epoch 0 / 40) train acc: 0.117000; val_acc: 0.091000\n",
      "(Iteration 101 / 61240) loss: 2.305658\n",
      "(Iteration 201 / 61240) loss: 2.305835\n",
      "(Iteration 301 / 61240) loss: 2.305634\n",
      "(Iteration 401 / 61240) loss: 2.305515\n",
      "(Iteration 501 / 61240) loss: 2.305335\n",
      "(Iteration 601 / 61240) loss: 2.305491\n",
      "(Iteration 701 / 61240) loss: 2.304993\n",
      "(Iteration 801 / 61240) loss: 2.305391\n",
      "(Iteration 901 / 61240) loss: 2.305390\n",
      "(Iteration 1001 / 61240) loss: 2.305080\n",
      "(Iteration 1101 / 61240) loss: 2.305054\n",
      "(Iteration 1201 / 61240) loss: 2.305422\n",
      "(Iteration 1301 / 61240) loss: 2.305139\n",
      "(Iteration 1401 / 61240) loss: 2.304417\n",
      "(Iteration 1501 / 61240) loss: 2.304678\n",
      "(Epoch 1 / 40) train acc: 0.127000; val_acc: 0.120000\n",
      "(Iteration 1601 / 61240) loss: 2.304906\n",
      "(Iteration 1701 / 61240) loss: 2.304760\n",
      "(Iteration 1801 / 61240) loss: 2.304576\n",
      "(Iteration 1901 / 61240) loss: 2.304927\n",
      "(Iteration 2001 / 61240) loss: 2.304908\n",
      "(Iteration 2101 / 61240) loss: 2.304659\n",
      "(Iteration 2201 / 61240) loss: 2.304570\n",
      "(Iteration 2301 / 61240) loss: 2.304790\n",
      "(Iteration 2401 / 61240) loss: 2.304621\n",
      "(Iteration 2501 / 61240) loss: 2.304611\n",
      "(Iteration 2601 / 61240) loss: 2.304923\n",
      "(Iteration 2701 / 61240) loss: 2.303782\n",
      "(Iteration 2801 / 61240) loss: 2.304202\n",
      "(Iteration 2901 / 61240) loss: 2.304852\n",
      "(Iteration 3001 / 61240) loss: 2.304460\n",
      "(Epoch 2 / 40) train acc: 0.105000; val_acc: 0.084000\n",
      "(Iteration 3101 / 61240) loss: 2.303654\n",
      "(Iteration 3201 / 61240) loss: 2.303895\n",
      "(Iteration 3301 / 61240) loss: 2.303763\n",
      "(Iteration 3401 / 61240) loss: 2.303868\n",
      "(Iteration 3501 / 61240) loss: 2.303740\n",
      "(Iteration 3601 / 61240) loss: 2.304411\n",
      "(Iteration 3701 / 61240) loss: 2.304628\n",
      "(Iteration 3801 / 61240) loss: 2.304089\n",
      "(Iteration 3901 / 61240) loss: 2.302746\n",
      "(Iteration 4001 / 61240) loss: 2.303583\n",
      "(Iteration 4101 / 61240) loss: 2.303129\n",
      "(Iteration 4201 / 61240) loss: 2.304290\n",
      "(Iteration 4301 / 61240) loss: 2.303821\n",
      "(Iteration 4401 / 61240) loss: 2.303516\n",
      "(Iteration 4501 / 61240) loss: 2.303734\n",
      "(Epoch 3 / 40) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 4601 / 61240) loss: 2.303921\n",
      "(Iteration 4701 / 61240) loss: 2.304046\n",
      "(Iteration 4801 / 61240) loss: 2.303012\n",
      "(Iteration 4901 / 61240) loss: 2.303281\n",
      "(Iteration 5001 / 61240) loss: 2.303391\n",
      "(Iteration 5101 / 61240) loss: 2.304131\n",
      "(Iteration 5201 / 61240) loss: 2.303242\n",
      "(Iteration 5301 / 61240) loss: 2.303032\n",
      "(Iteration 5401 / 61240) loss: 2.302961\n",
      "(Iteration 5501 / 61240) loss: 2.303241\n",
      "(Iteration 5601 / 61240) loss: 2.303092\n",
      "(Iteration 5701 / 61240) loss: 2.303942\n",
      "(Iteration 5801 / 61240) loss: 2.302645\n",
      "(Iteration 5901 / 61240) loss: 2.303046\n",
      "(Iteration 6001 / 61240) loss: 2.302784\n",
      "(Iteration 6101 / 61240) loss: 2.303013\n",
      "(Epoch 4 / 40) train acc: 0.107000; val_acc: 0.086000\n",
      "(Iteration 6201 / 61240) loss: 2.302596\n",
      "(Iteration 6301 / 61240) loss: 2.302289\n",
      "(Iteration 6401 / 61240) loss: 2.302333\n",
      "(Iteration 6501 / 61240) loss: 2.302509\n",
      "(Iteration 6601 / 61240) loss: 2.302671\n",
      "(Iteration 6701 / 61240) loss: 2.302570\n",
      "(Iteration 6801 / 61240) loss: 2.302980\n",
      "(Iteration 6901 / 61240) loss: 2.302288\n",
      "(Iteration 7001 / 61240) loss: 2.302179\n",
      "(Iteration 7101 / 61240) loss: 2.302785\n",
      "(Iteration 7201 / 61240) loss: 2.303137\n",
      "(Iteration 7301 / 61240) loss: 2.302630\n",
      "(Iteration 7401 / 61240) loss: 2.302843\n",
      "(Iteration 7501 / 61240) loss: 2.302866\n",
      "(Iteration 7601 / 61240) loss: 2.301977\n",
      "(Epoch 5 / 40) train acc: 0.127000; val_acc: 0.084000\n",
      "(Iteration 7701 / 61240) loss: 2.303384\n",
      "(Iteration 7801 / 61240) loss: 2.302636\n",
      "(Iteration 7901 / 61240) loss: 2.302671\n",
      "(Iteration 8001 / 61240) loss: 2.303439\n",
      "(Iteration 8101 / 61240) loss: 2.303349\n",
      "(Iteration 8201 / 61240) loss: 2.302039\n",
      "(Iteration 8301 / 61240) loss: 2.302148\n",
      "(Iteration 8401 / 61240) loss: 2.301607\n",
      "(Iteration 8501 / 61240) loss: 2.302383\n",
      "(Iteration 8601 / 61240) loss: 2.302078\n",
      "(Iteration 8701 / 61240) loss: 2.302541\n",
      "(Iteration 8801 / 61240) loss: 2.302886\n",
      "(Iteration 8901 / 61240) loss: 2.301704\n",
      "(Iteration 9001 / 61240) loss: 2.301571\n",
      "(Iteration 9101 / 61240) loss: 2.301304\n",
      "(Epoch 6 / 40) train acc: 0.152000; val_acc: 0.135000\n",
      "(Iteration 9201 / 61240) loss: 2.302607\n",
      "(Iteration 9301 / 61240) loss: 2.302202\n",
      "(Iteration 9401 / 61240) loss: 2.303031\n",
      "(Iteration 9501 / 61240) loss: 2.302496\n",
      "(Iteration 9601 / 61240) loss: 2.300205\n",
      "(Iteration 9701 / 61240) loss: 2.301633\n",
      "(Iteration 9801 / 61240) loss: 2.301203\n",
      "(Iteration 9901 / 61240) loss: 2.301588\n",
      "(Iteration 10001 / 61240) loss: 2.301032\n",
      "(Iteration 10101 / 61240) loss: 2.301775\n",
      "(Iteration 10201 / 61240) loss: 2.302717\n",
      "(Iteration 10301 / 61240) loss: 2.301296\n",
      "(Iteration 10401 / 61240) loss: 2.300627\n",
      "(Iteration 10501 / 61240) loss: 2.301151\n",
      "(Iteration 10601 / 61240) loss: 2.298893\n",
      "(Iteration 10701 / 61240) loss: 2.301537\n",
      "(Epoch 7 / 40) train acc: 0.196000; val_acc: 0.165000\n",
      "(Iteration 10801 / 61240) loss: 2.299757\n",
      "(Iteration 10901 / 61240) loss: 2.300694\n",
      "(Iteration 11001 / 61240) loss: 2.301081\n",
      "(Iteration 11101 / 61240) loss: 2.300411\n",
      "(Iteration 11201 / 61240) loss: 2.300769\n",
      "(Iteration 11301 / 61240) loss: 2.300100\n",
      "(Iteration 11401 / 61240) loss: 2.302448\n",
      "(Iteration 11501 / 61240) loss: 2.299039\n",
      "(Iteration 11601 / 61240) loss: 2.298866\n",
      "(Iteration 11701 / 61240) loss: 2.298238\n",
      "(Iteration 11801 / 61240) loss: 2.300030\n",
      "(Iteration 11901 / 61240) loss: 2.300997\n",
      "(Iteration 12001 / 61240) loss: 2.299340\n",
      "(Iteration 12101 / 61240) loss: 2.302129\n",
      "(Iteration 12201 / 61240) loss: 2.300183\n",
      "(Epoch 8 / 40) train acc: 0.196000; val_acc: 0.180000\n",
      "(Iteration 12301 / 61240) loss: 2.300804\n",
      "(Iteration 12401 / 61240) loss: 2.299628\n",
      "(Iteration 12501 / 61240) loss: 2.298058\n",
      "(Iteration 12601 / 61240) loss: 2.301717\n",
      "(Iteration 12701 / 61240) loss: 2.299965\n",
      "(Iteration 12801 / 61240) loss: 2.301774\n",
      "(Iteration 12901 / 61240) loss: 2.301136\n",
      "(Iteration 13001 / 61240) loss: 2.299036\n",
      "(Iteration 13101 / 61240) loss: 2.300860\n",
      "(Iteration 13201 / 61240) loss: 2.301065\n",
      "(Iteration 13301 / 61240) loss: 2.297695\n",
      "(Iteration 13401 / 61240) loss: 2.299326\n",
      "(Iteration 13501 / 61240) loss: 2.300100\n",
      "(Iteration 13601 / 61240) loss: 2.298901\n",
      "(Iteration 13701 / 61240) loss: 2.300020\n",
      "(Epoch 9 / 40) train acc: 0.223000; val_acc: 0.197000\n",
      "(Iteration 13801 / 61240) loss: 2.301467\n",
      "(Iteration 13901 / 61240) loss: 2.296538\n",
      "(Iteration 14001 / 61240) loss: 2.296469\n",
      "(Iteration 14101 / 61240) loss: 2.297512\n",
      "(Iteration 14201 / 61240) loss: 2.300549\n",
      "(Iteration 14301 / 61240) loss: 2.296177\n",
      "(Iteration 14401 / 61240) loss: 2.296557\n",
      "(Iteration 14501 / 61240) loss: 2.298313\n",
      "(Iteration 14601 / 61240) loss: 2.298910\n",
      "(Iteration 14701 / 61240) loss: 2.292625\n",
      "(Iteration 14801 / 61240) loss: 2.295737\n",
      "(Iteration 14901 / 61240) loss: 2.296978\n",
      "(Iteration 15001 / 61240) loss: 2.297610\n",
      "(Iteration 15101 / 61240) loss: 2.297858\n",
      "(Iteration 15201 / 61240) loss: 2.297610\n",
      "(Iteration 15301 / 61240) loss: 2.297900\n",
      "(Epoch 10 / 40) train acc: 0.252000; val_acc: 0.216000\n",
      "(Iteration 15401 / 61240) loss: 2.294639\n",
      "(Iteration 15501 / 61240) loss: 2.290681\n",
      "(Iteration 15601 / 61240) loss: 2.297282\n",
      "(Iteration 15701 / 61240) loss: 2.296379\n",
      "(Iteration 15801 / 61240) loss: 2.292362\n",
      "(Iteration 15901 / 61240) loss: 2.300469\n",
      "(Iteration 16001 / 61240) loss: 2.290332\n",
      "(Iteration 16101 / 61240) loss: 2.301238\n",
      "(Iteration 16201 / 61240) loss: 2.294375\n",
      "(Iteration 16301 / 61240) loss: 2.291510\n",
      "(Iteration 16401 / 61240) loss: 2.297753\n",
      "(Iteration 16501 / 61240) loss: 2.293613\n",
      "(Iteration 16601 / 61240) loss: 2.296723\n",
      "(Iteration 16701 / 61240) loss: 2.296900\n",
      "(Iteration 16801 / 61240) loss: 2.297223\n",
      "(Epoch 11 / 40) train acc: 0.253000; val_acc: 0.235000\n",
      "(Iteration 16901 / 61240) loss: 2.291089\n",
      "(Iteration 17001 / 61240) loss: 2.298598\n",
      "(Iteration 17101 / 61240) loss: 2.298706\n",
      "(Iteration 17201 / 61240) loss: 2.298487\n",
      "(Iteration 17301 / 61240) loss: 2.296519\n",
      "(Iteration 17401 / 61240) loss: 2.294915\n",
      "(Iteration 17501 / 61240) loss: 2.294990\n",
      "(Iteration 17601 / 61240) loss: 2.291220\n",
      "(Iteration 17701 / 61240) loss: 2.293418\n",
      "(Iteration 17801 / 61240) loss: 2.302316\n",
      "(Iteration 17901 / 61240) loss: 2.287505\n",
      "(Iteration 18001 / 61240) loss: 2.293977\n",
      "(Iteration 18101 / 61240) loss: 2.280479\n",
      "(Iteration 18201 / 61240) loss: 2.286430\n",
      "(Iteration 18301 / 61240) loss: 2.288385\n",
      "(Epoch 12 / 40) train acc: 0.255000; val_acc: 0.236000\n",
      "(Iteration 18401 / 61240) loss: 2.301406\n",
      "(Iteration 18501 / 61240) loss: 2.293181\n",
      "(Iteration 18601 / 61240) loss: 2.293413\n",
      "(Iteration 18701 / 61240) loss: 2.292448\n",
      "(Iteration 18801 / 61240) loss: 2.294174\n",
      "(Iteration 18901 / 61240) loss: 2.298104\n",
      "(Iteration 19001 / 61240) loss: 2.299224\n",
      "(Iteration 19101 / 61240) loss: 2.288529\n",
      "(Iteration 19201 / 61240) loss: 2.281300\n",
      "(Iteration 19301 / 61240) loss: 2.294964\n",
      "(Iteration 19401 / 61240) loss: 2.286827\n",
      "(Iteration 19501 / 61240) loss: 2.286700\n",
      "(Iteration 19601 / 61240) loss: 2.297076\n",
      "(Iteration 19701 / 61240) loss: 2.292119\n",
      "(Iteration 19801 / 61240) loss: 2.293644\n",
      "(Iteration 19901 / 61240) loss: 2.294889\n",
      "(Epoch 13 / 40) train acc: 0.258000; val_acc: 0.230000\n",
      "(Iteration 20001 / 61240) loss: 2.291383\n",
      "(Iteration 20101 / 61240) loss: 2.290231\n",
      "(Iteration 20201 / 61240) loss: 2.290049\n",
      "(Iteration 20301 / 61240) loss: 2.288373\n",
      "(Iteration 20401 / 61240) loss: 2.293726\n",
      "(Iteration 20501 / 61240) loss: 2.280366\n",
      "(Iteration 20601 / 61240) loss: 2.282082\n",
      "(Iteration 20701 / 61240) loss: 2.278943\n",
      "(Iteration 20801 / 61240) loss: 2.273673\n",
      "(Iteration 20901 / 61240) loss: 2.294627\n",
      "(Iteration 21001 / 61240) loss: 2.290105\n",
      "(Iteration 21101 / 61240) loss: 2.277648\n",
      "(Iteration 21201 / 61240) loss: 2.300279\n",
      "(Iteration 21301 / 61240) loss: 2.291300\n",
      "(Iteration 21401 / 61240) loss: 2.284989\n",
      "(Epoch 14 / 40) train acc: 0.242000; val_acc: 0.236000\n",
      "(Iteration 21501 / 61240) loss: 2.296137\n",
      "(Iteration 21601 / 61240) loss: 2.274599\n",
      "(Iteration 21701 / 61240) loss: 2.294753\n",
      "(Iteration 21801 / 61240) loss: 2.287799\n",
      "(Iteration 21901 / 61240) loss: 2.287155\n",
      "(Iteration 22001 / 61240) loss: 2.291777\n",
      "(Iteration 22101 / 61240) loss: 2.283945\n",
      "(Iteration 22201 / 61240) loss: 2.280725\n",
      "(Iteration 22301 / 61240) loss: 2.281238\n",
      "(Iteration 22401 / 61240) loss: 2.286594\n",
      "(Iteration 22501 / 61240) loss: 2.289683\n",
      "(Iteration 22601 / 61240) loss: 2.291355\n",
      "(Iteration 22701 / 61240) loss: 2.282724\n",
      "(Iteration 22801 / 61240) loss: 2.280150\n",
      "(Iteration 22901 / 61240) loss: 2.284385\n",
      "(Epoch 15 / 40) train acc: 0.230000; val_acc: 0.239000\n",
      "(Iteration 23001 / 61240) loss: 2.283746\n",
      "(Iteration 23101 / 61240) loss: 2.286061\n",
      "(Iteration 23201 / 61240) loss: 2.292110\n",
      "(Iteration 23301 / 61240) loss: 2.293718\n",
      "(Iteration 23401 / 61240) loss: 2.299901\n",
      "(Iteration 23501 / 61240) loss: 2.280328\n",
      "(Iteration 23601 / 61240) loss: 2.270124\n",
      "(Iteration 23701 / 61240) loss: 2.298972\n",
      "(Iteration 23801 / 61240) loss: 2.287597\n",
      "(Iteration 23901 / 61240) loss: 2.271138\n",
      "(Iteration 24001 / 61240) loss: 2.279777\n",
      "(Iteration 24101 / 61240) loss: 2.298818\n",
      "(Iteration 24201 / 61240) loss: 2.288366\n",
      "(Iteration 24301 / 61240) loss: 2.258681\n",
      "(Iteration 24401 / 61240) loss: 2.276253\n",
      "(Epoch 16 / 40) train acc: 0.221000; val_acc: 0.239000\n",
      "(Iteration 24501 / 61240) loss: 2.276895\n",
      "(Iteration 24601 / 61240) loss: 2.283669\n",
      "(Iteration 24701 / 61240) loss: 2.294330\n",
      "(Iteration 24801 / 61240) loss: 2.287191\n",
      "(Iteration 24901 / 61240) loss: 2.302376\n",
      "(Iteration 25001 / 61240) loss: 2.291053\n",
      "(Iteration 25101 / 61240) loss: 2.299549\n",
      "(Iteration 25201 / 61240) loss: 2.279482\n",
      "(Iteration 25301 / 61240) loss: 2.269894\n",
      "(Iteration 25401 / 61240) loss: 2.286455\n",
      "(Iteration 25501 / 61240) loss: 2.272529\n",
      "(Iteration 25601 / 61240) loss: 2.287061\n",
      "(Iteration 25701 / 61240) loss: 2.264842\n",
      "(Iteration 25801 / 61240) loss: 2.283372\n",
      "(Iteration 25901 / 61240) loss: 2.282612\n",
      "(Iteration 26001 / 61240) loss: 2.285742\n",
      "(Epoch 17 / 40) train acc: 0.232000; val_acc: 0.233000\n",
      "(Iteration 26101 / 61240) loss: 2.271727\n",
      "(Iteration 26201 / 61240) loss: 2.281498\n",
      "(Iteration 26301 / 61240) loss: 2.278119\n",
      "(Iteration 26401 / 61240) loss: 2.269943\n",
      "(Iteration 26501 / 61240) loss: 2.276305\n",
      "(Iteration 26601 / 61240) loss: 2.286868\n",
      "(Iteration 26701 / 61240) loss: 2.294071\n",
      "(Iteration 26801 / 61240) loss: 2.281604\n",
      "(Iteration 26901 / 61240) loss: 2.278874\n",
      "(Iteration 27001 / 61240) loss: 2.244974\n",
      "(Iteration 27101 / 61240) loss: 2.278500\n",
      "(Iteration 27201 / 61240) loss: 2.280686\n",
      "(Iteration 27301 / 61240) loss: 2.298848\n",
      "(Iteration 27401 / 61240) loss: 2.288632\n",
      "(Iteration 27501 / 61240) loss: 2.283506\n",
      "(Epoch 18 / 40) train acc: 0.239000; val_acc: 0.233000\n",
      "(Iteration 27601 / 61240) loss: 2.285771\n",
      "(Iteration 27701 / 61240) loss: 2.238527\n",
      "(Iteration 27801 / 61240) loss: 2.282835\n",
      "(Iteration 27901 / 61240) loss: 2.275737\n",
      "(Iteration 28001 / 61240) loss: 2.275698\n",
      "(Iteration 28101 / 61240) loss: 2.277582\n",
      "(Iteration 28201 / 61240) loss: 2.252788\n",
      "(Iteration 28301 / 61240) loss: 2.273906\n",
      "(Iteration 28401 / 61240) loss: 2.283383\n",
      "(Iteration 28501 / 61240) loss: 2.276298\n",
      "(Iteration 28601 / 61240) loss: 2.262413\n",
      "(Iteration 28701 / 61240) loss: 2.293359\n",
      "(Iteration 28801 / 61240) loss: 2.273250\n",
      "(Iteration 28901 / 61240) loss: 2.278457\n",
      "(Iteration 29001 / 61240) loss: 2.263967\n",
      "(Epoch 19 / 40) train acc: 0.252000; val_acc: 0.236000\n",
      "(Iteration 29101 / 61240) loss: 2.251124\n",
      "(Iteration 29201 / 61240) loss: 2.287840\n",
      "(Iteration 29301 / 61240) loss: 2.274851\n",
      "(Iteration 29401 / 61240) loss: 2.281348\n",
      "(Iteration 29501 / 61240) loss: 2.297435\n",
      "(Iteration 29601 / 61240) loss: 2.260597\n",
      "(Iteration 29701 / 61240) loss: 2.282033\n",
      "(Iteration 29801 / 61240) loss: 2.249688\n",
      "(Iteration 29901 / 61240) loss: 2.283048\n",
      "(Iteration 30001 / 61240) loss: 2.265176\n",
      "(Iteration 30101 / 61240) loss: 2.276998\n",
      "(Iteration 30201 / 61240) loss: 2.290763\n",
      "(Iteration 30301 / 61240) loss: 2.280304\n",
      "(Iteration 30401 / 61240) loss: 2.280069\n",
      "(Iteration 30501 / 61240) loss: 2.267558\n",
      "(Iteration 30601 / 61240) loss: 2.256827\n",
      "(Epoch 20 / 40) train acc: 0.245000; val_acc: 0.230000\n",
      "(Iteration 30701 / 61240) loss: 2.275092\n",
      "(Iteration 30801 / 61240) loss: 2.268419\n",
      "(Iteration 30901 / 61240) loss: 2.261016\n",
      "(Iteration 31001 / 61240) loss: 2.276958\n",
      "(Iteration 31101 / 61240) loss: 2.220718\n",
      "(Iteration 31201 / 61240) loss: 2.290734\n",
      "(Iteration 31301 / 61240) loss: 2.265266\n",
      "(Iteration 31401 / 61240) loss: 2.274678\n",
      "(Iteration 31501 / 61240) loss: 2.262745\n",
      "(Iteration 31601 / 61240) loss: 2.295464\n",
      "(Iteration 31701 / 61240) loss: 2.280150\n",
      "(Iteration 31801 / 61240) loss: 2.275560\n",
      "(Iteration 31901 / 61240) loss: 2.228606\n",
      "(Iteration 32001 / 61240) loss: 2.280603\n",
      "(Iteration 32101 / 61240) loss: 2.226049\n",
      "(Epoch 21 / 40) train acc: 0.258000; val_acc: 0.236000\n",
      "(Iteration 32201 / 61240) loss: 2.263899\n",
      "(Iteration 32301 / 61240) loss: 2.291179\n",
      "(Iteration 32401 / 61240) loss: 2.247375\n",
      "(Iteration 32501 / 61240) loss: 2.289312\n",
      "(Iteration 32601 / 61240) loss: 2.276388\n",
      "(Iteration 32701 / 61240) loss: 2.261614\n",
      "(Iteration 32801 / 61240) loss: 2.248357\n",
      "(Iteration 32901 / 61240) loss: 2.268727\n",
      "(Iteration 33001 / 61240) loss: 2.263128\n",
      "(Iteration 33101 / 61240) loss: 2.249186\n",
      "(Iteration 33201 / 61240) loss: 2.264751\n",
      "(Iteration 33301 / 61240) loss: 2.294710\n",
      "(Iteration 33401 / 61240) loss: 2.265170\n",
      "(Iteration 33501 / 61240) loss: 2.270100\n",
      "(Iteration 33601 / 61240) loss: 2.270730\n",
      "(Epoch 22 / 40) train acc: 0.204000; val_acc: 0.239000\n",
      "(Iteration 33701 / 61240) loss: 2.269375\n",
      "(Iteration 33801 / 61240) loss: 2.250616\n",
      "(Iteration 33901 / 61240) loss: 2.248105\n",
      "(Iteration 34001 / 61240) loss: 2.251366\n",
      "(Iteration 34101 / 61240) loss: 2.242420\n",
      "(Iteration 34201 / 61240) loss: 2.289842\n",
      "(Iteration 34301 / 61240) loss: 2.265272\n",
      "(Iteration 34401 / 61240) loss: 2.277811\n",
      "(Iteration 34501 / 61240) loss: 2.300506\n",
      "(Iteration 34601 / 61240) loss: 2.260405\n",
      "(Iteration 34701 / 61240) loss: 2.235088\n",
      "(Iteration 34801 / 61240) loss: 2.297726\n",
      "(Iteration 34901 / 61240) loss: 2.268502\n",
      "(Iteration 35001 / 61240) loss: 2.264265\n",
      "(Iteration 35101 / 61240) loss: 2.251977\n",
      "(Iteration 35201 / 61240) loss: 2.292397\n",
      "(Epoch 23 / 40) train acc: 0.235000; val_acc: 0.238000\n",
      "(Iteration 35301 / 61240) loss: 2.244754\n",
      "(Iteration 35401 / 61240) loss: 2.275109\n",
      "(Iteration 35501 / 61240) loss: 2.281872\n",
      "(Iteration 35601 / 61240) loss: 2.263106\n",
      "(Iteration 35701 / 61240) loss: 2.282902\n",
      "(Iteration 35801 / 61240) loss: 2.270760\n",
      "(Iteration 35901 / 61240) loss: 2.289691\n",
      "(Iteration 36001 / 61240) loss: 2.291643\n",
      "(Iteration 36101 / 61240) loss: 2.297939\n",
      "(Iteration 36201 / 61240) loss: 2.226259\n",
      "(Iteration 36301 / 61240) loss: 2.276483\n",
      "(Iteration 36401 / 61240) loss: 2.260796\n",
      "(Iteration 36501 / 61240) loss: 2.282742\n",
      "(Iteration 36601 / 61240) loss: 2.269285\n",
      "(Iteration 36701 / 61240) loss: 2.260106\n",
      "(Epoch 24 / 40) train acc: 0.241000; val_acc: 0.240000\n",
      "(Iteration 36801 / 61240) loss: 2.256077\n",
      "(Iteration 36901 / 61240) loss: 2.208705\n",
      "(Iteration 37001 / 61240) loss: 2.239129\n",
      "(Iteration 37101 / 61240) loss: 2.283700\n",
      "(Iteration 37201 / 61240) loss: 2.260521\n",
      "(Iteration 37301 / 61240) loss: 2.274987\n",
      "(Iteration 37401 / 61240) loss: 2.222506\n",
      "(Iteration 37501 / 61240) loss: 2.283054\n",
      "(Iteration 37601 / 61240) loss: 2.226155\n",
      "(Iteration 37701 / 61240) loss: 2.246735\n",
      "(Iteration 37801 / 61240) loss: 2.277605\n",
      "(Iteration 37901 / 61240) loss: 2.247086\n",
      "(Iteration 38001 / 61240) loss: 2.288370\n",
      "(Iteration 38101 / 61240) loss: 2.219289\n",
      "(Iteration 38201 / 61240) loss: 2.254937\n",
      "(Epoch 25 / 40) train acc: 0.241000; val_acc: 0.239000\n",
      "(Iteration 38301 / 61240) loss: 2.255340\n",
      "(Iteration 38401 / 61240) loss: 2.223513\n",
      "(Iteration 38501 / 61240) loss: 2.256093\n",
      "(Iteration 38601 / 61240) loss: 2.287392\n",
      "(Iteration 38701 / 61240) loss: 2.267906\n",
      "(Iteration 38801 / 61240) loss: 2.283665\n",
      "(Iteration 38901 / 61240) loss: 2.229893\n",
      "(Iteration 39001 / 61240) loss: 2.245076\n",
      "(Iteration 39101 / 61240) loss: 2.278718\n",
      "(Iteration 39201 / 61240) loss: 2.233614\n",
      "(Iteration 39301 / 61240) loss: 2.250370\n",
      "(Iteration 39401 / 61240) loss: 2.273663\n",
      "(Iteration 39501 / 61240) loss: 2.260371\n",
      "(Iteration 39601 / 61240) loss: 2.270715\n",
      "(Iteration 39701 / 61240) loss: 2.241121\n",
      "(Iteration 39801 / 61240) loss: 2.237727\n",
      "(Epoch 26 / 40) train acc: 0.236000; val_acc: 0.239000\n",
      "(Iteration 39901 / 61240) loss: 2.228748\n",
      "(Iteration 40001 / 61240) loss: 2.288403\n",
      "(Iteration 40101 / 61240) loss: 2.295450\n",
      "(Iteration 40201 / 61240) loss: 2.224008\n",
      "(Iteration 40301 / 61240) loss: 2.239088\n",
      "(Iteration 40401 / 61240) loss: 2.257805\n",
      "(Iteration 40501 / 61240) loss: 2.225563\n",
      "(Iteration 40601 / 61240) loss: 2.266959\n",
      "(Iteration 40701 / 61240) loss: 2.304626\n",
      "(Iteration 40801 / 61240) loss: 2.253369\n",
      "(Iteration 40901 / 61240) loss: 2.259347\n",
      "(Iteration 41001 / 61240) loss: 2.276122\n",
      "(Iteration 41101 / 61240) loss: 2.261542\n",
      "(Iteration 41201 / 61240) loss: 2.266658\n",
      "(Iteration 41301 / 61240) loss: 2.222068\n",
      "(Epoch 27 / 40) train acc: 0.236000; val_acc: 0.240000\n",
      "(Iteration 41401 / 61240) loss: 2.272393\n",
      "(Iteration 41501 / 61240) loss: 2.214479\n",
      "(Iteration 41601 / 61240) loss: 2.287642\n",
      "(Iteration 41701 / 61240) loss: 2.210827\n",
      "(Iteration 41801 / 61240) loss: 2.265084\n",
      "(Iteration 41901 / 61240) loss: 2.261494\n",
      "(Iteration 42001 / 61240) loss: 2.296447\n",
      "(Iteration 42101 / 61240) loss: 2.263021\n",
      "(Iteration 42201 / 61240) loss: 2.291111\n",
      "(Iteration 42301 / 61240) loss: 2.225831\n",
      "(Iteration 42401 / 61240) loss: 2.264207\n",
      "(Iteration 42501 / 61240) loss: 2.214592\n",
      "(Iteration 42601 / 61240) loss: 2.241345\n",
      "(Iteration 42701 / 61240) loss: 2.288446\n",
      "(Iteration 42801 / 61240) loss: 2.245525\n",
      "(Epoch 28 / 40) train acc: 0.234000; val_acc: 0.243000\n",
      "(Iteration 42901 / 61240) loss: 2.268766\n",
      "(Iteration 43001 / 61240) loss: 2.268682\n",
      "(Iteration 43101 / 61240) loss: 2.252480\n",
      "(Iteration 43201 / 61240) loss: 2.283137\n",
      "(Iteration 43301 / 61240) loss: 2.212476\n",
      "(Iteration 43401 / 61240) loss: 2.243431\n",
      "(Iteration 43501 / 61240) loss: 2.213251\n",
      "(Iteration 43601 / 61240) loss: 2.275830\n",
      "(Iteration 43701 / 61240) loss: 2.299725\n",
      "(Iteration 43801 / 61240) loss: 2.255838\n",
      "(Iteration 43901 / 61240) loss: 2.269637\n",
      "(Iteration 44001 / 61240) loss: 2.201409\n",
      "(Iteration 44101 / 61240) loss: 2.253847\n",
      "(Iteration 44201 / 61240) loss: 2.297462\n",
      "(Iteration 44301 / 61240) loss: 2.301718\n",
      "(Epoch 29 / 40) train acc: 0.252000; val_acc: 0.246000\n",
      "(Iteration 44401 / 61240) loss: 2.239597\n",
      "(Iteration 44501 / 61240) loss: 2.272567\n",
      "(Iteration 44601 / 61240) loss: 2.280456\n",
      "(Iteration 44701 / 61240) loss: 2.242394\n",
      "(Iteration 44801 / 61240) loss: 2.243436\n",
      "(Iteration 44901 / 61240) loss: 2.214842\n",
      "(Iteration 45001 / 61240) loss: 2.245140\n",
      "(Iteration 45101 / 61240) loss: 2.275283\n",
      "(Iteration 45201 / 61240) loss: 2.244218\n",
      "(Iteration 45301 / 61240) loss: 2.277577\n",
      "(Iteration 45401 / 61240) loss: 2.215543\n",
      "(Iteration 45501 / 61240) loss: 2.220851\n",
      "(Iteration 45601 / 61240) loss: 2.196876\n",
      "(Iteration 45701 / 61240) loss: 2.226102\n",
      "(Iteration 45801 / 61240) loss: 2.234757\n",
      "(Iteration 45901 / 61240) loss: 2.231323\n",
      "(Epoch 30 / 40) train acc: 0.258000; val_acc: 0.246000\n",
      "(Iteration 46001 / 61240) loss: 2.242991\n",
      "(Iteration 46101 / 61240) loss: 2.260739\n",
      "(Iteration 46201 / 61240) loss: 2.193248\n",
      "(Iteration 46301 / 61240) loss: 2.264874\n",
      "(Iteration 46401 / 61240) loss: 2.224270\n",
      "(Iteration 46501 / 61240) loss: 2.228759\n",
      "(Iteration 46601 / 61240) loss: 2.228248\n",
      "(Iteration 46701 / 61240) loss: 2.219133\n",
      "(Iteration 46801 / 61240) loss: 2.257367\n",
      "(Iteration 46901 / 61240) loss: 2.274147\n",
      "(Iteration 47001 / 61240) loss: 2.215787\n",
      "(Iteration 47101 / 61240) loss: 2.271527\n",
      "(Iteration 47201 / 61240) loss: 2.235023\n",
      "(Iteration 47301 / 61240) loss: 2.242997\n",
      "(Iteration 47401 / 61240) loss: 2.236597\n",
      "(Epoch 31 / 40) train acc: 0.234000; val_acc: 0.248000\n",
      "(Iteration 47501 / 61240) loss: 2.231361\n",
      "(Iteration 47601 / 61240) loss: 2.267924\n",
      "(Iteration 47701 / 61240) loss: 2.263956\n",
      "(Iteration 47801 / 61240) loss: 2.233665\n",
      "(Iteration 47901 / 61240) loss: 2.249446\n",
      "(Iteration 48001 / 61240) loss: 2.280771\n",
      "(Iteration 48101 / 61240) loss: 2.256967\n",
      "(Iteration 48201 / 61240) loss: 2.244602\n",
      "(Iteration 48301 / 61240) loss: 2.240765\n",
      "(Iteration 48401 / 61240) loss: 2.286729\n",
      "(Iteration 48501 / 61240) loss: 2.280260\n",
      "(Iteration 48601 / 61240) loss: 2.291247\n",
      "(Iteration 48701 / 61240) loss: 2.279096\n",
      "(Iteration 48801 / 61240) loss: 2.256157\n",
      "(Iteration 48901 / 61240) loss: 2.231825\n",
      "(Epoch 32 / 40) train acc: 0.248000; val_acc: 0.250000\n",
      "(Iteration 49001 / 61240) loss: 2.261884\n",
      "(Iteration 49101 / 61240) loss: 2.314186\n",
      "(Iteration 49201 / 61240) loss: 2.249132\n",
      "(Iteration 49301 / 61240) loss: 2.206904\n",
      "(Iteration 49401 / 61240) loss: 2.202295\n",
      "(Iteration 49501 / 61240) loss: 2.271997\n",
      "(Iteration 49601 / 61240) loss: 2.254352\n",
      "(Iteration 49701 / 61240) loss: 2.257999\n",
      "(Iteration 49801 / 61240) loss: 2.239546\n",
      "(Iteration 49901 / 61240) loss: 2.289273\n",
      "(Iteration 50001 / 61240) loss: 2.255965\n",
      "(Iteration 50101 / 61240) loss: 2.204719\n",
      "(Iteration 50201 / 61240) loss: 2.283276\n",
      "(Iteration 50301 / 61240) loss: 2.235507\n",
      "(Iteration 50401 / 61240) loss: 2.232937\n",
      "(Iteration 50501 / 61240) loss: 2.246695\n",
      "(Epoch 33 / 40) train acc: 0.224000; val_acc: 0.254000\n",
      "(Iteration 50601 / 61240) loss: 2.209607\n",
      "(Iteration 50701 / 61240) loss: 2.254074\n",
      "(Iteration 50801 / 61240) loss: 2.261384\n",
      "(Iteration 50901 / 61240) loss: 2.233871\n",
      "(Iteration 51001 / 61240) loss: 2.274077\n",
      "(Iteration 51101 / 61240) loss: 2.284947\n",
      "(Iteration 51201 / 61240) loss: 2.241584\n",
      "(Iteration 51301 / 61240) loss: 2.169174\n",
      "(Iteration 51401 / 61240) loss: 2.235671\n",
      "(Iteration 51501 / 61240) loss: 2.270057\n",
      "(Iteration 51601 / 61240) loss: 2.236705\n",
      "(Iteration 51701 / 61240) loss: 2.273408\n",
      "(Iteration 51801 / 61240) loss: 2.183176\n",
      "(Iteration 51901 / 61240) loss: 2.235128\n",
      "(Iteration 52001 / 61240) loss: 2.228566\n",
      "(Epoch 34 / 40) train acc: 0.272000; val_acc: 0.253000\n",
      "(Iteration 52101 / 61240) loss: 2.267006\n",
      "(Iteration 52201 / 61240) loss: 2.200914\n",
      "(Iteration 52301 / 61240) loss: 2.241625\n",
      "(Iteration 52401 / 61240) loss: 2.296871\n",
      "(Iteration 52501 / 61240) loss: 2.240995\n",
      "(Iteration 52601 / 61240) loss: 2.244941\n",
      "(Iteration 52701 / 61240) loss: 2.271536\n",
      "(Iteration 52801 / 61240) loss: 2.239947\n",
      "(Iteration 52901 / 61240) loss: 2.219339\n",
      "(Iteration 53001 / 61240) loss: 2.232270\n",
      "(Iteration 53101 / 61240) loss: 2.245852\n",
      "(Iteration 53201 / 61240) loss: 2.247817\n",
      "(Iteration 53301 / 61240) loss: 2.232389\n",
      "(Iteration 53401 / 61240) loss: 2.268264\n",
      "(Iteration 53501 / 61240) loss: 2.248441\n",
      "(Epoch 35 / 40) train acc: 0.243000; val_acc: 0.252000\n",
      "(Iteration 53601 / 61240) loss: 2.276198\n",
      "(Iteration 53701 / 61240) loss: 2.283659\n",
      "(Iteration 53801 / 61240) loss: 2.252981\n",
      "(Iteration 53901 / 61240) loss: 2.207600\n",
      "(Iteration 54001 / 61240) loss: 2.253012\n",
      "(Iteration 54101 / 61240) loss: 2.253298\n",
      "(Iteration 54201 / 61240) loss: 2.219474\n",
      "(Iteration 54301 / 61240) loss: 2.275409\n",
      "(Iteration 54401 / 61240) loss: 2.192414\n",
      "(Iteration 54501 / 61240) loss: 2.209255\n",
      "(Iteration 54601 / 61240) loss: 2.162191\n",
      "(Iteration 54701 / 61240) loss: 2.253823\n",
      "(Iteration 54801 / 61240) loss: 2.250154\n",
      "(Iteration 54901 / 61240) loss: 2.257787\n",
      "(Iteration 55001 / 61240) loss: 2.267114\n",
      "(Iteration 55101 / 61240) loss: 2.232509\n",
      "(Epoch 36 / 40) train acc: 0.233000; val_acc: 0.253000\n",
      "(Iteration 55201 / 61240) loss: 2.267745\n",
      "(Iteration 55301 / 61240) loss: 2.266507\n",
      "(Iteration 55401 / 61240) loss: 2.275739\n",
      "(Iteration 55501 / 61240) loss: 2.215677\n",
      "(Iteration 55601 / 61240) loss: 2.252110\n",
      "(Iteration 55701 / 61240) loss: 2.219438\n",
      "(Iteration 55801 / 61240) loss: 2.258121\n",
      "(Iteration 55901 / 61240) loss: 2.237815\n",
      "(Iteration 56001 / 61240) loss: 2.225823\n",
      "(Iteration 56101 / 61240) loss: 2.271443\n",
      "(Iteration 56201 / 61240) loss: 2.258507\n",
      "(Iteration 56301 / 61240) loss: 2.231845\n",
      "(Iteration 56401 / 61240) loss: 2.231042\n",
      "(Iteration 56501 / 61240) loss: 2.235437\n",
      "(Iteration 56601 / 61240) loss: 2.249092\n",
      "(Epoch 37 / 40) train acc: 0.255000; val_acc: 0.251000\n",
      "(Iteration 56701 / 61240) loss: 2.235181\n",
      "(Iteration 56801 / 61240) loss: 2.268380\n",
      "(Iteration 56901 / 61240) loss: 2.260072\n",
      "(Iteration 57001 / 61240) loss: 2.198714\n",
      "(Iteration 57101 / 61240) loss: 2.210468\n",
      "(Iteration 57201 / 61240) loss: 2.307045\n",
      "(Iteration 57301 / 61240) loss: 2.251471\n",
      "(Iteration 57401 / 61240) loss: 2.219999\n",
      "(Iteration 57501 / 61240) loss: 2.212783\n",
      "(Iteration 57601 / 61240) loss: 2.258581\n",
      "(Iteration 57701 / 61240) loss: 2.282167\n",
      "(Iteration 57801 / 61240) loss: 2.220698\n",
      "(Iteration 57901 / 61240) loss: 2.205919\n",
      "(Iteration 58001 / 61240) loss: 2.246511\n",
      "(Iteration 58101 / 61240) loss: 2.195535\n",
      "(Epoch 38 / 40) train acc: 0.264000; val_acc: 0.251000\n",
      "(Iteration 58201 / 61240) loss: 2.162961\n",
      "(Iteration 58301 / 61240) loss: 2.297960\n",
      "(Iteration 58401 / 61240) loss: 2.225874\n",
      "(Iteration 58501 / 61240) loss: 2.214726\n",
      "(Iteration 58601 / 61240) loss: 2.260974\n",
      "(Iteration 58701 / 61240) loss: 2.235998\n",
      "(Iteration 58801 / 61240) loss: 2.259114\n",
      "(Iteration 58901 / 61240) loss: 2.197724\n",
      "(Iteration 59001 / 61240) loss: 2.243042\n",
      "(Iteration 59101 / 61240) loss: 2.223901\n",
      "(Iteration 59201 / 61240) loss: 2.243348\n",
      "(Iteration 59301 / 61240) loss: 2.210527\n",
      "(Iteration 59401 / 61240) loss: 2.293794\n",
      "(Iteration 59501 / 61240) loss: 2.224225\n",
      "(Iteration 59601 / 61240) loss: 2.180763\n",
      "(Iteration 59701 / 61240) loss: 2.232325\n",
      "(Epoch 39 / 40) train acc: 0.255000; val_acc: 0.251000\n",
      "(Iteration 59801 / 61240) loss: 2.255344\n",
      "(Iteration 59901 / 61240) loss: 2.241553\n",
      "(Iteration 60001 / 61240) loss: 2.258220\n",
      "(Iteration 60101 / 61240) loss: 2.275729\n",
      "(Iteration 60201 / 61240) loss: 2.246689\n",
      "(Iteration 60301 / 61240) loss: 2.225499\n",
      "(Iteration 60401 / 61240) loss: 2.260121\n",
      "(Iteration 60501 / 61240) loss: 2.279237\n",
      "(Iteration 60601 / 61240) loss: 2.265320\n",
      "(Iteration 60701 / 61240) loss: 2.260788\n",
      "(Iteration 60801 / 61240) loss: 2.242144\n",
      "(Iteration 60901 / 61240) loss: 2.281292\n",
      "(Iteration 61001 / 61240) loss: 2.164274\n",
      "(Iteration 61101 / 61240) loss: 2.260432\n",
      "(Iteration 61201 / 61240) loss: 2.219643\n",
      "(Epoch 40 / 40) train acc: 0.240000; val_acc: 0.251000\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.001, 'num_epochs': 40, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 30600) loss: 2.302945\n",
      "(Epoch 0 / 40) train acc: 0.114000; val_acc: 0.084000\n",
      "(Iteration 101 / 30600) loss: 2.302928\n",
      "(Iteration 201 / 30600) loss: 2.302881\n",
      "(Iteration 301 / 30600) loss: 2.302829\n",
      "(Iteration 401 / 30600) loss: 2.302896\n",
      "(Iteration 501 / 30600) loss: 2.302732\n",
      "(Iteration 601 / 30600) loss: 2.302833\n",
      "(Iteration 701 / 30600) loss: 2.302653\n",
      "(Epoch 1 / 40) train acc: 0.128000; val_acc: 0.130000\n",
      "(Iteration 801 / 30600) loss: 2.302688\n",
      "(Iteration 901 / 30600) loss: 2.302491\n",
      "(Iteration 1001 / 30600) loss: 2.302874\n",
      "(Iteration 1101 / 30600) loss: 2.302630\n",
      "(Iteration 1201 / 30600) loss: 2.302569\n",
      "(Iteration 1301 / 30600) loss: 2.302730\n",
      "(Iteration 1401 / 30600) loss: 2.302730\n",
      "(Iteration 1501 / 30600) loss: 2.302636\n",
      "(Epoch 2 / 40) train acc: 0.091000; val_acc: 0.108000\n",
      "(Iteration 1601 / 30600) loss: 2.302539\n",
      "(Iteration 1701 / 30600) loss: 2.302535\n",
      "(Iteration 1801 / 30600) loss: 2.302727\n",
      "(Iteration 1901 / 30600) loss: 2.302511\n",
      "(Iteration 2001 / 30600) loss: 2.302692\n",
      "(Iteration 2101 / 30600) loss: 2.302424\n",
      "(Iteration 2201 / 30600) loss: 2.302517\n",
      "(Epoch 3 / 40) train acc: 0.116000; val_acc: 0.089000\n",
      "(Iteration 2301 / 30600) loss: 2.302504\n",
      "(Iteration 2401 / 30600) loss: 2.302426\n",
      "(Iteration 2501 / 30600) loss: 2.302318\n",
      "(Iteration 2601 / 30600) loss: 2.302464\n",
      "(Iteration 2701 / 30600) loss: 2.302568\n",
      "(Iteration 2801 / 30600) loss: 2.302320\n",
      "(Iteration 2901 / 30600) loss: 2.302215\n",
      "(Iteration 3001 / 30600) loss: 2.302375\n",
      "(Epoch 4 / 40) train acc: 0.166000; val_acc: 0.154000\n",
      "(Iteration 3101 / 30600) loss: 2.302087\n",
      "(Iteration 3201 / 30600) loss: 2.302552\n",
      "(Iteration 3301 / 30600) loss: 2.302340\n",
      "(Iteration 3401 / 30600) loss: 2.302107\n",
      "(Iteration 3501 / 30600) loss: 2.302358\n",
      "(Iteration 3601 / 30600) loss: 2.302253\n",
      "(Iteration 3701 / 30600) loss: 2.302142\n",
      "(Iteration 3801 / 30600) loss: 2.302395\n",
      "(Epoch 5 / 40) train acc: 0.132000; val_acc: 0.137000\n",
      "(Iteration 3901 / 30600) loss: 2.302238\n",
      "(Iteration 4001 / 30600) loss: 2.302180\n",
      "(Iteration 4101 / 30600) loss: 2.302127\n",
      "(Iteration 4201 / 30600) loss: 2.302062\n",
      "(Iteration 4301 / 30600) loss: 2.302204\n",
      "(Iteration 4401 / 30600) loss: 2.302188\n",
      "(Iteration 4501 / 30600) loss: 2.302320\n",
      "(Epoch 6 / 40) train acc: 0.276000; val_acc: 0.268000\n",
      "(Iteration 4601 / 30600) loss: 2.301916\n",
      "(Iteration 4701 / 30600) loss: 2.302184\n",
      "(Iteration 4801 / 30600) loss: 2.302101\n",
      "(Iteration 4901 / 30600) loss: 2.302113\n",
      "(Iteration 5001 / 30600) loss: 2.301794\n",
      "(Iteration 5101 / 30600) loss: 2.302020\n",
      "(Iteration 5201 / 30600) loss: 2.301623\n",
      "(Iteration 5301 / 30600) loss: 2.301630\n",
      "(Epoch 7 / 40) train acc: 0.256000; val_acc: 0.275000\n",
      "(Iteration 5401 / 30600) loss: 2.301737\n",
      "(Iteration 5501 / 30600) loss: 2.301864\n",
      "(Iteration 5601 / 30600) loss: 2.301674\n",
      "(Iteration 5701 / 30600) loss: 2.301357\n",
      "(Iteration 5801 / 30600) loss: 2.301069\n",
      "(Iteration 5901 / 30600) loss: 2.301395\n",
      "(Iteration 6001 / 30600) loss: 2.301307\n",
      "(Iteration 6101 / 30600) loss: 2.301192\n",
      "(Epoch 8 / 40) train acc: 0.240000; val_acc: 0.243000\n",
      "(Iteration 6201 / 30600) loss: 2.301507\n",
      "(Iteration 6301 / 30600) loss: 2.301409\n",
      "(Iteration 6401 / 30600) loss: 2.301681\n",
      "(Iteration 6501 / 30600) loss: 2.301222\n",
      "(Iteration 6601 / 30600) loss: 2.301412\n",
      "(Iteration 6701 / 30600) loss: 2.301495\n",
      "(Iteration 6801 / 30600) loss: 2.300878\n",
      "(Epoch 9 / 40) train acc: 0.221000; val_acc: 0.234000\n",
      "(Iteration 6901 / 30600) loss: 2.300305\n",
      "(Iteration 7001 / 30600) loss: 2.301032\n",
      "(Iteration 7101 / 30600) loss: 2.301068\n",
      "(Iteration 7201 / 30600) loss: 2.300995\n",
      "(Iteration 7301 / 30600) loss: 2.300752\n",
      "(Iteration 7401 / 30600) loss: 2.300622\n",
      "(Iteration 7501 / 30600) loss: 2.301114\n",
      "(Iteration 7601 / 30600) loss: 2.300809\n",
      "(Epoch 10 / 40) train acc: 0.237000; val_acc: 0.244000\n",
      "(Iteration 7701 / 30600) loss: 2.301061\n",
      "(Iteration 7801 / 30600) loss: 2.300773\n",
      "(Iteration 7901 / 30600) loss: 2.300440\n",
      "(Iteration 8001 / 30600) loss: 2.300550\n",
      "(Iteration 8101 / 30600) loss: 2.300608\n",
      "(Iteration 8201 / 30600) loss: 2.300635\n",
      "(Iteration 8301 / 30600) loss: 2.300161\n",
      "(Iteration 8401 / 30600) loss: 2.301146\n",
      "(Epoch 11 / 40) train acc: 0.255000; val_acc: 0.263000\n",
      "(Iteration 8501 / 30600) loss: 2.300631\n",
      "(Iteration 8601 / 30600) loss: 2.300063\n",
      "(Iteration 8701 / 30600) loss: 2.300388\n",
      "(Iteration 8801 / 30600) loss: 2.300884\n",
      "(Iteration 8901 / 30600) loss: 2.300626\n",
      "(Iteration 9001 / 30600) loss: 2.301092\n",
      "(Iteration 9101 / 30600) loss: 2.299625\n",
      "(Epoch 12 / 40) train acc: 0.234000; val_acc: 0.242000\n",
      "(Iteration 9201 / 30600) loss: 2.300461\n",
      "(Iteration 9301 / 30600) loss: 2.299754\n",
      "(Iteration 9401 / 30600) loss: 2.299389\n",
      "(Iteration 9501 / 30600) loss: 2.299341\n",
      "(Iteration 9601 / 30600) loss: 2.299929\n",
      "(Iteration 9701 / 30600) loss: 2.300184\n",
      "(Iteration 9801 / 30600) loss: 2.299999\n",
      "(Iteration 9901 / 30600) loss: 2.299890\n",
      "(Epoch 13 / 40) train acc: 0.231000; val_acc: 0.248000\n",
      "(Iteration 10001 / 30600) loss: 2.299844\n",
      "(Iteration 10101 / 30600) loss: 2.298119\n",
      "(Iteration 10201 / 30600) loss: 2.299169\n",
      "(Iteration 10301 / 30600) loss: 2.299727\n",
      "(Iteration 10401 / 30600) loss: 2.298962\n",
      "(Iteration 10501 / 30600) loss: 2.299556\n",
      "(Iteration 10601 / 30600) loss: 2.299576\n",
      "(Iteration 10701 / 30600) loss: 2.299095\n",
      "(Epoch 14 / 40) train acc: 0.294000; val_acc: 0.293000\n",
      "(Iteration 10801 / 30600) loss: 2.299932\n",
      "(Iteration 10901 / 30600) loss: 2.298969\n",
      "(Iteration 11001 / 30600) loss: 2.299502\n",
      "(Iteration 11101 / 30600) loss: 2.298381\n",
      "(Iteration 11201 / 30600) loss: 2.299247\n",
      "(Iteration 11301 / 30600) loss: 2.299025\n",
      "(Iteration 11401 / 30600) loss: 2.298551\n",
      "(Epoch 15 / 40) train acc: 0.284000; val_acc: 0.274000\n",
      "(Iteration 11501 / 30600) loss: 2.300116\n",
      "(Iteration 11601 / 30600) loss: 2.299002\n",
      "(Iteration 11701 / 30600) loss: 2.298988\n",
      "(Iteration 11801 / 30600) loss: 2.299330\n",
      "(Iteration 11901 / 30600) loss: 2.299461\n",
      "(Iteration 12001 / 30600) loss: 2.298990\n",
      "(Iteration 12101 / 30600) loss: 2.297882\n",
      "(Iteration 12201 / 30600) loss: 2.297697\n",
      "(Epoch 16 / 40) train acc: 0.260000; val_acc: 0.289000\n",
      "(Iteration 12301 / 30600) loss: 2.299505\n",
      "(Iteration 12401 / 30600) loss: 2.298025\n",
      "(Iteration 12501 / 30600) loss: 2.297370\n",
      "(Iteration 12601 / 30600) loss: 2.298242\n",
      "(Iteration 12701 / 30600) loss: 2.297708\n",
      "(Iteration 12801 / 30600) loss: 2.298241\n",
      "(Iteration 12901 / 30600) loss: 2.297779\n",
      "(Iteration 13001 / 30600) loss: 2.297642\n",
      "(Epoch 17 / 40) train acc: 0.288000; val_acc: 0.297000\n",
      "(Iteration 13101 / 30600) loss: 2.296633\n",
      "(Iteration 13201 / 30600) loss: 2.296814\n",
      "(Iteration 13301 / 30600) loss: 2.296732\n",
      "(Iteration 13401 / 30600) loss: 2.296975\n",
      "(Iteration 13501 / 30600) loss: 2.296130\n",
      "(Iteration 13601 / 30600) loss: 2.296713\n",
      "(Iteration 13701 / 30600) loss: 2.298516\n",
      "(Epoch 18 / 40) train acc: 0.301000; val_acc: 0.286000\n",
      "(Iteration 13801 / 30600) loss: 2.296843\n",
      "(Iteration 13901 / 30600) loss: 2.297740\n",
      "(Iteration 14001 / 30600) loss: 2.298254\n",
      "(Iteration 14101 / 30600) loss: 2.297778\n",
      "(Iteration 14201 / 30600) loss: 2.296043\n",
      "(Iteration 14301 / 30600) loss: 2.298087\n",
      "(Iteration 14401 / 30600) loss: 2.297845\n",
      "(Iteration 14501 / 30600) loss: 2.299344\n",
      "(Epoch 19 / 40) train acc: 0.280000; val_acc: 0.282000\n",
      "(Iteration 14601 / 30600) loss: 2.296547\n",
      "(Iteration 14701 / 30600) loss: 2.298351\n",
      "(Iteration 14801 / 30600) loss: 2.298736\n",
      "(Iteration 14901 / 30600) loss: 2.297461\n",
      "(Iteration 15001 / 30600) loss: 2.295151\n",
      "(Iteration 15101 / 30600) loss: 2.296232\n",
      "(Iteration 15201 / 30600) loss: 2.293926\n",
      "(Epoch 20 / 40) train acc: 0.290000; val_acc: 0.278000\n",
      "(Iteration 15301 / 30600) loss: 2.296023\n",
      "(Iteration 15401 / 30600) loss: 2.297752\n",
      "(Iteration 15501 / 30600) loss: 2.295610\n",
      "(Iteration 15601 / 30600) loss: 2.296422\n",
      "(Iteration 15701 / 30600) loss: 2.295940\n",
      "(Iteration 15801 / 30600) loss: 2.297801\n",
      "(Iteration 15901 / 30600) loss: 2.295114\n",
      "(Iteration 16001 / 30600) loss: 2.294533\n",
      "(Epoch 21 / 40) train acc: 0.283000; val_acc: 0.289000\n",
      "(Iteration 16101 / 30600) loss: 2.296744\n",
      "(Iteration 16201 / 30600) loss: 2.295252\n",
      "(Iteration 16301 / 30600) loss: 2.295738\n",
      "(Iteration 16401 / 30600) loss: 2.296628\n",
      "(Iteration 16501 / 30600) loss: 2.294932\n",
      "(Iteration 16601 / 30600) loss: 2.294802\n",
      "(Iteration 16701 / 30600) loss: 2.296954\n",
      "(Iteration 16801 / 30600) loss: 2.297683\n",
      "(Epoch 22 / 40) train acc: 0.328000; val_acc: 0.291000\n",
      "(Iteration 16901 / 30600) loss: 2.295985\n",
      "(Iteration 17001 / 30600) loss: 2.294404\n",
      "(Iteration 17101 / 30600) loss: 2.294185\n",
      "(Iteration 17201 / 30600) loss: 2.294291\n",
      "(Iteration 17301 / 30600) loss: 2.295704\n",
      "(Iteration 17401 / 30600) loss: 2.296300\n",
      "(Iteration 17501 / 30600) loss: 2.293046\n",
      "(Epoch 23 / 40) train acc: 0.314000; val_acc: 0.288000\n",
      "(Iteration 17601 / 30600) loss: 2.297316\n",
      "(Iteration 17701 / 30600) loss: 2.295433\n",
      "(Iteration 17801 / 30600) loss: 2.295148\n",
      "(Iteration 17901 / 30600) loss: 2.297061\n",
      "(Iteration 18001 / 30600) loss: 2.296786\n",
      "(Iteration 18101 / 30600) loss: 2.294352\n",
      "(Iteration 18201 / 30600) loss: 2.295147\n",
      "(Iteration 18301 / 30600) loss: 2.292890\n",
      "(Epoch 24 / 40) train acc: 0.280000; val_acc: 0.286000\n",
      "(Iteration 18401 / 30600) loss: 2.295442\n",
      "(Iteration 18501 / 30600) loss: 2.291260\n",
      "(Iteration 18601 / 30600) loss: 2.295927\n",
      "(Iteration 18701 / 30600) loss: 2.295545\n",
      "(Iteration 18801 / 30600) loss: 2.296085\n",
      "(Iteration 18901 / 30600) loss: 2.296794\n",
      "(Iteration 19001 / 30600) loss: 2.294548\n",
      "(Iteration 19101 / 30600) loss: 2.295739\n",
      "(Epoch 25 / 40) train acc: 0.306000; val_acc: 0.288000\n",
      "(Iteration 19201 / 30600) loss: 2.292784\n",
      "(Iteration 19301 / 30600) loss: 2.298109\n",
      "(Iteration 19401 / 30600) loss: 2.295844\n",
      "(Iteration 19501 / 30600) loss: 2.296711\n",
      "(Iteration 19601 / 30600) loss: 2.295750\n",
      "(Iteration 19701 / 30600) loss: 2.292424\n",
      "(Iteration 19801 / 30600) loss: 2.292936\n",
      "(Epoch 26 / 40) train acc: 0.322000; val_acc: 0.286000\n",
      "(Iteration 19901 / 30600) loss: 2.293158\n",
      "(Iteration 20001 / 30600) loss: 2.293730\n",
      "(Iteration 20101 / 30600) loss: 2.294259\n",
      "(Iteration 20201 / 30600) loss: 2.293026\n",
      "(Iteration 20301 / 30600) loss: 2.289916\n",
      "(Iteration 20401 / 30600) loss: 2.293748\n",
      "(Iteration 20501 / 30600) loss: 2.295732\n",
      "(Iteration 20601 / 30600) loss: 2.296840\n",
      "(Epoch 27 / 40) train acc: 0.321000; val_acc: 0.287000\n",
      "(Iteration 20701 / 30600) loss: 2.295318\n",
      "(Iteration 20801 / 30600) loss: 2.295739\n",
      "(Iteration 20901 / 30600) loss: 2.293519\n",
      "(Iteration 21001 / 30600) loss: 2.292575\n",
      "(Iteration 21101 / 30600) loss: 2.294048\n",
      "(Iteration 21201 / 30600) loss: 2.292975\n",
      "(Iteration 21301 / 30600) loss: 2.296653\n",
      "(Iteration 21401 / 30600) loss: 2.292167\n",
      "(Epoch 28 / 40) train acc: 0.313000; val_acc: 0.286000\n",
      "(Iteration 21501 / 30600) loss: 2.294455\n",
      "(Iteration 21601 / 30600) loss: 2.293095\n",
      "(Iteration 21701 / 30600) loss: 2.293538\n",
      "(Iteration 21801 / 30600) loss: 2.292364\n",
      "(Iteration 21901 / 30600) loss: 2.288919\n",
      "(Iteration 22001 / 30600) loss: 2.294234\n",
      "(Iteration 22101 / 30600) loss: 2.292769\n",
      "(Epoch 29 / 40) train acc: 0.306000; val_acc: 0.292000\n",
      "(Iteration 22201 / 30600) loss: 2.295414\n",
      "(Iteration 22301 / 30600) loss: 2.295283\n",
      "(Iteration 22401 / 30600) loss: 2.295185\n",
      "(Iteration 22501 / 30600) loss: 2.293566\n",
      "(Iteration 22601 / 30600) loss: 2.293723\n",
      "(Iteration 22701 / 30600) loss: 2.295338\n",
      "(Iteration 22801 / 30600) loss: 2.294683\n",
      "(Iteration 22901 / 30600) loss: 2.293311\n",
      "(Epoch 30 / 40) train acc: 0.313000; val_acc: 0.286000\n",
      "(Iteration 23001 / 30600) loss: 2.295446\n",
      "(Iteration 23101 / 30600) loss: 2.295791\n",
      "(Iteration 23201 / 30600) loss: 2.292235\n",
      "(Iteration 23301 / 30600) loss: 2.293603\n",
      "(Iteration 23401 / 30600) loss: 2.292498\n",
      "(Iteration 23501 / 30600) loss: 2.292981\n",
      "(Iteration 23601 / 30600) loss: 2.289350\n",
      "(Iteration 23701 / 30600) loss: 2.292413\n",
      "(Epoch 31 / 40) train acc: 0.309000; val_acc: 0.286000\n",
      "(Iteration 23801 / 30600) loss: 2.291870\n",
      "(Iteration 23901 / 30600) loss: 2.294826\n",
      "(Iteration 24001 / 30600) loss: 2.295022\n",
      "(Iteration 24101 / 30600) loss: 2.290749\n",
      "(Iteration 24201 / 30600) loss: 2.292446\n",
      "(Iteration 24301 / 30600) loss: 2.294867\n",
      "(Iteration 24401 / 30600) loss: 2.291816\n",
      "(Epoch 32 / 40) train acc: 0.322000; val_acc: 0.287000\n",
      "(Iteration 24501 / 30600) loss: 2.292311\n",
      "(Iteration 24601 / 30600) loss: 2.294856\n",
      "(Iteration 24701 / 30600) loss: 2.294407\n",
      "(Iteration 24801 / 30600) loss: 2.293792\n",
      "(Iteration 24901 / 30600) loss: 2.290628\n",
      "(Iteration 25001 / 30600) loss: 2.294922\n",
      "(Iteration 25101 / 30600) loss: 2.291855\n",
      "(Iteration 25201 / 30600) loss: 2.294012\n",
      "(Epoch 33 / 40) train acc: 0.330000; val_acc: 0.291000\n",
      "(Iteration 25301 / 30600) loss: 2.289631\n",
      "(Iteration 25401 / 30600) loss: 2.292616\n",
      "(Iteration 25501 / 30600) loss: 2.292047\n",
      "(Iteration 25601 / 30600) loss: 2.294787\n",
      "(Iteration 25701 / 30600) loss: 2.292242\n",
      "(Iteration 25801 / 30600) loss: 2.293526\n",
      "(Iteration 25901 / 30600) loss: 2.291896\n",
      "(Iteration 26001 / 30600) loss: 2.291566\n",
      "(Epoch 34 / 40) train acc: 0.329000; val_acc: 0.298000\n",
      "(Iteration 26101 / 30600) loss: 2.294715\n",
      "(Iteration 26201 / 30600) loss: 2.292994\n",
      "(Iteration 26301 / 30600) loss: 2.290853\n",
      "(Iteration 26401 / 30600) loss: 2.290926\n",
      "(Iteration 26501 / 30600) loss: 2.293039\n",
      "(Iteration 26601 / 30600) loss: 2.295638\n",
      "(Iteration 26701 / 30600) loss: 2.294690\n",
      "(Epoch 35 / 40) train acc: 0.303000; val_acc: 0.293000\n",
      "(Iteration 26801 / 30600) loss: 2.294718\n",
      "(Iteration 26901 / 30600) loss: 2.290808\n",
      "(Iteration 27001 / 30600) loss: 2.295128\n",
      "(Iteration 27101 / 30600) loss: 2.293452\n",
      "(Iteration 27201 / 30600) loss: 2.293194\n",
      "(Iteration 27301 / 30600) loss: 2.295653\n",
      "(Iteration 27401 / 30600) loss: 2.297310\n",
      "(Iteration 27501 / 30600) loss: 2.292424\n",
      "(Epoch 36 / 40) train acc: 0.299000; val_acc: 0.289000\n",
      "(Iteration 27601 / 30600) loss: 2.294310\n",
      "(Iteration 27701 / 30600) loss: 2.294588\n",
      "(Iteration 27801 / 30600) loss: 2.294527\n",
      "(Iteration 27901 / 30600) loss: 2.291289\n",
      "(Iteration 28001 / 30600) loss: 2.292607\n",
      "(Iteration 28101 / 30600) loss: 2.292814\n",
      "(Iteration 28201 / 30600) loss: 2.291749\n",
      "(Iteration 28301 / 30600) loss: 2.292101\n",
      "(Epoch 37 / 40) train acc: 0.310000; val_acc: 0.289000\n",
      "(Iteration 28401 / 30600) loss: 2.292804\n",
      "(Iteration 28501 / 30600) loss: 2.292588\n",
      "(Iteration 28601 / 30600) loss: 2.295447\n",
      "(Iteration 28701 / 30600) loss: 2.289763\n",
      "(Iteration 28801 / 30600) loss: 2.291019\n",
      "(Iteration 28901 / 30600) loss: 2.295216\n",
      "(Iteration 29001 / 30600) loss: 2.294381\n",
      "(Epoch 38 / 40) train acc: 0.296000; val_acc: 0.289000\n",
      "(Iteration 29101 / 30600) loss: 2.294443\n",
      "(Iteration 29201 / 30600) loss: 2.289850\n",
      "(Iteration 29301 / 30600) loss: 2.291096\n",
      "(Iteration 29401 / 30600) loss: 2.292985\n",
      "(Iteration 29501 / 30600) loss: 2.295114\n",
      "(Iteration 29601 / 30600) loss: 2.292913\n",
      "(Iteration 29701 / 30600) loss: 2.295446\n",
      "(Iteration 29801 / 30600) loss: 2.294776\n",
      "(Epoch 39 / 40) train acc: 0.298000; val_acc: 0.291000\n",
      "(Iteration 29901 / 30600) loss: 2.290447\n",
      "(Iteration 30001 / 30600) loss: 2.292492\n",
      "(Iteration 30101 / 30600) loss: 2.290182\n",
      "(Iteration 30201 / 30600) loss: 2.289796\n",
      "(Iteration 30301 / 30600) loss: 2.291944\n",
      "(Iteration 30401 / 30600) loss: 2.296132\n",
      "(Iteration 30501 / 30600) loss: 2.291450\n",
      "(Epoch 40 / 40) train acc: 0.299000; val_acc: 0.291000\n",
      "New best model found with validation accuracy: 0.2561\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.001, 'num_epochs': 40, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 61240) loss: 2.302892\n",
      "(Epoch 0 / 40) train acc: 0.090000; val_acc: 0.103000\n",
      "(Iteration 101 / 61240) loss: 2.302924\n",
      "(Iteration 201 / 61240) loss: 2.302946\n",
      "(Iteration 301 / 61240) loss: 2.302987\n",
      "(Iteration 401 / 61240) loss: 2.303096\n",
      "(Iteration 501 / 61240) loss: 2.303104\n",
      "(Iteration 601 / 61240) loss: 2.302905\n",
      "(Iteration 701 / 61240) loss: 2.302822\n",
      "(Iteration 801 / 61240) loss: 2.302711\n",
      "(Iteration 901 / 61240) loss: 2.302649\n",
      "(Iteration 1001 / 61240) loss: 2.302597\n",
      "(Iteration 1101 / 61240) loss: 2.302635\n",
      "(Iteration 1201 / 61240) loss: 2.302932\n",
      "(Iteration 1301 / 61240) loss: 2.302563\n",
      "(Iteration 1401 / 61240) loss: 2.302660\n",
      "(Iteration 1501 / 61240) loss: 2.302837\n",
      "(Epoch 1 / 40) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 1601 / 61240) loss: 2.302813\n",
      "(Iteration 1701 / 61240) loss: 2.302667\n",
      "(Iteration 1801 / 61240) loss: 2.302502\n",
      "(Iteration 1901 / 61240) loss: 2.302607\n",
      "(Iteration 2001 / 61240) loss: 2.302155\n",
      "(Iteration 2101 / 61240) loss: 2.302820\n",
      "(Iteration 2201 / 61240) loss: 2.302449\n",
      "(Iteration 2301 / 61240) loss: 2.302865\n",
      "(Iteration 2401 / 61240) loss: 2.301922\n",
      "(Iteration 2501 / 61240) loss: 2.301504\n",
      "(Iteration 2601 / 61240) loss: 2.301652\n",
      "(Iteration 2701 / 61240) loss: 2.302334\n",
      "(Iteration 2801 / 61240) loss: 2.301995\n",
      "(Iteration 2901 / 61240) loss: 2.302833\n",
      "(Iteration 3001 / 61240) loss: 2.302374\n",
      "(Epoch 2 / 40) train acc: 0.153000; val_acc: 0.105000\n",
      "(Iteration 3101 / 61240) loss: 2.302765\n",
      "(Iteration 3201 / 61240) loss: 2.302251\n",
      "(Iteration 3301 / 61240) loss: 2.301265\n",
      "(Iteration 3401 / 61240) loss: 2.302627\n",
      "(Iteration 3501 / 61240) loss: 2.302278\n",
      "(Iteration 3601 / 61240) loss: 2.302541\n",
      "(Iteration 3701 / 61240) loss: 2.301649\n",
      "(Iteration 3801 / 61240) loss: 2.302116\n",
      "(Iteration 3901 / 61240) loss: 2.302997\n",
      "(Iteration 4001 / 61240) loss: 2.301774\n",
      "(Iteration 4101 / 61240) loss: 2.301677\n",
      "(Iteration 4201 / 61240) loss: 2.301741\n",
      "(Iteration 4301 / 61240) loss: 2.303110\n",
      "(Iteration 4401 / 61240) loss: 2.302175\n",
      "(Iteration 4501 / 61240) loss: 2.300997\n",
      "(Epoch 3 / 40) train acc: 0.116000; val_acc: 0.107000\n",
      "(Iteration 4601 / 61240) loss: 2.301634\n",
      "(Iteration 4701 / 61240) loss: 2.301877\n",
      "(Iteration 4801 / 61240) loss: 2.301274\n",
      "(Iteration 4901 / 61240) loss: 2.301768\n",
      "(Iteration 5001 / 61240) loss: 2.302003\n",
      "(Iteration 5101 / 61240) loss: 2.300932\n",
      "(Iteration 5201 / 61240) loss: 2.300743\n",
      "(Iteration 5301 / 61240) loss: 2.301168\n",
      "(Iteration 5401 / 61240) loss: 2.301541\n",
      "(Iteration 5501 / 61240) loss: 2.301407\n",
      "(Iteration 5601 / 61240) loss: 2.301666\n",
      "(Iteration 5701 / 61240) loss: 2.302399\n",
      "(Iteration 5801 / 61240) loss: 2.301066\n",
      "(Iteration 5901 / 61240) loss: 2.299956\n",
      "(Iteration 6001 / 61240) loss: 2.300970\n",
      "(Iteration 6101 / 61240) loss: 2.300686\n",
      "(Epoch 4 / 40) train acc: 0.168000; val_acc: 0.178000\n",
      "(Iteration 6201 / 61240) loss: 2.300032\n",
      "(Iteration 6301 / 61240) loss: 2.300047\n",
      "(Iteration 6401 / 61240) loss: 2.300467\n",
      "(Iteration 6501 / 61240) loss: 2.299875\n",
      "(Iteration 6601 / 61240) loss: 2.301566\n",
      "(Iteration 6701 / 61240) loss: 2.299222\n",
      "(Iteration 6801 / 61240) loss: 2.299910\n",
      "(Iteration 6901 / 61240) loss: 2.298536\n",
      "(Iteration 7001 / 61240) loss: 2.300135\n",
      "(Iteration 7101 / 61240) loss: 2.300041\n",
      "(Iteration 7201 / 61240) loss: 2.298553\n",
      "(Iteration 7301 / 61240) loss: 2.298354\n",
      "(Iteration 7401 / 61240) loss: 2.299842\n",
      "(Iteration 7501 / 61240) loss: 2.297329\n",
      "(Iteration 7601 / 61240) loss: 2.299215\n",
      "(Epoch 5 / 40) train acc: 0.217000; val_acc: 0.200000\n",
      "(Iteration 7701 / 61240) loss: 2.299523\n",
      "(Iteration 7801 / 61240) loss: 2.299017\n",
      "(Iteration 7901 / 61240) loss: 2.298634\n",
      "(Iteration 8001 / 61240) loss: 2.296942\n",
      "(Iteration 8101 / 61240) loss: 2.295808\n",
      "(Iteration 8201 / 61240) loss: 2.297086\n",
      "(Iteration 8301 / 61240) loss: 2.296563\n",
      "(Iteration 8401 / 61240) loss: 2.297187\n",
      "(Iteration 8501 / 61240) loss: 2.297322\n",
      "(Iteration 8601 / 61240) loss: 2.295931\n",
      "(Iteration 8701 / 61240) loss: 2.297885\n",
      "(Iteration 8801 / 61240) loss: 2.298031\n",
      "(Iteration 8901 / 61240) loss: 2.296224\n",
      "(Iteration 9001 / 61240) loss: 2.296567\n",
      "(Iteration 9101 / 61240) loss: 2.296217\n",
      "(Epoch 6 / 40) train acc: 0.231000; val_acc: 0.220000\n",
      "(Iteration 9201 / 61240) loss: 2.296455\n",
      "(Iteration 9301 / 61240) loss: 2.294745\n",
      "(Iteration 9401 / 61240) loss: 2.296309\n",
      "(Iteration 9501 / 61240) loss: 2.291017\n",
      "(Iteration 9601 / 61240) loss: 2.296149\n",
      "(Iteration 9701 / 61240) loss: 2.291870\n",
      "(Iteration 9801 / 61240) loss: 2.293884\n",
      "(Iteration 9901 / 61240) loss: 2.291869\n",
      "(Iteration 10001 / 61240) loss: 2.296989\n",
      "(Iteration 10101 / 61240) loss: 2.293072\n",
      "(Iteration 10201 / 61240) loss: 2.296558\n",
      "(Iteration 10301 / 61240) loss: 2.294634\n",
      "(Iteration 10401 / 61240) loss: 2.289752\n",
      "(Iteration 10501 / 61240) loss: 2.293207\n",
      "(Iteration 10601 / 61240) loss: 2.291891\n",
      "(Iteration 10701 / 61240) loss: 2.295276\n",
      "(Epoch 7 / 40) train acc: 0.242000; val_acc: 0.248000\n",
      "(Iteration 10801 / 61240) loss: 2.292162\n",
      "(Iteration 10901 / 61240) loss: 2.290657\n",
      "(Iteration 11001 / 61240) loss: 2.286190\n",
      "(Iteration 11101 / 61240) loss: 2.296575\n",
      "(Iteration 11201 / 61240) loss: 2.286543\n",
      "(Iteration 11301 / 61240) loss: 2.294037\n",
      "(Iteration 11401 / 61240) loss: 2.290715\n",
      "(Iteration 11501 / 61240) loss: 2.291527\n",
      "(Iteration 11601 / 61240) loss: 2.290672\n",
      "(Iteration 11701 / 61240) loss: 2.284936\n",
      "(Iteration 11801 / 61240) loss: 2.279515\n",
      "(Iteration 11901 / 61240) loss: 2.277102\n",
      "(Iteration 12001 / 61240) loss: 2.286126\n",
      "(Iteration 12101 / 61240) loss: 2.295102\n",
      "(Iteration 12201 / 61240) loss: 2.285551\n",
      "(Epoch 8 / 40) train acc: 0.249000; val_acc: 0.255000\n",
      "(Iteration 12301 / 61240) loss: 2.290860\n",
      "(Iteration 12401 / 61240) loss: 2.295625\n",
      "(Iteration 12501 / 61240) loss: 2.283664\n",
      "(Iteration 12601 / 61240) loss: 2.280961\n",
      "(Iteration 12701 / 61240) loss: 2.278336\n",
      "(Iteration 12801 / 61240) loss: 2.285313\n",
      "(Iteration 12901 / 61240) loss: 2.284554\n",
      "(Iteration 13001 / 61240) loss: 2.289133\n",
      "(Iteration 13101 / 61240) loss: 2.276458\n",
      "(Iteration 13201 / 61240) loss: 2.279538\n",
      "(Iteration 13301 / 61240) loss: 2.258347\n",
      "(Iteration 13401 / 61240) loss: 2.273856\n",
      "(Iteration 13501 / 61240) loss: 2.266754\n",
      "(Iteration 13601 / 61240) loss: 2.269705\n",
      "(Iteration 13701 / 61240) loss: 2.283948\n",
      "(Epoch 9 / 40) train acc: 0.244000; val_acc: 0.264000\n",
      "(Iteration 13801 / 61240) loss: 2.284068\n",
      "(Iteration 13901 / 61240) loss: 2.272148\n",
      "(Iteration 14001 / 61240) loss: 2.268729\n",
      "(Iteration 14101 / 61240) loss: 2.268804\n",
      "(Iteration 14201 / 61240) loss: 2.272045\n",
      "(Iteration 14301 / 61240) loss: 2.265991\n",
      "(Iteration 14401 / 61240) loss: 2.273931\n",
      "(Iteration 14501 / 61240) loss: 2.272054\n",
      "(Iteration 14601 / 61240) loss: 2.250394\n",
      "(Iteration 14701 / 61240) loss: 2.276268\n",
      "(Iteration 14801 / 61240) loss: 2.278049\n",
      "(Iteration 14901 / 61240) loss: 2.262596\n",
      "(Iteration 15001 / 61240) loss: 2.269194\n",
      "(Iteration 15101 / 61240) loss: 2.271837\n",
      "(Iteration 15201 / 61240) loss: 2.268463\n",
      "(Iteration 15301 / 61240) loss: 2.267408\n",
      "(Epoch 10 / 40) train acc: 0.250000; val_acc: 0.270000\n",
      "(Iteration 15401 / 61240) loss: 2.267443\n",
      "(Iteration 15501 / 61240) loss: 2.259065\n",
      "(Iteration 15601 / 61240) loss: 2.273958\n",
      "(Iteration 15701 / 61240) loss: 2.284429\n",
      "(Iteration 15801 / 61240) loss: 2.279481\n",
      "(Iteration 15901 / 61240) loss: 2.268127\n",
      "(Iteration 16001 / 61240) loss: 2.281688\n",
      "(Iteration 16101 / 61240) loss: 2.248968\n",
      "(Iteration 16201 / 61240) loss: 2.267322\n",
      "(Iteration 16301 / 61240) loss: 2.255390\n",
      "(Iteration 16401 / 61240) loss: 2.254029\n",
      "(Iteration 16501 / 61240) loss: 2.259381\n",
      "(Iteration 16601 / 61240) loss: 2.250348\n",
      "(Iteration 16701 / 61240) loss: 2.267594\n",
      "(Iteration 16801 / 61240) loss: 2.233284\n",
      "(Epoch 11 / 40) train acc: 0.262000; val_acc: 0.263000\n",
      "(Iteration 16901 / 61240) loss: 2.236980\n",
      "(Iteration 17001 / 61240) loss: 2.268298\n",
      "(Iteration 17101 / 61240) loss: 2.237620\n",
      "(Iteration 17201 / 61240) loss: 2.248860\n",
      "(Iteration 17301 / 61240) loss: 2.242150\n",
      "(Iteration 17401 / 61240) loss: 2.244406\n",
      "(Iteration 17501 / 61240) loss: 2.215180\n",
      "(Iteration 17601 / 61240) loss: 2.236998\n",
      "(Iteration 17701 / 61240) loss: 2.229968\n",
      "(Iteration 17801 / 61240) loss: 2.215070\n",
      "(Iteration 17901 / 61240) loss: 2.237195\n",
      "(Iteration 18001 / 61240) loss: 2.205374\n",
      "(Iteration 18101 / 61240) loss: 2.142550\n",
      "(Iteration 18201 / 61240) loss: 2.219220\n",
      "(Iteration 18301 / 61240) loss: 2.220520\n",
      "(Epoch 12 / 40) train acc: 0.275000; val_acc: 0.255000\n",
      "(Iteration 18401 / 61240) loss: 2.197506\n",
      "(Iteration 18501 / 61240) loss: 2.240330\n",
      "(Iteration 18601 / 61240) loss: 2.255607\n",
      "(Iteration 18701 / 61240) loss: 2.237601\n",
      "(Iteration 18801 / 61240) loss: 2.243898\n",
      "(Iteration 18901 / 61240) loss: 2.211845\n",
      "(Iteration 19001 / 61240) loss: 2.256507\n",
      "(Iteration 19101 / 61240) loss: 2.230874\n",
      "(Iteration 19201 / 61240) loss: 2.251461\n",
      "(Iteration 19301 / 61240) loss: 2.204901\n",
      "(Iteration 19401 / 61240) loss: 2.196788\n",
      "(Iteration 19501 / 61240) loss: 2.217043\n",
      "(Iteration 19601 / 61240) loss: 2.246914\n",
      "(Iteration 19701 / 61240) loss: 2.236193\n",
      "(Iteration 19801 / 61240) loss: 2.216728\n",
      "(Iteration 19901 / 61240) loss: 2.190510\n",
      "(Epoch 13 / 40) train acc: 0.261000; val_acc: 0.259000\n",
      "(Iteration 20001 / 61240) loss: 2.242903\n",
      "(Iteration 20101 / 61240) loss: 2.243272\n",
      "(Iteration 20201 / 61240) loss: 2.218301\n",
      "(Iteration 20301 / 61240) loss: 2.229598\n",
      "(Iteration 20401 / 61240) loss: 2.215605\n",
      "(Iteration 20501 / 61240) loss: 2.170354\n",
      "(Iteration 20601 / 61240) loss: 2.247760\n",
      "(Iteration 20701 / 61240) loss: 2.192755\n",
      "(Iteration 20801 / 61240) loss: 2.161201\n",
      "(Iteration 20901 / 61240) loss: 2.141806\n",
      "(Iteration 21001 / 61240) loss: 2.213163\n",
      "(Iteration 21101 / 61240) loss: 2.189404\n",
      "(Iteration 21201 / 61240) loss: 2.215245\n",
      "(Iteration 21301 / 61240) loss: 2.215730\n",
      "(Iteration 21401 / 61240) loss: 2.221032\n",
      "(Epoch 14 / 40) train acc: 0.249000; val_acc: 0.251000\n",
      "(Iteration 21501 / 61240) loss: 2.157304\n",
      "(Iteration 21601 / 61240) loss: 2.201471\n",
      "(Iteration 21701 / 61240) loss: 2.161669\n",
      "(Iteration 21801 / 61240) loss: 2.248219\n",
      "(Iteration 21901 / 61240) loss: 2.186262\n",
      "(Iteration 22001 / 61240) loss: 2.188898\n",
      "(Iteration 22101 / 61240) loss: 2.207243\n",
      "(Iteration 22201 / 61240) loss: 2.261431\n",
      "(Iteration 22301 / 61240) loss: 2.176481\n",
      "(Iteration 22401 / 61240) loss: 2.165977\n",
      "(Iteration 22501 / 61240) loss: 2.212020\n",
      "(Iteration 22601 / 61240) loss: 2.153871\n",
      "(Iteration 22701 / 61240) loss: 2.198811\n",
      "(Iteration 22801 / 61240) loss: 2.169113\n",
      "(Iteration 22901 / 61240) loss: 2.161047\n",
      "(Epoch 15 / 40) train acc: 0.252000; val_acc: 0.256000\n",
      "(Iteration 23001 / 61240) loss: 2.164912\n",
      "(Iteration 23101 / 61240) loss: 2.167229\n",
      "(Iteration 23201 / 61240) loss: 2.207208\n",
      "(Iteration 23301 / 61240) loss: 2.185649\n",
      "(Iteration 23401 / 61240) loss: 2.149004\n",
      "(Iteration 23501 / 61240) loss: 2.156262\n",
      "(Iteration 23601 / 61240) loss: 2.185140\n",
      "(Iteration 23701 / 61240) loss: 2.163439\n",
      "(Iteration 23801 / 61240) loss: 2.191431\n",
      "(Iteration 23901 / 61240) loss: 2.144486\n",
      "(Iteration 24001 / 61240) loss: 2.144831\n",
      "(Iteration 24101 / 61240) loss: 2.193757\n",
      "(Iteration 24201 / 61240) loss: 2.150311\n",
      "(Iteration 24301 / 61240) loss: 2.185521\n",
      "(Iteration 24401 / 61240) loss: 2.130319\n",
      "(Epoch 16 / 40) train acc: 0.255000; val_acc: 0.256000\n",
      "(Iteration 24501 / 61240) loss: 2.118057\n",
      "(Iteration 24601 / 61240) loss: 2.158131\n",
      "(Iteration 24701 / 61240) loss: 2.158151\n",
      "(Iteration 24801 / 61240) loss: 2.150853\n",
      "(Iteration 24901 / 61240) loss: 2.254928\n",
      "(Iteration 25001 / 61240) loss: 2.147964\n",
      "(Iteration 25101 / 61240) loss: 2.208414\n",
      "(Iteration 25201 / 61240) loss: 2.176394\n",
      "(Iteration 25301 / 61240) loss: 2.204270\n",
      "(Iteration 25401 / 61240) loss: 2.162257\n",
      "(Iteration 25501 / 61240) loss: 2.141024\n",
      "(Iteration 25601 / 61240) loss: 2.165710\n",
      "(Iteration 25701 / 61240) loss: 2.166352\n",
      "(Iteration 25801 / 61240) loss: 2.147651\n",
      "(Iteration 25901 / 61240) loss: 2.118558\n",
      "(Iteration 26001 / 61240) loss: 2.125677\n",
      "(Epoch 17 / 40) train acc: 0.241000; val_acc: 0.261000\n",
      "(Iteration 26101 / 61240) loss: 2.144586\n",
      "(Iteration 26201 / 61240) loss: 2.142407\n",
      "(Iteration 26301 / 61240) loss: 2.201711\n",
      "(Iteration 26401 / 61240) loss: 2.088297\n",
      "(Iteration 26501 / 61240) loss: 2.131989\n",
      "(Iteration 26601 / 61240) loss: 2.087451\n",
      "(Iteration 26701 / 61240) loss: 2.158466\n",
      "(Iteration 26801 / 61240) loss: 2.129395\n",
      "(Iteration 26901 / 61240) loss: 2.129181\n",
      "(Iteration 27001 / 61240) loss: 2.165165\n",
      "(Iteration 27101 / 61240) loss: 2.139197\n",
      "(Iteration 27201 / 61240) loss: 2.110106\n",
      "(Iteration 27301 / 61240) loss: 2.182753\n",
      "(Iteration 27401 / 61240) loss: 2.200453\n",
      "(Iteration 27501 / 61240) loss: 2.175738\n",
      "(Epoch 18 / 40) train acc: 0.250000; val_acc: 0.265000\n",
      "(Iteration 27601 / 61240) loss: 2.119244\n",
      "(Iteration 27701 / 61240) loss: 2.188496\n",
      "(Iteration 27801 / 61240) loss: 2.135093\n",
      "(Iteration 27901 / 61240) loss: 2.126303\n",
      "(Iteration 28001 / 61240) loss: 2.138466\n",
      "(Iteration 28101 / 61240) loss: 2.088994\n",
      "(Iteration 28201 / 61240) loss: 2.120450\n",
      "(Iteration 28301 / 61240) loss: 2.195541\n",
      "(Iteration 28401 / 61240) loss: 2.160855\n",
      "(Iteration 28501 / 61240) loss: 2.139202\n",
      "(Iteration 28601 / 61240) loss: 2.121077\n",
      "(Iteration 28701 / 61240) loss: 2.135067\n",
      "(Iteration 28801 / 61240) loss: 2.133059\n",
      "(Iteration 28901 / 61240) loss: 2.163264\n",
      "(Iteration 29001 / 61240) loss: 2.148113\n",
      "(Epoch 19 / 40) train acc: 0.248000; val_acc: 0.267000\n",
      "(Iteration 29101 / 61240) loss: 2.065290\n",
      "(Iteration 29201 / 61240) loss: 2.041759\n",
      "(Iteration 29301 / 61240) loss: 2.102881\n",
      "(Iteration 29401 / 61240) loss: 2.110044\n",
      "(Iteration 29501 / 61240) loss: 2.219020\n",
      "(Iteration 29601 / 61240) loss: 2.215381\n",
      "(Iteration 29701 / 61240) loss: 2.097273\n",
      "(Iteration 29801 / 61240) loss: 2.120005\n",
      "(Iteration 29901 / 61240) loss: 2.171816\n",
      "(Iteration 30001 / 61240) loss: 2.102425\n",
      "(Iteration 30101 / 61240) loss: 2.096690\n",
      "(Iteration 30201 / 61240) loss: 2.145003\n",
      "(Iteration 30301 / 61240) loss: 2.231623\n",
      "(Iteration 30401 / 61240) loss: 1.987103\n",
      "(Iteration 30501 / 61240) loss: 2.101752\n",
      "(Iteration 30601 / 61240) loss: 2.125345\n",
      "(Epoch 20 / 40) train acc: 0.252000; val_acc: 0.264000\n",
      "(Iteration 30701 / 61240) loss: 2.126408\n",
      "(Iteration 30801 / 61240) loss: 2.091808\n",
      "(Iteration 30901 / 61240) loss: 2.102173\n",
      "(Iteration 31001 / 61240) loss: 2.166935\n",
      "(Iteration 31101 / 61240) loss: 2.068814\n",
      "(Iteration 31201 / 61240) loss: 2.159560\n",
      "(Iteration 31301 / 61240) loss: 2.072538\n",
      "(Iteration 31401 / 61240) loss: 2.146464\n",
      "(Iteration 31501 / 61240) loss: 2.035748\n",
      "(Iteration 31601 / 61240) loss: 2.045093\n",
      "(Iteration 31701 / 61240) loss: 2.089998\n",
      "(Iteration 31801 / 61240) loss: 2.129088\n",
      "(Iteration 31901 / 61240) loss: 2.145700\n",
      "(Iteration 32001 / 61240) loss: 2.122025\n",
      "(Iteration 32101 / 61240) loss: 2.083493\n",
      "(Epoch 21 / 40) train acc: 0.274000; val_acc: 0.266000\n",
      "(Iteration 32201 / 61240) loss: 2.132201\n",
      "(Iteration 32301 / 61240) loss: 2.184246\n",
      "(Iteration 32401 / 61240) loss: 2.141018\n",
      "(Iteration 32501 / 61240) loss: 2.042676\n",
      "(Iteration 32601 / 61240) loss: 1.956246\n",
      "(Iteration 32701 / 61240) loss: 2.151881\n",
      "(Iteration 32801 / 61240) loss: 2.163960\n",
      "(Iteration 32901 / 61240) loss: 2.077153\n",
      "(Iteration 33001 / 61240) loss: 2.127512\n",
      "(Iteration 33101 / 61240) loss: 2.131005\n",
      "(Iteration 33201 / 61240) loss: 2.045496\n",
      "(Iteration 33301 / 61240) loss: 1.992651\n",
      "(Iteration 33401 / 61240) loss: 2.121266\n",
      "(Iteration 33501 / 61240) loss: 2.008307\n",
      "(Iteration 33601 / 61240) loss: 2.050751\n",
      "(Epoch 22 / 40) train acc: 0.277000; val_acc: 0.268000\n",
      "(Iteration 33701 / 61240) loss: 2.002718\n",
      "(Iteration 33801 / 61240) loss: 2.103974\n",
      "(Iteration 33901 / 61240) loss: 2.106330\n",
      "(Iteration 34001 / 61240) loss: 2.216310\n",
      "(Iteration 34101 / 61240) loss: 2.189692\n",
      "(Iteration 34201 / 61240) loss: 2.115686\n",
      "(Iteration 34301 / 61240) loss: 2.181562\n",
      "(Iteration 34401 / 61240) loss: 1.969850\n",
      "(Iteration 34501 / 61240) loss: 2.238059\n",
      "(Iteration 34601 / 61240) loss: 2.134967\n",
      "(Iteration 34701 / 61240) loss: 2.160466\n",
      "(Iteration 34801 / 61240) loss: 2.007401\n",
      "(Iteration 34901 / 61240) loss: 2.111572\n",
      "(Iteration 35001 / 61240) loss: 2.056368\n",
      "(Iteration 35101 / 61240) loss: 2.092132\n",
      "(Iteration 35201 / 61240) loss: 2.079059\n",
      "(Epoch 23 / 40) train acc: 0.241000; val_acc: 0.266000\n",
      "(Iteration 35301 / 61240) loss: 2.145148\n",
      "(Iteration 35401 / 61240) loss: 2.029664\n",
      "(Iteration 35501 / 61240) loss: 2.111368\n",
      "(Iteration 35601 / 61240) loss: 2.034572\n",
      "(Iteration 35701 / 61240) loss: 2.280135\n",
      "(Iteration 35801 / 61240) loss: 2.149381\n",
      "(Iteration 35901 / 61240) loss: 2.092544\n",
      "(Iteration 36001 / 61240) loss: 2.179431\n",
      "(Iteration 36101 / 61240) loss: 2.150473\n",
      "(Iteration 36201 / 61240) loss: 2.025236\n",
      "(Iteration 36301 / 61240) loss: 2.027520\n",
      "(Iteration 36401 / 61240) loss: 2.085311\n",
      "(Iteration 36501 / 61240) loss: 2.097660\n",
      "(Iteration 36601 / 61240) loss: 2.085362\n",
      "(Iteration 36701 / 61240) loss: 2.190568\n",
      "(Epoch 24 / 40) train acc: 0.250000; val_acc: 0.264000\n",
      "(Iteration 36801 / 61240) loss: 2.138698\n",
      "(Iteration 36901 / 61240) loss: 2.116828\n",
      "(Iteration 37001 / 61240) loss: 2.021222\n",
      "(Iteration 37101 / 61240) loss: 2.034212\n",
      "(Iteration 37201 / 61240) loss: 2.069733\n",
      "(Iteration 37301 / 61240) loss: 2.018705\n",
      "(Iteration 37401 / 61240) loss: 2.003311\n",
      "(Iteration 37501 / 61240) loss: 2.205335\n",
      "(Iteration 37601 / 61240) loss: 1.923420\n",
      "(Iteration 37701 / 61240) loss: 2.051033\n",
      "(Iteration 37801 / 61240) loss: 2.148571\n",
      "(Iteration 37901 / 61240) loss: 2.077636\n",
      "(Iteration 38001 / 61240) loss: 2.114450\n",
      "(Iteration 38101 / 61240) loss: 2.110433\n",
      "(Iteration 38201 / 61240) loss: 2.104423\n",
      "(Epoch 25 / 40) train acc: 0.250000; val_acc: 0.266000\n",
      "(Iteration 38301 / 61240) loss: 1.936608\n",
      "(Iteration 38401 / 61240) loss: 2.042872\n",
      "(Iteration 38501 / 61240) loss: 1.996849\n",
      "(Iteration 38601 / 61240) loss: 2.066488\n",
      "(Iteration 38701 / 61240) loss: 2.045340\n",
      "(Iteration 38801 / 61240) loss: 2.141728\n",
      "(Iteration 38901 / 61240) loss: 1.995235\n",
      "(Iteration 39001 / 61240) loss: 2.128375\n",
      "(Iteration 39101 / 61240) loss: 2.064420\n",
      "(Iteration 39201 / 61240) loss: 2.049112\n",
      "(Iteration 39301 / 61240) loss: 1.935106\n",
      "(Iteration 39401 / 61240) loss: 2.125972\n",
      "(Iteration 39501 / 61240) loss: 2.081536\n",
      "(Iteration 39601 / 61240) loss: 2.191605\n",
      "(Iteration 39701 / 61240) loss: 2.127288\n",
      "(Iteration 39801 / 61240) loss: 2.047071\n",
      "(Epoch 26 / 40) train acc: 0.271000; val_acc: 0.268000\n",
      "(Iteration 39901 / 61240) loss: 2.036237\n",
      "(Iteration 40001 / 61240) loss: 1.946100\n",
      "(Iteration 40101 / 61240) loss: 2.164421\n",
      "(Iteration 40201 / 61240) loss: 1.965802\n",
      "(Iteration 40301 / 61240) loss: 2.001669\n",
      "(Iteration 40401 / 61240) loss: 2.136791\n",
      "(Iteration 40501 / 61240) loss: 2.115781\n",
      "(Iteration 40601 / 61240) loss: 2.102873\n",
      "(Iteration 40701 / 61240) loss: 2.057766\n",
      "(Iteration 40801 / 61240) loss: 2.067849\n",
      "(Iteration 40901 / 61240) loss: 2.060011\n",
      "(Iteration 41001 / 61240) loss: 1.966425\n",
      "(Iteration 41101 / 61240) loss: 2.110174\n",
      "(Iteration 41201 / 61240) loss: 2.067711\n",
      "(Iteration 41301 / 61240) loss: 2.104448\n",
      "(Epoch 27 / 40) train acc: 0.277000; val_acc: 0.268000\n",
      "(Iteration 41401 / 61240) loss: 2.078987\n",
      "(Iteration 41501 / 61240) loss: 2.060192\n",
      "(Iteration 41601 / 61240) loss: 2.093281\n",
      "(Iteration 41701 / 61240) loss: 2.022653\n",
      "(Iteration 41801 / 61240) loss: 2.129255\n",
      "(Iteration 41901 / 61240) loss: 2.177878\n",
      "(Iteration 42001 / 61240) loss: 2.061000\n",
      "(Iteration 42101 / 61240) loss: 2.102178\n",
      "(Iteration 42201 / 61240) loss: 2.160713\n",
      "(Iteration 42301 / 61240) loss: 2.055212\n",
      "(Iteration 42401 / 61240) loss: 2.097252\n",
      "(Iteration 42501 / 61240) loss: 2.073598\n",
      "(Iteration 42601 / 61240) loss: 2.085797\n",
      "(Iteration 42701 / 61240) loss: 1.984549\n",
      "(Iteration 42801 / 61240) loss: 2.160173\n",
      "(Epoch 28 / 40) train acc: 0.241000; val_acc: 0.266000\n",
      "(Iteration 42901 / 61240) loss: 2.091125\n",
      "(Iteration 43001 / 61240) loss: 2.029164\n",
      "(Iteration 43101 / 61240) loss: 2.004544\n",
      "(Iteration 43201 / 61240) loss: 2.079288\n",
      "(Iteration 43301 / 61240) loss: 1.917666\n",
      "(Iteration 43401 / 61240) loss: 2.093439\n",
      "(Iteration 43501 / 61240) loss: 2.084036\n",
      "(Iteration 43601 / 61240) loss: 1.916399\n",
      "(Iteration 43701 / 61240) loss: 2.001845\n",
      "(Iteration 43801 / 61240) loss: 2.021196\n",
      "(Iteration 43901 / 61240) loss: 2.087510\n",
      "(Iteration 44001 / 61240) loss: 2.067776\n",
      "(Iteration 44101 / 61240) loss: 2.066431\n",
      "(Iteration 44201 / 61240) loss: 2.083984\n",
      "(Iteration 44301 / 61240) loss: 2.018317\n",
      "(Epoch 29 / 40) train acc: 0.269000; val_acc: 0.268000\n",
      "(Iteration 44401 / 61240) loss: 2.038849\n",
      "(Iteration 44501 / 61240) loss: 2.011520\n",
      "(Iteration 44601 / 61240) loss: 2.092350\n",
      "(Iteration 44701 / 61240) loss: 2.060911\n",
      "(Iteration 44801 / 61240) loss: 2.056635\n",
      "(Iteration 44901 / 61240) loss: 2.127175\n",
      "(Iteration 45001 / 61240) loss: 2.182905\n",
      "(Iteration 45101 / 61240) loss: 2.197739\n",
      "(Iteration 45201 / 61240) loss: 2.005666\n",
      "(Iteration 45301 / 61240) loss: 2.055911\n",
      "(Iteration 45401 / 61240) loss: 2.012141\n",
      "(Iteration 45501 / 61240) loss: 1.989504\n",
      "(Iteration 45601 / 61240) loss: 2.101348\n",
      "(Iteration 45701 / 61240) loss: 2.097929\n",
      "(Iteration 45801 / 61240) loss: 2.013585\n",
      "(Iteration 45901 / 61240) loss: 1.927477\n",
      "(Epoch 30 / 40) train acc: 0.241000; val_acc: 0.272000\n",
      "(Iteration 46001 / 61240) loss: 2.101562\n",
      "(Iteration 46101 / 61240) loss: 2.012612\n",
      "(Iteration 46201 / 61240) loss: 2.109261\n",
      "(Iteration 46301 / 61240) loss: 2.113691\n",
      "(Iteration 46401 / 61240) loss: 2.044801\n",
      "(Iteration 46501 / 61240) loss: 2.158042\n",
      "(Iteration 46601 / 61240) loss: 2.022244\n",
      "(Iteration 46701 / 61240) loss: 2.200804\n",
      "(Iteration 46801 / 61240) loss: 2.105168\n",
      "(Iteration 46901 / 61240) loss: 2.052768\n",
      "(Iteration 47001 / 61240) loss: 1.990045\n",
      "(Iteration 47101 / 61240) loss: 2.053084\n",
      "(Iteration 47201 / 61240) loss: 2.059059\n",
      "(Iteration 47301 / 61240) loss: 2.077344\n",
      "(Iteration 47401 / 61240) loss: 2.067587\n",
      "(Epoch 31 / 40) train acc: 0.275000; val_acc: 0.275000\n",
      "(Iteration 47501 / 61240) loss: 2.105715\n",
      "(Iteration 47601 / 61240) loss: 2.024776\n",
      "(Iteration 47701 / 61240) loss: 2.110190\n",
      "(Iteration 47801 / 61240) loss: 1.967478\n",
      "(Iteration 47901 / 61240) loss: 2.025061\n",
      "(Iteration 48001 / 61240) loss: 2.212339\n",
      "(Iteration 48101 / 61240) loss: 1.975534\n",
      "(Iteration 48201 / 61240) loss: 2.086203\n",
      "(Iteration 48301 / 61240) loss: 2.110650\n",
      "(Iteration 48401 / 61240) loss: 2.085181\n",
      "(Iteration 48501 / 61240) loss: 1.989262\n",
      "(Iteration 48601 / 61240) loss: 1.989076\n",
      "(Iteration 48701 / 61240) loss: 2.103818\n",
      "(Iteration 48801 / 61240) loss: 2.064790\n",
      "(Iteration 48901 / 61240) loss: 1.942962\n",
      "(Epoch 32 / 40) train acc: 0.268000; val_acc: 0.278000\n",
      "(Iteration 49001 / 61240) loss: 2.019667\n",
      "(Iteration 49101 / 61240) loss: 2.177007\n",
      "(Iteration 49201 / 61240) loss: 2.082025\n",
      "(Iteration 49301 / 61240) loss: 2.089321\n",
      "(Iteration 49401 / 61240) loss: 2.120740\n",
      "(Iteration 49501 / 61240) loss: 2.088893\n",
      "(Iteration 49601 / 61240) loss: 2.132731\n",
      "(Iteration 49701 / 61240) loss: 1.922334\n",
      "(Iteration 49801 / 61240) loss: 1.936261\n",
      "(Iteration 49901 / 61240) loss: 2.117776\n",
      "(Iteration 50001 / 61240) loss: 2.041080\n",
      "(Iteration 50101 / 61240) loss: 2.058931\n",
      "(Iteration 50201 / 61240) loss: 2.014172\n",
      "(Iteration 50301 / 61240) loss: 1.963938\n",
      "(Iteration 50401 / 61240) loss: 2.088177\n",
      "(Iteration 50501 / 61240) loss: 2.122597\n",
      "(Epoch 33 / 40) train acc: 0.280000; val_acc: 0.279000\n",
      "(Iteration 50601 / 61240) loss: 2.027862\n",
      "(Iteration 50701 / 61240) loss: 2.057061\n",
      "(Iteration 50801 / 61240) loss: 2.105108\n",
      "(Iteration 50901 / 61240) loss: 2.095429\n",
      "(Iteration 51001 / 61240) loss: 2.005112\n",
      "(Iteration 51101 / 61240) loss: 2.127107\n",
      "(Iteration 51201 / 61240) loss: 1.955135\n",
      "(Iteration 51301 / 61240) loss: 1.987837\n",
      "(Iteration 51401 / 61240) loss: 2.072035\n",
      "(Iteration 51501 / 61240) loss: 1.916984\n",
      "(Iteration 51601 / 61240) loss: 2.099102\n",
      "(Iteration 51701 / 61240) loss: 2.047430\n",
      "(Iteration 51801 / 61240) loss: 1.986291\n",
      "(Iteration 51901 / 61240) loss: 1.997599\n",
      "(Iteration 52001 / 61240) loss: 2.021566\n",
      "(Epoch 34 / 40) train acc: 0.264000; val_acc: 0.279000\n",
      "(Iteration 52101 / 61240) loss: 2.013970\n",
      "(Iteration 52201 / 61240) loss: 2.027492\n",
      "(Iteration 52301 / 61240) loss: 1.988920\n",
      "(Iteration 52401 / 61240) loss: 1.997975\n",
      "(Iteration 52501 / 61240) loss: 2.040374\n",
      "(Iteration 52601 / 61240) loss: 2.156616\n",
      "(Iteration 52701 / 61240) loss: 2.084714\n",
      "(Iteration 52801 / 61240) loss: 2.089580\n",
      "(Iteration 52901 / 61240) loss: 1.985738\n",
      "(Iteration 53001 / 61240) loss: 2.081200\n",
      "(Iteration 53101 / 61240) loss: 1.973069\n",
      "(Iteration 53201 / 61240) loss: 2.057179\n",
      "(Iteration 53301 / 61240) loss: 1.969961\n",
      "(Iteration 53401 / 61240) loss: 2.088691\n",
      "(Iteration 53501 / 61240) loss: 1.985015\n",
      "(Epoch 35 / 40) train acc: 0.256000; val_acc: 0.280000\n",
      "(Iteration 53601 / 61240) loss: 2.029125\n",
      "(Iteration 53701 / 61240) loss: 1.982318\n",
      "(Iteration 53801 / 61240) loss: 1.994948\n",
      "(Iteration 53901 / 61240) loss: 1.980112\n",
      "(Iteration 54001 / 61240) loss: 2.072514\n",
      "(Iteration 54101 / 61240) loss: 2.084118\n",
      "(Iteration 54201 / 61240) loss: 2.062236\n",
      "(Iteration 54301 / 61240) loss: 1.926925\n",
      "(Iteration 54401 / 61240) loss: 2.165246\n",
      "(Iteration 54501 / 61240) loss: 1.915995\n",
      "(Iteration 54601 / 61240) loss: 2.122328\n",
      "(Iteration 54701 / 61240) loss: 2.119922\n",
      "(Iteration 54801 / 61240) loss: 1.990246\n",
      "(Iteration 54901 / 61240) loss: 2.088437\n",
      "(Iteration 55001 / 61240) loss: 1.978168\n",
      "(Iteration 55101 / 61240) loss: 2.151648\n",
      "(Epoch 36 / 40) train acc: 0.261000; val_acc: 0.280000\n",
      "(Iteration 55201 / 61240) loss: 1.999394\n",
      "(Iteration 55301 / 61240) loss: 2.059592\n",
      "(Iteration 55401 / 61240) loss: 2.014956\n",
      "(Iteration 55501 / 61240) loss: 1.985967\n",
      "(Iteration 55601 / 61240) loss: 1.989578\n",
      "(Iteration 55701 / 61240) loss: 2.042579\n",
      "(Iteration 55801 / 61240) loss: 2.170495\n",
      "(Iteration 55901 / 61240) loss: 2.048352\n",
      "(Iteration 56001 / 61240) loss: 2.004963\n",
      "(Iteration 56101 / 61240) loss: 1.995266\n",
      "(Iteration 56201 / 61240) loss: 2.103132\n",
      "(Iteration 56301 / 61240) loss: 2.023033\n",
      "(Iteration 56401 / 61240) loss: 1.978353\n",
      "(Iteration 56501 / 61240) loss: 2.062969\n",
      "(Iteration 56601 / 61240) loss: 1.862738\n",
      "(Epoch 37 / 40) train acc: 0.258000; val_acc: 0.281000\n",
      "(Iteration 56701 / 61240) loss: 2.146539\n",
      "(Iteration 56801 / 61240) loss: 1.986139\n",
      "(Iteration 56901 / 61240) loss: 2.087451\n",
      "(Iteration 57001 / 61240) loss: 2.098742\n",
      "(Iteration 57101 / 61240) loss: 1.981044\n",
      "(Iteration 57201 / 61240) loss: 2.079904\n",
      "(Iteration 57301 / 61240) loss: 1.997881\n",
      "(Iteration 57401 / 61240) loss: 2.096336\n",
      "(Iteration 57501 / 61240) loss: 2.109490\n",
      "(Iteration 57601 / 61240) loss: 2.040658\n",
      "(Iteration 57701 / 61240) loss: 2.031880\n",
      "(Iteration 57801 / 61240) loss: 2.008156\n",
      "(Iteration 57901 / 61240) loss: 2.224922\n",
      "(Iteration 58001 / 61240) loss: 2.000626\n",
      "(Iteration 58101 / 61240) loss: 2.113892\n",
      "(Epoch 38 / 40) train acc: 0.245000; val_acc: 0.280000\n",
      "(Iteration 58201 / 61240) loss: 2.046935\n",
      "(Iteration 58301 / 61240) loss: 2.121581\n",
      "(Iteration 58401 / 61240) loss: 1.952276\n",
      "(Iteration 58501 / 61240) loss: 2.007839\n",
      "(Iteration 58601 / 61240) loss: 2.043516\n",
      "(Iteration 58701 / 61240) loss: 2.097783\n",
      "(Iteration 58801 / 61240) loss: 2.061355\n",
      "(Iteration 58901 / 61240) loss: 1.941652\n",
      "(Iteration 59001 / 61240) loss: 2.110545\n",
      "(Iteration 59101 / 61240) loss: 2.141944\n",
      "(Iteration 59201 / 61240) loss: 1.981713\n",
      "(Iteration 59301 / 61240) loss: 1.971153\n",
      "(Iteration 59401 / 61240) loss: 2.124719\n",
      "(Iteration 59501 / 61240) loss: 2.110456\n",
      "(Iteration 59601 / 61240) loss: 2.152092\n",
      "(Iteration 59701 / 61240) loss: 1.964785\n",
      "(Epoch 39 / 40) train acc: 0.278000; val_acc: 0.280000\n",
      "(Iteration 59801 / 61240) loss: 2.000151\n",
      "(Iteration 59901 / 61240) loss: 1.953279\n",
      "(Iteration 60001 / 61240) loss: 1.886997\n",
      "(Iteration 60101 / 61240) loss: 2.002248\n",
      "(Iteration 60201 / 61240) loss: 2.068850\n",
      "(Iteration 60301 / 61240) loss: 2.101065\n",
      "(Iteration 60401 / 61240) loss: 2.130366\n",
      "(Iteration 60501 / 61240) loss: 1.973352\n",
      "(Iteration 60601 / 61240) loss: 2.038849\n",
      "(Iteration 60701 / 61240) loss: 1.983716\n",
      "(Iteration 60801 / 61240) loss: 2.041514\n",
      "(Iteration 60901 / 61240) loss: 2.080241\n",
      "(Iteration 61001 / 61240) loss: 1.931353\n",
      "(Iteration 61101 / 61240) loss: 2.108574\n",
      "(Iteration 61201 / 61240) loss: 2.127472\n",
      "(Epoch 40 / 40) train acc: 0.283000; val_acc: 0.282000\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.01, 'num_epochs': 20, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 15300) loss: 2.305887\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.098000\n",
      "(Iteration 101 / 15300) loss: 2.305744\n",
      "(Iteration 201 / 15300) loss: 2.303624\n",
      "(Iteration 301 / 15300) loss: 2.304732\n",
      "(Iteration 401 / 15300) loss: 2.303974\n",
      "(Iteration 501 / 15300) loss: 2.304156\n",
      "(Iteration 601 / 15300) loss: 2.301355\n",
      "(Iteration 701 / 15300) loss: 2.301174\n",
      "(Epoch 1 / 20) train acc: 0.194000; val_acc: 0.175000\n",
      "(Iteration 801 / 15300) loss: 2.301794\n",
      "(Iteration 901 / 15300) loss: 2.298195\n",
      "(Iteration 1001 / 15300) loss: 2.294190\n",
      "(Iteration 1101 / 15300) loss: 2.289307\n",
      "(Iteration 1201 / 15300) loss: 2.273063\n",
      "(Iteration 1301 / 15300) loss: 2.281356\n",
      "(Iteration 1401 / 15300) loss: 2.276946\n",
      "(Iteration 1501 / 15300) loss: 2.280043\n",
      "(Epoch 2 / 20) train acc: 0.226000; val_acc: 0.239000\n",
      "(Iteration 1601 / 15300) loss: 2.255900\n",
      "(Iteration 1701 / 15300) loss: 2.272556\n",
      "(Iteration 1801 / 15300) loss: 2.252826\n",
      "(Iteration 1901 / 15300) loss: 2.169039\n",
      "(Iteration 2001 / 15300) loss: 2.170153\n",
      "(Iteration 2101 / 15300) loss: 2.213451\n",
      "(Iteration 2201 / 15300) loss: 2.133508\n",
      "(Epoch 3 / 20) train acc: 0.237000; val_acc: 0.254000\n",
      "(Iteration 2301 / 15300) loss: 2.118541\n",
      "(Iteration 2401 / 15300) loss: 2.062627\n",
      "(Iteration 2501 / 15300) loss: 2.045620\n",
      "(Iteration 2601 / 15300) loss: 2.078317\n",
      "(Iteration 2701 / 15300) loss: 2.186255\n",
      "(Iteration 2801 / 15300) loss: 2.124827\n",
      "(Iteration 2901 / 15300) loss: 2.124702\n",
      "(Iteration 3001 / 15300) loss: 2.021169\n",
      "(Epoch 4 / 20) train acc: 0.301000; val_acc: 0.301000\n",
      "(Iteration 3101 / 15300) loss: 2.056444\n",
      "(Iteration 3201 / 15300) loss: 2.051490\n",
      "(Iteration 3301 / 15300) loss: 2.026594\n",
      "(Iteration 3401 / 15300) loss: 2.045805\n",
      "(Iteration 3501 / 15300) loss: 1.993900\n",
      "(Iteration 3601 / 15300) loss: 2.144427\n",
      "(Iteration 3701 / 15300) loss: 2.006298\n",
      "(Iteration 3801 / 15300) loss: 1.972936\n",
      "(Epoch 5 / 20) train acc: 0.317000; val_acc: 0.302000\n",
      "(Iteration 3901 / 15300) loss: 2.183141\n",
      "(Iteration 4001 / 15300) loss: 1.995705\n",
      "(Iteration 4101 / 15300) loss: 2.000211\n",
      "(Iteration 4201 / 15300) loss: 2.040758\n",
      "(Iteration 4301 / 15300) loss: 2.077990\n",
      "(Iteration 4401 / 15300) loss: 2.130083\n",
      "(Iteration 4501 / 15300) loss: 1.976578\n",
      "(Epoch 6 / 20) train acc: 0.329000; val_acc: 0.334000\n",
      "(Iteration 4601 / 15300) loss: 1.945257\n",
      "(Iteration 4701 / 15300) loss: 2.071366\n",
      "(Iteration 4801 / 15300) loss: 2.083384\n",
      "(Iteration 4901 / 15300) loss: 1.993110\n",
      "(Iteration 5001 / 15300) loss: 1.941348\n",
      "(Iteration 5101 / 15300) loss: 2.157190\n",
      "(Iteration 5201 / 15300) loss: 2.026532\n",
      "(Iteration 5301 / 15300) loss: 2.069506\n",
      "(Epoch 7 / 20) train acc: 0.350000; val_acc: 0.373000\n",
      "(Iteration 5401 / 15300) loss: 2.020661\n",
      "(Iteration 5501 / 15300) loss: 2.078195\n",
      "(Iteration 5601 / 15300) loss: 2.044360\n",
      "(Iteration 5701 / 15300) loss: 1.945313\n",
      "(Iteration 5801 / 15300) loss: 2.024294\n",
      "(Iteration 5901 / 15300) loss: 1.903806\n",
      "(Iteration 6001 / 15300) loss: 1.982721\n",
      "(Iteration 6101 / 15300) loss: 2.020303\n",
      "(Epoch 8 / 20) train acc: 0.365000; val_acc: 0.381000\n",
      "(Iteration 6201 / 15300) loss: 1.945019\n",
      "(Iteration 6301 / 15300) loss: 2.135262\n",
      "(Iteration 6401 / 15300) loss: 2.010838\n",
      "(Iteration 6501 / 15300) loss: 2.032217\n",
      "(Iteration 6601 / 15300) loss: 1.852020\n",
      "(Iteration 6701 / 15300) loss: 1.980416\n",
      "(Iteration 6801 / 15300) loss: 2.049259\n",
      "(Epoch 9 / 20) train acc: 0.386000; val_acc: 0.400000\n",
      "(Iteration 6901 / 15300) loss: 1.937880\n",
      "(Iteration 7001 / 15300) loss: 1.996360\n",
      "(Iteration 7101 / 15300) loss: 1.979108\n",
      "(Iteration 7201 / 15300) loss: 2.008027\n",
      "(Iteration 7301 / 15300) loss: 2.025057\n",
      "(Iteration 7401 / 15300) loss: 1.944332\n",
      "(Iteration 7501 / 15300) loss: 1.954532\n",
      "(Iteration 7601 / 15300) loss: 2.001289\n",
      "(Epoch 10 / 20) train acc: 0.409000; val_acc: 0.394000\n",
      "(Iteration 7701 / 15300) loss: 2.047354\n",
      "(Iteration 7801 / 15300) loss: 1.992158\n",
      "(Iteration 7901 / 15300) loss: 1.965149\n",
      "(Iteration 8001 / 15300) loss: 1.944322\n",
      "(Iteration 8101 / 15300) loss: 1.969212\n",
      "(Iteration 8201 / 15300) loss: 1.978074\n",
      "(Iteration 8301 / 15300) loss: 2.102505\n",
      "(Iteration 8401 / 15300) loss: 1.979818\n",
      "(Epoch 11 / 20) train acc: 0.426000; val_acc: 0.404000\n",
      "(Iteration 8501 / 15300) loss: 1.903450\n",
      "(Iteration 8601 / 15300) loss: 1.926174\n",
      "(Iteration 8701 / 15300) loss: 2.112632\n",
      "(Iteration 8801 / 15300) loss: 2.005683\n",
      "(Iteration 8901 / 15300) loss: 2.025076\n",
      "(Iteration 9001 / 15300) loss: 1.926826\n",
      "(Iteration 9101 / 15300) loss: 2.041170\n",
      "(Epoch 12 / 20) train acc: 0.431000; val_acc: 0.396000\n",
      "(Iteration 9201 / 15300) loss: 2.007881\n",
      "(Iteration 9301 / 15300) loss: 1.905249\n",
      "(Iteration 9401 / 15300) loss: 1.912364\n",
      "(Iteration 9501 / 15300) loss: 2.043297\n",
      "(Iteration 9601 / 15300) loss: 2.080851\n",
      "(Iteration 9701 / 15300) loss: 1.907041\n",
      "(Iteration 9801 / 15300) loss: 2.020157\n",
      "(Iteration 9901 / 15300) loss: 2.044961\n",
      "(Epoch 13 / 20) train acc: 0.425000; val_acc: 0.411000\n",
      "(Iteration 10001 / 15300) loss: 2.101371\n",
      "(Iteration 10101 / 15300) loss: 2.030667\n",
      "(Iteration 10201 / 15300) loss: 1.986801\n",
      "(Iteration 10301 / 15300) loss: 1.957369\n",
      "(Iteration 10401 / 15300) loss: 1.935324\n",
      "(Iteration 10501 / 15300) loss: 2.011738\n",
      "(Iteration 10601 / 15300) loss: 2.041610\n",
      "(Iteration 10701 / 15300) loss: 2.045160\n",
      "(Epoch 14 / 20) train acc: 0.425000; val_acc: 0.407000\n",
      "(Iteration 10801 / 15300) loss: 2.036595\n",
      "(Iteration 10901 / 15300) loss: 2.042187\n",
      "(Iteration 11001 / 15300) loss: 1.916139\n",
      "(Iteration 11101 / 15300) loss: 2.061388\n",
      "(Iteration 11201 / 15300) loss: 1.838003\n",
      "(Iteration 11301 / 15300) loss: 1.986805\n",
      "(Iteration 11401 / 15300) loss: 1.987393\n",
      "(Epoch 15 / 20) train acc: 0.429000; val_acc: 0.415000\n",
      "(Iteration 11501 / 15300) loss: 1.964593\n",
      "(Iteration 11601 / 15300) loss: 2.037132\n",
      "(Iteration 11701 / 15300) loss: 1.877675\n",
      "(Iteration 11801 / 15300) loss: 1.932799\n",
      "(Iteration 11901 / 15300) loss: 1.975352\n",
      "(Iteration 12001 / 15300) loss: 1.903693\n",
      "(Iteration 12101 / 15300) loss: 1.858543\n",
      "(Iteration 12201 / 15300) loss: 1.996598\n",
      "(Epoch 16 / 20) train acc: 0.419000; val_acc: 0.408000\n",
      "(Iteration 12301 / 15300) loss: 2.066320\n",
      "(Iteration 12401 / 15300) loss: 2.074332\n",
      "(Iteration 12501 / 15300) loss: 1.839627\n",
      "(Iteration 12601 / 15300) loss: 1.997559\n",
      "(Iteration 12701 / 15300) loss: 2.001308\n",
      "(Iteration 12801 / 15300) loss: 1.899970\n",
      "(Iteration 12901 / 15300) loss: 1.887132\n",
      "(Iteration 13001 / 15300) loss: 2.035917\n",
      "(Epoch 17 / 20) train acc: 0.444000; val_acc: 0.413000\n",
      "(Iteration 13101 / 15300) loss: 1.958285\n",
      "(Iteration 13201 / 15300) loss: 2.004021\n",
      "(Iteration 13301 / 15300) loss: 1.985952\n",
      "(Iteration 13401 / 15300) loss: 2.032940\n",
      "(Iteration 13501 / 15300) loss: 1.961475\n",
      "(Iteration 13601 / 15300) loss: 2.081672\n",
      "(Iteration 13701 / 15300) loss: 2.000121\n",
      "(Epoch 18 / 20) train acc: 0.423000; val_acc: 0.399000\n",
      "(Iteration 13801 / 15300) loss: 1.850715\n",
      "(Iteration 13901 / 15300) loss: 1.911110\n",
      "(Iteration 14001 / 15300) loss: 1.969793\n",
      "(Iteration 14101 / 15300) loss: 1.887157\n",
      "(Iteration 14201 / 15300) loss: 2.045663\n",
      "(Iteration 14301 / 15300) loss: 1.880933\n",
      "(Iteration 14401 / 15300) loss: 2.047539\n",
      "(Iteration 14501 / 15300) loss: 1.922644\n",
      "(Epoch 19 / 20) train acc: 0.443000; val_acc: 0.408000\n",
      "(Iteration 14601 / 15300) loss: 1.913077\n",
      "(Iteration 14701 / 15300) loss: 1.927872\n",
      "(Iteration 14801 / 15300) loss: 1.952832\n",
      "(Iteration 14901 / 15300) loss: 1.959965\n",
      "(Iteration 15001 / 15300) loss: 1.874469\n",
      "(Iteration 15101 / 15300) loss: 2.042920\n",
      "(Iteration 15201 / 15300) loss: 1.830125\n",
      "(Epoch 20 / 20) train acc: 0.406000; val_acc: 0.407000\n",
      "New best model found with validation accuracy: 0.3485\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.01, 'num_epochs': 20, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 30620) loss: 2.305839\n",
      "(Epoch 0 / 20) train acc: 0.087000; val_acc: 0.103000\n",
      "(Iteration 101 / 30620) loss: 2.304190\n",
      "(Iteration 201 / 30620) loss: 2.305986\n",
      "(Iteration 301 / 30620) loss: 2.303375\n",
      "(Iteration 401 / 30620) loss: 2.302479\n",
      "(Iteration 501 / 30620) loss: 2.306208\n",
      "(Iteration 601 / 30620) loss: 2.303041\n",
      "(Iteration 701 / 30620) loss: 2.300898\n",
      "(Iteration 801 / 30620) loss: 2.302963\n",
      "(Iteration 901 / 30620) loss: 2.298806\n",
      "(Iteration 1001 / 30620) loss: 2.298573\n",
      "(Iteration 1101 / 30620) loss: 2.288777\n",
      "(Iteration 1201 / 30620) loss: 2.279386\n",
      "(Iteration 1301 / 30620) loss: 2.265136\n",
      "(Iteration 1401 / 30620) loss: 2.269443\n",
      "(Iteration 1501 / 30620) loss: 2.246767\n",
      "(Epoch 1 / 20) train acc: 0.205000; val_acc: 0.209000\n",
      "(Iteration 1601 / 30620) loss: 2.240426\n",
      "(Iteration 1701 / 30620) loss: 2.159005\n",
      "(Iteration 1801 / 30620) loss: 2.237640\n",
      "(Iteration 1901 / 30620) loss: 2.118794\n",
      "(Iteration 2001 / 30620) loss: 2.282801\n",
      "(Iteration 2101 / 30620) loss: 2.143260\n",
      "(Iteration 2201 / 30620) loss: 2.141537\n",
      "(Iteration 2301 / 30620) loss: 2.186906\n",
      "(Iteration 2401 / 30620) loss: 2.180388\n",
      "(Iteration 2501 / 30620) loss: 2.054339\n",
      "(Iteration 2601 / 30620) loss: 2.021972\n",
      "(Iteration 2701 / 30620) loss: 1.859647\n",
      "(Iteration 2801 / 30620) loss: 2.094316\n",
      "(Iteration 2901 / 30620) loss: 2.120463\n",
      "(Iteration 3001 / 30620) loss: 1.998866\n",
      "(Epoch 2 / 20) train acc: 0.265000; val_acc: 0.293000\n",
      "(Iteration 3101 / 30620) loss: 1.932672\n",
      "(Iteration 3201 / 30620) loss: 2.076519\n",
      "(Iteration 3301 / 30620) loss: 2.029073\n",
      "(Iteration 3401 / 30620) loss: 1.946456\n",
      "(Iteration 3501 / 30620) loss: 2.103918\n",
      "(Iteration 3601 / 30620) loss: 2.085849\n",
      "(Iteration 3701 / 30620) loss: 1.928857\n",
      "(Iteration 3801 / 30620) loss: 1.934363\n",
      "(Iteration 3901 / 30620) loss: 2.155945\n",
      "(Iteration 4001 / 30620) loss: 2.115732\n",
      "(Iteration 4101 / 30620) loss: 2.080370\n",
      "(Iteration 4201 / 30620) loss: 2.059471\n",
      "(Iteration 4301 / 30620) loss: 1.946921\n",
      "(Iteration 4401 / 30620) loss: 2.015567\n",
      "(Iteration 4501 / 30620) loss: 2.099942\n",
      "(Epoch 3 / 20) train acc: 0.359000; val_acc: 0.373000\n",
      "(Iteration 4601 / 30620) loss: 2.069204\n",
      "(Iteration 4701 / 30620) loss: 2.173416\n",
      "(Iteration 4801 / 30620) loss: 2.093577\n",
      "(Iteration 4901 / 30620) loss: 2.046725\n",
      "(Iteration 5001 / 30620) loss: 2.023285\n",
      "(Iteration 5101 / 30620) loss: 2.155125\n",
      "(Iteration 5201 / 30620) loss: 1.868214\n",
      "(Iteration 5301 / 30620) loss: 2.058263\n",
      "(Iteration 5401 / 30620) loss: 2.053161\n",
      "(Iteration 5501 / 30620) loss: 2.059523\n",
      "(Iteration 5601 / 30620) loss: 2.209682\n",
      "(Iteration 5701 / 30620) loss: 1.974308\n",
      "(Iteration 5801 / 30620) loss: 1.914480\n",
      "(Iteration 5901 / 30620) loss: 2.026676\n",
      "(Iteration 6001 / 30620) loss: 2.012223\n",
      "(Iteration 6101 / 30620) loss: 1.912675\n",
      "(Epoch 4 / 20) train acc: 0.404000; val_acc: 0.386000\n",
      "(Iteration 6201 / 30620) loss: 1.823690\n",
      "(Iteration 6301 / 30620) loss: 1.990216\n",
      "(Iteration 6401 / 30620) loss: 2.136925\n",
      "(Iteration 6501 / 30620) loss: 2.063865\n",
      "(Iteration 6601 / 30620) loss: 1.925920\n",
      "(Iteration 6701 / 30620) loss: 2.028883\n",
      "(Iteration 6801 / 30620) loss: 2.054792\n",
      "(Iteration 6901 / 30620) loss: 1.913377\n",
      "(Iteration 7001 / 30620) loss: 1.977995\n",
      "(Iteration 7101 / 30620) loss: 2.057325\n",
      "(Iteration 7201 / 30620) loss: 2.031490\n",
      "(Iteration 7301 / 30620) loss: 1.967742\n",
      "(Iteration 7401 / 30620) loss: 1.907021\n",
      "(Iteration 7501 / 30620) loss: 1.955569\n",
      "(Iteration 7601 / 30620) loss: 1.949915\n",
      "(Epoch 5 / 20) train acc: 0.426000; val_acc: 0.399000\n",
      "(Iteration 7701 / 30620) loss: 2.005561\n",
      "(Iteration 7801 / 30620) loss: 2.055007\n",
      "(Iteration 7901 / 30620) loss: 2.002359\n",
      "(Iteration 8001 / 30620) loss: 1.850083\n",
      "(Iteration 8101 / 30620) loss: 1.870257\n",
      "(Iteration 8201 / 30620) loss: 1.935820\n",
      "(Iteration 8301 / 30620) loss: 1.975160\n",
      "(Iteration 8401 / 30620) loss: 1.975868\n",
      "(Iteration 8501 / 30620) loss: 1.954176\n",
      "(Iteration 8601 / 30620) loss: 1.976349\n",
      "(Iteration 8701 / 30620) loss: 2.054171\n",
      "(Iteration 8801 / 30620) loss: 1.815305\n",
      "(Iteration 8901 / 30620) loss: 2.159032\n",
      "(Iteration 9001 / 30620) loss: 2.022737\n",
      "(Iteration 9101 / 30620) loss: 2.105909\n",
      "(Epoch 6 / 20) train acc: 0.424000; val_acc: 0.422000\n",
      "(Iteration 9201 / 30620) loss: 1.863502\n",
      "(Iteration 9301 / 30620) loss: 1.970240\n",
      "(Iteration 9401 / 30620) loss: 2.052836\n",
      "(Iteration 9501 / 30620) loss: 1.879116\n",
      "(Iteration 9601 / 30620) loss: 2.043124\n",
      "(Iteration 9701 / 30620) loss: 2.007631\n",
      "(Iteration 9801 / 30620) loss: 1.960679\n",
      "(Iteration 9901 / 30620) loss: 1.979532\n",
      "(Iteration 10001 / 30620) loss: 2.122851\n",
      "(Iteration 10101 / 30620) loss: 1.957868\n",
      "(Iteration 10201 / 30620) loss: 2.019201\n",
      "(Iteration 10301 / 30620) loss: 2.112077\n",
      "(Iteration 10401 / 30620) loss: 1.893699\n",
      "(Iteration 10501 / 30620) loss: 2.078282\n",
      "(Iteration 10601 / 30620) loss: 1.855640\n",
      "(Iteration 10701 / 30620) loss: 1.994343\n",
      "(Epoch 7 / 20) train acc: 0.432000; val_acc: 0.429000\n",
      "(Iteration 10801 / 30620) loss: 1.832788\n",
      "(Iteration 10901 / 30620) loss: 1.918692\n",
      "(Iteration 11001 / 30620) loss: 2.055291\n",
      "(Iteration 11101 / 30620) loss: 2.015393\n",
      "(Iteration 11201 / 30620) loss: 1.926786\n",
      "(Iteration 11301 / 30620) loss: 1.905229\n",
      "(Iteration 11401 / 30620) loss: 2.026233\n",
      "(Iteration 11501 / 30620) loss: 1.832588\n",
      "(Iteration 11601 / 30620) loss: 1.956225\n",
      "(Iteration 11701 / 30620) loss: 1.907367\n",
      "(Iteration 11801 / 30620) loss: 1.950891\n",
      "(Iteration 11901 / 30620) loss: 1.920978\n",
      "(Iteration 12001 / 30620) loss: 2.004627\n",
      "(Iteration 12101 / 30620) loss: 2.055784\n",
      "(Iteration 12201 / 30620) loss: 2.114769\n",
      "(Epoch 8 / 20) train acc: 0.437000; val_acc: 0.429000\n",
      "(Iteration 12301 / 30620) loss: 1.885570\n",
      "(Iteration 12401 / 30620) loss: 2.086845\n",
      "(Iteration 12501 / 30620) loss: 1.973596\n",
      "(Iteration 12601 / 30620) loss: 1.797884\n",
      "(Iteration 12701 / 30620) loss: 1.945987\n",
      "(Iteration 12801 / 30620) loss: 1.856578\n",
      "(Iteration 12901 / 30620) loss: 1.970031\n",
      "(Iteration 13001 / 30620) loss: 1.938350\n",
      "(Iteration 13101 / 30620) loss: 1.966308\n",
      "(Iteration 13201 / 30620) loss: 1.924173\n",
      "(Iteration 13301 / 30620) loss: 2.121099\n",
      "(Iteration 13401 / 30620) loss: 1.958765\n",
      "(Iteration 13501 / 30620) loss: 2.007918\n",
      "(Iteration 13601 / 30620) loss: 2.151739\n",
      "(Iteration 13701 / 30620) loss: 1.808966\n",
      "(Epoch 9 / 20) train acc: 0.423000; val_acc: 0.417000\n",
      "(Iteration 13801 / 30620) loss: 1.899864\n",
      "(Iteration 13901 / 30620) loss: 1.839468\n",
      "(Iteration 14001 / 30620) loss: 2.052487\n",
      "(Iteration 14101 / 30620) loss: 2.036858\n",
      "(Iteration 14201 / 30620) loss: 2.033344\n",
      "(Iteration 14301 / 30620) loss: 1.848024\n",
      "(Iteration 14401 / 30620) loss: 1.878937\n",
      "(Iteration 14501 / 30620) loss: 1.981809\n",
      "(Iteration 14601 / 30620) loss: 1.757598\n",
      "(Iteration 14701 / 30620) loss: 1.928012\n",
      "(Iteration 14801 / 30620) loss: 1.982884\n",
      "(Iteration 14901 / 30620) loss: 2.060465\n",
      "(Iteration 15001 / 30620) loss: 2.047576\n",
      "(Iteration 15101 / 30620) loss: 2.042881\n",
      "(Iteration 15201 / 30620) loss: 1.874627\n",
      "(Iteration 15301 / 30620) loss: 1.984061\n",
      "(Epoch 10 / 20) train acc: 0.424000; val_acc: 0.425000\n",
      "(Iteration 15401 / 30620) loss: 2.045286\n",
      "(Iteration 15501 / 30620) loss: 1.966353\n",
      "(Iteration 15601 / 30620) loss: 1.969047\n",
      "(Iteration 15701 / 30620) loss: 1.827080\n",
      "(Iteration 15801 / 30620) loss: 1.878079\n",
      "(Iteration 15901 / 30620) loss: 1.852095\n",
      "(Iteration 16001 / 30620) loss: 1.843725\n",
      "(Iteration 16101 / 30620) loss: 1.945885\n",
      "(Iteration 16201 / 30620) loss: 1.956001\n",
      "(Iteration 16301 / 30620) loss: 1.983214\n",
      "(Iteration 16401 / 30620) loss: 1.985056\n",
      "(Iteration 16501 / 30620) loss: 1.891132\n",
      "(Iteration 16601 / 30620) loss: 2.018662\n",
      "(Iteration 16701 / 30620) loss: 1.946827\n",
      "(Iteration 16801 / 30620) loss: 1.934427\n",
      "(Epoch 11 / 20) train acc: 0.440000; val_acc: 0.426000\n",
      "(Iteration 16901 / 30620) loss: 1.813867\n",
      "(Iteration 17001 / 30620) loss: 2.016714\n",
      "(Iteration 17101 / 30620) loss: 1.918721\n",
      "(Iteration 17201 / 30620) loss: 2.042317\n",
      "(Iteration 17301 / 30620) loss: 1.962790\n",
      "(Iteration 17401 / 30620) loss: 2.051755\n",
      "(Iteration 17501 / 30620) loss: 1.904948\n",
      "(Iteration 17601 / 30620) loss: 2.013509\n",
      "(Iteration 17701 / 30620) loss: 1.920278\n",
      "(Iteration 17801 / 30620) loss: 2.116194\n",
      "(Iteration 17901 / 30620) loss: 1.881262\n",
      "(Iteration 18001 / 30620) loss: 1.868730\n",
      "(Iteration 18101 / 30620) loss: 2.049773\n",
      "(Iteration 18201 / 30620) loss: 2.001525\n",
      "(Iteration 18301 / 30620) loss: 1.920392\n",
      "(Epoch 12 / 20) train acc: 0.480000; val_acc: 0.428000\n",
      "(Iteration 18401 / 30620) loss: 1.991484\n",
      "(Iteration 18501 / 30620) loss: 1.917183\n",
      "(Iteration 18601 / 30620) loss: 1.853232\n",
      "(Iteration 18701 / 30620) loss: 1.944054\n",
      "(Iteration 18801 / 30620) loss: 1.871027\n",
      "(Iteration 18901 / 30620) loss: 2.023460\n",
      "(Iteration 19001 / 30620) loss: 2.069688\n",
      "(Iteration 19101 / 30620) loss: 2.095796\n",
      "(Iteration 19201 / 30620) loss: 2.111019\n",
      "(Iteration 19301 / 30620) loss: 2.045904\n",
      "(Iteration 19401 / 30620) loss: 1.974575\n",
      "(Iteration 19501 / 30620) loss: 1.875708\n",
      "(Iteration 19601 / 30620) loss: 2.017895\n",
      "(Iteration 19701 / 30620) loss: 2.112739\n",
      "(Iteration 19801 / 30620) loss: 1.813679\n",
      "(Iteration 19901 / 30620) loss: 1.843377\n",
      "(Epoch 13 / 20) train acc: 0.456000; val_acc: 0.428000\n",
      "(Iteration 20001 / 30620) loss: 1.946504\n",
      "(Iteration 20101 / 30620) loss: 1.821304\n",
      "(Iteration 20201 / 30620) loss: 1.876504\n",
      "(Iteration 20301 / 30620) loss: 2.111033\n",
      "(Iteration 20401 / 30620) loss: 1.882994\n",
      "(Iteration 20501 / 30620) loss: 1.951148\n",
      "(Iteration 20601 / 30620) loss: 1.922586\n",
      "(Iteration 20701 / 30620) loss: 1.897895\n",
      "(Iteration 20801 / 30620) loss: 2.023387\n",
      "(Iteration 20901 / 30620) loss: 1.930896\n",
      "(Iteration 21001 / 30620) loss: 2.208809\n",
      "(Iteration 21101 / 30620) loss: 1.793905\n",
      "(Iteration 21201 / 30620) loss: 2.070103\n",
      "(Iteration 21301 / 30620) loss: 1.860301\n",
      "(Iteration 21401 / 30620) loss: 1.806917\n",
      "(Epoch 14 / 20) train acc: 0.456000; val_acc: 0.430000\n",
      "(Iteration 21501 / 30620) loss: 1.925820\n",
      "(Iteration 21601 / 30620) loss: 1.939879\n",
      "(Iteration 21701 / 30620) loss: 2.050709\n",
      "(Iteration 21801 / 30620) loss: 2.092516\n",
      "(Iteration 21901 / 30620) loss: 1.959527\n",
      "(Iteration 22001 / 30620) loss: 2.057802\n",
      "(Iteration 22101 / 30620) loss: 2.089474\n",
      "(Iteration 22201 / 30620) loss: 1.953534\n",
      "(Iteration 22301 / 30620) loss: 1.950482\n",
      "(Iteration 22401 / 30620) loss: 2.133676\n",
      "(Iteration 22501 / 30620) loss: 2.030530\n",
      "(Iteration 22601 / 30620) loss: 2.035360\n",
      "(Iteration 22701 / 30620) loss: 1.900842\n",
      "(Iteration 22801 / 30620) loss: 2.038915\n",
      "(Iteration 22901 / 30620) loss: 1.956758\n",
      "(Epoch 15 / 20) train acc: 0.439000; val_acc: 0.431000\n",
      "(Iteration 23001 / 30620) loss: 1.929972\n",
      "(Iteration 23101 / 30620) loss: 2.020496\n",
      "(Iteration 23201 / 30620) loss: 1.772868\n",
      "(Iteration 23301 / 30620) loss: 1.961122\n",
      "(Iteration 23401 / 30620) loss: 1.960321\n",
      "(Iteration 23501 / 30620) loss: 2.075252\n",
      "(Iteration 23601 / 30620) loss: 2.264427\n",
      "(Iteration 23701 / 30620) loss: 1.782172\n",
      "(Iteration 23801 / 30620) loss: 1.880778\n",
      "(Iteration 23901 / 30620) loss: 2.020062\n",
      "(Iteration 24001 / 30620) loss: 1.951904\n",
      "(Iteration 24101 / 30620) loss: 2.068580\n",
      "(Iteration 24201 / 30620) loss: 2.109765\n",
      "(Iteration 24301 / 30620) loss: 1.974375\n",
      "(Iteration 24401 / 30620) loss: 1.982534\n",
      "(Epoch 16 / 20) train acc: 0.434000; val_acc: 0.432000\n",
      "(Iteration 24501 / 30620) loss: 2.008862\n",
      "(Iteration 24601 / 30620) loss: 1.955907\n",
      "(Iteration 24701 / 30620) loss: 2.109746\n",
      "(Iteration 24801 / 30620) loss: 1.871582\n",
      "(Iteration 24901 / 30620) loss: 2.034585\n",
      "(Iteration 25001 / 30620) loss: 1.993114\n",
      "(Iteration 25101 / 30620) loss: 2.116934\n",
      "(Iteration 25201 / 30620) loss: 1.840541\n",
      "(Iteration 25301 / 30620) loss: 1.968565\n",
      "(Iteration 25401 / 30620) loss: 1.970939\n",
      "(Iteration 25501 / 30620) loss: 1.954879\n",
      "(Iteration 25601 / 30620) loss: 1.950997\n",
      "(Iteration 25701 / 30620) loss: 1.857542\n",
      "(Iteration 25801 / 30620) loss: 1.912998\n",
      "(Iteration 25901 / 30620) loss: 1.945147\n",
      "(Iteration 26001 / 30620) loss: 2.002968\n",
      "(Epoch 17 / 20) train acc: 0.434000; val_acc: 0.435000\n",
      "(Iteration 26101 / 30620) loss: 1.932894\n",
      "(Iteration 26201 / 30620) loss: 2.004356\n",
      "(Iteration 26301 / 30620) loss: 1.938850\n",
      "(Iteration 26401 / 30620) loss: 2.074482\n",
      "(Iteration 26501 / 30620) loss: 1.934052\n",
      "(Iteration 26601 / 30620) loss: 1.818404\n",
      "(Iteration 26701 / 30620) loss: 1.996126\n",
      "(Iteration 26801 / 30620) loss: 1.827839\n",
      "(Iteration 26901 / 30620) loss: 1.795213\n",
      "(Iteration 27001 / 30620) loss: 2.036589\n",
      "(Iteration 27101 / 30620) loss: 1.781404\n",
      "(Iteration 27201 / 30620) loss: 1.829745\n",
      "(Iteration 27301 / 30620) loss: 1.915909\n",
      "(Iteration 27401 / 30620) loss: 2.157334\n",
      "(Iteration 27501 / 30620) loss: 1.846547\n",
      "(Epoch 18 / 20) train acc: 0.435000; val_acc: 0.430000\n",
      "(Iteration 27601 / 30620) loss: 2.021768\n",
      "(Iteration 27701 / 30620) loss: 2.021564\n",
      "(Iteration 27801 / 30620) loss: 1.980549\n",
      "(Iteration 27901 / 30620) loss: 1.833389\n",
      "(Iteration 28001 / 30620) loss: 1.947587\n",
      "(Iteration 28101 / 30620) loss: 2.063492\n",
      "(Iteration 28201 / 30620) loss: 1.886621\n",
      "(Iteration 28301 / 30620) loss: 2.002971\n",
      "(Iteration 28401 / 30620) loss: 1.957791\n",
      "(Iteration 28501 / 30620) loss: 1.919842\n",
      "(Iteration 28601 / 30620) loss: 1.923169\n",
      "(Iteration 28701 / 30620) loss: 1.935273\n",
      "(Iteration 28801 / 30620) loss: 1.969140\n",
      "(Iteration 28901 / 30620) loss: 1.900043\n",
      "(Iteration 29001 / 30620) loss: 1.870964\n",
      "(Epoch 19 / 20) train acc: 0.444000; val_acc: 0.434000\n",
      "(Iteration 29101 / 30620) loss: 1.904152\n",
      "(Iteration 29201 / 30620) loss: 1.995123\n",
      "(Iteration 29301 / 30620) loss: 1.990409\n",
      "(Iteration 29401 / 30620) loss: 1.946989\n",
      "(Iteration 29501 / 30620) loss: 1.797667\n",
      "(Iteration 29601 / 30620) loss: 1.895630\n",
      "(Iteration 29701 / 30620) loss: 1.898235\n",
      "(Iteration 29801 / 30620) loss: 2.070218\n",
      "(Iteration 29901 / 30620) loss: 1.918752\n",
      "(Iteration 30001 / 30620) loss: 1.968514\n",
      "(Iteration 30101 / 30620) loss: 1.904752\n",
      "(Iteration 30201 / 30620) loss: 2.054086\n",
      "(Iteration 30301 / 30620) loss: 1.832773\n",
      "(Iteration 30401 / 30620) loss: 1.812478\n",
      "(Iteration 30501 / 30620) loss: 1.939998\n",
      "(Iteration 30601 / 30620) loss: 1.849819\n",
      "(Epoch 20 / 20) train acc: 0.425000; val_acc: 0.431000\n",
      "New best model found with validation accuracy: 0.3900\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.01, 'num_epochs': 20, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 15300) loss: 2.302922\n",
      "(Epoch 0 / 20) train acc: 0.104000; val_acc: 0.112000\n",
      "(Iteration 101 / 15300) loss: 2.302660\n",
      "(Iteration 201 / 15300) loss: 2.302578\n",
      "(Iteration 301 / 15300) loss: 2.302018\n",
      "(Iteration 401 / 15300) loss: 2.302239\n",
      "(Iteration 501 / 15300) loss: 2.301070\n",
      "(Iteration 601 / 15300) loss: 2.299830\n",
      "(Iteration 701 / 15300) loss: 2.294890\n",
      "(Epoch 1 / 20) train acc: 0.224000; val_acc: 0.230000\n",
      "(Iteration 801 / 15300) loss: 2.287327\n",
      "(Iteration 901 / 15300) loss: 2.270581\n",
      "(Iteration 1001 / 15300) loss: 2.264943\n",
      "(Iteration 1101 / 15300) loss: 2.228684\n",
      "(Iteration 1201 / 15300) loss: 2.197192\n",
      "(Iteration 1301 / 15300) loss: 2.161618\n",
      "(Iteration 1401 / 15300) loss: 2.082839\n",
      "(Iteration 1501 / 15300) loss: 2.159596\n",
      "(Epoch 2 / 20) train acc: 0.275000; val_acc: 0.285000\n",
      "(Iteration 1601 / 15300) loss: 1.951901\n",
      "(Iteration 1701 / 15300) loss: 2.001610\n",
      "(Iteration 1801 / 15300) loss: 1.941206\n",
      "(Iteration 1901 / 15300) loss: 2.005591\n",
      "(Iteration 2001 / 15300) loss: 2.075246\n",
      "(Iteration 2101 / 15300) loss: 1.912637\n",
      "(Iteration 2201 / 15300) loss: 1.899308\n",
      "(Epoch 3 / 20) train acc: 0.341000; val_acc: 0.356000\n",
      "(Iteration 2301 / 15300) loss: 1.888144\n",
      "(Iteration 2401 / 15300) loss: 1.661537\n",
      "(Iteration 2501 / 15300) loss: 1.807226\n",
      "(Iteration 2601 / 15300) loss: 1.715031\n",
      "(Iteration 2701 / 15300) loss: 1.900254\n",
      "(Iteration 2801 / 15300) loss: 1.697067\n",
      "(Iteration 2901 / 15300) loss: 1.746164\n",
      "(Iteration 3001 / 15300) loss: 1.688753\n",
      "(Epoch 4 / 20) train acc: 0.418000; val_acc: 0.394000\n",
      "(Iteration 3101 / 15300) loss: 1.632399\n",
      "(Iteration 3201 / 15300) loss: 1.801051\n",
      "(Iteration 3301 / 15300) loss: 1.690324\n",
      "(Iteration 3401 / 15300) loss: 1.758795\n",
      "(Iteration 3501 / 15300) loss: 1.533315\n",
      "(Iteration 3601 / 15300) loss: 1.473662\n",
      "(Iteration 3701 / 15300) loss: 1.577424\n",
      "(Iteration 3801 / 15300) loss: 1.804356\n",
      "(Epoch 5 / 20) train acc: 0.431000; val_acc: 0.418000\n",
      "(Iteration 3901 / 15300) loss: 1.570425\n",
      "(Iteration 4001 / 15300) loss: 1.573281\n",
      "(Iteration 4101 / 15300) loss: 1.465326\n",
      "(Iteration 4201 / 15300) loss: 1.655833\n",
      "(Iteration 4301 / 15300) loss: 1.669993\n",
      "(Iteration 4401 / 15300) loss: 1.704504\n",
      "(Iteration 4501 / 15300) loss: 1.452175\n",
      "(Epoch 6 / 20) train acc: 0.462000; val_acc: 0.440000\n",
      "(Iteration 4601 / 15300) loss: 1.638260\n",
      "(Iteration 4701 / 15300) loss: 1.567994\n",
      "(Iteration 4801 / 15300) loss: 1.710369\n",
      "(Iteration 4901 / 15300) loss: 1.369350\n",
      "(Iteration 5001 / 15300) loss: 1.639352\n",
      "(Iteration 5101 / 15300) loss: 1.419236\n",
      "(Iteration 5201 / 15300) loss: 1.562125\n",
      "(Iteration 5301 / 15300) loss: 1.343969\n",
      "(Epoch 7 / 20) train acc: 0.463000; val_acc: 0.453000\n",
      "(Iteration 5401 / 15300) loss: 1.520619\n",
      "(Iteration 5501 / 15300) loss: 1.452666\n",
      "(Iteration 5601 / 15300) loss: 1.530588\n",
      "(Iteration 5701 / 15300) loss: 1.444571\n",
      "(Iteration 5801 / 15300) loss: 1.535193\n",
      "(Iteration 5901 / 15300) loss: 1.371694\n",
      "(Iteration 6001 / 15300) loss: 1.602588\n",
      "(Iteration 6101 / 15300) loss: 1.755907\n",
      "(Epoch 8 / 20) train acc: 0.474000; val_acc: 0.476000\n",
      "(Iteration 6201 / 15300) loss: 1.626812\n",
      "(Iteration 6301 / 15300) loss: 1.607426\n",
      "(Iteration 6401 / 15300) loss: 1.505428\n",
      "(Iteration 6501 / 15300) loss: 1.689450\n",
      "(Iteration 6601 / 15300) loss: 1.644853\n",
      "(Iteration 6701 / 15300) loss: 1.694387\n",
      "(Iteration 6801 / 15300) loss: 1.484704\n",
      "(Epoch 9 / 20) train acc: 0.503000; val_acc: 0.482000\n",
      "(Iteration 6901 / 15300) loss: 1.401977\n",
      "(Iteration 7001 / 15300) loss: 1.931417\n",
      "(Iteration 7101 / 15300) loss: 1.637411\n",
      "(Iteration 7201 / 15300) loss: 1.479820\n",
      "(Iteration 7301 / 15300) loss: 1.494457\n",
      "(Iteration 7401 / 15300) loss: 1.443735\n",
      "(Iteration 7501 / 15300) loss: 1.489164\n",
      "(Iteration 7601 / 15300) loss: 1.504536\n",
      "(Epoch 10 / 20) train acc: 0.474000; val_acc: 0.484000\n",
      "(Iteration 7701 / 15300) loss: 1.252341\n",
      "(Iteration 7801 / 15300) loss: 1.503778\n",
      "(Iteration 7901 / 15300) loss: 1.576062\n",
      "(Iteration 8001 / 15300) loss: 1.695454\n",
      "(Iteration 8101 / 15300) loss: 1.433738\n",
      "(Iteration 8201 / 15300) loss: 1.476652\n",
      "(Iteration 8301 / 15300) loss: 1.503095\n",
      "(Iteration 8401 / 15300) loss: 1.593002\n",
      "(Epoch 11 / 20) train acc: 0.521000; val_acc: 0.492000\n",
      "(Iteration 8501 / 15300) loss: 1.525514\n",
      "(Iteration 8601 / 15300) loss: 1.402380\n",
      "(Iteration 8701 / 15300) loss: 1.462158\n",
      "(Iteration 8801 / 15300) loss: 1.667576\n",
      "(Iteration 8901 / 15300) loss: 1.510020\n",
      "(Iteration 9001 / 15300) loss: 1.648650\n",
      "(Iteration 9101 / 15300) loss: 1.625011\n",
      "(Epoch 12 / 20) train acc: 0.472000; val_acc: 0.496000\n",
      "(Iteration 9201 / 15300) loss: 1.589659\n",
      "(Iteration 9301 / 15300) loss: 1.493254\n",
      "(Iteration 9401 / 15300) loss: 1.511643\n",
      "(Iteration 9501 / 15300) loss: 1.718306\n",
      "(Iteration 9601 / 15300) loss: 1.561036\n",
      "(Iteration 9701 / 15300) loss: 1.612442\n",
      "(Iteration 9801 / 15300) loss: 1.244950\n",
      "(Iteration 9901 / 15300) loss: 1.320675\n",
      "(Epoch 13 / 20) train acc: 0.483000; val_acc: 0.500000\n",
      "(Iteration 10001 / 15300) loss: 1.569028\n",
      "(Iteration 10101 / 15300) loss: 1.576791\n",
      "(Iteration 10201 / 15300) loss: 1.447604\n",
      "(Iteration 10301 / 15300) loss: 1.590303\n",
      "(Iteration 10401 / 15300) loss: 1.566955\n",
      "(Iteration 10501 / 15300) loss: 1.407790\n",
      "(Iteration 10601 / 15300) loss: 1.572634\n",
      "(Iteration 10701 / 15300) loss: 1.522954\n",
      "(Epoch 14 / 20) train acc: 0.514000; val_acc: 0.501000\n",
      "(Iteration 10801 / 15300) loss: 1.508053\n",
      "(Iteration 10901 / 15300) loss: 1.548520\n",
      "(Iteration 11001 / 15300) loss: 1.466994\n",
      "(Iteration 11101 / 15300) loss: 1.626444\n",
      "(Iteration 11201 / 15300) loss: 1.283603\n",
      "(Iteration 11301 / 15300) loss: 1.533762\n",
      "(Iteration 11401 / 15300) loss: 1.581188\n",
      "(Epoch 15 / 20) train acc: 0.495000; val_acc: 0.500000\n",
      "(Iteration 11501 / 15300) loss: 1.599554\n",
      "(Iteration 11601 / 15300) loss: 1.586622\n",
      "(Iteration 11701 / 15300) loss: 1.628601\n",
      "(Iteration 11801 / 15300) loss: 1.334835\n",
      "(Iteration 11901 / 15300) loss: 1.319940\n",
      "(Iteration 12001 / 15300) loss: 1.440557\n",
      "(Iteration 12101 / 15300) loss: 1.286238\n",
      "(Iteration 12201 / 15300) loss: 1.631400\n",
      "(Epoch 16 / 20) train acc: 0.508000; val_acc: 0.503000\n",
      "(Iteration 12301 / 15300) loss: 1.375879\n",
      "(Iteration 12401 / 15300) loss: 1.612201\n",
      "(Iteration 12501 / 15300) loss: 1.370820\n",
      "(Iteration 12601 / 15300) loss: 1.535206\n",
      "(Iteration 12701 / 15300) loss: 1.554712\n",
      "(Iteration 12801 / 15300) loss: 1.595056\n",
      "(Iteration 12901 / 15300) loss: 1.359072\n",
      "(Iteration 13001 / 15300) loss: 1.443606\n",
      "(Epoch 17 / 20) train acc: 0.518000; val_acc: 0.505000\n",
      "(Iteration 13101 / 15300) loss: 1.386315\n",
      "(Iteration 13201 / 15300) loss: 1.548336\n",
      "(Iteration 13301 / 15300) loss: 1.513980\n",
      "(Iteration 13401 / 15300) loss: 1.541412\n",
      "(Iteration 13501 / 15300) loss: 1.513557\n",
      "(Iteration 13601 / 15300) loss: 1.482156\n",
      "(Iteration 13701 / 15300) loss: 1.340572\n",
      "(Epoch 18 / 20) train acc: 0.500000; val_acc: 0.505000\n",
      "(Iteration 13801 / 15300) loss: 1.342765\n",
      "(Iteration 13901 / 15300) loss: 1.452766\n",
      "(Iteration 14001 / 15300) loss: 1.678309\n",
      "(Iteration 14101 / 15300) loss: 1.362622\n",
      "(Iteration 14201 / 15300) loss: 1.419772\n",
      "(Iteration 14301 / 15300) loss: 1.439971\n",
      "(Iteration 14401 / 15300) loss: 1.458629\n",
      "(Iteration 14501 / 15300) loss: 1.498378\n",
      "(Epoch 19 / 20) train acc: 0.509000; val_acc: 0.510000\n",
      "(Iteration 14601 / 15300) loss: 1.405952\n",
      "(Iteration 14701 / 15300) loss: 1.541992\n",
      "(Iteration 14801 / 15300) loss: 1.402915\n",
      "(Iteration 14901 / 15300) loss: 1.419224\n",
      "(Iteration 15001 / 15300) loss: 1.682635\n",
      "(Iteration 15101 / 15300) loss: 1.308590\n",
      "(Iteration 15201 / 15300) loss: 1.534442\n",
      "(Epoch 20 / 20) train acc: 0.516000; val_acc: 0.510000\n",
      "New best model found with validation accuracy: 0.4358\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.01, 'num_epochs': 20, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 30620) loss: 2.302864\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.111000\n",
      "(Iteration 101 / 30620) loss: 2.302966\n",
      "(Iteration 201 / 30620) loss: 2.302980\n",
      "(Iteration 301 / 30620) loss: 2.303459\n",
      "(Iteration 401 / 30620) loss: 2.300706\n",
      "(Iteration 501 / 30620) loss: 2.298431\n",
      "(Iteration 601 / 30620) loss: 2.298096\n",
      "(Iteration 701 / 30620) loss: 2.297842\n",
      "(Iteration 801 / 30620) loss: 2.292541\n",
      "(Iteration 901 / 30620) loss: 2.280824\n",
      "(Iteration 1001 / 30620) loss: 2.262366\n",
      "(Iteration 1101 / 30620) loss: 2.230278\n",
      "(Iteration 1201 / 30620) loss: 2.158101\n",
      "(Iteration 1301 / 30620) loss: 2.214423\n",
      "(Iteration 1401 / 30620) loss: 2.047431\n",
      "(Iteration 1501 / 30620) loss: 2.067830\n",
      "(Epoch 1 / 20) train acc: 0.280000; val_acc: 0.292000\n",
      "(Iteration 1601 / 30620) loss: 1.906315\n",
      "(Iteration 1701 / 30620) loss: 1.930246\n",
      "(Iteration 1801 / 30620) loss: 1.871591\n",
      "(Iteration 1901 / 30620) loss: 1.836118\n",
      "(Iteration 2001 / 30620) loss: 1.763968\n",
      "(Iteration 2101 / 30620) loss: 1.958357\n",
      "(Iteration 2201 / 30620) loss: 1.809315\n",
      "(Iteration 2301 / 30620) loss: 1.902537\n",
      "(Iteration 2401 / 30620) loss: 1.735960\n",
      "(Iteration 2501 / 30620) loss: 1.948498\n",
      "(Iteration 2601 / 30620) loss: 1.799607\n",
      "(Iteration 2701 / 30620) loss: 1.654016\n",
      "(Iteration 2801 / 30620) loss: 1.794635\n",
      "(Iteration 2901 / 30620) loss: 1.669101\n",
      "(Iteration 3001 / 30620) loss: 1.442142\n",
      "(Epoch 2 / 20) train acc: 0.435000; val_acc: 0.418000\n",
      "(Iteration 3101 / 30620) loss: 1.779019\n",
      "(Iteration 3201 / 30620) loss: 1.536420\n",
      "(Iteration 3301 / 30620) loss: 1.476708\n",
      "(Iteration 3401 / 30620) loss: 1.598961\n",
      "(Iteration 3501 / 30620) loss: 1.562976\n",
      "(Iteration 3601 / 30620) loss: 1.452791\n",
      "(Iteration 3701 / 30620) loss: 1.643160\n",
      "(Iteration 3801 / 30620) loss: 1.542688\n",
      "(Iteration 3901 / 30620) loss: 1.493008\n",
      "(Iteration 4001 / 30620) loss: 1.642269\n",
      "(Iteration 4101 / 30620) loss: 1.494589\n",
      "(Iteration 4201 / 30620) loss: 1.547921\n",
      "(Iteration 4301 / 30620) loss: 1.784352\n",
      "(Iteration 4401 / 30620) loss: 1.628185\n",
      "(Iteration 4501 / 30620) loss: 1.476951\n",
      "(Epoch 3 / 20) train acc: 0.469000; val_acc: 0.471000\n",
      "(Iteration 4601 / 30620) loss: 1.343764\n",
      "(Iteration 4701 / 30620) loss: 1.590844\n",
      "(Iteration 4801 / 30620) loss: 1.459303\n",
      "(Iteration 4901 / 30620) loss: 1.533619\n",
      "(Iteration 5001 / 30620) loss: 1.642381\n",
      "(Iteration 5101 / 30620) loss: 1.598598\n",
      "(Iteration 5201 / 30620) loss: 1.567050\n",
      "(Iteration 5301 / 30620) loss: 1.617600\n",
      "(Iteration 5401 / 30620) loss: 1.514060\n",
      "(Iteration 5501 / 30620) loss: 1.527575\n",
      "(Iteration 5601 / 30620) loss: 1.423557\n",
      "(Iteration 5701 / 30620) loss: 1.336918\n",
      "(Iteration 5801 / 30620) loss: 1.321885\n",
      "(Iteration 5901 / 30620) loss: 1.797193\n",
      "(Iteration 6001 / 30620) loss: 1.489776\n",
      "(Iteration 6101 / 30620) loss: 1.486556\n",
      "(Epoch 4 / 20) train acc: 0.520000; val_acc: 0.493000\n",
      "(Iteration 6201 / 30620) loss: 1.462759\n",
      "(Iteration 6301 / 30620) loss: 1.229868\n",
      "(Iteration 6401 / 30620) loss: 1.728805\n",
      "(Iteration 6501 / 30620) loss: 1.689812\n",
      "(Iteration 6601 / 30620) loss: 1.507840\n",
      "(Iteration 6701 / 30620) loss: 1.719094\n",
      "(Iteration 6801 / 30620) loss: 1.555994\n",
      "(Iteration 6901 / 30620) loss: 1.706211\n",
      "(Iteration 7001 / 30620) loss: 1.602780\n",
      "(Iteration 7101 / 30620) loss: 1.526707\n",
      "(Iteration 7201 / 30620) loss: 1.329723\n",
      "(Iteration 7301 / 30620) loss: 1.635958\n",
      "(Iteration 7401 / 30620) loss: 1.422492\n",
      "(Iteration 7501 / 30620) loss: 1.555189\n",
      "(Iteration 7601 / 30620) loss: 1.686496\n",
      "(Epoch 5 / 20) train acc: 0.533000; val_acc: 0.497000\n",
      "(Iteration 7701 / 30620) loss: 1.476502\n",
      "(Iteration 7801 / 30620) loss: 1.417340\n",
      "(Iteration 7901 / 30620) loss: 1.608116\n",
      "(Iteration 8001 / 30620) loss: 1.266144\n",
      "(Iteration 8101 / 30620) loss: 1.511296\n",
      "(Iteration 8201 / 30620) loss: 1.388985\n",
      "(Iteration 8301 / 30620) loss: 1.345026\n",
      "(Iteration 8401 / 30620) loss: 1.256854\n",
      "(Iteration 8501 / 30620) loss: 1.383153\n",
      "(Iteration 8601 / 30620) loss: 1.663202\n",
      "(Iteration 8701 / 30620) loss: 1.334414\n",
      "(Iteration 8801 / 30620) loss: 1.617870\n",
      "(Iteration 8901 / 30620) loss: 1.705488\n",
      "(Iteration 9001 / 30620) loss: 1.631726\n",
      "(Iteration 9101 / 30620) loss: 1.252481\n",
      "(Epoch 6 / 20) train acc: 0.508000; val_acc: 0.511000\n",
      "(Iteration 9201 / 30620) loss: 1.396907\n",
      "(Iteration 9301 / 30620) loss: 1.564977\n",
      "(Iteration 9401 / 30620) loss: 1.499565\n",
      "(Iteration 9501 / 30620) loss: 1.369324\n",
      "(Iteration 9601 / 30620) loss: 1.559041\n",
      "(Iteration 9701 / 30620) loss: 1.688385\n",
      "(Iteration 9801 / 30620) loss: 1.435699\n",
      "(Iteration 9901 / 30620) loss: 1.147745\n",
      "(Iteration 10001 / 30620) loss: 1.578849\n",
      "(Iteration 10101 / 30620) loss: 1.371377\n",
      "(Iteration 10201 / 30620) loss: 1.246640\n",
      "(Iteration 10301 / 30620) loss: 1.301505\n",
      "(Iteration 10401 / 30620) loss: 1.712741\n",
      "(Iteration 10501 / 30620) loss: 1.263617\n",
      "(Iteration 10601 / 30620) loss: 1.236623\n",
      "(Iteration 10701 / 30620) loss: 1.412522\n",
      "(Epoch 7 / 20) train acc: 0.509000; val_acc: 0.514000\n",
      "(Iteration 10801 / 30620) loss: 1.365789\n",
      "(Iteration 10901 / 30620) loss: 1.412275\n",
      "(Iteration 11001 / 30620) loss: 1.498198\n",
      "(Iteration 11101 / 30620) loss: 1.389016\n",
      "(Iteration 11201 / 30620) loss: 1.358173\n",
      "(Iteration 11301 / 30620) loss: 1.693330\n",
      "(Iteration 11401 / 30620) loss: 1.496639\n",
      "(Iteration 11501 / 30620) loss: 1.417038\n",
      "(Iteration 11601 / 30620) loss: 1.599905\n",
      "(Iteration 11701 / 30620) loss: 1.622784\n",
      "(Iteration 11801 / 30620) loss: 1.495085\n",
      "(Iteration 11901 / 30620) loss: 1.547608\n",
      "(Iteration 12001 / 30620) loss: 1.471736\n",
      "(Iteration 12101 / 30620) loss: 1.477042\n",
      "(Iteration 12201 / 30620) loss: 1.178241\n",
      "(Epoch 8 / 20) train acc: 0.525000; val_acc: 0.520000\n",
      "(Iteration 12301 / 30620) loss: 1.408745\n",
      "(Iteration 12401 / 30620) loss: 1.369445\n",
      "(Iteration 12501 / 30620) loss: 1.295164\n",
      "(Iteration 12601 / 30620) loss: 1.751627\n",
      "(Iteration 12701 / 30620) loss: 1.352959\n",
      "(Iteration 12801 / 30620) loss: 1.277624\n",
      "(Iteration 12901 / 30620) loss: 1.745719\n",
      "(Iteration 13001 / 30620) loss: 1.396563\n",
      "(Iteration 13101 / 30620) loss: 1.383618\n",
      "(Iteration 13201 / 30620) loss: 1.583814\n",
      "(Iteration 13301 / 30620) loss: 1.849279\n",
      "(Iteration 13401 / 30620) loss: 1.560283\n",
      "(Iteration 13501 / 30620) loss: 1.482971\n",
      "(Iteration 13601 / 30620) loss: 1.791156\n",
      "(Iteration 13701 / 30620) loss: 1.411566\n",
      "(Epoch 9 / 20) train acc: 0.549000; val_acc: 0.509000\n",
      "(Iteration 13801 / 30620) loss: 1.355594\n",
      "(Iteration 13901 / 30620) loss: 1.208495\n",
      "(Iteration 14001 / 30620) loss: 1.410624\n",
      "(Iteration 14101 / 30620) loss: 1.715413\n",
      "(Iteration 14201 / 30620) loss: 1.588140\n",
      "(Iteration 14301 / 30620) loss: 1.092420\n",
      "(Iteration 14401 / 30620) loss: 1.205884\n",
      "(Iteration 14501 / 30620) loss: 1.323787\n",
      "(Iteration 14601 / 30620) loss: 1.307919\n",
      "(Iteration 14701 / 30620) loss: 1.389937\n",
      "(Iteration 14801 / 30620) loss: 1.478904\n",
      "(Iteration 14901 / 30620) loss: 1.722456\n",
      "(Iteration 15001 / 30620) loss: 2.002640\n",
      "(Iteration 15101 / 30620) loss: 1.279368\n",
      "(Iteration 15201 / 30620) loss: 1.456338\n",
      "(Iteration 15301 / 30620) loss: 1.302023\n",
      "(Epoch 10 / 20) train acc: 0.546000; val_acc: 0.523000\n",
      "(Iteration 15401 / 30620) loss: 1.585848\n",
      "(Iteration 15501 / 30620) loss: 1.658916\n",
      "(Iteration 15601 / 30620) loss: 1.356705\n",
      "(Iteration 15701 / 30620) loss: 1.487034\n",
      "(Iteration 15801 / 30620) loss: 1.478026\n",
      "(Iteration 15901 / 30620) loss: 1.325111\n",
      "(Iteration 16001 / 30620) loss: 1.381374\n",
      "(Iteration 16101 / 30620) loss: 1.469030\n",
      "(Iteration 16201 / 30620) loss: 1.147567\n",
      "(Iteration 16301 / 30620) loss: 1.365713\n",
      "(Iteration 16401 / 30620) loss: 1.462257\n",
      "(Iteration 16501 / 30620) loss: 1.515041\n",
      "(Iteration 16601 / 30620) loss: 1.368584\n",
      "(Iteration 16701 / 30620) loss: 1.294350\n",
      "(Iteration 16801 / 30620) loss: 1.329028\n",
      "(Epoch 11 / 20) train acc: 0.526000; val_acc: 0.519000\n",
      "(Iteration 16901 / 30620) loss: 1.372001\n",
      "(Iteration 17001 / 30620) loss: 1.194825\n",
      "(Iteration 17101 / 30620) loss: 1.348093\n",
      "(Iteration 17201 / 30620) loss: 1.769088\n",
      "(Iteration 17301 / 30620) loss: 1.378176\n",
      "(Iteration 17401 / 30620) loss: 1.244693\n",
      "(Iteration 17501 / 30620) loss: 1.500350\n",
      "(Iteration 17601 / 30620) loss: 1.572526\n",
      "(Iteration 17701 / 30620) loss: 1.555253\n",
      "(Iteration 17801 / 30620) loss: 1.452593\n",
      "(Iteration 17901 / 30620) loss: 1.525342\n",
      "(Iteration 18001 / 30620) loss: 1.310747\n",
      "(Iteration 18101 / 30620) loss: 1.312649\n",
      "(Iteration 18201 / 30620) loss: 1.493693\n",
      "(Iteration 18301 / 30620) loss: 1.400039\n",
      "(Epoch 12 / 20) train acc: 0.530000; val_acc: 0.511000\n",
      "(Iteration 18401 / 30620) loss: 1.586724\n",
      "(Iteration 18501 / 30620) loss: 1.362100\n",
      "(Iteration 18601 / 30620) loss: 1.502649\n",
      "(Iteration 18701 / 30620) loss: 1.418730\n",
      "(Iteration 18801 / 30620) loss: 1.556251\n",
      "(Iteration 18901 / 30620) loss: 1.478927\n",
      "(Iteration 19001 / 30620) loss: 1.385485\n",
      "(Iteration 19101 / 30620) loss: 1.253125\n",
      "(Iteration 19201 / 30620) loss: 1.585132\n",
      "(Iteration 19301 / 30620) loss: 1.461214\n",
      "(Iteration 19401 / 30620) loss: 1.840789\n",
      "(Iteration 19501 / 30620) loss: 1.555056\n",
      "(Iteration 19601 / 30620) loss: 1.438481\n",
      "(Iteration 19701 / 30620) loss: 1.674046\n",
      "(Iteration 19801 / 30620) loss: 1.500883\n",
      "(Iteration 19901 / 30620) loss: 1.458181\n",
      "(Epoch 13 / 20) train acc: 0.518000; val_acc: 0.525000\n",
      "(Iteration 20001 / 30620) loss: 1.441686\n",
      "(Iteration 20101 / 30620) loss: 1.851331\n",
      "(Iteration 20201 / 30620) loss: 1.537090\n",
      "(Iteration 20301 / 30620) loss: 1.194953\n",
      "(Iteration 20401 / 30620) loss: 1.688849\n",
      "(Iteration 20501 / 30620) loss: 1.597951\n",
      "(Iteration 20601 / 30620) loss: 1.314059\n",
      "(Iteration 20701 / 30620) loss: 1.409474\n",
      "(Iteration 20801 / 30620) loss: 1.098070\n",
      "(Iteration 20901 / 30620) loss: 1.405903\n",
      "(Iteration 21001 / 30620) loss: 1.197522\n",
      "(Iteration 21101 / 30620) loss: 1.342510\n",
      "(Iteration 21201 / 30620) loss: 1.514467\n",
      "(Iteration 21301 / 30620) loss: 1.624758\n",
      "(Iteration 21401 / 30620) loss: 1.508281\n",
      "(Epoch 14 / 20) train acc: 0.534000; val_acc: 0.514000\n",
      "(Iteration 21501 / 30620) loss: 1.303586\n",
      "(Iteration 21601 / 30620) loss: 1.390583\n",
      "(Iteration 21701 / 30620) loss: 1.332675\n",
      "(Iteration 21801 / 30620) loss: 1.486307\n",
      "(Iteration 21901 / 30620) loss: 1.366842\n",
      "(Iteration 22001 / 30620) loss: 1.481239\n",
      "(Iteration 22101 / 30620) loss: 1.106745\n",
      "(Iteration 22201 / 30620) loss: 1.716625\n",
      "(Iteration 22301 / 30620) loss: 1.425584\n",
      "(Iteration 22401 / 30620) loss: 1.362347\n",
      "(Iteration 22501 / 30620) loss: 1.376999\n",
      "(Iteration 22601 / 30620) loss: 1.497429\n",
      "(Iteration 22701 / 30620) loss: 1.493308\n",
      "(Iteration 22801 / 30620) loss: 1.719867\n",
      "(Iteration 22901 / 30620) loss: 1.356222\n",
      "(Epoch 15 / 20) train acc: 0.526000; val_acc: 0.522000\n",
      "(Iteration 23001 / 30620) loss: 1.513497\n",
      "(Iteration 23101 / 30620) loss: 1.322433\n",
      "(Iteration 23201 / 30620) loss: 1.674632\n",
      "(Iteration 23301 / 30620) loss: 1.341509\n",
      "(Iteration 23401 / 30620) loss: 1.555293\n",
      "(Iteration 23501 / 30620) loss: 1.356274\n",
      "(Iteration 23601 / 30620) loss: 1.476168\n",
      "(Iteration 23701 / 30620) loss: 1.731504\n",
      "(Iteration 23801 / 30620) loss: 1.386772\n",
      "(Iteration 23901 / 30620) loss: 1.359614\n",
      "(Iteration 24001 / 30620) loss: 1.558126\n",
      "(Iteration 24101 / 30620) loss: 1.489283\n",
      "(Iteration 24201 / 30620) loss: 1.265114\n",
      "(Iteration 24301 / 30620) loss: 1.488853\n",
      "(Iteration 24401 / 30620) loss: 1.800076\n",
      "(Epoch 16 / 20) train acc: 0.540000; val_acc: 0.525000\n",
      "(Iteration 24501 / 30620) loss: 1.365954\n",
      "(Iteration 24601 / 30620) loss: 1.811998\n",
      "(Iteration 24701 / 30620) loss: 1.367054\n",
      "(Iteration 24801 / 30620) loss: 1.460259\n",
      "(Iteration 24901 / 30620) loss: 1.256509\n",
      "(Iteration 25001 / 30620) loss: 1.495976\n",
      "(Iteration 25101 / 30620) loss: 1.440835\n",
      "(Iteration 25201 / 30620) loss: 1.327366\n",
      "(Iteration 25301 / 30620) loss: 1.290110\n",
      "(Iteration 25401 / 30620) loss: 1.413926\n",
      "(Iteration 25501 / 30620) loss: 1.117505\n",
      "(Iteration 25601 / 30620) loss: 1.345458\n",
      "(Iteration 25701 / 30620) loss: 1.369594\n",
      "(Iteration 25801 / 30620) loss: 1.645958\n",
      "(Iteration 25901 / 30620) loss: 1.440759\n",
      "(Iteration 26001 / 30620) loss: 1.195069\n",
      "(Epoch 17 / 20) train acc: 0.517000; val_acc: 0.523000\n",
      "(Iteration 26101 / 30620) loss: 1.404955\n",
      "(Iteration 26201 / 30620) loss: 1.652044\n",
      "(Iteration 26301 / 30620) loss: 1.446559\n",
      "(Iteration 26401 / 30620) loss: 1.402559\n",
      "(Iteration 26501 / 30620) loss: 1.403000\n",
      "(Iteration 26601 / 30620) loss: 1.518121\n",
      "(Iteration 26701 / 30620) loss: 1.243797\n",
      "(Iteration 26801 / 30620) loss: 1.649468\n",
      "(Iteration 26901 / 30620) loss: 1.289306\n",
      "(Iteration 27001 / 30620) loss: 1.438079\n",
      "(Iteration 27101 / 30620) loss: 1.427161\n",
      "(Iteration 27201 / 30620) loss: 1.418331\n",
      "(Iteration 27301 / 30620) loss: 1.424350\n",
      "(Iteration 27401 / 30620) loss: 1.536360\n",
      "(Iteration 27501 / 30620) loss: 1.456213\n",
      "(Epoch 18 / 20) train acc: 0.561000; val_acc: 0.523000\n",
      "(Iteration 27601 / 30620) loss: 1.482887\n",
      "(Iteration 27701 / 30620) loss: 1.204914\n",
      "(Iteration 27801 / 30620) loss: 1.275326\n",
      "(Iteration 27901 / 30620) loss: 1.476684\n",
      "(Iteration 28001 / 30620) loss: 1.267060\n",
      "(Iteration 28101 / 30620) loss: 1.339896\n",
      "(Iteration 28201 / 30620) loss: 1.526440\n",
      "(Iteration 28301 / 30620) loss: 1.331402\n",
      "(Iteration 28401 / 30620) loss: 1.495724\n",
      "(Iteration 28501 / 30620) loss: 1.489934\n",
      "(Iteration 28601 / 30620) loss: 1.508738\n",
      "(Iteration 28701 / 30620) loss: 1.788164\n",
      "(Iteration 28801 / 30620) loss: 1.159440\n",
      "(Iteration 28901 / 30620) loss: 1.354753\n",
      "(Iteration 29001 / 30620) loss: 1.425442\n",
      "(Epoch 19 / 20) train acc: 0.551000; val_acc: 0.523000\n",
      "(Iteration 29101 / 30620) loss: 1.478570\n",
      "(Iteration 29201 / 30620) loss: 1.452151\n",
      "(Iteration 29301 / 30620) loss: 1.221793\n",
      "(Iteration 29401 / 30620) loss: 1.501357\n",
      "(Iteration 29501 / 30620) loss: 1.518530\n",
      "(Iteration 29601 / 30620) loss: 1.345735\n",
      "(Iteration 29701 / 30620) loss: 1.349281\n",
      "(Iteration 29801 / 30620) loss: 1.507040\n",
      "(Iteration 29901 / 30620) loss: 1.372092\n",
      "(Iteration 30001 / 30620) loss: 1.433529\n",
      "(Iteration 30101 / 30620) loss: 1.577002\n",
      "(Iteration 30201 / 30620) loss: 1.639181\n",
      "(Iteration 30301 / 30620) loss: 1.387305\n",
      "(Iteration 30401 / 30620) loss: 1.576758\n",
      "(Iteration 30501 / 30620) loss: 1.330261\n",
      "(Iteration 30601 / 30620) loss: 1.659091\n",
      "(Epoch 20 / 20) train acc: 0.548000; val_acc: 0.516000\n",
      "New best model found with validation accuracy: 0.4790\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.01, 'num_epochs': 30, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 22950) loss: 2.305870\n",
      "(Epoch 0 / 30) train acc: 0.102000; val_acc: 0.081000\n",
      "(Iteration 101 / 22950) loss: 2.305311\n",
      "(Iteration 201 / 22950) loss: 2.304938\n",
      "(Iteration 301 / 22950) loss: 2.304553\n",
      "(Iteration 401 / 22950) loss: 2.302621\n",
      "(Iteration 501 / 22950) loss: 2.303774\n",
      "(Iteration 601 / 22950) loss: 2.301752\n",
      "(Iteration 701 / 22950) loss: 2.298199\n",
      "(Epoch 1 / 30) train acc: 0.092000; val_acc: 0.087000\n",
      "(Iteration 801 / 22950) loss: 2.301290\n",
      "(Iteration 901 / 22950) loss: 2.298750\n",
      "(Iteration 1001 / 22950) loss: 2.296792\n",
      "(Iteration 1101 / 22950) loss: 2.292168\n",
      "(Iteration 1201 / 22950) loss: 2.288987\n",
      "(Iteration 1301 / 22950) loss: 2.274940\n",
      "(Iteration 1401 / 22950) loss: 2.277833\n",
      "(Iteration 1501 / 22950) loss: 2.257100\n",
      "(Epoch 2 / 30) train acc: 0.210000; val_acc: 0.214000\n",
      "(Iteration 1601 / 22950) loss: 2.270839\n",
      "(Iteration 1701 / 22950) loss: 2.231693\n",
      "(Iteration 1801 / 22950) loss: 2.215169\n",
      "(Iteration 1901 / 22950) loss: 2.158055\n",
      "(Iteration 2001 / 22950) loss: 2.195820\n",
      "(Iteration 2101 / 22950) loss: 2.206065\n",
      "(Iteration 2201 / 22950) loss: 2.214329\n",
      "(Epoch 3 / 30) train acc: 0.273000; val_acc: 0.284000\n",
      "(Iteration 2301 / 22950) loss: 2.195647\n",
      "(Iteration 2401 / 22950) loss: 2.076090\n",
      "(Iteration 2501 / 22950) loss: 2.124759\n",
      "(Iteration 2601 / 22950) loss: 2.149692\n",
      "(Iteration 2701 / 22950) loss: 2.155867\n",
      "(Iteration 2801 / 22950) loss: 2.081187\n",
      "(Iteration 2901 / 22950) loss: 2.004175\n",
      "(Iteration 3001 / 22950) loss: 2.149245\n",
      "(Epoch 4 / 30) train acc: 0.253000; val_acc: 0.296000\n",
      "(Iteration 3101 / 22950) loss: 2.104965\n",
      "(Iteration 3201 / 22950) loss: 1.983016\n",
      "(Iteration 3301 / 22950) loss: 2.096316\n",
      "(Iteration 3401 / 22950) loss: 2.023227\n",
      "(Iteration 3501 / 22950) loss: 2.066977\n",
      "(Iteration 3601 / 22950) loss: 2.026109\n",
      "(Iteration 3701 / 22950) loss: 2.228732\n",
      "(Iteration 3801 / 22950) loss: 2.120790\n",
      "(Epoch 5 / 30) train acc: 0.299000; val_acc: 0.315000\n",
      "(Iteration 3901 / 22950) loss: 2.065767\n",
      "(Iteration 4001 / 22950) loss: 2.071847\n",
      "(Iteration 4101 / 22950) loss: 1.968691\n",
      "(Iteration 4201 / 22950) loss: 2.055153\n",
      "(Iteration 4301 / 22950) loss: 2.021845\n",
      "(Iteration 4401 / 22950) loss: 2.132997\n",
      "(Iteration 4501 / 22950) loss: 2.004393\n",
      "(Epoch 6 / 30) train acc: 0.329000; val_acc: 0.331000\n",
      "(Iteration 4601 / 22950) loss: 1.987481\n",
      "(Iteration 4701 / 22950) loss: 2.011910\n",
      "(Iteration 4801 / 22950) loss: 2.027431\n",
      "(Iteration 4901 / 22950) loss: 2.056876\n",
      "(Iteration 5001 / 22950) loss: 2.042807\n",
      "(Iteration 5101 / 22950) loss: 2.008720\n",
      "(Iteration 5201 / 22950) loss: 2.067922\n",
      "(Iteration 5301 / 22950) loss: 1.978750\n",
      "(Epoch 7 / 30) train acc: 0.339000; val_acc: 0.366000\n",
      "(Iteration 5401 / 22950) loss: 1.991226\n",
      "(Iteration 5501 / 22950) loss: 2.005790\n",
      "(Iteration 5601 / 22950) loss: 2.068021\n",
      "(Iteration 5701 / 22950) loss: 1.977895\n",
      "(Iteration 5801 / 22950) loss: 2.177336\n",
      "(Iteration 5901 / 22950) loss: 2.061342\n",
      "(Iteration 6001 / 22950) loss: 1.957893\n",
      "(Iteration 6101 / 22950) loss: 1.944516\n",
      "(Epoch 8 / 30) train acc: 0.378000; val_acc: 0.381000\n",
      "(Iteration 6201 / 22950) loss: 2.106602\n",
      "(Iteration 6301 / 22950) loss: 2.015599\n",
      "(Iteration 6401 / 22950) loss: 2.112649\n",
      "(Iteration 6501 / 22950) loss: 1.938992\n",
      "(Iteration 6601 / 22950) loss: 2.004396\n",
      "(Iteration 6701 / 22950) loss: 1.985789\n",
      "(Iteration 6801 / 22950) loss: 2.058088\n",
      "(Epoch 9 / 30) train acc: 0.331000; val_acc: 0.385000\n",
      "(Iteration 6901 / 22950) loss: 2.129178\n",
      "(Iteration 7001 / 22950) loss: 2.074239\n",
      "(Iteration 7101 / 22950) loss: 1.945770\n",
      "(Iteration 7201 / 22950) loss: 1.955871\n",
      "(Iteration 7301 / 22950) loss: 1.911152\n",
      "(Iteration 7401 / 22950) loss: 2.055033\n",
      "(Iteration 7501 / 22950) loss: 1.976528\n",
      "(Iteration 7601 / 22950) loss: 2.027245\n",
      "(Epoch 10 / 30) train acc: 0.402000; val_acc: 0.404000\n",
      "(Iteration 7701 / 22950) loss: 1.964103\n",
      "(Iteration 7801 / 22950) loss: 1.920921\n",
      "(Iteration 7901 / 22950) loss: 2.106432\n",
      "(Iteration 8001 / 22950) loss: 1.994439\n",
      "(Iteration 8101 / 22950) loss: 2.083209\n",
      "(Iteration 8201 / 22950) loss: 2.030014\n",
      "(Iteration 8301 / 22950) loss: 2.110931\n",
      "(Iteration 8401 / 22950) loss: 2.141406\n",
      "(Epoch 11 / 30) train acc: 0.410000; val_acc: 0.406000\n",
      "(Iteration 8501 / 22950) loss: 2.014592\n",
      "(Iteration 8601 / 22950) loss: 2.000809\n",
      "(Iteration 8701 / 22950) loss: 1.986168\n",
      "(Iteration 8801 / 22950) loss: 1.982008\n",
      "(Iteration 8901 / 22950) loss: 1.932669\n",
      "(Iteration 9001 / 22950) loss: 1.969573\n",
      "(Iteration 9101 / 22950) loss: 2.019198\n",
      "(Epoch 12 / 30) train acc: 0.408000; val_acc: 0.413000\n",
      "(Iteration 9201 / 22950) loss: 2.159305\n",
      "(Iteration 9301 / 22950) loss: 1.998810\n",
      "(Iteration 9401 / 22950) loss: 2.081132\n",
      "(Iteration 9501 / 22950) loss: 1.915964\n",
      "(Iteration 9601 / 22950) loss: 1.947009\n",
      "(Iteration 9701 / 22950) loss: 1.937777\n",
      "(Iteration 9801 / 22950) loss: 1.899616\n",
      "(Iteration 9901 / 22950) loss: 2.022450\n",
      "(Epoch 13 / 30) train acc: 0.419000; val_acc: 0.408000\n",
      "(Iteration 10001 / 22950) loss: 1.880440\n",
      "(Iteration 10101 / 22950) loss: 2.015554\n",
      "(Iteration 10201 / 22950) loss: 2.058366\n",
      "(Iteration 10301 / 22950) loss: 2.016682\n",
      "(Iteration 10401 / 22950) loss: 2.003241\n",
      "(Iteration 10501 / 22950) loss: 1.946908\n",
      "(Iteration 10601 / 22950) loss: 1.931938\n",
      "(Iteration 10701 / 22950) loss: 1.932646\n",
      "(Epoch 14 / 30) train acc: 0.422000; val_acc: 0.405000\n",
      "(Iteration 10801 / 22950) loss: 1.909642\n",
      "(Iteration 10901 / 22950) loss: 2.128566\n",
      "(Iteration 11001 / 22950) loss: 2.078144\n",
      "(Iteration 11101 / 22950) loss: 2.009990\n",
      "(Iteration 11201 / 22950) loss: 1.973582\n",
      "(Iteration 11301 / 22950) loss: 1.974203\n",
      "(Iteration 11401 / 22950) loss: 2.019080\n",
      "(Epoch 15 / 30) train acc: 0.415000; val_acc: 0.411000\n",
      "(Iteration 11501 / 22950) loss: 1.977271\n",
      "(Iteration 11601 / 22950) loss: 1.920324\n",
      "(Iteration 11701 / 22950) loss: 2.108995\n",
      "(Iteration 11801 / 22950) loss: 2.033424\n",
      "(Iteration 11901 / 22950) loss: 1.936877\n",
      "(Iteration 12001 / 22950) loss: 2.027891\n",
      "(Iteration 12101 / 22950) loss: 1.997330\n",
      "(Iteration 12201 / 22950) loss: 2.019515\n",
      "(Epoch 16 / 30) train acc: 0.427000; val_acc: 0.414000\n",
      "(Iteration 12301 / 22950) loss: 2.040808\n",
      "(Iteration 12401 / 22950) loss: 1.953848\n",
      "(Iteration 12501 / 22950) loss: 1.983369\n",
      "(Iteration 12601 / 22950) loss: 2.003894\n",
      "(Iteration 12701 / 22950) loss: 2.011592\n",
      "(Iteration 12801 / 22950) loss: 2.026546\n",
      "(Iteration 12901 / 22950) loss: 1.974286\n",
      "(Iteration 13001 / 22950) loss: 1.994311\n",
      "(Epoch 17 / 30) train acc: 0.428000; val_acc: 0.413000\n",
      "(Iteration 13101 / 22950) loss: 1.973422\n",
      "(Iteration 13201 / 22950) loss: 1.950531\n",
      "(Iteration 13301 / 22950) loss: 1.960131\n",
      "(Iteration 13401 / 22950) loss: 1.955642\n",
      "(Iteration 13501 / 22950) loss: 1.886144\n",
      "(Iteration 13601 / 22950) loss: 1.958945\n",
      "(Iteration 13701 / 22950) loss: 1.933071\n",
      "(Epoch 18 / 30) train acc: 0.424000; val_acc: 0.410000\n",
      "(Iteration 13801 / 22950) loss: 1.982414\n",
      "(Iteration 13901 / 22950) loss: 1.857821\n",
      "(Iteration 14001 / 22950) loss: 1.983501\n",
      "(Iteration 14101 / 22950) loss: 1.987178\n",
      "(Iteration 14201 / 22950) loss: 1.989228\n",
      "(Iteration 14301 / 22950) loss: 1.926765\n",
      "(Iteration 14401 / 22950) loss: 2.007956\n",
      "(Iteration 14501 / 22950) loss: 2.041584\n",
      "(Epoch 19 / 30) train acc: 0.428000; val_acc: 0.405000\n",
      "(Iteration 14601 / 22950) loss: 1.847455\n",
      "(Iteration 14701 / 22950) loss: 2.074085\n",
      "(Iteration 14801 / 22950) loss: 1.813374\n",
      "(Iteration 14901 / 22950) loss: 1.914429\n",
      "(Iteration 15001 / 22950) loss: 2.003586\n",
      "(Iteration 15101 / 22950) loss: 1.924111\n",
      "(Iteration 15201 / 22950) loss: 1.993813\n",
      "(Epoch 20 / 30) train acc: 0.408000; val_acc: 0.414000\n",
      "(Iteration 15301 / 22950) loss: 1.973803\n",
      "(Iteration 15401 / 22950) loss: 2.001044\n",
      "(Iteration 15501 / 22950) loss: 2.102035\n",
      "(Iteration 15601 / 22950) loss: 2.102643\n",
      "(Iteration 15701 / 22950) loss: 1.841045\n",
      "(Iteration 15801 / 22950) loss: 2.037248\n",
      "(Iteration 15901 / 22950) loss: 1.930504\n",
      "(Iteration 16001 / 22950) loss: 1.983953\n",
      "(Epoch 21 / 30) train acc: 0.436000; val_acc: 0.412000\n",
      "(Iteration 16101 / 22950) loss: 2.001654\n",
      "(Iteration 16201 / 22950) loss: 1.941361\n",
      "(Iteration 16301 / 22950) loss: 1.952416\n",
      "(Iteration 16401 / 22950) loss: 1.916916\n",
      "(Iteration 16501 / 22950) loss: 2.000892\n",
      "(Iteration 16601 / 22950) loss: 2.029756\n",
      "(Iteration 16701 / 22950) loss: 2.011683\n",
      "(Iteration 16801 / 22950) loss: 2.027674\n",
      "(Epoch 22 / 30) train acc: 0.418000; val_acc: 0.417000\n",
      "(Iteration 16901 / 22950) loss: 1.972331\n",
      "(Iteration 17001 / 22950) loss: 2.053525\n",
      "(Iteration 17101 / 22950) loss: 2.013175\n",
      "(Iteration 17201 / 22950) loss: 2.000812\n",
      "(Iteration 17301 / 22950) loss: 1.978414\n",
      "(Iteration 17401 / 22950) loss: 1.950860\n",
      "(Iteration 17501 / 22950) loss: 1.983080\n",
      "(Epoch 23 / 30) train acc: 0.382000; val_acc: 0.413000\n",
      "(Iteration 17601 / 22950) loss: 1.948084\n",
      "(Iteration 17701 / 22950) loss: 1.896283\n",
      "(Iteration 17801 / 22950) loss: 1.990158\n",
      "(Iteration 17901 / 22950) loss: 1.926550\n",
      "(Iteration 18001 / 22950) loss: 2.005839\n",
      "(Iteration 18101 / 22950) loss: 1.844151\n",
      "(Iteration 18201 / 22950) loss: 1.931528\n",
      "(Iteration 18301 / 22950) loss: 1.986314\n",
      "(Epoch 24 / 30) train acc: 0.420000; val_acc: 0.413000\n",
      "(Iteration 18401 / 22950) loss: 1.977227\n",
      "(Iteration 18501 / 22950) loss: 1.940120\n",
      "(Iteration 18601 / 22950) loss: 1.885592\n",
      "(Iteration 18701 / 22950) loss: 1.936611\n",
      "(Iteration 18801 / 22950) loss: 2.055787\n",
      "(Iteration 18901 / 22950) loss: 1.884629\n",
      "(Iteration 19001 / 22950) loss: 2.053166\n",
      "(Iteration 19101 / 22950) loss: 1.960090\n",
      "(Epoch 25 / 30) train acc: 0.433000; val_acc: 0.415000\n",
      "(Iteration 19201 / 22950) loss: 1.968806\n",
      "(Iteration 19301 / 22950) loss: 1.963691\n",
      "(Iteration 19401 / 22950) loss: 2.067044\n",
      "(Iteration 19501 / 22950) loss: 1.930794\n",
      "(Iteration 19601 / 22950) loss: 2.090522\n",
      "(Iteration 19701 / 22950) loss: 1.974300\n",
      "(Iteration 19801 / 22950) loss: 1.986872\n",
      "(Epoch 26 / 30) train acc: 0.413000; val_acc: 0.418000\n",
      "(Iteration 19901 / 22950) loss: 2.060856\n",
      "(Iteration 20001 / 22950) loss: 1.925992\n",
      "(Iteration 20101 / 22950) loss: 2.025857\n",
      "(Iteration 20201 / 22950) loss: 2.039420\n",
      "(Iteration 20301 / 22950) loss: 1.896703\n",
      "(Iteration 20401 / 22950) loss: 1.988876\n",
      "(Iteration 20501 / 22950) loss: 2.080377\n",
      "(Iteration 20601 / 22950) loss: 1.980096\n",
      "(Epoch 27 / 30) train acc: 0.441000; val_acc: 0.417000\n",
      "(Iteration 20701 / 22950) loss: 2.041066\n",
      "(Iteration 20801 / 22950) loss: 1.945960\n",
      "(Iteration 20901 / 22950) loss: 1.992183\n",
      "(Iteration 21001 / 22950) loss: 1.964187\n",
      "(Iteration 21101 / 22950) loss: 2.032915\n",
      "(Iteration 21201 / 22950) loss: 1.887139\n",
      "(Iteration 21301 / 22950) loss: 1.961350\n",
      "(Iteration 21401 / 22950) loss: 1.971877\n",
      "(Epoch 28 / 30) train acc: 0.396000; val_acc: 0.419000\n",
      "(Iteration 21501 / 22950) loss: 1.934472\n",
      "(Iteration 21601 / 22950) loss: 2.077847\n",
      "(Iteration 21701 / 22950) loss: 1.887370\n",
      "(Iteration 21801 / 22950) loss: 1.967672\n",
      "(Iteration 21901 / 22950) loss: 1.953022\n",
      "(Iteration 22001 / 22950) loss: 1.923240\n",
      "(Iteration 22101 / 22950) loss: 1.911730\n",
      "(Epoch 29 / 30) train acc: 0.429000; val_acc: 0.418000\n",
      "(Iteration 22201 / 22950) loss: 2.033309\n",
      "(Iteration 22301 / 22950) loss: 1.970030\n",
      "(Iteration 22401 / 22950) loss: 2.071148\n",
      "(Iteration 22501 / 22950) loss: 2.073267\n",
      "(Iteration 22601 / 22950) loss: 1.858178\n",
      "(Iteration 22701 / 22950) loss: 1.984840\n",
      "(Iteration 22801 / 22950) loss: 1.972539\n",
      "(Iteration 22901 / 22950) loss: 1.970925\n",
      "(Epoch 30 / 30) train acc: 0.407000; val_acc: 0.419000\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.01, 'num_epochs': 30, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 45930) loss: 2.305824\n",
      "(Epoch 0 / 30) train acc: 0.115000; val_acc: 0.112000\n",
      "(Iteration 101 / 45930) loss: 2.305589\n",
      "(Iteration 201 / 45930) loss: 2.304747\n",
      "(Iteration 301 / 45930) loss: 2.302524\n",
      "(Iteration 401 / 45930) loss: 2.303582\n",
      "(Iteration 501 / 45930) loss: 2.304898\n",
      "(Iteration 601 / 45930) loss: 2.303064\n",
      "(Iteration 701 / 45930) loss: 2.302142\n",
      "(Iteration 801 / 45930) loss: 2.299862\n",
      "(Iteration 901 / 45930) loss: 2.301176\n",
      "(Iteration 1001 / 45930) loss: 2.295288\n",
      "(Iteration 1101 / 45930) loss: 2.298174\n",
      "(Iteration 1201 / 45930) loss: 2.298087\n",
      "(Iteration 1301 / 45930) loss: 2.286762\n",
      "(Iteration 1401 / 45930) loss: 2.256658\n",
      "(Iteration 1501 / 45930) loss: 2.269693\n",
      "(Epoch 1 / 30) train acc: 0.218000; val_acc: 0.230000\n",
      "(Iteration 1601 / 45930) loss: 2.192549\n",
      "(Iteration 1701 / 45930) loss: 2.273310\n",
      "(Iteration 1801 / 45930) loss: 2.093716\n",
      "(Iteration 1901 / 45930) loss: 2.219121\n",
      "(Iteration 2001 / 45930) loss: 2.310706\n",
      "(Iteration 2101 / 45930) loss: 2.085623\n",
      "(Iteration 2201 / 45930) loss: 2.199710\n",
      "(Iteration 2301 / 45930) loss: 2.196398\n",
      "(Iteration 2401 / 45930) loss: 2.045343\n",
      "(Iteration 2501 / 45930) loss: 2.131861\n",
      "(Iteration 2601 / 45930) loss: 2.273290\n",
      "(Iteration 2701 / 45930) loss: 1.999010\n",
      "(Iteration 2801 / 45930) loss: 2.099905\n",
      "(Iteration 2901 / 45930) loss: 2.168640\n",
      "(Iteration 3001 / 45930) loss: 2.077998\n",
      "(Epoch 2 / 30) train acc: 0.293000; val_acc: 0.296000\n",
      "(Iteration 3101 / 45930) loss: 2.038201\n",
      "(Iteration 3201 / 45930) loss: 2.009453\n",
      "(Iteration 3301 / 45930) loss: 2.066655\n",
      "(Iteration 3401 / 45930) loss: 1.876814\n",
      "(Iteration 3501 / 45930) loss: 2.115930\n",
      "(Iteration 3601 / 45930) loss: 2.016954\n",
      "(Iteration 3701 / 45930) loss: 1.986363\n",
      "(Iteration 3801 / 45930) loss: 2.159369\n",
      "(Iteration 3901 / 45930) loss: 2.114926\n",
      "(Iteration 4001 / 45930) loss: 2.012851\n",
      "(Iteration 4101 / 45930) loss: 2.020230\n",
      "(Iteration 4201 / 45930) loss: 2.141707\n",
      "(Iteration 4301 / 45930) loss: 2.223453\n",
      "(Iteration 4401 / 45930) loss: 2.129055\n",
      "(Iteration 4501 / 45930) loss: 1.950640\n",
      "(Epoch 3 / 30) train acc: 0.361000; val_acc: 0.368000\n",
      "(Iteration 4601 / 45930) loss: 1.975239\n",
      "(Iteration 4701 / 45930) loss: 2.238081\n",
      "(Iteration 4801 / 45930) loss: 2.032766\n",
      "(Iteration 4901 / 45930) loss: 1.902903\n",
      "(Iteration 5001 / 45930) loss: 2.076311\n",
      "(Iteration 5101 / 45930) loss: 1.887355\n",
      "(Iteration 5201 / 45930) loss: 2.087334\n",
      "(Iteration 5301 / 45930) loss: 2.025145\n",
      "(Iteration 5401 / 45930) loss: 1.963946\n",
      "(Iteration 5501 / 45930) loss: 2.010841\n",
      "(Iteration 5601 / 45930) loss: 2.036056\n",
      "(Iteration 5701 / 45930) loss: 1.909329\n",
      "(Iteration 5801 / 45930) loss: 1.982506\n",
      "(Iteration 5901 / 45930) loss: 2.011026\n",
      "(Iteration 6001 / 45930) loss: 1.874026\n",
      "(Iteration 6101 / 45930) loss: 2.033684\n",
      "(Epoch 4 / 30) train acc: 0.397000; val_acc: 0.380000\n",
      "(Iteration 6201 / 45930) loss: 2.098512\n",
      "(Iteration 6301 / 45930) loss: 1.927365\n",
      "(Iteration 6401 / 45930) loss: 1.916381\n",
      "(Iteration 6501 / 45930) loss: 1.997374\n",
      "(Iteration 6601 / 45930) loss: 1.916314\n",
      "(Iteration 6701 / 45930) loss: 1.947461\n",
      "(Iteration 6801 / 45930) loss: 1.803100\n",
      "(Iteration 6901 / 45930) loss: 1.878031\n",
      "(Iteration 7001 / 45930) loss: 1.815592\n",
      "(Iteration 7101 / 45930) loss: 2.139170\n",
      "(Iteration 7201 / 45930) loss: 1.980745\n",
      "(Iteration 7301 / 45930) loss: 2.065825\n",
      "(Iteration 7401 / 45930) loss: 1.768414\n",
      "(Iteration 7501 / 45930) loss: 2.088652\n",
      "(Iteration 7601 / 45930) loss: 2.086727\n",
      "(Epoch 5 / 30) train acc: 0.419000; val_acc: 0.414000\n",
      "(Iteration 7701 / 45930) loss: 1.999486\n",
      "(Iteration 7801 / 45930) loss: 2.043642\n",
      "(Iteration 7901 / 45930) loss: 1.893447\n",
      "(Iteration 8001 / 45930) loss: 1.829196\n",
      "(Iteration 8101 / 45930) loss: 1.963479\n",
      "(Iteration 8201 / 45930) loss: 1.992978\n",
      "(Iteration 8301 / 45930) loss: 2.038134\n",
      "(Iteration 8401 / 45930) loss: 2.084130\n",
      "(Iteration 8501 / 45930) loss: 2.045194\n",
      "(Iteration 8601 / 45930) loss: 1.838277\n",
      "(Iteration 8701 / 45930) loss: 1.831527\n",
      "(Iteration 8801 / 45930) loss: 1.983432\n",
      "(Iteration 8901 / 45930) loss: 2.057795\n",
      "(Iteration 9001 / 45930) loss: 2.091909\n",
      "(Iteration 9101 / 45930) loss: 2.041206\n",
      "(Epoch 6 / 30) train acc: 0.430000; val_acc: 0.426000\n",
      "(Iteration 9201 / 45930) loss: 1.905383\n",
      "(Iteration 9301 / 45930) loss: 1.955746\n",
      "(Iteration 9401 / 45930) loss: 1.973666\n",
      "(Iteration 9501 / 45930) loss: 1.823863\n",
      "(Iteration 9601 / 45930) loss: 1.991318\n",
      "(Iteration 9701 / 45930) loss: 1.925479\n",
      "(Iteration 9801 / 45930) loss: 2.002026\n",
      "(Iteration 9901 / 45930) loss: 1.969183\n",
      "(Iteration 10001 / 45930) loss: 1.955423\n",
      "(Iteration 10101 / 45930) loss: 1.931142\n",
      "(Iteration 10201 / 45930) loss: 2.107329\n",
      "(Iteration 10301 / 45930) loss: 1.888676\n",
      "(Iteration 10401 / 45930) loss: 1.884125\n",
      "(Iteration 10501 / 45930) loss: 1.953093\n",
      "(Iteration 10601 / 45930) loss: 1.812860\n",
      "(Iteration 10701 / 45930) loss: 1.916547\n",
      "(Epoch 7 / 30) train acc: 0.425000; val_acc: 0.420000\n",
      "(Iteration 10801 / 45930) loss: 2.010493\n",
      "(Iteration 10901 / 45930) loss: 1.949230\n",
      "(Iteration 11001 / 45930) loss: 1.826521\n",
      "(Iteration 11101 / 45930) loss: 2.062416\n",
      "(Iteration 11201 / 45930) loss: 2.014768\n",
      "(Iteration 11301 / 45930) loss: 1.874896\n",
      "(Iteration 11401 / 45930) loss: 2.044124\n",
      "(Iteration 11501 / 45930) loss: 2.120462\n",
      "(Iteration 11601 / 45930) loss: 1.998886\n",
      "(Iteration 11701 / 45930) loss: 1.860001\n",
      "(Iteration 11801 / 45930) loss: 2.013589\n",
      "(Iteration 11901 / 45930) loss: 2.003218\n",
      "(Iteration 12001 / 45930) loss: 1.855733\n",
      "(Iteration 12101 / 45930) loss: 1.922572\n",
      "(Iteration 12201 / 45930) loss: 1.872254\n",
      "(Epoch 8 / 30) train acc: 0.411000; val_acc: 0.419000\n",
      "(Iteration 12301 / 45930) loss: 1.945768\n",
      "(Iteration 12401 / 45930) loss: 1.900466\n",
      "(Iteration 12501 / 45930) loss: 1.901066\n",
      "(Iteration 12601 / 45930) loss: 1.860929\n",
      "(Iteration 12701 / 45930) loss: 1.941237\n",
      "(Iteration 12801 / 45930) loss: 1.921425\n",
      "(Iteration 12901 / 45930) loss: 1.752670\n",
      "(Iteration 13001 / 45930) loss: 1.848526\n",
      "(Iteration 13101 / 45930) loss: 2.125675\n",
      "(Iteration 13201 / 45930) loss: 2.124518\n",
      "(Iteration 13301 / 45930) loss: 1.930804\n",
      "(Iteration 13401 / 45930) loss: 1.904723\n",
      "(Iteration 13501 / 45930) loss: 1.917456\n",
      "(Iteration 13601 / 45930) loss: 1.963963\n",
      "(Iteration 13701 / 45930) loss: 1.994084\n",
      "(Epoch 9 / 30) train acc: 0.448000; val_acc: 0.416000\n",
      "(Iteration 13801 / 45930) loss: 1.971171\n",
      "(Iteration 13901 / 45930) loss: 1.873602\n",
      "(Iteration 14001 / 45930) loss: 1.880272\n",
      "(Iteration 14101 / 45930) loss: 1.882763\n",
      "(Iteration 14201 / 45930) loss: 2.057152\n",
      "(Iteration 14301 / 45930) loss: 1.978742\n",
      "(Iteration 14401 / 45930) loss: 2.026236\n",
      "(Iteration 14501 / 45930) loss: 1.966002\n",
      "(Iteration 14601 / 45930) loss: 1.982465\n",
      "(Iteration 14701 / 45930) loss: 1.914793\n",
      "(Iteration 14801 / 45930) loss: 1.955611\n",
      "(Iteration 14901 / 45930) loss: 2.053887\n",
      "(Iteration 15001 / 45930) loss: 2.033249\n",
      "(Iteration 15101 / 45930) loss: 1.998130\n",
      "(Iteration 15201 / 45930) loss: 1.950032\n",
      "(Iteration 15301 / 45930) loss: 1.875061\n",
      "(Epoch 10 / 30) train acc: 0.441000; val_acc: 0.424000\n",
      "(Iteration 15401 / 45930) loss: 1.858758\n",
      "(Iteration 15501 / 45930) loss: 1.868466\n",
      "(Iteration 15601 / 45930) loss: 1.832054\n",
      "(Iteration 15701 / 45930) loss: 1.915835\n",
      "(Iteration 15801 / 45930) loss: 2.034098\n",
      "(Iteration 15901 / 45930) loss: 1.843429\n",
      "(Iteration 16001 / 45930) loss: 1.875762\n",
      "(Iteration 16101 / 45930) loss: 1.926521\n",
      "(Iteration 16201 / 45930) loss: 1.953582\n",
      "(Iteration 16301 / 45930) loss: 1.992450\n",
      "(Iteration 16401 / 45930) loss: 1.873313\n",
      "(Iteration 16501 / 45930) loss: 2.075613\n",
      "(Iteration 16601 / 45930) loss: 1.888672\n",
      "(Iteration 16701 / 45930) loss: 2.129302\n",
      "(Iteration 16801 / 45930) loss: 1.883436\n",
      "(Epoch 11 / 30) train acc: 0.436000; val_acc: 0.429000\n",
      "(Iteration 16901 / 45930) loss: 2.013037\n",
      "(Iteration 17001 / 45930) loss: 2.082122\n",
      "(Iteration 17101 / 45930) loss: 1.868545\n",
      "(Iteration 17201 / 45930) loss: 2.113407\n",
      "(Iteration 17301 / 45930) loss: 1.935106\n",
      "(Iteration 17401 / 45930) loss: 1.849799\n",
      "(Iteration 17501 / 45930) loss: 1.810800\n",
      "(Iteration 17601 / 45930) loss: 2.085722\n",
      "(Iteration 17701 / 45930) loss: 2.005275\n",
      "(Iteration 17801 / 45930) loss: 2.062560\n",
      "(Iteration 17901 / 45930) loss: 1.828241\n",
      "(Iteration 18001 / 45930) loss: 2.044209\n",
      "(Iteration 18101 / 45930) loss: 2.017348\n",
      "(Iteration 18201 / 45930) loss: 1.988910\n",
      "(Iteration 18301 / 45930) loss: 2.028508\n",
      "(Epoch 12 / 30) train acc: 0.442000; val_acc: 0.431000\n",
      "(Iteration 18401 / 45930) loss: 2.059410\n",
      "(Iteration 18501 / 45930) loss: 1.990232\n",
      "(Iteration 18601 / 45930) loss: 2.001586\n",
      "(Iteration 18701 / 45930) loss: 2.114361\n",
      "(Iteration 18801 / 45930) loss: 2.074141\n",
      "(Iteration 18901 / 45930) loss: 1.877306\n",
      "(Iteration 19001 / 45930) loss: 1.881127\n",
      "(Iteration 19101 / 45930) loss: 2.038843\n",
      "(Iteration 19201 / 45930) loss: 1.839254\n",
      "(Iteration 19301 / 45930) loss: 2.143880\n",
      "(Iteration 19401 / 45930) loss: 2.002180\n",
      "(Iteration 19501 / 45930) loss: 1.888425\n",
      "(Iteration 19601 / 45930) loss: 2.032119\n",
      "(Iteration 19701 / 45930) loss: 1.863153\n",
      "(Iteration 19801 / 45930) loss: 1.827384\n",
      "(Iteration 19901 / 45930) loss: 1.883163\n",
      "(Epoch 13 / 30) train acc: 0.454000; val_acc: 0.435000\n",
      "(Iteration 20001 / 45930) loss: 2.090760\n",
      "(Iteration 20101 / 45930) loss: 1.995491\n",
      "(Iteration 20201 / 45930) loss: 1.868463\n",
      "(Iteration 20301 / 45930) loss: 1.953878\n",
      "(Iteration 20401 / 45930) loss: 1.888797\n",
      "(Iteration 20501 / 45930) loss: 2.086766\n",
      "(Iteration 20601 / 45930) loss: 2.135052\n",
      "(Iteration 20701 / 45930) loss: 1.730524\n",
      "(Iteration 20801 / 45930) loss: 1.990126\n",
      "(Iteration 20901 / 45930) loss: 1.954162\n",
      "(Iteration 21001 / 45930) loss: 2.013961\n",
      "(Iteration 21101 / 45930) loss: 2.064906\n",
      "(Iteration 21201 / 45930) loss: 2.055452\n",
      "(Iteration 21301 / 45930) loss: 1.946437\n",
      "(Iteration 21401 / 45930) loss: 1.857750\n",
      "(Epoch 14 / 30) train acc: 0.437000; val_acc: 0.431000\n",
      "(Iteration 21501 / 45930) loss: 1.769587\n",
      "(Iteration 21601 / 45930) loss: 2.020849\n",
      "(Iteration 21701 / 45930) loss: 1.807889\n",
      "(Iteration 21801 / 45930) loss: 1.932139\n",
      "(Iteration 21901 / 45930) loss: 1.968645\n",
      "(Iteration 22001 / 45930) loss: 1.966795\n",
      "(Iteration 22101 / 45930) loss: 1.882189\n",
      "(Iteration 22201 / 45930) loss: 1.793103\n",
      "(Iteration 22301 / 45930) loss: 1.910512\n",
      "(Iteration 22401 / 45930) loss: 1.916657\n",
      "(Iteration 22501 / 45930) loss: 1.951362\n",
      "(Iteration 22601 / 45930) loss: 1.922925\n",
      "(Iteration 22701 / 45930) loss: 2.093011\n",
      "(Iteration 22801 / 45930) loss: 1.907585\n",
      "(Iteration 22901 / 45930) loss: 2.120575\n",
      "(Epoch 15 / 30) train acc: 0.445000; val_acc: 0.435000\n",
      "(Iteration 23001 / 45930) loss: 1.861036\n",
      "(Iteration 23101 / 45930) loss: 1.982028\n",
      "(Iteration 23201 / 45930) loss: 1.951976\n",
      "(Iteration 23301 / 45930) loss: 2.024545\n",
      "(Iteration 23401 / 45930) loss: 1.815514\n",
      "(Iteration 23501 / 45930) loss: 1.971448\n",
      "(Iteration 23601 / 45930) loss: 1.833213\n",
      "(Iteration 23701 / 45930) loss: 1.876273\n",
      "(Iteration 23801 / 45930) loss: 2.032033\n",
      "(Iteration 23901 / 45930) loss: 2.030661\n",
      "(Iteration 24001 / 45930) loss: 1.963671\n",
      "(Iteration 24101 / 45930) loss: 1.975194\n",
      "(Iteration 24201 / 45930) loss: 1.945188\n",
      "(Iteration 24301 / 45930) loss: 2.102103\n",
      "(Iteration 24401 / 45930) loss: 1.797241\n",
      "(Epoch 16 / 30) train acc: 0.450000; val_acc: 0.431000\n",
      "(Iteration 24501 / 45930) loss: 2.133517\n",
      "(Iteration 24601 / 45930) loss: 1.961915\n",
      "(Iteration 24701 / 45930) loss: 1.994358\n",
      "(Iteration 24801 / 45930) loss: 1.765640\n",
      "(Iteration 24901 / 45930) loss: 1.906028\n",
      "(Iteration 25001 / 45930) loss: 1.925130\n",
      "(Iteration 25101 / 45930) loss: 2.055431\n",
      "(Iteration 25201 / 45930) loss: 2.000166\n",
      "(Iteration 25301 / 45930) loss: 1.801707\n",
      "(Iteration 25401 / 45930) loss: 1.913785\n",
      "(Iteration 25501 / 45930) loss: 2.037524\n",
      "(Iteration 25601 / 45930) loss: 1.881627\n",
      "(Iteration 25701 / 45930) loss: 1.974046\n",
      "(Iteration 25801 / 45930) loss: 1.888095\n",
      "(Iteration 25901 / 45930) loss: 2.127454\n",
      "(Iteration 26001 / 45930) loss: 1.987492\n",
      "(Epoch 17 / 30) train acc: 0.431000; val_acc: 0.437000\n",
      "(Iteration 26101 / 45930) loss: 1.986083\n",
      "(Iteration 26201 / 45930) loss: 2.157095\n",
      "(Iteration 26301 / 45930) loss: 1.969555\n",
      "(Iteration 26401 / 45930) loss: 1.851690\n",
      "(Iteration 26501 / 45930) loss: 1.810085\n",
      "(Iteration 26601 / 45930) loss: 1.950346\n",
      "(Iteration 26701 / 45930) loss: 1.776520\n",
      "(Iteration 26801 / 45930) loss: 1.972805\n",
      "(Iteration 26901 / 45930) loss: 1.906103\n",
      "(Iteration 27001 / 45930) loss: 2.056015\n",
      "(Iteration 27101 / 45930) loss: 1.838074\n",
      "(Iteration 27201 / 45930) loss: 1.759870\n",
      "(Iteration 27301 / 45930) loss: 1.922503\n",
      "(Iteration 27401 / 45930) loss: 1.935497\n",
      "(Iteration 27501 / 45930) loss: 1.745676\n",
      "(Epoch 18 / 30) train acc: 0.425000; val_acc: 0.431000\n",
      "(Iteration 27601 / 45930) loss: 1.878525\n",
      "(Iteration 27701 / 45930) loss: 2.080261\n",
      "(Iteration 27801 / 45930) loss: 1.821277\n",
      "(Iteration 27901 / 45930) loss: 1.891159\n",
      "(Iteration 28001 / 45930) loss: 1.781924\n",
      "(Iteration 28101 / 45930) loss: 2.012120\n",
      "(Iteration 28201 / 45930) loss: 1.874965\n",
      "(Iteration 28301 / 45930) loss: 1.906631\n",
      "(Iteration 28401 / 45930) loss: 1.900884\n",
      "(Iteration 28501 / 45930) loss: 2.023348\n",
      "(Iteration 28601 / 45930) loss: 1.973848\n",
      "(Iteration 28701 / 45930) loss: 1.878636\n",
      "(Iteration 28801 / 45930) loss: 2.110323\n",
      "(Iteration 28901 / 45930) loss: 1.777654\n",
      "(Iteration 29001 / 45930) loss: 2.085672\n",
      "(Epoch 19 / 30) train acc: 0.448000; val_acc: 0.436000\n",
      "(Iteration 29101 / 45930) loss: 2.053722\n",
      "(Iteration 29201 / 45930) loss: 2.170765\n",
      "(Iteration 29301 / 45930) loss: 1.916388\n",
      "(Iteration 29401 / 45930) loss: 1.870158\n",
      "(Iteration 29501 / 45930) loss: 1.980396\n",
      "(Iteration 29601 / 45930) loss: 1.951142\n",
      "(Iteration 29701 / 45930) loss: 1.999581\n",
      "(Iteration 29801 / 45930) loss: 1.909663\n",
      "(Iteration 29901 / 45930) loss: 1.809873\n",
      "(Iteration 30001 / 45930) loss: 1.770249\n",
      "(Iteration 30101 / 45930) loss: 1.864157\n",
      "(Iteration 30201 / 45930) loss: 1.934556\n",
      "(Iteration 30301 / 45930) loss: 1.933395\n",
      "(Iteration 30401 / 45930) loss: 1.915947\n",
      "(Iteration 30501 / 45930) loss: 2.035655\n",
      "(Iteration 30601 / 45930) loss: 1.911076\n",
      "(Epoch 20 / 30) train acc: 0.463000; val_acc: 0.441000\n",
      "(Iteration 30701 / 45930) loss: 2.014149\n",
      "(Iteration 30801 / 45930) loss: 1.935995\n",
      "(Iteration 30901 / 45930) loss: 1.964761\n",
      "(Iteration 31001 / 45930) loss: 1.891924\n",
      "(Iteration 31101 / 45930) loss: 1.719383\n",
      "(Iteration 31201 / 45930) loss: 1.935215\n",
      "(Iteration 31301 / 45930) loss: 1.900473\n",
      "(Iteration 31401 / 45930) loss: 2.070034\n",
      "(Iteration 31501 / 45930) loss: 1.867859\n",
      "(Iteration 31601 / 45930) loss: 1.878825\n",
      "(Iteration 31701 / 45930) loss: 2.187374\n",
      "(Iteration 31801 / 45930) loss: 1.862887\n",
      "(Iteration 31901 / 45930) loss: 2.063564\n",
      "(Iteration 32001 / 45930) loss: 2.005104\n",
      "(Iteration 32101 / 45930) loss: 2.075937\n",
      "(Epoch 21 / 30) train acc: 0.452000; val_acc: 0.442000\n",
      "(Iteration 32201 / 45930) loss: 2.052646\n",
      "(Iteration 32301 / 45930) loss: 1.846530\n",
      "(Iteration 32401 / 45930) loss: 2.045390\n",
      "(Iteration 32501 / 45930) loss: 1.923273\n",
      "(Iteration 32601 / 45930) loss: 1.885704\n",
      "(Iteration 32701 / 45930) loss: 2.103351\n",
      "(Iteration 32801 / 45930) loss: 1.789183\n",
      "(Iteration 32901 / 45930) loss: 1.820675\n",
      "(Iteration 33001 / 45930) loss: 2.009666\n",
      "(Iteration 33101 / 45930) loss: 1.895931\n",
      "(Iteration 33201 / 45930) loss: 1.780549\n",
      "(Iteration 33301 / 45930) loss: 2.113925\n",
      "(Iteration 33401 / 45930) loss: 1.751669\n",
      "(Iteration 33501 / 45930) loss: 1.941191\n",
      "(Iteration 33601 / 45930) loss: 1.926632\n",
      "(Epoch 22 / 30) train acc: 0.484000; val_acc: 0.435000\n",
      "(Iteration 33701 / 45930) loss: 1.958963\n",
      "(Iteration 33801 / 45930) loss: 2.042424\n",
      "(Iteration 33901 / 45930) loss: 1.978416\n",
      "(Iteration 34001 / 45930) loss: 1.904431\n",
      "(Iteration 34101 / 45930) loss: 1.938589\n",
      "(Iteration 34201 / 45930) loss: 1.994014\n",
      "(Iteration 34301 / 45930) loss: 2.060384\n",
      "(Iteration 34401 / 45930) loss: 1.912206\n",
      "(Iteration 34501 / 45930) loss: 1.918358\n",
      "(Iteration 34601 / 45930) loss: 2.238795\n",
      "(Iteration 34701 / 45930) loss: 1.932495\n",
      "(Iteration 34801 / 45930) loss: 2.077402\n",
      "(Iteration 34901 / 45930) loss: 1.933819\n",
      "(Iteration 35001 / 45930) loss: 2.211247\n",
      "(Iteration 35101 / 45930) loss: 1.905226\n",
      "(Iteration 35201 / 45930) loss: 1.946290\n",
      "(Epoch 23 / 30) train acc: 0.455000; val_acc: 0.439000\n",
      "(Iteration 35301 / 45930) loss: 2.069490\n",
      "(Iteration 35401 / 45930) loss: 1.863303\n",
      "(Iteration 35501 / 45930) loss: 1.826462\n",
      "(Iteration 35601 / 45930) loss: 1.985576\n",
      "(Iteration 35701 / 45930) loss: 1.878315\n",
      "(Iteration 35801 / 45930) loss: 2.081726\n",
      "(Iteration 35901 / 45930) loss: 2.146703\n",
      "(Iteration 36001 / 45930) loss: 1.853059\n",
      "(Iteration 36101 / 45930) loss: 2.006705\n",
      "(Iteration 36201 / 45930) loss: 1.845747\n",
      "(Iteration 36301 / 45930) loss: 2.109496\n",
      "(Iteration 36401 / 45930) loss: 1.896738\n",
      "(Iteration 36501 / 45930) loss: 2.028539\n",
      "(Iteration 36601 / 45930) loss: 1.923025\n",
      "(Iteration 36701 / 45930) loss: 1.925805\n",
      "(Epoch 24 / 30) train acc: 0.439000; val_acc: 0.441000\n",
      "(Iteration 36801 / 45930) loss: 1.901134\n",
      "(Iteration 36901 / 45930) loss: 1.979364\n",
      "(Iteration 37001 / 45930) loss: 2.143473\n",
      "(Iteration 37101 / 45930) loss: 1.922870\n",
      "(Iteration 37201 / 45930) loss: 2.058358\n",
      "(Iteration 37301 / 45930) loss: 2.095942\n",
      "(Iteration 37401 / 45930) loss: 1.910152\n",
      "(Iteration 37501 / 45930) loss: 1.992982\n",
      "(Iteration 37601 / 45930) loss: 1.931648\n",
      "(Iteration 37701 / 45930) loss: 2.067510\n",
      "(Iteration 37801 / 45930) loss: 1.880760\n",
      "(Iteration 37901 / 45930) loss: 1.911327\n",
      "(Iteration 38001 / 45930) loss: 1.968574\n",
      "(Iteration 38101 / 45930) loss: 1.915624\n",
      "(Iteration 38201 / 45930) loss: 2.133495\n",
      "(Epoch 25 / 30) train acc: 0.429000; val_acc: 0.440000\n",
      "(Iteration 38301 / 45930) loss: 2.151909\n",
      "(Iteration 38401 / 45930) loss: 1.894803\n",
      "(Iteration 38501 / 45930) loss: 1.816762\n",
      "(Iteration 38601 / 45930) loss: 1.861859\n",
      "(Iteration 38701 / 45930) loss: 1.941363\n",
      "(Iteration 38801 / 45930) loss: 2.080349\n",
      "(Iteration 38901 / 45930) loss: 2.076493\n",
      "(Iteration 39001 / 45930) loss: 1.818552\n",
      "(Iteration 39101 / 45930) loss: 1.940903\n",
      "(Iteration 39201 / 45930) loss: 2.035297\n",
      "(Iteration 39301 / 45930) loss: 2.119557\n",
      "(Iteration 39401 / 45930) loss: 2.126536\n",
      "(Iteration 39501 / 45930) loss: 1.909621\n",
      "(Iteration 39601 / 45930) loss: 1.897681\n",
      "(Iteration 39701 / 45930) loss: 1.957493\n",
      "(Iteration 39801 / 45930) loss: 1.768474\n",
      "(Epoch 26 / 30) train acc: 0.448000; val_acc: 0.439000\n",
      "(Iteration 39901 / 45930) loss: 2.040397\n",
      "(Iteration 40001 / 45930) loss: 1.988165\n",
      "(Iteration 40101 / 45930) loss: 1.911356\n",
      "(Iteration 40201 / 45930) loss: 1.902482\n",
      "(Iteration 40301 / 45930) loss: 2.007723\n",
      "(Iteration 40401 / 45930) loss: 2.013354\n",
      "(Iteration 40501 / 45930) loss: 1.812611\n",
      "(Iteration 40601 / 45930) loss: 1.849587\n",
      "(Iteration 40701 / 45930) loss: 1.927806\n",
      "(Iteration 40801 / 45930) loss: 1.968966\n",
      "(Iteration 40901 / 45930) loss: 2.071710\n",
      "(Iteration 41001 / 45930) loss: 2.094868\n",
      "(Iteration 41101 / 45930) loss: 1.848336\n",
      "(Iteration 41201 / 45930) loss: 1.998467\n",
      "(Iteration 41301 / 45930) loss: 1.918287\n",
      "(Epoch 27 / 30) train acc: 0.450000; val_acc: 0.437000\n",
      "(Iteration 41401 / 45930) loss: 1.928035\n",
      "(Iteration 41501 / 45930) loss: 2.060054\n",
      "(Iteration 41601 / 45930) loss: 2.119808\n",
      "(Iteration 41701 / 45930) loss: 1.858597\n",
      "(Iteration 41801 / 45930) loss: 2.022727\n",
      "(Iteration 41901 / 45930) loss: 1.803891\n",
      "(Iteration 42001 / 45930) loss: 1.912650\n",
      "(Iteration 42101 / 45930) loss: 1.945979\n",
      "(Iteration 42201 / 45930) loss: 1.965149\n",
      "(Iteration 42301 / 45930) loss: 1.995750\n",
      "(Iteration 42401 / 45930) loss: 2.045013\n",
      "(Iteration 42501 / 45930) loss: 2.046783\n",
      "(Iteration 42601 / 45930) loss: 1.992406\n",
      "(Iteration 42701 / 45930) loss: 1.913173\n",
      "(Iteration 42801 / 45930) loss: 1.829111\n",
      "(Epoch 28 / 30) train acc: 0.461000; val_acc: 0.438000\n",
      "(Iteration 42901 / 45930) loss: 1.864575\n",
      "(Iteration 43001 / 45930) loss: 1.765572\n",
      "(Iteration 43101 / 45930) loss: 1.930526\n",
      "(Iteration 43201 / 45930) loss: 1.881916\n",
      "(Iteration 43301 / 45930) loss: 2.072636\n",
      "(Iteration 43401 / 45930) loss: 1.929810\n",
      "(Iteration 43501 / 45930) loss: 2.042683\n",
      "(Iteration 43601 / 45930) loss: 1.995178\n",
      "(Iteration 43701 / 45930) loss: 1.889841\n",
      "(Iteration 43801 / 45930) loss: 2.027410\n",
      "(Iteration 43901 / 45930) loss: 2.091692\n",
      "(Iteration 44001 / 45930) loss: 1.960050\n",
      "(Iteration 44101 / 45930) loss: 1.960946\n",
      "(Iteration 44201 / 45930) loss: 2.055883\n",
      "(Iteration 44301 / 45930) loss: 2.030070\n",
      "(Epoch 29 / 30) train acc: 0.448000; val_acc: 0.443000\n",
      "(Iteration 44401 / 45930) loss: 1.937226\n",
      "(Iteration 44501 / 45930) loss: 2.021307\n",
      "(Iteration 44601 / 45930) loss: 1.694485\n",
      "(Iteration 44701 / 45930) loss: 2.006138\n",
      "(Iteration 44801 / 45930) loss: 2.039319\n",
      "(Iteration 44901 / 45930) loss: 1.985501\n",
      "(Iteration 45001 / 45930) loss: 1.970985\n",
      "(Iteration 45101 / 45930) loss: 1.790828\n",
      "(Iteration 45201 / 45930) loss: 1.997835\n",
      "(Iteration 45301 / 45930) loss: 1.887991\n",
      "(Iteration 45401 / 45930) loss: 1.952454\n",
      "(Iteration 45501 / 45930) loss: 1.893737\n",
      "(Iteration 45601 / 45930) loss: 1.938933\n",
      "(Iteration 45701 / 45930) loss: 2.081865\n",
      "(Iteration 45801 / 45930) loss: 2.048083\n",
      "(Iteration 45901 / 45930) loss: 1.970341\n",
      "(Epoch 30 / 30) train acc: 0.476000; val_acc: 0.439000\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.01, 'num_epochs': 30, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 22950) loss: 2.302866\n",
      "(Epoch 0 / 30) train acc: 0.086000; val_acc: 0.107000\n",
      "(Iteration 101 / 22950) loss: 2.303394\n",
      "(Iteration 201 / 22950) loss: 2.302007\n",
      "(Iteration 301 / 22950) loss: 2.302930\n",
      "(Iteration 401 / 22950) loss: 2.302140\n",
      "(Iteration 501 / 22950) loss: 2.301412\n",
      "(Iteration 601 / 22950) loss: 2.299275\n",
      "(Iteration 701 / 22950) loss: 2.294468\n",
      "(Epoch 1 / 30) train acc: 0.220000; val_acc: 0.206000\n",
      "(Iteration 801 / 22950) loss: 2.287800\n",
      "(Iteration 901 / 22950) loss: 2.275532\n",
      "(Iteration 1001 / 22950) loss: 2.264621\n",
      "(Iteration 1101 / 22950) loss: 2.227495\n",
      "(Iteration 1201 / 22950) loss: 2.196452\n",
      "(Iteration 1301 / 22950) loss: 2.103019\n",
      "(Iteration 1401 / 22950) loss: 2.042359\n",
      "(Iteration 1501 / 22950) loss: 2.051538\n",
      "(Epoch 2 / 30) train acc: 0.261000; val_acc: 0.275000\n",
      "(Iteration 1601 / 22950) loss: 1.951570\n",
      "(Iteration 1701 / 22950) loss: 2.058713\n",
      "(Iteration 1801 / 22950) loss: 1.937295\n",
      "(Iteration 1901 / 22950) loss: 1.947816\n",
      "(Iteration 2001 / 22950) loss: 1.911911\n",
      "(Iteration 2101 / 22950) loss: 1.938198\n",
      "(Iteration 2201 / 22950) loss: 1.741381\n",
      "(Epoch 3 / 30) train acc: 0.338000; val_acc: 0.340000\n",
      "(Iteration 2301 / 22950) loss: 1.871050\n",
      "(Iteration 2401 / 22950) loss: 1.950545\n",
      "(Iteration 2501 / 22950) loss: 1.786635\n",
      "(Iteration 2601 / 22950) loss: 1.850603\n",
      "(Iteration 2701 / 22950) loss: 1.747352\n",
      "(Iteration 2801 / 22950) loss: 1.743539\n",
      "(Iteration 2901 / 22950) loss: 1.687390\n",
      "(Iteration 3001 / 22950) loss: 1.688076\n",
      "(Epoch 4 / 30) train acc: 0.420000; val_acc: 0.406000\n",
      "(Iteration 3101 / 22950) loss: 1.674884\n",
      "(Iteration 3201 / 22950) loss: 1.703916\n",
      "(Iteration 3301 / 22950) loss: 1.693816\n",
      "(Iteration 3401 / 22950) loss: 1.696145\n",
      "(Iteration 3501 / 22950) loss: 1.558386\n",
      "(Iteration 3601 / 22950) loss: 1.631738\n",
      "(Iteration 3701 / 22950) loss: 1.867144\n",
      "(Iteration 3801 / 22950) loss: 1.544917\n",
      "(Epoch 5 / 30) train acc: 0.428000; val_acc: 0.425000\n",
      "(Iteration 3901 / 22950) loss: 1.673166\n",
      "(Iteration 4001 / 22950) loss: 1.595939\n",
      "(Iteration 4101 / 22950) loss: 1.678993\n",
      "(Iteration 4201 / 22950) loss: 1.699207\n",
      "(Iteration 4301 / 22950) loss: 1.626816\n",
      "(Iteration 4401 / 22950) loss: 1.249456\n",
      "(Iteration 4501 / 22950) loss: 1.743026\n",
      "(Epoch 6 / 30) train acc: 0.428000; val_acc: 0.447000\n",
      "(Iteration 4601 / 22950) loss: 1.352766\n",
      "(Iteration 4701 / 22950) loss: 1.643428\n",
      "(Iteration 4801 / 22950) loss: 1.555266\n",
      "(Iteration 4901 / 22950) loss: 1.597095\n",
      "(Iteration 5001 / 22950) loss: 1.460104\n",
      "(Iteration 5101 / 22950) loss: 1.481434\n",
      "(Iteration 5201 / 22950) loss: 1.722119\n",
      "(Iteration 5301 / 22950) loss: 1.591469\n",
      "(Epoch 7 / 30) train acc: 0.488000; val_acc: 0.455000\n",
      "(Iteration 5401 / 22950) loss: 1.623914\n",
      "(Iteration 5501 / 22950) loss: 1.722560\n",
      "(Iteration 5601 / 22950) loss: 1.501787\n",
      "(Iteration 5701 / 22950) loss: 1.407691\n",
      "(Iteration 5801 / 22950) loss: 1.324634\n",
      "(Iteration 5901 / 22950) loss: 1.424758\n",
      "(Iteration 6001 / 22950) loss: 1.548670\n",
      "(Iteration 6101 / 22950) loss: 1.449831\n",
      "(Epoch 8 / 30) train acc: 0.484000; val_acc: 0.473000\n",
      "(Iteration 6201 / 22950) loss: 1.396274\n",
      "(Iteration 6301 / 22950) loss: 1.608275\n",
      "(Iteration 6401 / 22950) loss: 1.439130\n",
      "(Iteration 6501 / 22950) loss: 1.455471\n",
      "(Iteration 6601 / 22950) loss: 1.373475\n",
      "(Iteration 6701 / 22950) loss: 1.536061\n",
      "(Iteration 6801 / 22950) loss: 1.551623\n",
      "(Epoch 9 / 30) train acc: 0.494000; val_acc: 0.482000\n",
      "(Iteration 6901 / 22950) loss: 1.396466\n",
      "(Iteration 7001 / 22950) loss: 1.578832\n",
      "(Iteration 7101 / 22950) loss: 1.495641\n",
      "(Iteration 7201 / 22950) loss: 1.499244\n",
      "(Iteration 7301 / 22950) loss: 1.409893\n",
      "(Iteration 7401 / 22950) loss: 1.357828\n",
      "(Iteration 7501 / 22950) loss: 1.458791\n",
      "(Iteration 7601 / 22950) loss: 1.337583\n",
      "(Epoch 10 / 30) train acc: 0.478000; val_acc: 0.486000\n",
      "(Iteration 7701 / 22950) loss: 1.405774\n",
      "(Iteration 7801 / 22950) loss: 1.405333\n",
      "(Iteration 7901 / 22950) loss: 1.324673\n",
      "(Iteration 8001 / 22950) loss: 1.602549\n",
      "(Iteration 8101 / 22950) loss: 1.384253\n",
      "(Iteration 8201 / 22950) loss: 1.451103\n",
      "(Iteration 8301 / 22950) loss: 1.642567\n",
      "(Iteration 8401 / 22950) loss: 1.601689\n",
      "(Epoch 11 / 30) train acc: 0.512000; val_acc: 0.491000\n",
      "(Iteration 8501 / 22950) loss: 1.553133\n",
      "(Iteration 8601 / 22950) loss: 1.475468\n",
      "(Iteration 8701 / 22950) loss: 1.441281\n",
      "(Iteration 8801 / 22950) loss: 1.531677\n",
      "(Iteration 8901 / 22950) loss: 1.538829\n",
      "(Iteration 9001 / 22950) loss: 1.228958\n",
      "(Iteration 9101 / 22950) loss: 1.436896\n",
      "(Epoch 12 / 30) train acc: 0.471000; val_acc: 0.492000\n",
      "(Iteration 9201 / 22950) loss: 1.480896\n",
      "(Iteration 9301 / 22950) loss: 1.353849\n",
      "(Iteration 9401 / 22950) loss: 1.435590\n",
      "(Iteration 9501 / 22950) loss: 1.507359\n",
      "(Iteration 9601 / 22950) loss: 1.392558\n",
      "(Iteration 9701 / 22950) loss: 1.550572\n",
      "(Iteration 9801 / 22950) loss: 1.286777\n",
      "(Iteration 9901 / 22950) loss: 1.578817\n",
      "(Epoch 13 / 30) train acc: 0.502000; val_acc: 0.494000\n",
      "(Iteration 10001 / 22950) loss: 1.462584\n",
      "(Iteration 10101 / 22950) loss: 1.611230\n",
      "(Iteration 10201 / 22950) loss: 1.666149\n",
      "(Iteration 10301 / 22950) loss: 1.460549\n",
      "(Iteration 10401 / 22950) loss: 1.253723\n",
      "(Iteration 10501 / 22950) loss: 1.308136\n",
      "(Iteration 10601 / 22950) loss: 1.379850\n",
      "(Iteration 10701 / 22950) loss: 1.389979\n",
      "(Epoch 14 / 30) train acc: 0.480000; val_acc: 0.497000\n",
      "(Iteration 10801 / 22950) loss: 1.630310\n",
      "(Iteration 10901 / 22950) loss: 1.518144\n",
      "(Iteration 11001 / 22950) loss: 1.460802\n",
      "(Iteration 11101 / 22950) loss: 1.382086\n",
      "(Iteration 11201 / 22950) loss: 1.439736\n",
      "(Iteration 11301 / 22950) loss: 1.535138\n",
      "(Iteration 11401 / 22950) loss: 1.585736\n",
      "(Epoch 15 / 30) train acc: 0.503000; val_acc: 0.501000\n",
      "(Iteration 11501 / 22950) loss: 1.417734\n",
      "(Iteration 11601 / 22950) loss: 1.329150\n",
      "(Iteration 11701 / 22950) loss: 1.339209\n",
      "(Iteration 11801 / 22950) loss: 1.540283\n",
      "(Iteration 11901 / 22950) loss: 1.407363\n",
      "(Iteration 12001 / 22950) loss: 1.420196\n",
      "(Iteration 12101 / 22950) loss: 1.572895\n",
      "(Iteration 12201 / 22950) loss: 1.531503\n",
      "(Epoch 16 / 30) train acc: 0.527000; val_acc: 0.501000\n",
      "(Iteration 12301 / 22950) loss: 1.444358\n",
      "(Iteration 12401 / 22950) loss: 1.407386\n",
      "(Iteration 12501 / 22950) loss: 1.643134\n",
      "(Iteration 12601 / 22950) loss: 1.413299\n",
      "(Iteration 12701 / 22950) loss: 1.517137\n",
      "(Iteration 12801 / 22950) loss: 1.616119\n",
      "(Iteration 12901 / 22950) loss: 1.355878\n",
      "(Iteration 13001 / 22950) loss: 1.752877\n",
      "(Epoch 17 / 30) train acc: 0.537000; val_acc: 0.507000\n",
      "(Iteration 13101 / 22950) loss: 1.516251\n",
      "(Iteration 13201 / 22950) loss: 1.506477\n",
      "(Iteration 13301 / 22950) loss: 1.284211\n",
      "(Iteration 13401 / 22950) loss: 1.763065\n",
      "(Iteration 13501 / 22950) loss: 1.465149\n",
      "(Iteration 13601 / 22950) loss: 1.371534\n",
      "(Iteration 13701 / 22950) loss: 1.485421\n",
      "(Epoch 18 / 30) train acc: 0.513000; val_acc: 0.509000\n",
      "(Iteration 13801 / 22950) loss: 1.672977\n",
      "(Iteration 13901 / 22950) loss: 1.324462\n",
      "(Iteration 14001 / 22950) loss: 1.541728\n",
      "(Iteration 14101 / 22950) loss: 1.555852\n",
      "(Iteration 14201 / 22950) loss: 1.417639\n",
      "(Iteration 14301 / 22950) loss: 1.349941\n",
      "(Iteration 14401 / 22950) loss: 1.390647\n",
      "(Iteration 14501 / 22950) loss: 1.401360\n",
      "(Epoch 19 / 30) train acc: 0.496000; val_acc: 0.512000\n",
      "(Iteration 14601 / 22950) loss: 1.463290\n",
      "(Iteration 14701 / 22950) loss: 1.471635\n",
      "(Iteration 14801 / 22950) loss: 1.470468\n",
      "(Iteration 14901 / 22950) loss: 1.390387\n",
      "(Iteration 15001 / 22950) loss: 1.424255\n",
      "(Iteration 15101 / 22950) loss: 1.372214\n",
      "(Iteration 15201 / 22950) loss: 1.319733\n",
      "(Epoch 20 / 30) train acc: 0.529000; val_acc: 0.511000\n",
      "(Iteration 15301 / 22950) loss: 1.465502\n",
      "(Iteration 15401 / 22950) loss: 1.267153\n",
      "(Iteration 15501 / 22950) loss: 1.297047\n",
      "(Iteration 15601 / 22950) loss: 1.530360\n",
      "(Iteration 15701 / 22950) loss: 1.541345\n",
      "(Iteration 15801 / 22950) loss: 1.341698\n",
      "(Iteration 15901 / 22950) loss: 1.480235\n",
      "(Iteration 16001 / 22950) loss: 1.484139\n",
      "(Epoch 21 / 30) train acc: 0.489000; val_acc: 0.510000\n",
      "(Iteration 16101 / 22950) loss: 1.525108\n",
      "(Iteration 16201 / 22950) loss: 1.266587\n",
      "(Iteration 16301 / 22950) loss: 1.465672\n",
      "(Iteration 16401 / 22950) loss: 1.398414\n",
      "(Iteration 16501 / 22950) loss: 1.594033\n",
      "(Iteration 16601 / 22950) loss: 1.571015\n",
      "(Iteration 16701 / 22950) loss: 1.531300\n",
      "(Iteration 16801 / 22950) loss: 1.453233\n",
      "(Epoch 22 / 30) train acc: 0.544000; val_acc: 0.509000\n",
      "(Iteration 16901 / 22950) loss: 1.293464\n",
      "(Iteration 17001 / 22950) loss: 1.498846\n",
      "(Iteration 17101 / 22950) loss: 1.525366\n",
      "(Iteration 17201 / 22950) loss: 1.695215\n",
      "(Iteration 17301 / 22950) loss: 1.577861\n",
      "(Iteration 17401 / 22950) loss: 1.426423\n",
      "(Iteration 17501 / 22950) loss: 1.493586\n",
      "(Epoch 23 / 30) train acc: 0.507000; val_acc: 0.509000\n",
      "(Iteration 17601 / 22950) loss: 1.610681\n",
      "(Iteration 17701 / 22950) loss: 1.325072\n",
      "(Iteration 17801 / 22950) loss: 1.338857\n",
      "(Iteration 17901 / 22950) loss: 1.527997\n",
      "(Iteration 18001 / 22950) loss: 1.520651\n",
      "(Iteration 18101 / 22950) loss: 1.406172\n",
      "(Iteration 18201 / 22950) loss: 1.458353\n",
      "(Iteration 18301 / 22950) loss: 1.392455\n",
      "(Epoch 24 / 30) train acc: 0.519000; val_acc: 0.511000\n",
      "(Iteration 18401 / 22950) loss: 1.546819\n",
      "(Iteration 18501 / 22950) loss: 1.550492\n",
      "(Iteration 18601 / 22950) loss: 1.575920\n",
      "(Iteration 18701 / 22950) loss: 1.526849\n",
      "(Iteration 18801 / 22950) loss: 1.320771\n",
      "(Iteration 18901 / 22950) loss: 1.599315\n",
      "(Iteration 19001 / 22950) loss: 1.405105\n",
      "(Iteration 19101 / 22950) loss: 1.447973\n",
      "(Epoch 25 / 30) train acc: 0.494000; val_acc: 0.510000\n",
      "(Iteration 19201 / 22950) loss: 1.344870\n",
      "(Iteration 19301 / 22950) loss: 1.595625\n",
      "(Iteration 19401 / 22950) loss: 1.405103\n",
      "(Iteration 19501 / 22950) loss: 1.509923\n",
      "(Iteration 19601 / 22950) loss: 1.482022\n",
      "(Iteration 19701 / 22950) loss: 1.435221\n",
      "(Iteration 19801 / 22950) loss: 1.598823\n",
      "(Epoch 26 / 30) train acc: 0.518000; val_acc: 0.510000\n",
      "(Iteration 19901 / 22950) loss: 1.444403\n",
      "(Iteration 20001 / 22950) loss: 1.432778\n",
      "(Iteration 20101 / 22950) loss: 1.211833\n",
      "(Iteration 20201 / 22950) loss: 1.500583\n",
      "(Iteration 20301 / 22950) loss: 1.406866\n",
      "(Iteration 20401 / 22950) loss: 1.359999\n",
      "(Iteration 20501 / 22950) loss: 1.521557\n",
      "(Iteration 20601 / 22950) loss: 1.609986\n",
      "(Epoch 27 / 30) train acc: 0.523000; val_acc: 0.511000\n",
      "(Iteration 20701 / 22950) loss: 1.414375\n",
      "(Iteration 20801 / 22950) loss: 1.405836\n",
      "(Iteration 20901 / 22950) loss: 1.281170\n",
      "(Iteration 21001 / 22950) loss: 1.435363\n",
      "(Iteration 21101 / 22950) loss: 1.468577\n",
      "(Iteration 21201 / 22950) loss: 1.477544\n",
      "(Iteration 21301 / 22950) loss: 1.384236\n",
      "(Iteration 21401 / 22950) loss: 1.413941\n",
      "(Epoch 28 / 30) train acc: 0.533000; val_acc: 0.511000\n",
      "(Iteration 21501 / 22950) loss: 1.400079\n",
      "(Iteration 21601 / 22950) loss: 1.274508\n",
      "(Iteration 21701 / 22950) loss: 1.474117\n",
      "(Iteration 21801 / 22950) loss: 1.494375\n",
      "(Iteration 21901 / 22950) loss: 1.589658\n",
      "(Iteration 22001 / 22950) loss: 1.535208\n",
      "(Iteration 22101 / 22950) loss: 1.423008\n",
      "(Epoch 29 / 30) train acc: 0.517000; val_acc: 0.510000\n",
      "(Iteration 22201 / 22950) loss: 1.575363\n",
      "(Iteration 22301 / 22950) loss: 1.595829\n",
      "(Iteration 22401 / 22950) loss: 1.383949\n",
      "(Iteration 22501 / 22950) loss: 1.430971\n",
      "(Iteration 22601 / 22950) loss: 1.441282\n",
      "(Iteration 22701 / 22950) loss: 1.225314\n",
      "(Iteration 22801 / 22950) loss: 1.312046\n",
      "(Iteration 22901 / 22950) loss: 1.504544\n",
      "(Epoch 30 / 30) train acc: 0.566000; val_acc: 0.511000\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.01, 'num_epochs': 30, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 45930) loss: 2.302924\n",
      "(Epoch 0 / 30) train acc: 0.111000; val_acc: 0.110000\n",
      "(Iteration 101 / 45930) loss: 2.302122\n",
      "(Iteration 201 / 45930) loss: 2.303109\n",
      "(Iteration 301 / 45930) loss: 2.304433\n",
      "(Iteration 401 / 45930) loss: 2.302171\n",
      "(Iteration 501 / 45930) loss: 2.298071\n",
      "(Iteration 601 / 45930) loss: 2.298258\n",
      "(Iteration 701 / 45930) loss: 2.297110\n",
      "(Iteration 801 / 45930) loss: 2.289758\n",
      "(Iteration 901 / 45930) loss: 2.293059\n",
      "(Iteration 1001 / 45930) loss: 2.263864\n",
      "(Iteration 1101 / 45930) loss: 2.200171\n",
      "(Iteration 1201 / 45930) loss: 2.145765\n",
      "(Iteration 1301 / 45930) loss: 2.118759\n",
      "(Iteration 1401 / 45930) loss: 2.089069\n",
      "(Iteration 1501 / 45930) loss: 2.167578\n",
      "(Epoch 1 / 30) train acc: 0.262000; val_acc: 0.277000\n",
      "(Iteration 1601 / 45930) loss: 2.028195\n",
      "(Iteration 1701 / 45930) loss: 1.860497\n",
      "(Iteration 1801 / 45930) loss: 1.947645\n",
      "(Iteration 1901 / 45930) loss: 1.899502\n",
      "(Iteration 2001 / 45930) loss: 1.843923\n",
      "(Iteration 2101 / 45930) loss: 1.775525\n",
      "(Iteration 2201 / 45930) loss: 2.007575\n",
      "(Iteration 2301 / 45930) loss: 1.853628\n",
      "(Iteration 2401 / 45930) loss: 1.855341\n",
      "(Iteration 2501 / 45930) loss: 1.593375\n",
      "(Iteration 2601 / 45930) loss: 1.658633\n",
      "(Iteration 2701 / 45930) loss: 1.731517\n",
      "(Iteration 2801 / 45930) loss: 1.579749\n",
      "(Iteration 2901 / 45930) loss: 1.599367\n",
      "(Iteration 3001 / 45930) loss: 1.716811\n",
      "(Epoch 2 / 30) train acc: 0.440000; val_acc: 0.406000\n",
      "(Iteration 3101 / 45930) loss: 1.598733\n",
      "(Iteration 3201 / 45930) loss: 1.472859\n",
      "(Iteration 3301 / 45930) loss: 1.664936\n",
      "(Iteration 3401 / 45930) loss: 1.765898\n",
      "(Iteration 3501 / 45930) loss: 1.624622\n",
      "(Iteration 3601 / 45930) loss: 1.663117\n",
      "(Iteration 3701 / 45930) loss: 1.500525\n",
      "(Iteration 3801 / 45930) loss: 1.487879\n",
      "(Iteration 3901 / 45930) loss: 1.757020\n",
      "(Iteration 4001 / 45930) loss: 1.414113\n",
      "(Iteration 4101 / 45930) loss: 1.502378\n",
      "(Iteration 4201 / 45930) loss: 1.622541\n",
      "(Iteration 4301 / 45930) loss: 1.510606\n",
      "(Iteration 4401 / 45930) loss: 1.574759\n",
      "(Iteration 4501 / 45930) loss: 1.674316\n",
      "(Epoch 3 / 30) train acc: 0.492000; val_acc: 0.472000\n",
      "(Iteration 4601 / 45930) loss: 2.003150\n",
      "(Iteration 4701 / 45930) loss: 2.005145\n",
      "(Iteration 4801 / 45930) loss: 1.606246\n",
      "(Iteration 4901 / 45930) loss: 1.460829\n",
      "(Iteration 5001 / 45930) loss: 1.783924\n",
      "(Iteration 5101 / 45930) loss: 1.564890\n",
      "(Iteration 5201 / 45930) loss: 1.554567\n",
      "(Iteration 5301 / 45930) loss: 1.615067\n",
      "(Iteration 5401 / 45930) loss: 1.544542\n",
      "(Iteration 5501 / 45930) loss: 1.587338\n",
      "(Iteration 5601 / 45930) loss: 1.511279\n",
      "(Iteration 5701 / 45930) loss: 1.430774\n",
      "(Iteration 5801 / 45930) loss: 1.431182\n",
      "(Iteration 5901 / 45930) loss: 1.548681\n",
      "(Iteration 6001 / 45930) loss: 1.521025\n",
      "(Iteration 6101 / 45930) loss: 1.057432\n",
      "(Epoch 4 / 30) train acc: 0.500000; val_acc: 0.507000\n",
      "(Iteration 6201 / 45930) loss: 1.669765\n",
      "(Iteration 6301 / 45930) loss: 1.677487\n",
      "(Iteration 6401 / 45930) loss: 1.582690\n",
      "(Iteration 6501 / 45930) loss: 1.482720\n",
      "(Iteration 6601 / 45930) loss: 1.335528\n",
      "(Iteration 6701 / 45930) loss: 1.350808\n",
      "(Iteration 6801 / 45930) loss: 1.301711\n",
      "(Iteration 6901 / 45930) loss: 1.528692\n",
      "(Iteration 7001 / 45930) loss: 1.383756\n",
      "(Iteration 7101 / 45930) loss: 1.812444\n",
      "(Iteration 7201 / 45930) loss: 1.499065\n",
      "(Iteration 7301 / 45930) loss: 1.739961\n",
      "(Iteration 7401 / 45930) loss: 1.472551\n",
      "(Iteration 7501 / 45930) loss: 1.597611\n",
      "(Iteration 7601 / 45930) loss: 1.225514\n",
      "(Epoch 5 / 30) train acc: 0.504000; val_acc: 0.505000\n",
      "(Iteration 7701 / 45930) loss: 1.596646\n",
      "(Iteration 7801 / 45930) loss: 1.571468\n",
      "(Iteration 7901 / 45930) loss: 1.484849\n",
      "(Iteration 8001 / 45930) loss: 1.664761\n",
      "(Iteration 8101 / 45930) loss: 1.430243\n",
      "(Iteration 8201 / 45930) loss: 1.396436\n",
      "(Iteration 8301 / 45930) loss: 1.367981\n",
      "(Iteration 8401 / 45930) loss: 1.443409\n",
      "(Iteration 8501 / 45930) loss: 1.558466\n",
      "(Iteration 8601 / 45930) loss: 1.478255\n",
      "(Iteration 8701 / 45930) loss: 1.792678\n",
      "(Iteration 8801 / 45930) loss: 1.413977\n",
      "(Iteration 8901 / 45930) loss: 1.266285\n",
      "(Iteration 9001 / 45930) loss: 1.597969\n",
      "(Iteration 9101 / 45930) loss: 1.544652\n",
      "(Epoch 6 / 30) train acc: 0.532000; val_acc: 0.517000\n",
      "(Iteration 9201 / 45930) loss: 1.479230\n",
      "(Iteration 9301 / 45930) loss: 1.777719\n",
      "(Iteration 9401 / 45930) loss: 1.513374\n",
      "(Iteration 9501 / 45930) loss: 1.261806\n",
      "(Iteration 9601 / 45930) loss: 1.818809\n",
      "(Iteration 9701 / 45930) loss: 1.873873\n",
      "(Iteration 9801 / 45930) loss: 1.566391\n",
      "(Iteration 9901 / 45930) loss: 1.722551\n",
      "(Iteration 10001 / 45930) loss: 1.464003\n",
      "(Iteration 10101 / 45930) loss: 1.514754\n",
      "(Iteration 10201 / 45930) loss: 1.626039\n",
      "(Iteration 10301 / 45930) loss: 1.293787\n",
      "(Iteration 10401 / 45930) loss: 1.425923\n",
      "(Iteration 10501 / 45930) loss: 1.665925\n",
      "(Iteration 10601 / 45930) loss: 1.328783\n",
      "(Iteration 10701 / 45930) loss: 1.458157\n",
      "(Epoch 7 / 30) train acc: 0.547000; val_acc: 0.508000\n",
      "(Iteration 10801 / 45930) loss: 1.273662\n",
      "(Iteration 10901 / 45930) loss: 1.497849\n",
      "(Iteration 11001 / 45930) loss: 1.326971\n",
      "(Iteration 11101 / 45930) loss: 1.649829\n",
      "(Iteration 11201 / 45930) loss: 1.505802\n",
      "(Iteration 11301 / 45930) loss: 1.420010\n",
      "(Iteration 11401 / 45930) loss: 1.549185\n",
      "(Iteration 11501 / 45930) loss: 1.429417\n",
      "(Iteration 11601 / 45930) loss: 1.453505\n",
      "(Iteration 11701 / 45930) loss: 1.361513\n",
      "(Iteration 11801 / 45930) loss: 1.485883\n",
      "(Iteration 11901 / 45930) loss: 1.766851\n",
      "(Iteration 12001 / 45930) loss: 1.428504\n",
      "(Iteration 12101 / 45930) loss: 1.478590\n",
      "(Iteration 12201 / 45930) loss: 1.545244\n",
      "(Epoch 8 / 30) train acc: 0.522000; val_acc: 0.515000\n",
      "(Iteration 12301 / 45930) loss: 1.646960\n",
      "(Iteration 12401 / 45930) loss: 1.576848\n",
      "(Iteration 12501 / 45930) loss: 1.424730\n",
      "(Iteration 12601 / 45930) loss: 1.453695\n",
      "(Iteration 12701 / 45930) loss: 1.415395\n",
      "(Iteration 12801 / 45930) loss: 1.588629\n",
      "(Iteration 12901 / 45930) loss: 1.539317\n",
      "(Iteration 13001 / 45930) loss: 1.439229\n",
      "(Iteration 13101 / 45930) loss: 1.754142\n",
      "(Iteration 13201 / 45930) loss: 1.451312\n",
      "(Iteration 13301 / 45930) loss: 1.255498\n",
      "(Iteration 13401 / 45930) loss: 1.507104\n",
      "(Iteration 13501 / 45930) loss: 1.369003\n",
      "(Iteration 13601 / 45930) loss: 1.651728\n",
      "(Iteration 13701 / 45930) loss: 1.425615\n",
      "(Epoch 9 / 30) train acc: 0.554000; val_acc: 0.516000\n",
      "(Iteration 13801 / 45930) loss: 1.227720\n",
      "(Iteration 13901 / 45930) loss: 1.642628\n",
      "(Iteration 14001 / 45930) loss: 1.412594\n",
      "(Iteration 14101 / 45930) loss: 1.187599\n",
      "(Iteration 14201 / 45930) loss: 1.394548\n",
      "(Iteration 14301 / 45930) loss: 1.684372\n",
      "(Iteration 14401 / 45930) loss: 1.399598\n",
      "(Iteration 14501 / 45930) loss: 1.419729\n",
      "(Iteration 14601 / 45930) loss: 1.542342\n",
      "(Iteration 14701 / 45930) loss: 1.297710\n",
      "(Iteration 14801 / 45930) loss: 1.424827\n",
      "(Iteration 14901 / 45930) loss: 1.615229\n",
      "(Iteration 15001 / 45930) loss: 1.290492\n",
      "(Iteration 15101 / 45930) loss: 1.431436\n",
      "(Iteration 15201 / 45930) loss: 1.462567\n",
      "(Iteration 15301 / 45930) loss: 1.439835\n",
      "(Epoch 10 / 30) train acc: 0.522000; val_acc: 0.512000\n",
      "(Iteration 15401 / 45930) loss: 1.342154\n",
      "(Iteration 15501 / 45930) loss: 1.363388\n",
      "(Iteration 15601 / 45930) loss: 1.453334\n",
      "(Iteration 15701 / 45930) loss: 1.589692\n",
      "(Iteration 15801 / 45930) loss: 1.413221\n",
      "(Iteration 15901 / 45930) loss: 1.349590\n",
      "(Iteration 16001 / 45930) loss: 1.350042\n",
      "(Iteration 16101 / 45930) loss: 1.543588\n",
      "(Iteration 16201 / 45930) loss: 1.334596\n",
      "(Iteration 16301 / 45930) loss: 1.486954\n",
      "(Iteration 16401 / 45930) loss: 1.643191\n",
      "(Iteration 16501 / 45930) loss: 1.590513\n",
      "(Iteration 16601 / 45930) loss: 1.320395\n",
      "(Iteration 16701 / 45930) loss: 1.387101\n",
      "(Iteration 16801 / 45930) loss: 1.471016\n",
      "(Epoch 11 / 30) train acc: 0.552000; val_acc: 0.519000\n",
      "(Iteration 16901 / 45930) loss: 1.620350\n",
      "(Iteration 17001 / 45930) loss: 1.556177\n",
      "(Iteration 17101 / 45930) loss: 1.372895\n",
      "(Iteration 17201 / 45930) loss: 1.286536\n",
      "(Iteration 17301 / 45930) loss: 1.439871\n",
      "(Iteration 17401 / 45930) loss: 1.593186\n",
      "(Iteration 17501 / 45930) loss: 1.697802\n",
      "(Iteration 17601 / 45930) loss: 1.457938\n",
      "(Iteration 17701 / 45930) loss: 1.331962\n",
      "(Iteration 17801 / 45930) loss: 1.485795\n",
      "(Iteration 17901 / 45930) loss: 1.476413\n",
      "(Iteration 18001 / 45930) loss: 1.447808\n",
      "(Iteration 18101 / 45930) loss: 1.487583\n",
      "(Iteration 18201 / 45930) loss: 1.436371\n",
      "(Iteration 18301 / 45930) loss: 1.566050\n",
      "(Epoch 12 / 30) train acc: 0.538000; val_acc: 0.525000\n",
      "(Iteration 18401 / 45930) loss: 1.358283\n",
      "(Iteration 18501 / 45930) loss: 1.248795\n",
      "(Iteration 18601 / 45930) loss: 1.388598\n",
      "(Iteration 18701 / 45930) loss: 1.515926\n",
      "(Iteration 18801 / 45930) loss: 1.683616\n",
      "(Iteration 18901 / 45930) loss: 1.471530\n",
      "(Iteration 19001 / 45930) loss: 1.721458\n",
      "(Iteration 19101 / 45930) loss: 1.166047\n",
      "(Iteration 19201 / 45930) loss: 1.556416\n",
      "(Iteration 19301 / 45930) loss: 1.505179\n",
      "(Iteration 19401 / 45930) loss: 1.667872\n",
      "(Iteration 19501 / 45930) loss: 1.245522\n",
      "(Iteration 19601 / 45930) loss: 1.370621\n",
      "(Iteration 19701 / 45930) loss: 1.431201\n",
      "(Iteration 19801 / 45930) loss: 1.425486\n",
      "(Iteration 19901 / 45930) loss: 1.172547\n",
      "(Epoch 13 / 30) train acc: 0.545000; val_acc: 0.517000\n",
      "(Iteration 20001 / 45930) loss: 1.878600\n",
      "(Iteration 20101 / 45930) loss: 1.504779\n",
      "(Iteration 20201 / 45930) loss: 1.234264\n",
      "(Iteration 20301 / 45930) loss: 1.391550\n",
      "(Iteration 20401 / 45930) loss: 1.518791\n",
      "(Iteration 20501 / 45930) loss: 1.530734\n",
      "(Iteration 20601 / 45930) loss: 1.394292\n",
      "(Iteration 20701 / 45930) loss: 1.594914\n",
      "(Iteration 20801 / 45930) loss: 1.273641\n",
      "(Iteration 20901 / 45930) loss: 1.648550\n",
      "(Iteration 21001 / 45930) loss: 1.329058\n",
      "(Iteration 21101 / 45930) loss: 1.798907\n",
      "(Iteration 21201 / 45930) loss: 1.332927\n",
      "(Iteration 21301 / 45930) loss: 1.512292\n",
      "(Iteration 21401 / 45930) loss: 1.655045\n",
      "(Epoch 14 / 30) train acc: 0.520000; val_acc: 0.520000\n",
      "(Iteration 21501 / 45930) loss: 1.298954\n",
      "(Iteration 21601 / 45930) loss: 1.731417\n",
      "(Iteration 21701 / 45930) loss: 1.208355\n",
      "(Iteration 21801 / 45930) loss: 1.482635\n",
      "(Iteration 21901 / 45930) loss: 1.716729\n",
      "(Iteration 22001 / 45930) loss: 1.660203\n",
      "(Iteration 22101 / 45930) loss: 1.385119\n",
      "(Iteration 22201 / 45930) loss: 1.249539\n",
      "(Iteration 22301 / 45930) loss: 1.716352\n",
      "(Iteration 22401 / 45930) loss: 1.546543\n",
      "(Iteration 22501 / 45930) loss: 1.405739\n",
      "(Iteration 22601 / 45930) loss: 1.367567\n",
      "(Iteration 22701 / 45930) loss: 1.318267\n",
      "(Iteration 22801 / 45930) loss: 1.515466\n",
      "(Iteration 22901 / 45930) loss: 1.455381\n",
      "(Epoch 15 / 30) train acc: 0.524000; val_acc: 0.515000\n",
      "(Iteration 23001 / 45930) loss: 1.333670\n",
      "(Iteration 23101 / 45930) loss: 1.544916\n",
      "(Iteration 23201 / 45930) loss: 1.392950\n",
      "(Iteration 23301 / 45930) loss: 1.553862\n",
      "(Iteration 23401 / 45930) loss: 1.337411\n",
      "(Iteration 23501 / 45930) loss: 1.532125\n",
      "(Iteration 23601 / 45930) loss: 1.693012\n",
      "(Iteration 23701 / 45930) loss: 1.457578\n",
      "(Iteration 23801 / 45930) loss: 1.158966\n",
      "(Iteration 23901 / 45930) loss: 1.610520\n",
      "(Iteration 24001 / 45930) loss: 1.503951\n",
      "(Iteration 24101 / 45930) loss: 1.547631\n",
      "(Iteration 24201 / 45930) loss: 1.460343\n",
      "(Iteration 24301 / 45930) loss: 1.712145\n",
      "(Iteration 24401 / 45930) loss: 1.397416\n",
      "(Epoch 16 / 30) train acc: 0.549000; val_acc: 0.515000\n",
      "(Iteration 24501 / 45930) loss: 1.574894\n",
      "(Iteration 24601 / 45930) loss: 1.391831\n",
      "(Iteration 24701 / 45930) loss: 1.071809\n",
      "(Iteration 24801 / 45930) loss: 1.583083\n",
      "(Iteration 24901 / 45930) loss: 1.250819\n",
      "(Iteration 25001 / 45930) loss: 1.412444\n",
      "(Iteration 25101 / 45930) loss: 1.467581\n",
      "(Iteration 25201 / 45930) loss: 1.414236\n",
      "(Iteration 25301 / 45930) loss: 1.541302\n",
      "(Iteration 25401 / 45930) loss: 1.391734\n",
      "(Iteration 25501 / 45930) loss: 1.376891\n",
      "(Iteration 25601 / 45930) loss: 1.648670\n",
      "(Iteration 25701 / 45930) loss: 1.375782\n",
      "(Iteration 25801 / 45930) loss: 1.458271\n",
      "(Iteration 25901 / 45930) loss: 1.655422\n",
      "(Iteration 26001 / 45930) loss: 1.605633\n",
      "(Epoch 17 / 30) train acc: 0.531000; val_acc: 0.520000\n",
      "(Iteration 26101 / 45930) loss: 1.372449\n",
      "(Iteration 26201 / 45930) loss: 1.375606\n",
      "(Iteration 26301 / 45930) loss: 1.300569\n",
      "(Iteration 26401 / 45930) loss: 1.405811\n",
      "(Iteration 26501 / 45930) loss: 1.311663\n",
      "(Iteration 26601 / 45930) loss: 1.311039\n",
      "(Iteration 26701 / 45930) loss: 1.486288\n",
      "(Iteration 26801 / 45930) loss: 1.256307\n",
      "(Iteration 26901 / 45930) loss: 1.382122\n",
      "(Iteration 27001 / 45930) loss: 1.728728\n",
      "(Iteration 27101 / 45930) loss: 1.493240\n",
      "(Iteration 27201 / 45930) loss: 1.316305\n",
      "(Iteration 27301 / 45930) loss: 1.472589\n",
      "(Iteration 27401 / 45930) loss: 1.424757\n",
      "(Iteration 27501 / 45930) loss: 1.585972\n",
      "(Epoch 18 / 30) train acc: 0.531000; val_acc: 0.517000\n",
      "(Iteration 27601 / 45930) loss: 1.292641\n",
      "(Iteration 27701 / 45930) loss: 1.427651\n",
      "(Iteration 27801 / 45930) loss: 1.382784\n",
      "(Iteration 27901 / 45930) loss: 1.423455\n",
      "(Iteration 28001 / 45930) loss: 1.600970\n",
      "(Iteration 28101 / 45930) loss: 1.253199\n",
      "(Iteration 28201 / 45930) loss: 1.414441\n",
      "(Iteration 28301 / 45930) loss: 1.455952\n",
      "(Iteration 28401 / 45930) loss: 1.384484\n",
      "(Iteration 28501 / 45930) loss: 1.728589\n",
      "(Iteration 28601 / 45930) loss: 1.245983\n",
      "(Iteration 28701 / 45930) loss: 1.432068\n",
      "(Iteration 28801 / 45930) loss: 1.655663\n",
      "(Iteration 28901 / 45930) loss: 1.397002\n",
      "(Iteration 29001 / 45930) loss: 1.348802\n",
      "(Epoch 19 / 30) train acc: 0.529000; val_acc: 0.518000\n",
      "(Iteration 29101 / 45930) loss: 1.553293\n",
      "(Iteration 29201 / 45930) loss: 1.721163\n",
      "(Iteration 29301 / 45930) loss: 1.166096\n",
      "(Iteration 29401 / 45930) loss: 1.734363\n",
      "(Iteration 29501 / 45930) loss: 1.313301\n",
      "(Iteration 29601 / 45930) loss: 1.221102\n",
      "(Iteration 29701 / 45930) loss: 1.236242\n",
      "(Iteration 29801 / 45930) loss: 1.363601\n",
      "(Iteration 29901 / 45930) loss: 1.470759\n",
      "(Iteration 30001 / 45930) loss: 1.264870\n",
      "(Iteration 30101 / 45930) loss: 1.361215\n",
      "(Iteration 30201 / 45930) loss: 1.163703\n",
      "(Iteration 30301 / 45930) loss: 1.367260\n",
      "(Iteration 30401 / 45930) loss: 1.873743\n",
      "(Iteration 30501 / 45930) loss: 1.674914\n",
      "(Iteration 30601 / 45930) loss: 1.155322\n",
      "(Epoch 20 / 30) train acc: 0.550000; val_acc: 0.522000\n",
      "(Iteration 30701 / 45930) loss: 1.352549\n",
      "(Iteration 30801 / 45930) loss: 1.522526\n",
      "(Iteration 30901 / 45930) loss: 1.515008\n",
      "(Iteration 31001 / 45930) loss: 1.418822\n",
      "(Iteration 31101 / 45930) loss: 1.240075\n",
      "(Iteration 31201 / 45930) loss: 1.248297\n",
      "(Iteration 31301 / 45930) loss: 1.183268\n",
      "(Iteration 31401 / 45930) loss: 1.143364\n",
      "(Iteration 31501 / 45930) loss: 1.586671\n",
      "(Iteration 31601 / 45930) loss: 1.425745\n",
      "(Iteration 31701 / 45930) loss: 1.591897\n",
      "(Iteration 31801 / 45930) loss: 1.356592\n",
      "(Iteration 31901 / 45930) loss: 1.408856\n",
      "(Iteration 32001 / 45930) loss: 1.070189\n",
      "(Iteration 32101 / 45930) loss: 1.457522\n",
      "(Epoch 21 / 30) train acc: 0.534000; val_acc: 0.519000\n",
      "(Iteration 32201 / 45930) loss: 1.460842\n",
      "(Iteration 32301 / 45930) loss: 1.537227\n",
      "(Iteration 32401 / 45930) loss: 1.654683\n",
      "(Iteration 32501 / 45930) loss: 1.546481\n",
      "(Iteration 32601 / 45930) loss: 1.723498\n",
      "(Iteration 32701 / 45930) loss: 1.389203\n",
      "(Iteration 32801 / 45930) loss: 1.522080\n",
      "(Iteration 32901 / 45930) loss: 1.600802\n",
      "(Iteration 33001 / 45930) loss: 1.464439\n",
      "(Iteration 33101 / 45930) loss: 1.300914\n",
      "(Iteration 33201 / 45930) loss: 1.416355\n",
      "(Iteration 33301 / 45930) loss: 1.525821\n",
      "(Iteration 33401 / 45930) loss: 1.634787\n",
      "(Iteration 33501 / 45930) loss: 1.435588\n",
      "(Iteration 33601 / 45930) loss: 1.446131\n",
      "(Epoch 22 / 30) train acc: 0.532000; val_acc: 0.517000\n",
      "(Iteration 33701 / 45930) loss: 1.633190\n",
      "(Iteration 33801 / 45930) loss: 1.630183\n",
      "(Iteration 33901 / 45930) loss: 1.584918\n",
      "(Iteration 34001 / 45930) loss: 1.485029\n",
      "(Iteration 34101 / 45930) loss: 1.536817\n",
      "(Iteration 34201 / 45930) loss: 1.507778\n",
      "(Iteration 34301 / 45930) loss: 1.393759\n",
      "(Iteration 34401 / 45930) loss: 1.680086\n",
      "(Iteration 34501 / 45930) loss: 1.205211\n",
      "(Iteration 34601 / 45930) loss: 1.715069\n",
      "(Iteration 34701 / 45930) loss: 1.318267\n",
      "(Iteration 34801 / 45930) loss: 1.228672\n",
      "(Iteration 34901 / 45930) loss: 1.158400\n",
      "(Iteration 35001 / 45930) loss: 1.371966\n",
      "(Iteration 35101 / 45930) loss: 1.525066\n",
      "(Iteration 35201 / 45930) loss: 1.443521\n",
      "(Epoch 23 / 30) train acc: 0.533000; val_acc: 0.514000\n",
      "(Iteration 35301 / 45930) loss: 1.341513\n",
      "(Iteration 35401 / 45930) loss: 1.471329\n",
      "(Iteration 35501 / 45930) loss: 1.525759\n",
      "(Iteration 35601 / 45930) loss: 1.522977\n",
      "(Iteration 35701 / 45930) loss: 1.531192\n",
      "(Iteration 35801 / 45930) loss: 1.229263\n",
      "(Iteration 35901 / 45930) loss: 1.400348\n",
      "(Iteration 36001 / 45930) loss: 1.318890\n",
      "(Iteration 36101 / 45930) loss: 1.167316\n",
      "(Iteration 36201 / 45930) loss: 1.454662\n",
      "(Iteration 36301 / 45930) loss: 1.376604\n",
      "(Iteration 36401 / 45930) loss: 1.303268\n",
      "(Iteration 36501 / 45930) loss: 1.207861\n",
      "(Iteration 36601 / 45930) loss: 1.735648\n",
      "(Iteration 36701 / 45930) loss: 1.077640\n",
      "(Epoch 24 / 30) train acc: 0.523000; val_acc: 0.515000\n",
      "(Iteration 36801 / 45930) loss: 1.447835\n",
      "(Iteration 36901 / 45930) loss: 1.487494\n",
      "(Iteration 37001 / 45930) loss: 1.690424\n",
      "(Iteration 37101 / 45930) loss: 1.195229\n",
      "(Iteration 37201 / 45930) loss: 1.256223\n",
      "(Iteration 37301 / 45930) loss: 1.632286\n",
      "(Iteration 37401 / 45930) loss: 1.604223\n",
      "(Iteration 37501 / 45930) loss: 1.695649\n",
      "(Iteration 37601 / 45930) loss: 1.628046\n",
      "(Iteration 37701 / 45930) loss: 1.712008\n",
      "(Iteration 37801 / 45930) loss: 1.795748\n",
      "(Iteration 37901 / 45930) loss: 1.361793\n",
      "(Iteration 38001 / 45930) loss: 1.888395\n",
      "(Iteration 38101 / 45930) loss: 1.464540\n",
      "(Iteration 38201 / 45930) loss: 1.553439\n",
      "(Epoch 25 / 30) train acc: 0.546000; val_acc: 0.520000\n",
      "(Iteration 38301 / 45930) loss: 1.418626\n",
      "(Iteration 38401 / 45930) loss: 1.445647\n",
      "(Iteration 38501 / 45930) loss: 1.562119\n",
      "(Iteration 38601 / 45930) loss: 1.508061\n",
      "(Iteration 38701 / 45930) loss: 1.671626\n",
      "(Iteration 38801 / 45930) loss: 1.708993\n",
      "(Iteration 38901 / 45930) loss: 1.384492\n",
      "(Iteration 39001 / 45930) loss: 1.231140\n",
      "(Iteration 39101 / 45930) loss: 1.370364\n",
      "(Iteration 39201 / 45930) loss: 1.203879\n",
      "(Iteration 39301 / 45930) loss: 1.136430\n",
      "(Iteration 39401 / 45930) loss: 1.455196\n",
      "(Iteration 39501 / 45930) loss: 1.521952\n",
      "(Iteration 39601 / 45930) loss: 1.165358\n",
      "(Iteration 39701 / 45930) loss: 1.378803\n",
      "(Iteration 39801 / 45930) loss: 1.834900\n",
      "(Epoch 26 / 30) train acc: 0.530000; val_acc: 0.517000\n",
      "(Iteration 39901 / 45930) loss: 1.692301\n",
      "(Iteration 40001 / 45930) loss: 1.339164\n",
      "(Iteration 40101 / 45930) loss: 1.766196\n",
      "(Iteration 40201 / 45930) loss: 1.384824\n",
      "(Iteration 40301 / 45930) loss: 1.270945\n",
      "(Iteration 40401 / 45930) loss: 1.337515\n",
      "(Iteration 40501 / 45930) loss: 1.373072\n",
      "(Iteration 40601 / 45930) loss: 1.448705\n",
      "(Iteration 40701 / 45930) loss: 1.558016\n",
      "(Iteration 40801 / 45930) loss: 1.382709\n",
      "(Iteration 40901 / 45930) loss: 1.561121\n",
      "(Iteration 41001 / 45930) loss: 1.463193\n",
      "(Iteration 41101 / 45930) loss: 1.414163\n",
      "(Iteration 41201 / 45930) loss: 1.384133\n",
      "(Iteration 41301 / 45930) loss: 1.386296\n",
      "(Epoch 27 / 30) train acc: 0.536000; val_acc: 0.519000\n",
      "(Iteration 41401 / 45930) loss: 1.619652\n",
      "(Iteration 41501 / 45930) loss: 1.556783\n",
      "(Iteration 41601 / 45930) loss: 1.369164\n",
      "(Iteration 41701 / 45930) loss: 1.555424\n",
      "(Iteration 41801 / 45930) loss: 1.636515\n",
      "(Iteration 41901 / 45930) loss: 1.486589\n",
      "(Iteration 42001 / 45930) loss: 1.582081\n",
      "(Iteration 42101 / 45930) loss: 1.826521\n",
      "(Iteration 42201 / 45930) loss: 1.556594\n",
      "(Iteration 42301 / 45930) loss: 1.694558\n",
      "(Iteration 42401 / 45930) loss: 1.530093\n",
      "(Iteration 42501 / 45930) loss: 1.492506\n",
      "(Iteration 42601 / 45930) loss: 1.473029\n",
      "(Iteration 42701 / 45930) loss: 1.816646\n",
      "(Iteration 42801 / 45930) loss: 1.288080\n",
      "(Epoch 28 / 30) train acc: 0.541000; val_acc: 0.523000\n",
      "(Iteration 42901 / 45930) loss: 1.378973\n",
      "(Iteration 43001 / 45930) loss: 1.588528\n",
      "(Iteration 43101 / 45930) loss: 1.323357\n",
      "(Iteration 43201 / 45930) loss: 1.438932\n",
      "(Iteration 43301 / 45930) loss: 1.477237\n",
      "(Iteration 43401 / 45930) loss: 1.355886\n",
      "(Iteration 43501 / 45930) loss: 1.676771\n",
      "(Iteration 43601 / 45930) loss: 1.484549\n",
      "(Iteration 43701 / 45930) loss: 1.235288\n",
      "(Iteration 43801 / 45930) loss: 1.011351\n",
      "(Iteration 43901 / 45930) loss: 1.395403\n",
      "(Iteration 44001 / 45930) loss: 1.356730\n",
      "(Iteration 44101 / 45930) loss: 1.353207\n",
      "(Iteration 44201 / 45930) loss: 1.532993\n",
      "(Iteration 44301 / 45930) loss: 1.328965\n",
      "(Epoch 29 / 30) train acc: 0.562000; val_acc: 0.522000\n",
      "(Iteration 44401 / 45930) loss: 1.348695\n",
      "(Iteration 44501 / 45930) loss: 1.158032\n",
      "(Iteration 44601 / 45930) loss: 1.289095\n",
      "(Iteration 44701 / 45930) loss: 1.249931\n",
      "(Iteration 44801 / 45930) loss: 1.337495\n",
      "(Iteration 44901 / 45930) loss: 1.413886\n",
      "(Iteration 45001 / 45930) loss: 1.369516\n",
      "(Iteration 45101 / 45930) loss: 1.518451\n",
      "(Iteration 45201 / 45930) loss: 1.492014\n",
      "(Iteration 45301 / 45930) loss: 1.507879\n",
      "(Iteration 45401 / 45930) loss: 1.638434\n",
      "(Iteration 45501 / 45930) loss: 1.474274\n",
      "(Iteration 45601 / 45930) loss: 1.377363\n",
      "(Iteration 45701 / 45930) loss: 1.812386\n",
      "(Iteration 45801 / 45930) loss: 1.519010\n",
      "(Iteration 45901 / 45930) loss: 1.743867\n",
      "(Epoch 30 / 30) train acc: 0.536000; val_acc: 0.522000\n",
      "New best model found with validation accuracy: 0.4910\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.01, 'num_epochs': 40, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 30600) loss: 2.305858\n",
      "(Epoch 0 / 40) train acc: 0.094000; val_acc: 0.102000\n",
      "(Iteration 101 / 30600) loss: 2.304932\n",
      "(Iteration 201 / 30600) loss: 2.303873\n",
      "(Iteration 301 / 30600) loss: 2.305137\n",
      "(Iteration 401 / 30600) loss: 2.304271\n",
      "(Iteration 501 / 30600) loss: 2.303639\n",
      "(Iteration 601 / 30600) loss: 2.302065\n",
      "(Iteration 701 / 30600) loss: 2.302379\n",
      "(Epoch 1 / 40) train acc: 0.117000; val_acc: 0.141000\n",
      "(Iteration 801 / 30600) loss: 2.302393\n",
      "(Iteration 901 / 30600) loss: 2.302271\n",
      "(Iteration 1001 / 30600) loss: 2.296781\n",
      "(Iteration 1101 / 30600) loss: 2.295136\n",
      "(Iteration 1201 / 30600) loss: 2.287795\n",
      "(Iteration 1301 / 30600) loss: 2.277901\n",
      "(Iteration 1401 / 30600) loss: 2.270360\n",
      "(Iteration 1501 / 30600) loss: 2.260595\n",
      "(Epoch 2 / 40) train acc: 0.229000; val_acc: 0.232000\n",
      "(Iteration 1601 / 30600) loss: 2.206375\n",
      "(Iteration 1701 / 30600) loss: 2.234855\n",
      "(Iteration 1801 / 30600) loss: 2.216519\n",
      "(Iteration 1901 / 30600) loss: 2.265584\n",
      "(Iteration 2001 / 30600) loss: 2.212136\n",
      "(Iteration 2101 / 30600) loss: 2.095176\n",
      "(Iteration 2201 / 30600) loss: 2.172460\n",
      "(Epoch 3 / 40) train acc: 0.258000; val_acc: 0.275000\n",
      "(Iteration 2301 / 30600) loss: 2.146134\n",
      "(Iteration 2401 / 30600) loss: 2.040363\n",
      "(Iteration 2501 / 30600) loss: 2.162009\n",
      "(Iteration 2601 / 30600) loss: 2.156323\n",
      "(Iteration 2701 / 30600) loss: 2.093285\n",
      "(Iteration 2801 / 30600) loss: 2.154932\n",
      "(Iteration 2901 / 30600) loss: 2.088847\n",
      "(Iteration 3001 / 30600) loss: 2.027419\n",
      "(Epoch 4 / 40) train acc: 0.291000; val_acc: 0.301000\n",
      "(Iteration 3101 / 30600) loss: 2.109135\n",
      "(Iteration 3201 / 30600) loss: 2.083543\n",
      "(Iteration 3301 / 30600) loss: 2.121172\n",
      "(Iteration 3401 / 30600) loss: 2.072864\n",
      "(Iteration 3501 / 30600) loss: 2.063649\n",
      "(Iteration 3601 / 30600) loss: 2.037026\n",
      "(Iteration 3701 / 30600) loss: 2.016531\n",
      "(Iteration 3801 / 30600) loss: 2.109043\n",
      "(Epoch 5 / 40) train acc: 0.296000; val_acc: 0.299000\n",
      "(Iteration 3901 / 30600) loss: 1.958185\n",
      "(Iteration 4001 / 30600) loss: 1.980307\n",
      "(Iteration 4101 / 30600) loss: 2.053199\n",
      "(Iteration 4201 / 30600) loss: 2.040015\n",
      "(Iteration 4301 / 30600) loss: 2.100917\n",
      "(Iteration 4401 / 30600) loss: 2.081664\n",
      "(Iteration 4501 / 30600) loss: 2.079714\n",
      "(Epoch 6 / 40) train acc: 0.342000; val_acc: 0.328000\n",
      "(Iteration 4601 / 30600) loss: 2.132623\n",
      "(Iteration 4701 / 30600) loss: 2.083496\n",
      "(Iteration 4801 / 30600) loss: 2.030456\n",
      "(Iteration 4901 / 30600) loss: 2.020957\n",
      "(Iteration 5001 / 30600) loss: 2.031708\n",
      "(Iteration 5101 / 30600) loss: 2.066628\n",
      "(Iteration 5201 / 30600) loss: 1.975961\n",
      "(Iteration 5301 / 30600) loss: 2.123000\n",
      "(Epoch 7 / 40) train acc: 0.373000; val_acc: 0.361000\n",
      "(Iteration 5401 / 30600) loss: 1.946571\n",
      "(Iteration 5501 / 30600) loss: 2.171574\n",
      "(Iteration 5601 / 30600) loss: 2.078934\n",
      "(Iteration 5701 / 30600) loss: 1.945849\n",
      "(Iteration 5801 / 30600) loss: 1.963131\n",
      "(Iteration 5901 / 30600) loss: 2.131988\n",
      "(Iteration 6001 / 30600) loss: 1.980155\n",
      "(Iteration 6101 / 30600) loss: 2.093234\n",
      "(Epoch 8 / 40) train acc: 0.373000; val_acc: 0.370000\n",
      "(Iteration 6201 / 30600) loss: 2.096262\n",
      "(Iteration 6301 / 30600) loss: 2.058117\n",
      "(Iteration 6401 / 30600) loss: 2.020242\n",
      "(Iteration 6501 / 30600) loss: 2.098731\n",
      "(Iteration 6601 / 30600) loss: 2.004248\n",
      "(Iteration 6701 / 30600) loss: 2.103599\n",
      "(Iteration 6801 / 30600) loss: 1.980551\n",
      "(Epoch 9 / 40) train acc: 0.390000; val_acc: 0.382000\n",
      "(Iteration 6901 / 30600) loss: 2.050515\n",
      "(Iteration 7001 / 30600) loss: 1.959888\n",
      "(Iteration 7101 / 30600) loss: 2.067746\n",
      "(Iteration 7201 / 30600) loss: 2.055991\n",
      "(Iteration 7301 / 30600) loss: 1.909515\n",
      "(Iteration 7401 / 30600) loss: 2.005394\n",
      "(Iteration 7501 / 30600) loss: 1.967093\n",
      "(Iteration 7601 / 30600) loss: 1.980625\n",
      "(Epoch 10 / 40) train acc: 0.389000; val_acc: 0.401000\n",
      "(Iteration 7701 / 30600) loss: 1.893784\n",
      "(Iteration 7801 / 30600) loss: 1.972280\n",
      "(Iteration 7901 / 30600) loss: 1.945289\n",
      "(Iteration 8001 / 30600) loss: 1.993094\n",
      "(Iteration 8101 / 30600) loss: 2.022808\n",
      "(Iteration 8201 / 30600) loss: 2.018343\n",
      "(Iteration 8301 / 30600) loss: 2.035453\n",
      "(Iteration 8401 / 30600) loss: 2.008688\n",
      "(Epoch 11 / 40) train acc: 0.415000; val_acc: 0.410000\n",
      "(Iteration 8501 / 30600) loss: 1.991412\n",
      "(Iteration 8601 / 30600) loss: 2.056733\n",
      "(Iteration 8701 / 30600) loss: 2.006939\n",
      "(Iteration 8801 / 30600) loss: 1.953091\n",
      "(Iteration 8901 / 30600) loss: 1.935511\n",
      "(Iteration 9001 / 30600) loss: 1.964233\n",
      "(Iteration 9101 / 30600) loss: 1.921992\n",
      "(Epoch 12 / 40) train acc: 0.395000; val_acc: 0.409000\n",
      "(Iteration 9201 / 30600) loss: 1.880793\n",
      "(Iteration 9301 / 30600) loss: 2.003770\n",
      "(Iteration 9401 / 30600) loss: 2.042565\n",
      "(Iteration 9501 / 30600) loss: 2.012499\n",
      "(Iteration 9601 / 30600) loss: 2.082003\n",
      "(Iteration 9701 / 30600) loss: 1.969485\n",
      "(Iteration 9801 / 30600) loss: 1.895900\n",
      "(Iteration 9901 / 30600) loss: 1.998533\n",
      "(Epoch 13 / 40) train acc: 0.386000; val_acc: 0.411000\n",
      "(Iteration 10001 / 30600) loss: 2.029300\n",
      "(Iteration 10101 / 30600) loss: 1.984722\n",
      "(Iteration 10201 / 30600) loss: 1.924938\n",
      "(Iteration 10301 / 30600) loss: 1.931775\n",
      "(Iteration 10401 / 30600) loss: 1.987921\n",
      "(Iteration 10501 / 30600) loss: 1.995681\n",
      "(Iteration 10601 / 30600) loss: 1.992858\n",
      "(Iteration 10701 / 30600) loss: 2.004199\n",
      "(Epoch 14 / 40) train acc: 0.418000; val_acc: 0.404000\n",
      "(Iteration 10801 / 30600) loss: 1.928958\n",
      "(Iteration 10901 / 30600) loss: 2.127500\n",
      "(Iteration 11001 / 30600) loss: 1.994271\n",
      "(Iteration 11101 / 30600) loss: 2.083907\n",
      "(Iteration 11201 / 30600) loss: 2.082317\n",
      "(Iteration 11301 / 30600) loss: 1.978626\n",
      "(Iteration 11401 / 30600) loss: 2.114324\n",
      "(Epoch 15 / 40) train acc: 0.414000; val_acc: 0.414000\n",
      "(Iteration 11501 / 30600) loss: 1.975960\n",
      "(Iteration 11601 / 30600) loss: 1.973810\n",
      "(Iteration 11701 / 30600) loss: 1.970543\n",
      "(Iteration 11801 / 30600) loss: 2.043259\n",
      "(Iteration 11901 / 30600) loss: 1.959906\n",
      "(Iteration 12001 / 30600) loss: 1.991557\n",
      "(Iteration 12101 / 30600) loss: 2.096925\n",
      "(Iteration 12201 / 30600) loss: 2.069955\n",
      "(Epoch 16 / 40) train acc: 0.431000; val_acc: 0.402000\n",
      "(Iteration 12301 / 30600) loss: 2.051771\n",
      "(Iteration 12401 / 30600) loss: 2.008510\n",
      "(Iteration 12501 / 30600) loss: 1.963378\n",
      "(Iteration 12601 / 30600) loss: 2.033404\n",
      "(Iteration 12701 / 30600) loss: 1.997446\n",
      "(Iteration 12801 / 30600) loss: 1.939734\n",
      "(Iteration 12901 / 30600) loss: 1.983739\n",
      "(Iteration 13001 / 30600) loss: 1.898712\n",
      "(Epoch 17 / 40) train acc: 0.414000; val_acc: 0.413000\n",
      "(Iteration 13101 / 30600) loss: 1.899618\n",
      "(Iteration 13201 / 30600) loss: 2.007591\n",
      "(Iteration 13301 / 30600) loss: 1.950536\n",
      "(Iteration 13401 / 30600) loss: 1.932946\n",
      "(Iteration 13501 / 30600) loss: 1.985963\n",
      "(Iteration 13601 / 30600) loss: 1.952318\n",
      "(Iteration 13701 / 30600) loss: 1.960647\n",
      "(Epoch 18 / 40) train acc: 0.425000; val_acc: 0.409000\n",
      "(Iteration 13801 / 30600) loss: 2.036839\n",
      "(Iteration 13901 / 30600) loss: 1.921029\n",
      "(Iteration 14001 / 30600) loss: 2.046901\n",
      "(Iteration 14101 / 30600) loss: 1.816759\n",
      "(Iteration 14201 / 30600) loss: 1.913415\n",
      "(Iteration 14301 / 30600) loss: 1.893071\n",
      "(Iteration 14401 / 30600) loss: 2.000565\n",
      "(Iteration 14501 / 30600) loss: 2.031058\n",
      "(Epoch 19 / 40) train acc: 0.424000; val_acc: 0.406000\n",
      "(Iteration 14601 / 30600) loss: 2.102151\n",
      "(Iteration 14701 / 30600) loss: 1.828457\n",
      "(Iteration 14801 / 30600) loss: 1.903234\n",
      "(Iteration 14901 / 30600) loss: 1.943099\n",
      "(Iteration 15001 / 30600) loss: 1.921759\n",
      "(Iteration 15101 / 30600) loss: 1.946593\n",
      "(Iteration 15201 / 30600) loss: 2.016059\n",
      "(Epoch 20 / 40) train acc: 0.428000; val_acc: 0.409000\n",
      "(Iteration 15301 / 30600) loss: 2.177482\n",
      "(Iteration 15401 / 30600) loss: 1.955532\n",
      "(Iteration 15501 / 30600) loss: 1.986805\n",
      "(Iteration 15601 / 30600) loss: 2.057474\n",
      "(Iteration 15701 / 30600) loss: 1.894156\n",
      "(Iteration 15801 / 30600) loss: 2.005906\n",
      "(Iteration 15901 / 30600) loss: 2.041015\n",
      "(Iteration 16001 / 30600) loss: 1.960239\n",
      "(Epoch 21 / 40) train acc: 0.412000; val_acc: 0.418000\n",
      "(Iteration 16101 / 30600) loss: 1.992923\n",
      "(Iteration 16201 / 30600) loss: 1.894069\n",
      "(Iteration 16301 / 30600) loss: 2.046169\n",
      "(Iteration 16401 / 30600) loss: 1.994763\n",
      "(Iteration 16501 / 30600) loss: 1.964280\n",
      "(Iteration 16601 / 30600) loss: 2.063044\n",
      "(Iteration 16701 / 30600) loss: 2.058205\n",
      "(Iteration 16801 / 30600) loss: 2.017652\n",
      "(Epoch 22 / 40) train acc: 0.415000; val_acc: 0.415000\n",
      "(Iteration 16901 / 30600) loss: 1.933728\n",
      "(Iteration 17001 / 30600) loss: 1.907640\n",
      "(Iteration 17101 / 30600) loss: 1.947087\n",
      "(Iteration 17201 / 30600) loss: 2.101644\n",
      "(Iteration 17301 / 30600) loss: 1.891917\n",
      "(Iteration 17401 / 30600) loss: 1.974914\n",
      "(Iteration 17501 / 30600) loss: 1.861965\n",
      "(Epoch 23 / 40) train acc: 0.438000; val_acc: 0.410000\n",
      "(Iteration 17601 / 30600) loss: 2.014027\n",
      "(Iteration 17701 / 30600) loss: 1.915547\n",
      "(Iteration 17801 / 30600) loss: 1.960302\n",
      "(Iteration 17901 / 30600) loss: 1.990633\n",
      "(Iteration 18001 / 30600) loss: 1.947524\n",
      "(Iteration 18101 / 30600) loss: 1.981683\n",
      "(Iteration 18201 / 30600) loss: 1.981223\n",
      "(Iteration 18301 / 30600) loss: 1.998064\n",
      "(Epoch 24 / 40) train acc: 0.413000; val_acc: 0.412000\n",
      "(Iteration 18401 / 30600) loss: 2.067933\n",
      "(Iteration 18501 / 30600) loss: 1.971767\n",
      "(Iteration 18601 / 30600) loss: 1.978227\n",
      "(Iteration 18701 / 30600) loss: 1.984403\n",
      "(Iteration 18801 / 30600) loss: 1.997462\n",
      "(Iteration 18901 / 30600) loss: 1.986982\n",
      "(Iteration 19001 / 30600) loss: 1.898188\n",
      "(Iteration 19101 / 30600) loss: 1.985572\n",
      "(Epoch 25 / 40) train acc: 0.434000; val_acc: 0.418000\n",
      "(Iteration 19201 / 30600) loss: 1.949578\n",
      "(Iteration 19301 / 30600) loss: 1.905073\n",
      "(Iteration 19401 / 30600) loss: 1.950151\n",
      "(Iteration 19501 / 30600) loss: 1.936679\n",
      "(Iteration 19601 / 30600) loss: 1.962420\n",
      "(Iteration 19701 / 30600) loss: 1.942688\n",
      "(Iteration 19801 / 30600) loss: 1.931568\n",
      "(Epoch 26 / 40) train acc: 0.416000; val_acc: 0.420000\n",
      "(Iteration 19901 / 30600) loss: 1.879861\n",
      "(Iteration 20001 / 30600) loss: 1.986912\n",
      "(Iteration 20101 / 30600) loss: 2.062219\n",
      "(Iteration 20201 / 30600) loss: 1.977724\n",
      "(Iteration 20301 / 30600) loss: 1.897127\n",
      "(Iteration 20401 / 30600) loss: 2.037226\n",
      "(Iteration 20501 / 30600) loss: 1.918778\n",
      "(Iteration 20601 / 30600) loss: 2.107373\n",
      "(Epoch 27 / 40) train acc: 0.405000; val_acc: 0.423000\n",
      "(Iteration 20701 / 30600) loss: 1.922084\n",
      "(Iteration 20801 / 30600) loss: 2.038214\n",
      "(Iteration 20901 / 30600) loss: 2.056369\n",
      "(Iteration 21001 / 30600) loss: 1.948079\n",
      "(Iteration 21101 / 30600) loss: 1.935203\n",
      "(Iteration 21201 / 30600) loss: 1.977830\n",
      "(Iteration 21301 / 30600) loss: 1.944569\n",
      "(Iteration 21401 / 30600) loss: 1.917101\n",
      "(Epoch 28 / 40) train acc: 0.445000; val_acc: 0.424000\n",
      "(Iteration 21501 / 30600) loss: 1.806098\n",
      "(Iteration 21601 / 30600) loss: 1.925909\n",
      "(Iteration 21701 / 30600) loss: 1.997314\n",
      "(Iteration 21801 / 30600) loss: 2.044967\n",
      "(Iteration 21901 / 30600) loss: 1.979616\n",
      "(Iteration 22001 / 30600) loss: 1.940776\n",
      "(Iteration 22101 / 30600) loss: 1.986574\n",
      "(Epoch 29 / 40) train acc: 0.446000; val_acc: 0.423000\n",
      "(Iteration 22201 / 30600) loss: 1.932920\n",
      "(Iteration 22301 / 30600) loss: 2.097446\n",
      "(Iteration 22401 / 30600) loss: 2.131058\n",
      "(Iteration 22501 / 30600) loss: 2.056677\n",
      "(Iteration 22601 / 30600) loss: 1.902315\n",
      "(Iteration 22701 / 30600) loss: 2.147545\n",
      "(Iteration 22801 / 30600) loss: 2.027478\n",
      "(Iteration 22901 / 30600) loss: 1.972589\n",
      "(Epoch 30 / 40) train acc: 0.435000; val_acc: 0.423000\n",
      "(Iteration 23001 / 30600) loss: 1.923887\n",
      "(Iteration 23101 / 30600) loss: 1.916523\n",
      "(Iteration 23201 / 30600) loss: 1.931414\n",
      "(Iteration 23301 / 30600) loss: 2.030368\n",
      "(Iteration 23401 / 30600) loss: 2.081185\n",
      "(Iteration 23501 / 30600) loss: 2.024728\n",
      "(Iteration 23601 / 30600) loss: 1.831322\n",
      "(Iteration 23701 / 30600) loss: 1.995987\n",
      "(Epoch 31 / 40) train acc: 0.422000; val_acc: 0.418000\n",
      "(Iteration 23801 / 30600) loss: 1.950410\n",
      "(Iteration 23901 / 30600) loss: 1.948708\n",
      "(Iteration 24001 / 30600) loss: 1.890695\n",
      "(Iteration 24101 / 30600) loss: 1.880395\n",
      "(Iteration 24201 / 30600) loss: 1.992088\n",
      "(Iteration 24301 / 30600) loss: 1.874285\n",
      "(Iteration 24401 / 30600) loss: 2.019745\n",
      "(Epoch 32 / 40) train acc: 0.437000; val_acc: 0.420000\n",
      "(Iteration 24501 / 30600) loss: 1.938987\n",
      "(Iteration 24601 / 30600) loss: 1.924169\n",
      "(Iteration 24701 / 30600) loss: 1.926013\n",
      "(Iteration 24801 / 30600) loss: 1.956621\n",
      "(Iteration 24901 / 30600) loss: 2.014416\n",
      "(Iteration 25001 / 30600) loss: 1.851218\n",
      "(Iteration 25101 / 30600) loss: 2.000596\n",
      "(Iteration 25201 / 30600) loss: 2.087613\n",
      "(Epoch 33 / 40) train acc: 0.446000; val_acc: 0.417000\n",
      "(Iteration 25301 / 30600) loss: 1.993283\n",
      "(Iteration 25401 / 30600) loss: 2.007768\n",
      "(Iteration 25501 / 30600) loss: 1.975750\n",
      "(Iteration 25601 / 30600) loss: 2.014860\n",
      "(Iteration 25701 / 30600) loss: 2.100261\n",
      "(Iteration 25801 / 30600) loss: 1.961941\n",
      "(Iteration 25901 / 30600) loss: 1.863183\n",
      "(Iteration 26001 / 30600) loss: 1.848339\n",
      "(Epoch 34 / 40) train acc: 0.398000; val_acc: 0.417000\n",
      "(Iteration 26101 / 30600) loss: 1.950223\n",
      "(Iteration 26201 / 30600) loss: 1.794756\n",
      "(Iteration 26301 / 30600) loss: 1.897121\n",
      "(Iteration 26401 / 30600) loss: 2.072479\n",
      "(Iteration 26501 / 30600) loss: 1.865196\n",
      "(Iteration 26601 / 30600) loss: 1.977353\n",
      "(Iteration 26701 / 30600) loss: 1.874062\n",
      "(Epoch 35 / 40) train acc: 0.401000; val_acc: 0.416000\n",
      "(Iteration 26801 / 30600) loss: 1.973646\n",
      "(Iteration 26901 / 30600) loss: 1.954129\n",
      "(Iteration 27001 / 30600) loss: 2.054448\n",
      "(Iteration 27101 / 30600) loss: 2.008865\n",
      "(Iteration 27201 / 30600) loss: 2.066630\n",
      "(Iteration 27301 / 30600) loss: 2.022575\n",
      "(Iteration 27401 / 30600) loss: 1.936734\n",
      "(Iteration 27501 / 30600) loss: 1.955625\n",
      "(Epoch 36 / 40) train acc: 0.416000; val_acc: 0.417000\n",
      "(Iteration 27601 / 30600) loss: 1.986449\n",
      "(Iteration 27701 / 30600) loss: 2.024570\n",
      "(Iteration 27801 / 30600) loss: 2.015784\n",
      "(Iteration 27901 / 30600) loss: 1.944367\n",
      "(Iteration 28001 / 30600) loss: 1.871872\n",
      "(Iteration 28101 / 30600) loss: 2.013304\n",
      "(Iteration 28201 / 30600) loss: 1.959731\n",
      "(Iteration 28301 / 30600) loss: 1.921701\n",
      "(Epoch 37 / 40) train acc: 0.444000; val_acc: 0.417000\n",
      "(Iteration 28401 / 30600) loss: 2.187483\n",
      "(Iteration 28501 / 30600) loss: 1.901373\n",
      "(Iteration 28601 / 30600) loss: 2.006129\n",
      "(Iteration 28701 / 30600) loss: 2.083267\n",
      "(Iteration 28801 / 30600) loss: 1.973206\n",
      "(Iteration 28901 / 30600) loss: 2.023525\n",
      "(Iteration 29001 / 30600) loss: 2.002374\n",
      "(Epoch 38 / 40) train acc: 0.420000; val_acc: 0.416000\n",
      "(Iteration 29101 / 30600) loss: 1.916066\n",
      "(Iteration 29201 / 30600) loss: 1.918282\n",
      "(Iteration 29301 / 30600) loss: 1.926773\n",
      "(Iteration 29401 / 30600) loss: 1.992865\n",
      "(Iteration 29501 / 30600) loss: 2.012924\n",
      "(Iteration 29601 / 30600) loss: 1.907696\n",
      "(Iteration 29701 / 30600) loss: 1.944362\n",
      "(Iteration 29801 / 30600) loss: 1.871092\n",
      "(Epoch 39 / 40) train acc: 0.446000; val_acc: 0.416000\n",
      "(Iteration 29901 / 30600) loss: 2.037101\n",
      "(Iteration 30001 / 30600) loss: 1.988848\n",
      "(Iteration 30101 / 30600) loss: 1.997050\n",
      "(Iteration 30201 / 30600) loss: 2.033271\n",
      "(Iteration 30301 / 30600) loss: 2.020578\n",
      "(Iteration 30401 / 30600) loss: 2.035396\n",
      "(Iteration 30501 / 30600) loss: 1.947915\n",
      "(Epoch 40 / 40) train acc: 0.415000; val_acc: 0.416000\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.01, 'num_epochs': 40, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 61240) loss: 2.305896\n",
      "(Epoch 0 / 40) train acc: 0.116000; val_acc: 0.119000\n",
      "(Iteration 101 / 61240) loss: 2.303952\n",
      "(Iteration 201 / 61240) loss: 2.306090\n",
      "(Iteration 301 / 61240) loss: 2.303658\n",
      "(Iteration 401 / 61240) loss: 2.303796\n",
      "(Iteration 501 / 61240) loss: 2.304620\n",
      "(Iteration 601 / 61240) loss: 2.303340\n",
      "(Iteration 701 / 61240) loss: 2.304160\n",
      "(Iteration 801 / 61240) loss: 2.305855\n",
      "(Iteration 901 / 61240) loss: 2.296264\n",
      "(Iteration 1001 / 61240) loss: 2.294839\n",
      "(Iteration 1101 / 61240) loss: 2.296883\n",
      "(Iteration 1201 / 61240) loss: 2.293365\n",
      "(Iteration 1301 / 61240) loss: 2.287796\n",
      "(Iteration 1401 / 61240) loss: 2.267556\n",
      "(Iteration 1501 / 61240) loss: 2.241484\n",
      "(Epoch 1 / 40) train acc: 0.239000; val_acc: 0.207000\n",
      "(Iteration 1601 / 61240) loss: 2.220043\n",
      "(Iteration 1701 / 61240) loss: 2.217619\n",
      "(Iteration 1801 / 61240) loss: 2.178608\n",
      "(Iteration 1901 / 61240) loss: 2.127021\n",
      "(Iteration 2001 / 61240) loss: 2.105167\n",
      "(Iteration 2101 / 61240) loss: 2.107752\n",
      "(Iteration 2201 / 61240) loss: 2.152548\n",
      "(Iteration 2301 / 61240) loss: 2.090035\n",
      "(Iteration 2401 / 61240) loss: 2.089754\n",
      "(Iteration 2501 / 61240) loss: 2.043049\n",
      "(Iteration 2601 / 61240) loss: 2.045674\n",
      "(Iteration 2701 / 61240) loss: 2.039675\n",
      "(Iteration 2801 / 61240) loss: 2.115198\n",
      "(Iteration 2901 / 61240) loss: 2.115016\n",
      "(Iteration 3001 / 61240) loss: 2.007873\n",
      "(Epoch 2 / 40) train acc: 0.287000; val_acc: 0.294000\n",
      "(Iteration 3101 / 61240) loss: 2.029877\n",
      "(Iteration 3201 / 61240) loss: 1.882286\n",
      "(Iteration 3301 / 61240) loss: 2.150651\n",
      "(Iteration 3401 / 61240) loss: 1.973822\n",
      "(Iteration 3501 / 61240) loss: 2.063783\n",
      "(Iteration 3601 / 61240) loss: 2.009181\n",
      "(Iteration 3701 / 61240) loss: 2.090818\n",
      "(Iteration 3801 / 61240) loss: 2.080930\n",
      "(Iteration 3901 / 61240) loss: 2.064084\n",
      "(Iteration 4001 / 61240) loss: 2.115899\n",
      "(Iteration 4101 / 61240) loss: 2.141127\n",
      "(Iteration 4201 / 61240) loss: 1.975532\n",
      "(Iteration 4301 / 61240) loss: 1.966834\n",
      "(Iteration 4401 / 61240) loss: 1.995321\n",
      "(Iteration 4501 / 61240) loss: 2.046128\n",
      "(Epoch 3 / 40) train acc: 0.375000; val_acc: 0.372000\n",
      "(Iteration 4601 / 61240) loss: 2.075858\n",
      "(Iteration 4701 / 61240) loss: 1.891353\n",
      "(Iteration 4801 / 61240) loss: 2.060863\n",
      "(Iteration 4901 / 61240) loss: 2.017137\n",
      "(Iteration 5001 / 61240) loss: 2.019270\n",
      "(Iteration 5101 / 61240) loss: 2.065028\n",
      "(Iteration 5201 / 61240) loss: 2.043460\n",
      "(Iteration 5301 / 61240) loss: 1.990495\n",
      "(Iteration 5401 / 61240) loss: 1.854008\n",
      "(Iteration 5501 / 61240) loss: 2.044766\n",
      "(Iteration 5601 / 61240) loss: 2.122850\n",
      "(Iteration 5701 / 61240) loss: 1.920092\n",
      "(Iteration 5801 / 61240) loss: 2.092170\n",
      "(Iteration 5901 / 61240) loss: 1.981156\n",
      "(Iteration 6001 / 61240) loss: 1.910298\n",
      "(Iteration 6101 / 61240) loss: 2.192050\n",
      "(Epoch 4 / 40) train acc: 0.403000; val_acc: 0.403000\n",
      "(Iteration 6201 / 61240) loss: 2.123380\n",
      "(Iteration 6301 / 61240) loss: 1.913102\n",
      "(Iteration 6401 / 61240) loss: 2.069420\n",
      "(Iteration 6501 / 61240) loss: 2.054183\n",
      "(Iteration 6601 / 61240) loss: 1.884590\n",
      "(Iteration 6701 / 61240) loss: 1.984501\n",
      "(Iteration 6801 / 61240) loss: 1.875974\n",
      "(Iteration 6901 / 61240) loss: 1.880972\n",
      "(Iteration 7001 / 61240) loss: 1.922323\n",
      "(Iteration 7101 / 61240) loss: 1.992754\n",
      "(Iteration 7201 / 61240) loss: 1.938728\n",
      "(Iteration 7301 / 61240) loss: 2.114838\n",
      "(Iteration 7401 / 61240) loss: 1.979778\n",
      "(Iteration 7501 / 61240) loss: 1.942250\n",
      "(Iteration 7601 / 61240) loss: 1.942767\n",
      "(Epoch 5 / 40) train acc: 0.408000; val_acc: 0.410000\n",
      "(Iteration 7701 / 61240) loss: 2.050748\n",
      "(Iteration 7801 / 61240) loss: 1.968441\n",
      "(Iteration 7901 / 61240) loss: 1.999457\n",
      "(Iteration 8001 / 61240) loss: 2.010534\n",
      "(Iteration 8101 / 61240) loss: 2.045371\n",
      "(Iteration 8201 / 61240) loss: 1.974355\n",
      "(Iteration 8301 / 61240) loss: 2.006610\n",
      "(Iteration 8401 / 61240) loss: 1.926438\n",
      "(Iteration 8501 / 61240) loss: 1.894753\n",
      "(Iteration 8601 / 61240) loss: 1.827969\n",
      "(Iteration 8701 / 61240) loss: 2.030741\n",
      "(Iteration 8801 / 61240) loss: 1.959637\n",
      "(Iteration 8901 / 61240) loss: 2.057510\n",
      "(Iteration 9001 / 61240) loss: 1.852750\n",
      "(Iteration 9101 / 61240) loss: 2.021919\n",
      "(Epoch 6 / 40) train acc: 0.402000; val_acc: 0.425000\n",
      "(Iteration 9201 / 61240) loss: 2.027732\n",
      "(Iteration 9301 / 61240) loss: 1.880620\n",
      "(Iteration 9401 / 61240) loss: 1.894145\n",
      "(Iteration 9501 / 61240) loss: 1.942348\n",
      "(Iteration 9601 / 61240) loss: 1.944225\n",
      "(Iteration 9701 / 61240) loss: 2.023735\n",
      "(Iteration 9801 / 61240) loss: 1.968496\n",
      "(Iteration 9901 / 61240) loss: 2.011028\n",
      "(Iteration 10001 / 61240) loss: 1.983716\n",
      "(Iteration 10101 / 61240) loss: 2.064431\n",
      "(Iteration 10201 / 61240) loss: 1.806869\n",
      "(Iteration 10301 / 61240) loss: 1.973553\n",
      "(Iteration 10401 / 61240) loss: 2.041895\n",
      "(Iteration 10501 / 61240) loss: 1.965746\n",
      "(Iteration 10601 / 61240) loss: 2.074379\n",
      "(Iteration 10701 / 61240) loss: 2.098200\n",
      "(Epoch 7 / 40) train acc: 0.415000; val_acc: 0.412000\n",
      "(Iteration 10801 / 61240) loss: 1.994570\n",
      "(Iteration 10901 / 61240) loss: 2.027889\n",
      "(Iteration 11001 / 61240) loss: 2.053772\n",
      "(Iteration 11101 / 61240) loss: 2.034603\n",
      "(Iteration 11201 / 61240) loss: 2.203399\n",
      "(Iteration 11301 / 61240) loss: 1.928565\n",
      "(Iteration 11401 / 61240) loss: 1.825571\n",
      "(Iteration 11501 / 61240) loss: 1.974932\n",
      "(Iteration 11601 / 61240) loss: 1.981872\n",
      "(Iteration 11701 / 61240) loss: 1.988425\n",
      "(Iteration 11801 / 61240) loss: 1.872485\n",
      "(Iteration 11901 / 61240) loss: 2.062708\n",
      "(Iteration 12001 / 61240) loss: 2.065450\n",
      "(Iteration 12101 / 61240) loss: 1.935881\n",
      "(Iteration 12201 / 61240) loss: 1.923681\n",
      "(Epoch 8 / 40) train acc: 0.452000; val_acc: 0.432000\n",
      "(Iteration 12301 / 61240) loss: 1.933932\n",
      "(Iteration 12401 / 61240) loss: 1.853128\n",
      "(Iteration 12501 / 61240) loss: 1.956287\n",
      "(Iteration 12601 / 61240) loss: 1.953062\n",
      "(Iteration 12701 / 61240) loss: 1.963535\n",
      "(Iteration 12801 / 61240) loss: 1.973634\n",
      "(Iteration 12901 / 61240) loss: 1.964013\n",
      "(Iteration 13001 / 61240) loss: 2.134221\n",
      "(Iteration 13101 / 61240) loss: 2.020842\n",
      "(Iteration 13201 / 61240) loss: 1.750133\n",
      "(Iteration 13301 / 61240) loss: 2.062389\n",
      "(Iteration 13401 / 61240) loss: 1.986402\n",
      "(Iteration 13501 / 61240) loss: 1.873930\n",
      "(Iteration 13601 / 61240) loss: 1.747601\n",
      "(Iteration 13701 / 61240) loss: 2.076091\n",
      "(Epoch 9 / 40) train acc: 0.457000; val_acc: 0.410000\n",
      "(Iteration 13801 / 61240) loss: 1.924503\n",
      "(Iteration 13901 / 61240) loss: 2.018784\n",
      "(Iteration 14001 / 61240) loss: 1.992277\n",
      "(Iteration 14101 / 61240) loss: 1.874477\n",
      "(Iteration 14201 / 61240) loss: 2.141888\n",
      "(Iteration 14301 / 61240) loss: 2.024700\n",
      "(Iteration 14401 / 61240) loss: 1.841138\n",
      "(Iteration 14501 / 61240) loss: 2.085871\n",
      "(Iteration 14601 / 61240) loss: 2.024442\n",
      "(Iteration 14701 / 61240) loss: 2.009385\n",
      "(Iteration 14801 / 61240) loss: 1.931642\n",
      "(Iteration 14901 / 61240) loss: 1.911985\n",
      "(Iteration 15001 / 61240) loss: 2.030564\n",
      "(Iteration 15101 / 61240) loss: 2.141366\n",
      "(Iteration 15201 / 61240) loss: 1.785111\n",
      "(Iteration 15301 / 61240) loss: 2.180078\n",
      "(Epoch 10 / 40) train acc: 0.428000; val_acc: 0.426000\n",
      "(Iteration 15401 / 61240) loss: 1.988739\n",
      "(Iteration 15501 / 61240) loss: 1.949513\n",
      "(Iteration 15601 / 61240) loss: 2.015828\n",
      "(Iteration 15701 / 61240) loss: 1.830980\n",
      "(Iteration 15801 / 61240) loss: 2.096418\n",
      "(Iteration 15901 / 61240) loss: 2.047894\n",
      "(Iteration 16001 / 61240) loss: 1.946160\n",
      "(Iteration 16101 / 61240) loss: 1.776671\n",
      "(Iteration 16201 / 61240) loss: 1.794281\n",
      "(Iteration 16301 / 61240) loss: 1.826761\n",
      "(Iteration 16401 / 61240) loss: 1.823253\n",
      "(Iteration 16501 / 61240) loss: 1.831055\n",
      "(Iteration 16601 / 61240) loss: 2.057395\n",
      "(Iteration 16701 / 61240) loss: 1.993388\n",
      "(Iteration 16801 / 61240) loss: 1.977330\n",
      "(Epoch 11 / 40) train acc: 0.433000; val_acc: 0.433000\n",
      "(Iteration 16901 / 61240) loss: 1.850370\n",
      "(Iteration 17001 / 61240) loss: 2.030818\n",
      "(Iteration 17101 / 61240) loss: 1.816491\n",
      "(Iteration 17201 / 61240) loss: 1.888727\n",
      "(Iteration 17301 / 61240) loss: 2.074926\n",
      "(Iteration 17401 / 61240) loss: 2.030624\n",
      "(Iteration 17501 / 61240) loss: 2.105480\n",
      "(Iteration 17601 / 61240) loss: 1.962293\n",
      "(Iteration 17701 / 61240) loss: 1.895583\n",
      "(Iteration 17801 / 61240) loss: 1.986005\n",
      "(Iteration 17901 / 61240) loss: 2.020764\n",
      "(Iteration 18001 / 61240) loss: 2.189876\n",
      "(Iteration 18101 / 61240) loss: 2.136519\n",
      "(Iteration 18201 / 61240) loss: 1.979058\n",
      "(Iteration 18301 / 61240) loss: 1.939989\n",
      "(Epoch 12 / 40) train acc: 0.445000; val_acc: 0.439000\n",
      "(Iteration 18401 / 61240) loss: 1.866773\n",
      "(Iteration 18501 / 61240) loss: 1.976024\n",
      "(Iteration 18601 / 61240) loss: 1.966657\n",
      "(Iteration 18701 / 61240) loss: 1.960341\n",
      "(Iteration 18801 / 61240) loss: 1.839859\n",
      "(Iteration 18901 / 61240) loss: 1.906171\n",
      "(Iteration 19001 / 61240) loss: 1.757042\n",
      "(Iteration 19101 / 61240) loss: 1.860445\n",
      "(Iteration 19201 / 61240) loss: 1.975141\n",
      "(Iteration 19301 / 61240) loss: 1.964441\n",
      "(Iteration 19401 / 61240) loss: 1.994705\n",
      "(Iteration 19501 / 61240) loss: 2.024167\n",
      "(Iteration 19601 / 61240) loss: 2.093905\n",
      "(Iteration 19701 / 61240) loss: 1.921900\n",
      "(Iteration 19801 / 61240) loss: 1.891683\n",
      "(Iteration 19901 / 61240) loss: 1.920252\n",
      "(Epoch 13 / 40) train acc: 0.434000; val_acc: 0.433000\n",
      "(Iteration 20001 / 61240) loss: 2.101903\n",
      "(Iteration 20101 / 61240) loss: 1.923367\n",
      "(Iteration 20201 / 61240) loss: 1.824580\n",
      "(Iteration 20301 / 61240) loss: 1.974841\n",
      "(Iteration 20401 / 61240) loss: 1.895970\n",
      "(Iteration 20501 / 61240) loss: 1.928216\n",
      "(Iteration 20601 / 61240) loss: 1.859908\n",
      "(Iteration 20701 / 61240) loss: 1.934741\n",
      "(Iteration 20801 / 61240) loss: 2.019341\n",
      "(Iteration 20901 / 61240) loss: 2.115595\n",
      "(Iteration 21001 / 61240) loss: 1.845206\n",
      "(Iteration 21101 / 61240) loss: 1.799551\n",
      "(Iteration 21201 / 61240) loss: 2.001048\n",
      "(Iteration 21301 / 61240) loss: 1.935459\n",
      "(Iteration 21401 / 61240) loss: 2.022251\n",
      "(Epoch 14 / 40) train acc: 0.427000; val_acc: 0.440000\n",
      "(Iteration 21501 / 61240) loss: 1.932310\n",
      "(Iteration 21601 / 61240) loss: 2.116300\n",
      "(Iteration 21701 / 61240) loss: 2.044580\n",
      "(Iteration 21801 / 61240) loss: 2.280039\n",
      "(Iteration 21901 / 61240) loss: 1.904761\n",
      "(Iteration 22001 / 61240) loss: 1.811947\n",
      "(Iteration 22101 / 61240) loss: 2.020107\n",
      "(Iteration 22201 / 61240) loss: 2.098300\n",
      "(Iteration 22301 / 61240) loss: 2.069105\n",
      "(Iteration 22401 / 61240) loss: 1.858176\n",
      "(Iteration 22501 / 61240) loss: 1.741551\n",
      "(Iteration 22601 / 61240) loss: 1.964481\n",
      "(Iteration 22701 / 61240) loss: 2.114342\n",
      "(Iteration 22801 / 61240) loss: 1.955883\n",
      "(Iteration 22901 / 61240) loss: 1.866839\n",
      "(Epoch 15 / 40) train acc: 0.454000; val_acc: 0.432000\n",
      "(Iteration 23001 / 61240) loss: 1.938049\n",
      "(Iteration 23101 / 61240) loss: 1.889273\n",
      "(Iteration 23201 / 61240) loss: 1.981145\n",
      "(Iteration 23301 / 61240) loss: 1.994117\n",
      "(Iteration 23401 / 61240) loss: 1.964376\n",
      "(Iteration 23501 / 61240) loss: 1.941406\n",
      "(Iteration 23601 / 61240) loss: 1.911305\n",
      "(Iteration 23701 / 61240) loss: 1.791396\n",
      "(Iteration 23801 / 61240) loss: 1.967587\n",
      "(Iteration 23901 / 61240) loss: 1.836617\n",
      "(Iteration 24001 / 61240) loss: 1.935107\n",
      "(Iteration 24101 / 61240) loss: 1.980062\n",
      "(Iteration 24201 / 61240) loss: 1.926109\n",
      "(Iteration 24301 / 61240) loss: 1.967854\n",
      "(Iteration 24401 / 61240) loss: 1.759457\n",
      "(Epoch 16 / 40) train acc: 0.466000; val_acc: 0.437000\n",
      "(Iteration 24501 / 61240) loss: 1.863982\n",
      "(Iteration 24601 / 61240) loss: 2.203185\n",
      "(Iteration 24701 / 61240) loss: 1.929892\n",
      "(Iteration 24801 / 61240) loss: 2.062784\n",
      "(Iteration 24901 / 61240) loss: 1.990429\n",
      "(Iteration 25001 / 61240) loss: 1.897222\n",
      "(Iteration 25101 / 61240) loss: 1.713495\n",
      "(Iteration 25201 / 61240) loss: 2.070439\n",
      "(Iteration 25301 / 61240) loss: 2.083062\n",
      "(Iteration 25401 / 61240) loss: 1.866321\n",
      "(Iteration 25501 / 61240) loss: 1.903655\n",
      "(Iteration 25601 / 61240) loss: 2.003698\n",
      "(Iteration 25701 / 61240) loss: 1.770156\n",
      "(Iteration 25801 / 61240) loss: 1.947950\n",
      "(Iteration 25901 / 61240) loss: 2.132171\n",
      "(Iteration 26001 / 61240) loss: 2.111187\n",
      "(Epoch 17 / 40) train acc: 0.445000; val_acc: 0.438000\n",
      "(Iteration 26101 / 61240) loss: 1.994115\n",
      "(Iteration 26201 / 61240) loss: 2.018227\n",
      "(Iteration 26301 / 61240) loss: 1.925205\n",
      "(Iteration 26401 / 61240) loss: 1.935267\n",
      "(Iteration 26501 / 61240) loss: 1.963093\n",
      "(Iteration 26601 / 61240) loss: 1.919899\n",
      "(Iteration 26701 / 61240) loss: 1.661093\n",
      "(Iteration 26801 / 61240) loss: 1.827163\n",
      "(Iteration 26901 / 61240) loss: 1.958783\n",
      "(Iteration 27001 / 61240) loss: 2.003572\n",
      "(Iteration 27101 / 61240) loss: 1.723146\n",
      "(Iteration 27201 / 61240) loss: 1.769098\n",
      "(Iteration 27301 / 61240) loss: 1.833923\n",
      "(Iteration 27401 / 61240) loss: 1.782679\n",
      "(Iteration 27501 / 61240) loss: 2.133663\n",
      "(Epoch 18 / 40) train acc: 0.455000; val_acc: 0.437000\n",
      "(Iteration 27601 / 61240) loss: 2.006472\n",
      "(Iteration 27701 / 61240) loss: 2.054174\n",
      "(Iteration 27801 / 61240) loss: 1.964416\n",
      "(Iteration 27901 / 61240) loss: 2.072846\n",
      "(Iteration 28001 / 61240) loss: 2.016102\n",
      "(Iteration 28101 / 61240) loss: 1.957878\n",
      "(Iteration 28201 / 61240) loss: 1.832112\n",
      "(Iteration 28301 / 61240) loss: 2.151673\n",
      "(Iteration 28401 / 61240) loss: 2.014483\n",
      "(Iteration 28501 / 61240) loss: 1.757707\n",
      "(Iteration 28601 / 61240) loss: 1.951965\n",
      "(Iteration 28701 / 61240) loss: 1.781337\n",
      "(Iteration 28801 / 61240) loss: 1.947360\n",
      "(Iteration 28901 / 61240) loss: 1.972963\n",
      "(Iteration 29001 / 61240) loss: 1.761969\n",
      "(Epoch 19 / 40) train acc: 0.445000; val_acc: 0.426000\n",
      "(Iteration 29101 / 61240) loss: 1.981165\n",
      "(Iteration 29201 / 61240) loss: 1.967515\n",
      "(Iteration 29301 / 61240) loss: 2.060796\n",
      "(Iteration 29401 / 61240) loss: 1.786501\n",
      "(Iteration 29501 / 61240) loss: 1.985682\n",
      "(Iteration 29601 / 61240) loss: 1.968612\n",
      "(Iteration 29701 / 61240) loss: 1.996266\n",
      "(Iteration 29801 / 61240) loss: 2.054903\n",
      "(Iteration 29901 / 61240) loss: 1.956078\n",
      "(Iteration 30001 / 61240) loss: 2.085542\n",
      "(Iteration 30101 / 61240) loss: 1.932788\n",
      "(Iteration 30201 / 61240) loss: 1.987520\n",
      "(Iteration 30301 / 61240) loss: 1.877025\n",
      "(Iteration 30401 / 61240) loss: 1.845798\n",
      "(Iteration 30501 / 61240) loss: 2.075296\n",
      "(Iteration 30601 / 61240) loss: 1.857880\n",
      "(Epoch 20 / 40) train acc: 0.450000; val_acc: 0.434000\n",
      "(Iteration 30701 / 61240) loss: 2.004245\n",
      "(Iteration 30801 / 61240) loss: 1.831706\n",
      "(Iteration 30901 / 61240) loss: 1.936860\n",
      "(Iteration 31001 / 61240) loss: 1.979682\n",
      "(Iteration 31101 / 61240) loss: 1.979248\n",
      "(Iteration 31201 / 61240) loss: 1.997148\n",
      "(Iteration 31301 / 61240) loss: 2.081022\n",
      "(Iteration 31401 / 61240) loss: 1.994249\n",
      "(Iteration 31501 / 61240) loss: 1.767309\n",
      "(Iteration 31601 / 61240) loss: 1.882900\n",
      "(Iteration 31701 / 61240) loss: 1.952536\n",
      "(Iteration 31801 / 61240) loss: 1.957662\n",
      "(Iteration 31901 / 61240) loss: 1.971821\n",
      "(Iteration 32001 / 61240) loss: 2.058966\n",
      "(Iteration 32101 / 61240) loss: 2.149354\n",
      "(Epoch 21 / 40) train acc: 0.426000; val_acc: 0.434000\n",
      "(Iteration 32201 / 61240) loss: 1.821112\n",
      "(Iteration 32301 / 61240) loss: 1.807290\n",
      "(Iteration 32401 / 61240) loss: 1.778276\n",
      "(Iteration 32501 / 61240) loss: 1.919689\n",
      "(Iteration 32601 / 61240) loss: 1.896896\n",
      "(Iteration 32701 / 61240) loss: 2.052301\n",
      "(Iteration 32801 / 61240) loss: 1.804759\n",
      "(Iteration 32901 / 61240) loss: 1.966615\n",
      "(Iteration 33001 / 61240) loss: 1.990268\n",
      "(Iteration 33101 / 61240) loss: 2.039086\n",
      "(Iteration 33201 / 61240) loss: 1.870085\n",
      "(Iteration 33301 / 61240) loss: 1.990072\n",
      "(Iteration 33401 / 61240) loss: 1.904224\n",
      "(Iteration 33501 / 61240) loss: 2.100330\n",
      "(Iteration 33601 / 61240) loss: 2.015648\n",
      "(Epoch 22 / 40) train acc: 0.453000; val_acc: 0.434000\n",
      "(Iteration 33701 / 61240) loss: 2.055797\n",
      "(Iteration 33801 / 61240) loss: 1.795646\n",
      "(Iteration 33901 / 61240) loss: 1.998293\n",
      "(Iteration 34001 / 61240) loss: 1.964792\n",
      "(Iteration 34101 / 61240) loss: 2.009130\n",
      "(Iteration 34201 / 61240) loss: 1.970361\n",
      "(Iteration 34301 / 61240) loss: 2.025721\n",
      "(Iteration 34401 / 61240) loss: 1.903916\n",
      "(Iteration 34501 / 61240) loss: 1.800190\n",
      "(Iteration 34601 / 61240) loss: 1.830323\n",
      "(Iteration 34701 / 61240) loss: 1.996896\n",
      "(Iteration 34801 / 61240) loss: 2.056401\n",
      "(Iteration 34901 / 61240) loss: 2.002345\n",
      "(Iteration 35001 / 61240) loss: 1.861724\n",
      "(Iteration 35101 / 61240) loss: 2.056544\n",
      "(Iteration 35201 / 61240) loss: 1.883687\n",
      "(Epoch 23 / 40) train acc: 0.465000; val_acc: 0.438000\n",
      "(Iteration 35301 / 61240) loss: 1.973182\n",
      "(Iteration 35401 / 61240) loss: 1.899291\n",
      "(Iteration 35501 / 61240) loss: 2.091282\n",
      "(Iteration 35601 / 61240) loss: 1.869949\n",
      "(Iteration 35701 / 61240) loss: 1.805637\n",
      "(Iteration 35801 / 61240) loss: 2.027277\n",
      "(Iteration 35901 / 61240) loss: 1.917272\n",
      "(Iteration 36001 / 61240) loss: 1.940105\n",
      "(Iteration 36101 / 61240) loss: 2.190504\n",
      "(Iteration 36201 / 61240) loss: 1.972707\n",
      "(Iteration 36301 / 61240) loss: 1.935064\n",
      "(Iteration 36401 / 61240) loss: 1.864661\n",
      "(Iteration 36501 / 61240) loss: 1.965239\n",
      "(Iteration 36601 / 61240) loss: 2.030326\n",
      "(Iteration 36701 / 61240) loss: 2.010819\n",
      "(Epoch 24 / 40) train acc: 0.446000; val_acc: 0.436000\n",
      "(Iteration 36801 / 61240) loss: 1.820531\n",
      "(Iteration 36901 / 61240) loss: 1.866873\n",
      "(Iteration 37001 / 61240) loss: 1.954668\n",
      "(Iteration 37101 / 61240) loss: 1.837470\n",
      "(Iteration 37201 / 61240) loss: 2.139946\n",
      "(Iteration 37301 / 61240) loss: 1.900870\n",
      "(Iteration 37401 / 61240) loss: 2.056436\n",
      "(Iteration 37501 / 61240) loss: 1.943455\n",
      "(Iteration 37601 / 61240) loss: 1.855699\n",
      "(Iteration 37701 / 61240) loss: 1.877827\n",
      "(Iteration 37801 / 61240) loss: 2.044817\n",
      "(Iteration 37901 / 61240) loss: 1.840868\n",
      "(Iteration 38001 / 61240) loss: 1.955888\n",
      "(Iteration 38101 / 61240) loss: 2.146679\n",
      "(Iteration 38201 / 61240) loss: 2.029370\n",
      "(Epoch 25 / 40) train acc: 0.459000; val_acc: 0.440000\n",
      "(Iteration 38301 / 61240) loss: 1.906863\n",
      "(Iteration 38401 / 61240) loss: 1.910151\n",
      "(Iteration 38501 / 61240) loss: 1.924284\n",
      "(Iteration 38601 / 61240) loss: 1.815919\n",
      "(Iteration 38701 / 61240) loss: 1.981030\n",
      "(Iteration 38801 / 61240) loss: 1.800474\n",
      "(Iteration 38901 / 61240) loss: 1.938039\n",
      "(Iteration 39001 / 61240) loss: 1.791089\n",
      "(Iteration 39101 / 61240) loss: 2.104943\n",
      "(Iteration 39201 / 61240) loss: 1.910703\n",
      "(Iteration 39301 / 61240) loss: 1.937371\n",
      "(Iteration 39401 / 61240) loss: 1.920413\n",
      "(Iteration 39501 / 61240) loss: 1.885146\n",
      "(Iteration 39601 / 61240) loss: 2.020121\n",
      "(Iteration 39701 / 61240) loss: 1.884125\n",
      "(Iteration 39801 / 61240) loss: 2.002853\n",
      "(Epoch 26 / 40) train acc: 0.425000; val_acc: 0.438000\n",
      "(Iteration 39901 / 61240) loss: 1.854194\n",
      "(Iteration 40001 / 61240) loss: 1.869516\n",
      "(Iteration 40101 / 61240) loss: 1.925767\n",
      "(Iteration 40201 / 61240) loss: 1.795537\n",
      "(Iteration 40301 / 61240) loss: 1.964165\n",
      "(Iteration 40401 / 61240) loss: 2.014736\n",
      "(Iteration 40501 / 61240) loss: 1.838702\n",
      "(Iteration 40601 / 61240) loss: 1.911918\n",
      "(Iteration 40701 / 61240) loss: 2.081912\n",
      "(Iteration 40801 / 61240) loss: 1.944682\n",
      "(Iteration 40901 / 61240) loss: 2.138068\n",
      "(Iteration 41001 / 61240) loss: 1.860702\n",
      "(Iteration 41101 / 61240) loss: 1.957288\n",
      "(Iteration 41201 / 61240) loss: 2.144820\n",
      "(Iteration 41301 / 61240) loss: 1.885058\n",
      "(Epoch 27 / 40) train acc: 0.463000; val_acc: 0.442000\n",
      "(Iteration 41401 / 61240) loss: 1.996146\n",
      "(Iteration 41501 / 61240) loss: 1.854487\n",
      "(Iteration 41601 / 61240) loss: 1.862570\n",
      "(Iteration 41701 / 61240) loss: 1.890333\n",
      "(Iteration 41801 / 61240) loss: 1.844748\n",
      "(Iteration 41901 / 61240) loss: 2.010297\n",
      "(Iteration 42001 / 61240) loss: 1.899140\n",
      "(Iteration 42101 / 61240) loss: 2.008881\n",
      "(Iteration 42201 / 61240) loss: 1.869925\n",
      "(Iteration 42301 / 61240) loss: 2.030501\n",
      "(Iteration 42401 / 61240) loss: 1.915205\n",
      "(Iteration 42501 / 61240) loss: 2.065720\n",
      "(Iteration 42601 / 61240) loss: 1.964456\n",
      "(Iteration 42701 / 61240) loss: 1.967623\n",
      "(Iteration 42801 / 61240) loss: 2.064855\n",
      "(Epoch 28 / 40) train acc: 0.443000; val_acc: 0.436000\n",
      "(Iteration 42901 / 61240) loss: 1.796705\n",
      "(Iteration 43001 / 61240) loss: 1.948582\n",
      "(Iteration 43101 / 61240) loss: 1.852836\n",
      "(Iteration 43201 / 61240) loss: 1.907954\n",
      "(Iteration 43301 / 61240) loss: 1.975711\n",
      "(Iteration 43401 / 61240) loss: 2.031549\n",
      "(Iteration 43501 / 61240) loss: 1.878146\n",
      "(Iteration 43601 / 61240) loss: 1.981878\n",
      "(Iteration 43701 / 61240) loss: 1.869175\n",
      "(Iteration 43801 / 61240) loss: 2.071458\n",
      "(Iteration 43901 / 61240) loss: 1.900566\n",
      "(Iteration 44001 / 61240) loss: 1.777067\n",
      "(Iteration 44101 / 61240) loss: 1.970227\n",
      "(Iteration 44201 / 61240) loss: 2.039026\n",
      "(Iteration 44301 / 61240) loss: 1.928749\n",
      "(Epoch 29 / 40) train acc: 0.428000; val_acc: 0.438000\n",
      "(Iteration 44401 / 61240) loss: 1.861365\n",
      "(Iteration 44501 / 61240) loss: 1.874154\n",
      "(Iteration 44601 / 61240) loss: 2.036717\n",
      "(Iteration 44701 / 61240) loss: 2.072411\n",
      "(Iteration 44801 / 61240) loss: 1.951602\n",
      "(Iteration 44901 / 61240) loss: 1.884237\n",
      "(Iteration 45001 / 61240) loss: 2.158920\n",
      "(Iteration 45101 / 61240) loss: 1.969817\n",
      "(Iteration 45201 / 61240) loss: 1.944241\n",
      "(Iteration 45301 / 61240) loss: 1.754179\n",
      "(Iteration 45401 / 61240) loss: 1.884552\n",
      "(Iteration 45501 / 61240) loss: 1.951373\n",
      "(Iteration 45601 / 61240) loss: 2.008567\n",
      "(Iteration 45701 / 61240) loss: 1.950946\n",
      "(Iteration 45801 / 61240) loss: 1.950239\n",
      "(Iteration 45901 / 61240) loss: 1.960449\n",
      "(Epoch 30 / 40) train acc: 0.429000; val_acc: 0.432000\n",
      "(Iteration 46001 / 61240) loss: 1.739604\n",
      "(Iteration 46101 / 61240) loss: 2.066439\n",
      "(Iteration 46201 / 61240) loss: 1.823805\n",
      "(Iteration 46301 / 61240) loss: 1.881161\n",
      "(Iteration 46401 / 61240) loss: 1.930056\n",
      "(Iteration 46501 / 61240) loss: 1.959823\n",
      "(Iteration 46601 / 61240) loss: 1.885166\n",
      "(Iteration 46701 / 61240) loss: 1.795274\n",
      "(Iteration 46801 / 61240) loss: 2.089495\n",
      "(Iteration 46901 / 61240) loss: 1.752799\n",
      "(Iteration 47001 / 61240) loss: 2.047583\n",
      "(Iteration 47101 / 61240) loss: 1.855158\n",
      "(Iteration 47201 / 61240) loss: 1.925316\n",
      "(Iteration 47301 / 61240) loss: 1.823168\n",
      "(Iteration 47401 / 61240) loss: 1.963109\n",
      "(Epoch 31 / 40) train acc: 0.456000; val_acc: 0.436000\n",
      "(Iteration 47501 / 61240) loss: 2.047344\n",
      "(Iteration 47601 / 61240) loss: 1.866815\n",
      "(Iteration 47701 / 61240) loss: 1.883968\n",
      "(Iteration 47801 / 61240) loss: 1.925787\n",
      "(Iteration 47901 / 61240) loss: 2.094422\n",
      "(Iteration 48001 / 61240) loss: 1.798675\n",
      "(Iteration 48101 / 61240) loss: 2.092675\n",
      "(Iteration 48201 / 61240) loss: 1.884433\n",
      "(Iteration 48301 / 61240) loss: 1.940651\n",
      "(Iteration 48401 / 61240) loss: 1.995069\n",
      "(Iteration 48501 / 61240) loss: 1.861620\n",
      "(Iteration 48601 / 61240) loss: 2.066667\n",
      "(Iteration 48701 / 61240) loss: 2.076161\n",
      "(Iteration 48801 / 61240) loss: 1.978260\n",
      "(Iteration 48901 / 61240) loss: 2.074466\n",
      "(Epoch 32 / 40) train acc: 0.462000; val_acc: 0.439000\n",
      "(Iteration 49001 / 61240) loss: 1.911324\n",
      "(Iteration 49101 / 61240) loss: 1.947241\n",
      "(Iteration 49201 / 61240) loss: 1.829388\n",
      "(Iteration 49301 / 61240) loss: 1.987374\n",
      "(Iteration 49401 / 61240) loss: 2.077989\n",
      "(Iteration 49501 / 61240) loss: 2.106225\n",
      "(Iteration 49601 / 61240) loss: 2.007193\n",
      "(Iteration 49701 / 61240) loss: 1.967982\n",
      "(Iteration 49801 / 61240) loss: 1.954177\n",
      "(Iteration 49901 / 61240) loss: 1.930274\n",
      "(Iteration 50001 / 61240) loss: 1.864576\n",
      "(Iteration 50101 / 61240) loss: 2.073011\n",
      "(Iteration 50201 / 61240) loss: 1.995643\n",
      "(Iteration 50301 / 61240) loss: 1.962890\n",
      "(Iteration 50401 / 61240) loss: 1.844039\n",
      "(Iteration 50501 / 61240) loss: 1.968502\n",
      "(Epoch 33 / 40) train acc: 0.426000; val_acc: 0.439000\n",
      "(Iteration 50601 / 61240) loss: 2.004242\n",
      "(Iteration 50701 / 61240) loss: 1.933116\n",
      "(Iteration 50801 / 61240) loss: 2.035295\n",
      "(Iteration 50901 / 61240) loss: 1.967677\n",
      "(Iteration 51001 / 61240) loss: 2.027694\n",
      "(Iteration 51101 / 61240) loss: 1.868083\n",
      "(Iteration 51201 / 61240) loss: 1.990598\n",
      "(Iteration 51301 / 61240) loss: 1.902625\n",
      "(Iteration 51401 / 61240) loss: 1.813914\n",
      "(Iteration 51501 / 61240) loss: 2.010656\n",
      "(Iteration 51601 / 61240) loss: 1.899140\n",
      "(Iteration 51701 / 61240) loss: 2.150826\n",
      "(Iteration 51801 / 61240) loss: 1.842526\n",
      "(Iteration 51901 / 61240) loss: 1.904385\n",
      "(Iteration 52001 / 61240) loss: 1.946109\n",
      "(Epoch 34 / 40) train acc: 0.481000; val_acc: 0.437000\n",
      "(Iteration 52101 / 61240) loss: 2.043781\n",
      "(Iteration 52201 / 61240) loss: 2.093683\n",
      "(Iteration 52301 / 61240) loss: 1.928165\n",
      "(Iteration 52401 / 61240) loss: 1.984031\n",
      "(Iteration 52501 / 61240) loss: 1.969051\n",
      "(Iteration 52601 / 61240) loss: 1.825668\n",
      "(Iteration 52701 / 61240) loss: 2.002920\n",
      "(Iteration 52801 / 61240) loss: 1.941401\n",
      "(Iteration 52901 / 61240) loss: 2.195905\n",
      "(Iteration 53001 / 61240) loss: 2.043896\n",
      "(Iteration 53101 / 61240) loss: 1.911167\n",
      "(Iteration 53201 / 61240) loss: 1.844515\n",
      "(Iteration 53301 / 61240) loss: 1.868272\n",
      "(Iteration 53401 / 61240) loss: 1.970599\n",
      "(Iteration 53501 / 61240) loss: 1.934115\n",
      "(Epoch 35 / 40) train acc: 0.458000; val_acc: 0.438000\n",
      "(Iteration 53601 / 61240) loss: 1.931136\n",
      "(Iteration 53701 / 61240) loss: 2.086599\n",
      "(Iteration 53801 / 61240) loss: 2.014825\n",
      "(Iteration 53901 / 61240) loss: 1.872423\n",
      "(Iteration 54001 / 61240) loss: 1.902717\n",
      "(Iteration 54101 / 61240) loss: 1.899421\n",
      "(Iteration 54201 / 61240) loss: 1.838940\n",
      "(Iteration 54301 / 61240) loss: 2.006792\n",
      "(Iteration 54401 / 61240) loss: 2.141220\n",
      "(Iteration 54501 / 61240) loss: 1.973393\n",
      "(Iteration 54601 / 61240) loss: 1.887535\n",
      "(Iteration 54701 / 61240) loss: 1.919234\n",
      "(Iteration 54801 / 61240) loss: 1.994076\n",
      "(Iteration 54901 / 61240) loss: 1.845269\n",
      "(Iteration 55001 / 61240) loss: 1.922819\n",
      "(Iteration 55101 / 61240) loss: 1.876218\n",
      "(Epoch 36 / 40) train acc: 0.425000; val_acc: 0.440000\n",
      "(Iteration 55201 / 61240) loss: 1.880770\n",
      "(Iteration 55301 / 61240) loss: 2.052066\n",
      "(Iteration 55401 / 61240) loss: 1.960877\n",
      "(Iteration 55501 / 61240) loss: 1.826481\n",
      "(Iteration 55601 / 61240) loss: 2.022641\n",
      "(Iteration 55701 / 61240) loss: 1.877597\n",
      "(Iteration 55801 / 61240) loss: 1.951868\n",
      "(Iteration 55901 / 61240) loss: 1.910759\n",
      "(Iteration 56001 / 61240) loss: 1.814646\n",
      "(Iteration 56101 / 61240) loss: 1.758927\n",
      "(Iteration 56201 / 61240) loss: 2.063978\n",
      "(Iteration 56301 / 61240) loss: 1.990172\n",
      "(Iteration 56401 / 61240) loss: 2.009403\n",
      "(Iteration 56501 / 61240) loss: 1.951465\n",
      "(Iteration 56601 / 61240) loss: 1.989618\n",
      "(Epoch 37 / 40) train acc: 0.421000; val_acc: 0.437000\n",
      "(Iteration 56701 / 61240) loss: 1.865138\n",
      "(Iteration 56801 / 61240) loss: 1.965019\n",
      "(Iteration 56901 / 61240) loss: 1.871698\n",
      "(Iteration 57001 / 61240) loss: 1.819122\n",
      "(Iteration 57101 / 61240) loss: 1.995388\n",
      "(Iteration 57201 / 61240) loss: 1.951887\n",
      "(Iteration 57301 / 61240) loss: 1.921158\n",
      "(Iteration 57401 / 61240) loss: 1.906291\n",
      "(Iteration 57501 / 61240) loss: 1.912983\n",
      "(Iteration 57601 / 61240) loss: 1.907872\n",
      "(Iteration 57701 / 61240) loss: 2.018641\n",
      "(Iteration 57801 / 61240) loss: 2.085721\n",
      "(Iteration 57901 / 61240) loss: 2.066696\n",
      "(Iteration 58001 / 61240) loss: 2.070513\n",
      "(Iteration 58101 / 61240) loss: 1.975730\n",
      "(Epoch 38 / 40) train acc: 0.431000; val_acc: 0.439000\n",
      "(Iteration 58201 / 61240) loss: 1.947689\n",
      "(Iteration 58301 / 61240) loss: 1.903210\n",
      "(Iteration 58401 / 61240) loss: 1.965377\n",
      "(Iteration 58501 / 61240) loss: 2.078660\n",
      "(Iteration 58601 / 61240) loss: 1.847608\n",
      "(Iteration 58701 / 61240) loss: 1.705118\n",
      "(Iteration 58801 / 61240) loss: 2.022926\n",
      "(Iteration 58901 / 61240) loss: 1.854100\n",
      "(Iteration 59001 / 61240) loss: 1.988643\n",
      "(Iteration 59101 / 61240) loss: 1.865795\n",
      "(Iteration 59201 / 61240) loss: 1.853375\n",
      "(Iteration 59301 / 61240) loss: 2.021543\n",
      "(Iteration 59401 / 61240) loss: 1.812601\n",
      "(Iteration 59501 / 61240) loss: 1.954114\n",
      "(Iteration 59601 / 61240) loss: 2.089078\n",
      "(Iteration 59701 / 61240) loss: 1.893839\n",
      "(Epoch 39 / 40) train acc: 0.436000; val_acc: 0.439000\n",
      "(Iteration 59801 / 61240) loss: 2.050172\n",
      "(Iteration 59901 / 61240) loss: 2.001738\n",
      "(Iteration 60001 / 61240) loss: 1.928234\n",
      "(Iteration 60101 / 61240) loss: 1.857720\n",
      "(Iteration 60201 / 61240) loss: 1.902865\n",
      "(Iteration 60301 / 61240) loss: 2.008795\n",
      "(Iteration 60401 / 61240) loss: 1.924628\n",
      "(Iteration 60501 / 61240) loss: 1.886927\n",
      "(Iteration 60601 / 61240) loss: 2.129385\n",
      "(Iteration 60701 / 61240) loss: 1.997702\n",
      "(Iteration 60801 / 61240) loss: 1.907141\n",
      "(Iteration 60901 / 61240) loss: 2.079308\n",
      "(Iteration 61001 / 61240) loss: 1.805318\n",
      "(Iteration 61101 / 61240) loss: 1.889642\n",
      "(Iteration 61201 / 61240) loss: 2.100349\n",
      "(Epoch 40 / 40) train acc: 0.452000; val_acc: 0.439000\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.01, 'num_epochs': 40, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 30600) loss: 2.302898\n",
      "(Epoch 0 / 40) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 101 / 30600) loss: 2.302192\n",
      "(Iteration 201 / 30600) loss: 2.303248\n",
      "(Iteration 301 / 30600) loss: 2.302015\n",
      "(Iteration 401 / 30600) loss: 2.301310\n",
      "(Iteration 501 / 30600) loss: 2.301274\n",
      "(Iteration 601 / 30600) loss: 2.299782\n",
      "(Iteration 701 / 30600) loss: 2.297994\n",
      "(Epoch 1 / 40) train acc: 0.257000; val_acc: 0.251000\n",
      "(Iteration 801 / 30600) loss: 2.295088\n",
      "(Iteration 901 / 30600) loss: 2.285450\n",
      "(Iteration 1001 / 30600) loss: 2.270526\n",
      "(Iteration 1101 / 30600) loss: 2.243675\n",
      "(Iteration 1201 / 30600) loss: 2.216813\n",
      "(Iteration 1301 / 30600) loss: 2.125435\n",
      "(Iteration 1401 / 30600) loss: 2.107184\n",
      "(Iteration 1501 / 30600) loss: 2.057214\n",
      "(Epoch 2 / 40) train acc: 0.267000; val_acc: 0.272000\n",
      "(Iteration 1601 / 30600) loss: 2.013918\n",
      "(Iteration 1701 / 30600) loss: 1.927739\n",
      "(Iteration 1801 / 30600) loss: 2.037523\n",
      "(Iteration 1901 / 30600) loss: 1.849075\n",
      "(Iteration 2001 / 30600) loss: 1.826528\n",
      "(Iteration 2101 / 30600) loss: 1.869546\n",
      "(Iteration 2201 / 30600) loss: 1.862692\n",
      "(Epoch 3 / 40) train acc: 0.360000; val_acc: 0.344000\n",
      "(Iteration 2301 / 30600) loss: 1.802488\n",
      "(Iteration 2401 / 30600) loss: 1.800116\n",
      "(Iteration 2501 / 30600) loss: 1.700018\n",
      "(Iteration 2601 / 30600) loss: 1.796155\n",
      "(Iteration 2701 / 30600) loss: 1.622568\n",
      "(Iteration 2801 / 30600) loss: 1.765708\n",
      "(Iteration 2901 / 30600) loss: 1.694290\n",
      "(Iteration 3001 / 30600) loss: 1.684020\n",
      "(Epoch 4 / 40) train acc: 0.399000; val_acc: 0.393000\n",
      "(Iteration 3101 / 30600) loss: 1.669215\n",
      "(Iteration 3201 / 30600) loss: 1.547238\n",
      "(Iteration 3301 / 30600) loss: 1.682604\n",
      "(Iteration 3401 / 30600) loss: 1.496412\n",
      "(Iteration 3501 / 30600) loss: 1.725784\n",
      "(Iteration 3601 / 30600) loss: 1.554067\n",
      "(Iteration 3701 / 30600) loss: 1.676157\n",
      "(Iteration 3801 / 30600) loss: 1.697926\n",
      "(Epoch 5 / 40) train acc: 0.420000; val_acc: 0.436000\n",
      "(Iteration 3901 / 30600) loss: 1.569639\n",
      "(Iteration 4001 / 30600) loss: 1.541905\n",
      "(Iteration 4101 / 30600) loss: 1.578570\n",
      "(Iteration 4201 / 30600) loss: 1.708676\n",
      "(Iteration 4301 / 30600) loss: 1.610069\n",
      "(Iteration 4401 / 30600) loss: 1.708787\n",
      "(Iteration 4501 / 30600) loss: 1.671205\n",
      "(Epoch 6 / 40) train acc: 0.432000; val_acc: 0.454000\n",
      "(Iteration 4601 / 30600) loss: 1.655789\n",
      "(Iteration 4701 / 30600) loss: 1.603547\n",
      "(Iteration 4801 / 30600) loss: 1.545777\n",
      "(Iteration 4901 / 30600) loss: 1.627175\n",
      "(Iteration 5001 / 30600) loss: 1.562552\n",
      "(Iteration 5101 / 30600) loss: 1.498033\n",
      "(Iteration 5201 / 30600) loss: 1.494274\n",
      "(Iteration 5301 / 30600) loss: 1.640011\n",
      "(Epoch 7 / 40) train acc: 0.492000; val_acc: 0.472000\n",
      "(Iteration 5401 / 30600) loss: 1.249104\n",
      "(Iteration 5501 / 30600) loss: 1.634098\n",
      "(Iteration 5601 / 30600) loss: 1.569028\n",
      "(Iteration 5701 / 30600) loss: 1.478546\n",
      "(Iteration 5801 / 30600) loss: 1.458384\n",
      "(Iteration 5901 / 30600) loss: 1.662869\n",
      "(Iteration 6001 / 30600) loss: 1.610019\n",
      "(Iteration 6101 / 30600) loss: 1.448765\n",
      "(Epoch 8 / 40) train acc: 0.449000; val_acc: 0.481000\n",
      "(Iteration 6201 / 30600) loss: 1.746147\n",
      "(Iteration 6301 / 30600) loss: 1.489489\n",
      "(Iteration 6401 / 30600) loss: 1.476477\n",
      "(Iteration 6501 / 30600) loss: 1.474284\n",
      "(Iteration 6601 / 30600) loss: 1.484173\n",
      "(Iteration 6701 / 30600) loss: 1.419310\n",
      "(Iteration 6801 / 30600) loss: 1.423974\n",
      "(Epoch 9 / 40) train acc: 0.524000; val_acc: 0.486000\n",
      "(Iteration 6901 / 30600) loss: 1.528221\n",
      "(Iteration 7001 / 30600) loss: 1.264029\n",
      "(Iteration 7101 / 30600) loss: 1.379528\n",
      "(Iteration 7201 / 30600) loss: 1.423044\n",
      "(Iteration 7301 / 30600) loss: 1.604519\n",
      "(Iteration 7401 / 30600) loss: 1.462200\n",
      "(Iteration 7501 / 30600) loss: 1.608856\n",
      "(Iteration 7601 / 30600) loss: 1.426812\n",
      "(Epoch 10 / 40) train acc: 0.506000; val_acc: 0.488000\n",
      "(Iteration 7701 / 30600) loss: 1.386474\n",
      "(Iteration 7801 / 30600) loss: 1.434414\n",
      "(Iteration 7901 / 30600) loss: 1.400082\n",
      "(Iteration 8001 / 30600) loss: 1.410204\n",
      "(Iteration 8101 / 30600) loss: 1.434899\n",
      "(Iteration 8201 / 30600) loss: 1.528016\n",
      "(Iteration 8301 / 30600) loss: 1.556138\n",
      "(Iteration 8401 / 30600) loss: 1.408316\n",
      "(Epoch 11 / 40) train acc: 0.476000; val_acc: 0.493000\n",
      "(Iteration 8501 / 30600) loss: 1.640753\n",
      "(Iteration 8601 / 30600) loss: 1.499650\n",
      "(Iteration 8701 / 30600) loss: 1.562258\n",
      "(Iteration 8801 / 30600) loss: 1.515946\n",
      "(Iteration 8901 / 30600) loss: 1.454744\n",
      "(Iteration 9001 / 30600) loss: 1.547985\n",
      "(Iteration 9101 / 30600) loss: 1.394631\n",
      "(Epoch 12 / 40) train acc: 0.491000; val_acc: 0.498000\n",
      "(Iteration 9201 / 30600) loss: 1.488738\n",
      "(Iteration 9301 / 30600) loss: 1.481637\n",
      "(Iteration 9401 / 30600) loss: 1.748544\n",
      "(Iteration 9501 / 30600) loss: 1.511192\n",
      "(Iteration 9601 / 30600) loss: 1.530732\n",
      "(Iteration 9701 / 30600) loss: 1.527823\n",
      "(Iteration 9801 / 30600) loss: 1.551102\n",
      "(Iteration 9901 / 30600) loss: 1.518483\n",
      "(Epoch 13 / 40) train acc: 0.484000; val_acc: 0.500000\n",
      "(Iteration 10001 / 30600) loss: 1.550965\n",
      "(Iteration 10101 / 30600) loss: 1.639031\n",
      "(Iteration 10201 / 30600) loss: 1.419369\n",
      "(Iteration 10301 / 30600) loss: 1.707722\n",
      "(Iteration 10401 / 30600) loss: 1.460554\n",
      "(Iteration 10501 / 30600) loss: 1.498284\n",
      "(Iteration 10601 / 30600) loss: 1.282854\n",
      "(Iteration 10701 / 30600) loss: 1.626914\n",
      "(Epoch 14 / 40) train acc: 0.503000; val_acc: 0.505000\n",
      "(Iteration 10801 / 30600) loss: 1.476349\n",
      "(Iteration 10901 / 30600) loss: 1.366249\n",
      "(Iteration 11001 / 30600) loss: 1.379678\n",
      "(Iteration 11101 / 30600) loss: 1.650342\n",
      "(Iteration 11201 / 30600) loss: 1.618031\n",
      "(Iteration 11301 / 30600) loss: 1.352570\n",
      "(Iteration 11401 / 30600) loss: 1.502114\n",
      "(Epoch 15 / 40) train acc: 0.500000; val_acc: 0.507000\n",
      "(Iteration 11501 / 30600) loss: 1.459308\n",
      "(Iteration 11601 / 30600) loss: 1.401005\n",
      "(Iteration 11701 / 30600) loss: 1.412996\n",
      "(Iteration 11801 / 30600) loss: 1.729214\n",
      "(Iteration 11901 / 30600) loss: 1.514214\n",
      "(Iteration 12001 / 30600) loss: 1.626945\n",
      "(Iteration 12101 / 30600) loss: 1.279345\n",
      "(Iteration 12201 / 30600) loss: 1.362206\n",
      "(Epoch 16 / 40) train acc: 0.499000; val_acc: 0.506000\n",
      "(Iteration 12301 / 30600) loss: 1.695658\n",
      "(Iteration 12401 / 30600) loss: 1.248478\n",
      "(Iteration 12501 / 30600) loss: 1.510654\n",
      "(Iteration 12601 / 30600) loss: 1.444919\n",
      "(Iteration 12701 / 30600) loss: 1.483607\n",
      "(Iteration 12801 / 30600) loss: 1.620548\n",
      "(Iteration 12901 / 30600) loss: 1.310693\n",
      "(Iteration 13001 / 30600) loss: 1.521516\n",
      "(Epoch 17 / 40) train acc: 0.527000; val_acc: 0.507000\n",
      "(Iteration 13101 / 30600) loss: 1.586144\n",
      "(Iteration 13201 / 30600) loss: 1.602367\n",
      "(Iteration 13301 / 30600) loss: 1.671655\n",
      "(Iteration 13401 / 30600) loss: 1.520670\n",
      "(Iteration 13501 / 30600) loss: 1.717003\n",
      "(Iteration 13601 / 30600) loss: 1.296962\n",
      "(Iteration 13701 / 30600) loss: 1.330552\n",
      "(Epoch 18 / 40) train acc: 0.526000; val_acc: 0.508000\n",
      "(Iteration 13801 / 30600) loss: 1.266344\n",
      "(Iteration 13901 / 30600) loss: 1.418581\n",
      "(Iteration 14001 / 30600) loss: 1.720522\n",
      "(Iteration 14101 / 30600) loss: 1.667699\n",
      "(Iteration 14201 / 30600) loss: 1.527226\n",
      "(Iteration 14301 / 30600) loss: 1.463277\n",
      "(Iteration 14401 / 30600) loss: 1.353140\n",
      "(Iteration 14501 / 30600) loss: 1.364468\n",
      "(Epoch 19 / 40) train acc: 0.499000; val_acc: 0.511000\n",
      "(Iteration 14601 / 30600) loss: 1.490846\n",
      "(Iteration 14701 / 30600) loss: 1.223267\n",
      "(Iteration 14801 / 30600) loss: 1.542559\n",
      "(Iteration 14901 / 30600) loss: 1.415537\n",
      "(Iteration 15001 / 30600) loss: 1.539725\n",
      "(Iteration 15101 / 30600) loss: 1.523779\n",
      "(Iteration 15201 / 30600) loss: 1.296993\n",
      "(Epoch 20 / 40) train acc: 0.516000; val_acc: 0.509000\n",
      "(Iteration 15301 / 30600) loss: 1.633207\n",
      "(Iteration 15401 / 30600) loss: 1.434242\n",
      "(Iteration 15501 / 30600) loss: 1.515758\n",
      "(Iteration 15601 / 30600) loss: 1.463406\n",
      "(Iteration 15701 / 30600) loss: 1.398717\n",
      "(Iteration 15801 / 30600) loss: 1.456295\n",
      "(Iteration 15901 / 30600) loss: 1.725218\n",
      "(Iteration 16001 / 30600) loss: 1.526280\n",
      "(Epoch 21 / 40) train acc: 0.518000; val_acc: 0.513000\n",
      "(Iteration 16101 / 30600) loss: 1.634701\n",
      "(Iteration 16201 / 30600) loss: 1.508331\n",
      "(Iteration 16301 / 30600) loss: 1.472939\n",
      "(Iteration 16401 / 30600) loss: 1.430781\n",
      "(Iteration 16501 / 30600) loss: 1.638030\n",
      "(Iteration 16601 / 30600) loss: 1.401417\n",
      "(Iteration 16701 / 30600) loss: 1.605615\n",
      "(Iteration 16801 / 30600) loss: 1.350413\n",
      "(Epoch 22 / 40) train acc: 0.516000; val_acc: 0.510000\n",
      "(Iteration 16901 / 30600) loss: 1.487263\n",
      "(Iteration 17001 / 30600) loss: 1.284445\n",
      "(Iteration 17101 / 30600) loss: 1.471197\n",
      "(Iteration 17201 / 30600) loss: 1.392466\n",
      "(Iteration 17301 / 30600) loss: 1.427285\n",
      "(Iteration 17401 / 30600) loss: 1.457672\n",
      "(Iteration 17501 / 30600) loss: 1.415188\n",
      "(Epoch 23 / 40) train acc: 0.519000; val_acc: 0.514000\n",
      "(Iteration 17601 / 30600) loss: 1.571510\n",
      "(Iteration 17701 / 30600) loss: 1.436677\n",
      "(Iteration 17801 / 30600) loss: 1.500830\n",
      "(Iteration 17901 / 30600) loss: 1.386120\n",
      "(Iteration 18001 / 30600) loss: 1.393786\n",
      "(Iteration 18101 / 30600) loss: 1.587001\n",
      "(Iteration 18201 / 30600) loss: 1.532269\n",
      "(Iteration 18301 / 30600) loss: 1.454360\n",
      "(Epoch 24 / 40) train acc: 0.513000; val_acc: 0.509000\n",
      "(Iteration 18401 / 30600) loss: 1.392422\n",
      "(Iteration 18501 / 30600) loss: 1.320933\n",
      "(Iteration 18601 / 30600) loss: 1.296714\n",
      "(Iteration 18701 / 30600) loss: 1.348647\n",
      "(Iteration 18801 / 30600) loss: 1.364909\n",
      "(Iteration 18901 / 30600) loss: 1.529433\n",
      "(Iteration 19001 / 30600) loss: 1.351960\n",
      "(Iteration 19101 / 30600) loss: 1.458791\n",
      "(Epoch 25 / 40) train acc: 0.528000; val_acc: 0.510000\n",
      "(Iteration 19201 / 30600) loss: 1.504099\n",
      "(Iteration 19301 / 30600) loss: 1.490553\n",
      "(Iteration 19401 / 30600) loss: 1.581200\n",
      "(Iteration 19501 / 30600) loss: 1.461778\n",
      "(Iteration 19601 / 30600) loss: 1.391372\n",
      "(Iteration 19701 / 30600) loss: 1.717039\n",
      "(Iteration 19801 / 30600) loss: 1.461942\n",
      "(Epoch 26 / 40) train acc: 0.541000; val_acc: 0.509000\n",
      "(Iteration 19901 / 30600) loss: 1.420269\n",
      "(Iteration 20001 / 30600) loss: 1.352628\n",
      "(Iteration 20101 / 30600) loss: 1.326565\n",
      "(Iteration 20201 / 30600) loss: 1.490845\n",
      "(Iteration 20301 / 30600) loss: 1.378238\n",
      "(Iteration 20401 / 30600) loss: 1.404747\n",
      "(Iteration 20501 / 30600) loss: 1.281818\n",
      "(Iteration 20601 / 30600) loss: 1.536383\n",
      "(Epoch 27 / 40) train acc: 0.498000; val_acc: 0.510000\n",
      "(Iteration 20701 / 30600) loss: 1.606829\n",
      "(Iteration 20801 / 30600) loss: 1.380335\n",
      "(Iteration 20901 / 30600) loss: 1.340880\n",
      "(Iteration 21001 / 30600) loss: 1.521101\n",
      "(Iteration 21101 / 30600) loss: 1.461886\n",
      "(Iteration 21201 / 30600) loss: 1.535340\n",
      "(Iteration 21301 / 30600) loss: 1.391089\n",
      "(Iteration 21401 / 30600) loss: 1.429588\n",
      "(Epoch 28 / 40) train acc: 0.521000; val_acc: 0.507000\n",
      "(Iteration 21501 / 30600) loss: 1.671924\n",
      "(Iteration 21601 / 30600) loss: 1.524084\n",
      "(Iteration 21701 / 30600) loss: 1.259326\n",
      "(Iteration 21801 / 30600) loss: 1.454457\n",
      "(Iteration 21901 / 30600) loss: 1.432189\n",
      "(Iteration 22001 / 30600) loss: 1.434893\n",
      "(Iteration 22101 / 30600) loss: 1.638418\n",
      "(Epoch 29 / 40) train acc: 0.517000; val_acc: 0.510000\n",
      "(Iteration 22201 / 30600) loss: 1.458837\n",
      "(Iteration 22301 / 30600) loss: 1.425737\n",
      "(Iteration 22401 / 30600) loss: 1.329722\n",
      "(Iteration 22501 / 30600) loss: 1.443822\n",
      "(Iteration 22601 / 30600) loss: 1.551482\n",
      "(Iteration 22701 / 30600) loss: 1.441291\n",
      "(Iteration 22801 / 30600) loss: 1.615060\n",
      "(Iteration 22901 / 30600) loss: 1.541331\n",
      "(Epoch 30 / 40) train acc: 0.539000; val_acc: 0.508000\n",
      "(Iteration 23001 / 30600) loss: 1.332781\n",
      "(Iteration 23101 / 30600) loss: 1.281904\n",
      "(Iteration 23201 / 30600) loss: 1.329118\n",
      "(Iteration 23301 / 30600) loss: 1.519574\n",
      "(Iteration 23401 / 30600) loss: 1.377379\n",
      "(Iteration 23501 / 30600) loss: 1.583018\n",
      "(Iteration 23601 / 30600) loss: 1.609885\n",
      "(Iteration 23701 / 30600) loss: 1.504969\n",
      "(Epoch 31 / 40) train acc: 0.524000; val_acc: 0.509000\n",
      "(Iteration 23801 / 30600) loss: 1.428870\n",
      "(Iteration 23901 / 30600) loss: 1.337275\n",
      "(Iteration 24001 / 30600) loss: 1.415283\n",
      "(Iteration 24101 / 30600) loss: 1.484267\n",
      "(Iteration 24201 / 30600) loss: 1.191763\n",
      "(Iteration 24301 / 30600) loss: 1.447019\n",
      "(Iteration 24401 / 30600) loss: 1.450014\n",
      "(Epoch 32 / 40) train acc: 0.515000; val_acc: 0.511000\n",
      "(Iteration 24501 / 30600) loss: 1.570428\n",
      "(Iteration 24601 / 30600) loss: 1.412037\n",
      "(Iteration 24701 / 30600) loss: 1.426534\n",
      "(Iteration 24801 / 30600) loss: 1.404102\n",
      "(Iteration 24901 / 30600) loss: 1.554485\n",
      "(Iteration 25001 / 30600) loss: 1.787829\n",
      "(Iteration 25101 / 30600) loss: 1.355987\n",
      "(Iteration 25201 / 30600) loss: 1.639537\n",
      "(Epoch 33 / 40) train acc: 0.548000; val_acc: 0.510000\n",
      "(Iteration 25301 / 30600) loss: 1.384748\n",
      "(Iteration 25401 / 30600) loss: 1.367569\n",
      "(Iteration 25501 / 30600) loss: 1.317360\n",
      "(Iteration 25601 / 30600) loss: 1.388089\n",
      "(Iteration 25701 / 30600) loss: 1.663160\n",
      "(Iteration 25801 / 30600) loss: 1.473607\n",
      "(Iteration 25901 / 30600) loss: 1.391004\n",
      "(Iteration 26001 / 30600) loss: 1.325440\n",
      "(Epoch 34 / 40) train acc: 0.513000; val_acc: 0.509000\n",
      "(Iteration 26101 / 30600) loss: 1.736608\n",
      "(Iteration 26201 / 30600) loss: 1.333121\n",
      "(Iteration 26301 / 30600) loss: 1.439239\n",
      "(Iteration 26401 / 30600) loss: 1.497474\n",
      "(Iteration 26501 / 30600) loss: 1.359258\n",
      "(Iteration 26601 / 30600) loss: 1.577212\n",
      "(Iteration 26701 / 30600) loss: 1.599329\n",
      "(Epoch 35 / 40) train acc: 0.544000; val_acc: 0.508000\n",
      "(Iteration 26801 / 30600) loss: 1.564982\n",
      "(Iteration 26901 / 30600) loss: 1.426966\n",
      "(Iteration 27001 / 30600) loss: 1.355915\n",
      "(Iteration 27101 / 30600) loss: 1.413222\n",
      "(Iteration 27201 / 30600) loss: 1.591800\n",
      "(Iteration 27301 / 30600) loss: 1.595916\n",
      "(Iteration 27401 / 30600) loss: 1.555952\n",
      "(Iteration 27501 / 30600) loss: 1.632869\n",
      "(Epoch 36 / 40) train acc: 0.545000; val_acc: 0.507000\n",
      "(Iteration 27601 / 30600) loss: 1.533189\n",
      "(Iteration 27701 / 30600) loss: 1.505162\n",
      "(Iteration 27801 / 30600) loss: 1.624910\n",
      "(Iteration 27901 / 30600) loss: 1.395511\n",
      "(Iteration 28001 / 30600) loss: 1.593032\n",
      "(Iteration 28101 / 30600) loss: 1.553325\n",
      "(Iteration 28201 / 30600) loss: 1.582991\n",
      "(Iteration 28301 / 30600) loss: 1.463770\n",
      "(Epoch 37 / 40) train acc: 0.533000; val_acc: 0.508000\n",
      "(Iteration 28401 / 30600) loss: 1.555342\n",
      "(Iteration 28501 / 30600) loss: 1.452507\n",
      "(Iteration 28601 / 30600) loss: 1.545186\n",
      "(Iteration 28701 / 30600) loss: 1.561914\n",
      "(Iteration 28801 / 30600) loss: 1.596928\n",
      "(Iteration 28901 / 30600) loss: 1.438407\n",
      "(Iteration 29001 / 30600) loss: 1.383812\n",
      "(Epoch 38 / 40) train acc: 0.520000; val_acc: 0.508000\n",
      "(Iteration 29101 / 30600) loss: 1.529289\n",
      "(Iteration 29201 / 30600) loss: 1.326591\n",
      "(Iteration 29301 / 30600) loss: 1.577217\n",
      "(Iteration 29401 / 30600) loss: 1.654320\n",
      "(Iteration 29501 / 30600) loss: 1.639812\n",
      "(Iteration 29601 / 30600) loss: 1.612672\n",
      "(Iteration 29701 / 30600) loss: 1.361948\n",
      "(Iteration 29801 / 30600) loss: 1.533595\n",
      "(Epoch 39 / 40) train acc: 0.530000; val_acc: 0.507000\n",
      "(Iteration 29901 / 30600) loss: 1.406196\n",
      "(Iteration 30001 / 30600) loss: 1.607120\n",
      "(Iteration 30101 / 30600) loss: 1.440956\n",
      "(Iteration 30201 / 30600) loss: 1.539426\n",
      "(Iteration 30301 / 30600) loss: 1.531917\n",
      "(Iteration 30401 / 30600) loss: 1.513111\n",
      "(Iteration 30501 / 30600) loss: 1.670150\n",
      "(Epoch 40 / 40) train acc: 0.517000; val_acc: 0.509000\n",
      "Training with parameters: {'hidden_size': 400, 'learning_rate': 0.01, 'num_epochs': 40, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 61240) loss: 2.302879\n",
      "(Epoch 0 / 40) train acc: 0.095000; val_acc: 0.121000\n",
      "(Iteration 101 / 61240) loss: 2.302882\n",
      "(Iteration 201 / 61240) loss: 2.300968\n",
      "(Iteration 301 / 61240) loss: 2.299941\n",
      "(Iteration 401 / 61240) loss: 2.299831\n",
      "(Iteration 501 / 61240) loss: 2.300866\n",
      "(Iteration 601 / 61240) loss: 2.301378\n",
      "(Iteration 701 / 61240) loss: 2.296456\n",
      "(Iteration 801 / 61240) loss: 2.291788\n",
      "(Iteration 901 / 61240) loss: 2.274215\n",
      "(Iteration 1001 / 61240) loss: 2.272901\n",
      "(Iteration 1101 / 61240) loss: 2.241853\n",
      "(Iteration 1201 / 61240) loss: 2.198257\n",
      "(Iteration 1301 / 61240) loss: 2.084458\n",
      "(Iteration 1401 / 61240) loss: 2.021142\n",
      "(Iteration 1501 / 61240) loss: 1.862381\n",
      "(Epoch 1 / 40) train acc: 0.287000; val_acc: 0.274000\n",
      "(Iteration 1601 / 61240) loss: 1.946633\n",
      "(Iteration 1701 / 61240) loss: 2.122795\n",
      "(Iteration 1801 / 61240) loss: 1.983679\n",
      "(Iteration 1901 / 61240) loss: 1.935709\n",
      "(Iteration 2001 / 61240) loss: 2.013939\n",
      "(Iteration 2101 / 61240) loss: 1.745659\n",
      "(Iteration 2201 / 61240) loss: 1.703319\n",
      "(Iteration 2301 / 61240) loss: 1.707825\n",
      "(Iteration 2401 / 61240) loss: 1.871475\n",
      "(Iteration 2501 / 61240) loss: 1.632571\n",
      "(Iteration 2601 / 61240) loss: 1.585860\n",
      "(Iteration 2701 / 61240) loss: 1.550868\n",
      "(Iteration 2801 / 61240) loss: 1.692764\n",
      "(Iteration 2901 / 61240) loss: 1.609240\n",
      "(Iteration 3001 / 61240) loss: 1.563655\n",
      "(Epoch 2 / 40) train acc: 0.442000; val_acc: 0.422000\n",
      "(Iteration 3101 / 61240) loss: 2.006007\n",
      "(Iteration 3201 / 61240) loss: 1.614107\n",
      "(Iteration 3301 / 61240) loss: 1.596419\n",
      "(Iteration 3401 / 61240) loss: 1.671297\n",
      "(Iteration 3501 / 61240) loss: 1.845712\n",
      "(Iteration 3601 / 61240) loss: 1.800858\n",
      "(Iteration 3701 / 61240) loss: 1.304537\n",
      "(Iteration 3801 / 61240) loss: 1.480216\n",
      "(Iteration 3901 / 61240) loss: 1.614770\n",
      "(Iteration 4001 / 61240) loss: 1.553481\n",
      "(Iteration 4101 / 61240) loss: 1.873169\n",
      "(Iteration 4201 / 61240) loss: 1.473630\n",
      "(Iteration 4301 / 61240) loss: 1.744084\n",
      "(Iteration 4401 / 61240) loss: 1.640943\n",
      "(Iteration 4501 / 61240) loss: 1.398788\n",
      "(Epoch 3 / 40) train acc: 0.483000; val_acc: 0.461000\n",
      "(Iteration 4601 / 61240) loss: 1.671006\n",
      "(Iteration 4701 / 61240) loss: 1.490509\n",
      "(Iteration 4801 / 61240) loss: 1.609389\n",
      "(Iteration 4901 / 61240) loss: 1.491833\n",
      "(Iteration 5001 / 61240) loss: 1.668828\n",
      "(Iteration 5101 / 61240) loss: 1.516676\n",
      "(Iteration 5201 / 61240) loss: 1.483198\n",
      "(Iteration 5301 / 61240) loss: 1.667371\n",
      "(Iteration 5401 / 61240) loss: 1.474633\n",
      "(Iteration 5501 / 61240) loss: 1.545405\n",
      "(Iteration 5601 / 61240) loss: 1.269380\n",
      "(Iteration 5701 / 61240) loss: 1.850389\n",
      "(Iteration 5801 / 61240) loss: 1.821562\n",
      "(Iteration 5901 / 61240) loss: 1.307874\n",
      "(Iteration 6001 / 61240) loss: 1.532175\n",
      "(Iteration 6101 / 61240) loss: 1.347524\n",
      "(Epoch 4 / 40) train acc: 0.505000; val_acc: 0.483000\n",
      "(Iteration 6201 / 61240) loss: 1.617372\n",
      "(Iteration 6301 / 61240) loss: 1.725032\n",
      "(Iteration 6401 / 61240) loss: 1.324714\n",
      "(Iteration 6501 / 61240) loss: 1.667298\n",
      "(Iteration 6601 / 61240) loss: 1.386950\n",
      "(Iteration 6701 / 61240) loss: 1.830127\n",
      "(Iteration 6801 / 61240) loss: 1.389071\n",
      "(Iteration 6901 / 61240) loss: 1.438906\n",
      "(Iteration 7001 / 61240) loss: 1.168231\n",
      "(Iteration 7101 / 61240) loss: 1.903774\n",
      "(Iteration 7201 / 61240) loss: 1.294459\n",
      "(Iteration 7301 / 61240) loss: 1.542924\n",
      "(Iteration 7401 / 61240) loss: 1.444820\n",
      "(Iteration 7501 / 61240) loss: 1.572669\n",
      "(Iteration 7601 / 61240) loss: 1.416131\n",
      "(Epoch 5 / 40) train acc: 0.503000; val_acc: 0.504000\n",
      "(Iteration 7701 / 61240) loss: 1.558513\n",
      "(Iteration 7801 / 61240) loss: 1.437997\n",
      "(Iteration 7901 / 61240) loss: 1.450185\n",
      "(Iteration 8001 / 61240) loss: 1.379250\n",
      "(Iteration 8101 / 61240) loss: 1.478270\n",
      "(Iteration 8201 / 61240) loss: 1.609306\n",
      "(Iteration 8301 / 61240) loss: 1.580800\n",
      "(Iteration 8401 / 61240) loss: 1.314120\n",
      "(Iteration 8501 / 61240) loss: 1.366725\n",
      "(Iteration 8601 / 61240) loss: 1.735222\n",
      "(Iteration 8701 / 61240) loss: 1.171326\n",
      "(Iteration 8801 / 61240) loss: 1.495652\n",
      "(Iteration 8901 / 61240) loss: 1.537140\n",
      "(Iteration 9001 / 61240) loss: 1.631960\n",
      "(Iteration 9101 / 61240) loss: 1.453619\n",
      "(Epoch 6 / 40) train acc: 0.509000; val_acc: 0.500000\n",
      "(Iteration 9201 / 61240) loss: 1.678622\n",
      "(Iteration 9301 / 61240) loss: 1.513478\n",
      "(Iteration 9401 / 61240) loss: 1.157275\n",
      "(Iteration 9501 / 61240) loss: 1.645483\n",
      "(Iteration 9601 / 61240) loss: 1.487729\n",
      "(Iteration 9701 / 61240) loss: 1.430939\n",
      "(Iteration 9801 / 61240) loss: 1.677299\n",
      "(Iteration 9901 / 61240) loss: 1.565298\n",
      "(Iteration 10001 / 61240) loss: 1.641857\n",
      "(Iteration 10101 / 61240) loss: 1.765359\n",
      "(Iteration 10201 / 61240) loss: 1.534965\n",
      "(Iteration 10301 / 61240) loss: 1.507677\n",
      "(Iteration 10401 / 61240) loss: 1.087367\n",
      "(Iteration 10501 / 61240) loss: 1.191601\n",
      "(Iteration 10601 / 61240) loss: 1.637053\n",
      "(Iteration 10701 / 61240) loss: 1.353543\n",
      "(Epoch 7 / 40) train acc: 0.513000; val_acc: 0.519000\n",
      "(Iteration 10801 / 61240) loss: 1.536884\n",
      "(Iteration 10901 / 61240) loss: 1.396260\n",
      "(Iteration 11001 / 61240) loss: 1.789517\n",
      "(Iteration 11101 / 61240) loss: 1.595863\n",
      "(Iteration 11201 / 61240) loss: 1.444700\n",
      "(Iteration 11301 / 61240) loss: 1.847104\n",
      "(Iteration 11401 / 61240) loss: 1.599493\n",
      "(Iteration 11501 / 61240) loss: 1.523117\n",
      "(Iteration 11601 / 61240) loss: 1.750729\n",
      "(Iteration 11701 / 61240) loss: 1.376017\n",
      "(Iteration 11801 / 61240) loss: 1.272844\n",
      "(Iteration 11901 / 61240) loss: 1.396507\n",
      "(Iteration 12001 / 61240) loss: 1.599517\n",
      "(Iteration 12101 / 61240) loss: 1.465062\n",
      "(Iteration 12201 / 61240) loss: 1.481423\n",
      "(Epoch 8 / 40) train acc: 0.511000; val_acc: 0.511000\n",
      "(Iteration 12301 / 61240) loss: 1.517012\n",
      "(Iteration 12401 / 61240) loss: 1.169249\n",
      "(Iteration 12501 / 61240) loss: 1.634645\n",
      "(Iteration 12601 / 61240) loss: 1.666421\n",
      "(Iteration 12701 / 61240) loss: 1.121636\n",
      "(Iteration 12801 / 61240) loss: 1.260566\n",
      "(Iteration 12901 / 61240) loss: 1.668138\n",
      "(Iteration 13001 / 61240) loss: 1.442923\n",
      "(Iteration 13101 / 61240) loss: 1.505761\n",
      "(Iteration 13201 / 61240) loss: 1.745856\n",
      "(Iteration 13301 / 61240) loss: 1.571154\n",
      "(Iteration 13401 / 61240) loss: 1.531750\n",
      "(Iteration 13501 / 61240) loss: 1.700243\n",
      "(Iteration 13601 / 61240) loss: 1.549968\n",
      "(Iteration 13701 / 61240) loss: 1.376788\n",
      "(Epoch 9 / 40) train acc: 0.520000; val_acc: 0.515000\n",
      "(Iteration 13801 / 61240) loss: 1.765280\n",
      "(Iteration 13901 / 61240) loss: 1.878334\n",
      "(Iteration 14001 / 61240) loss: 1.387315\n",
      "(Iteration 14101 / 61240) loss: 1.603368\n",
      "(Iteration 14201 / 61240) loss: 1.378486\n",
      "(Iteration 14301 / 61240) loss: 1.466135\n",
      "(Iteration 14401 / 61240) loss: 1.411821\n",
      "(Iteration 14501 / 61240) loss: 1.467030\n",
      "(Iteration 14601 / 61240) loss: 1.565559\n",
      "(Iteration 14701 / 61240) loss: 1.677363\n",
      "(Iteration 14801 / 61240) loss: 1.215941\n",
      "(Iteration 14901 / 61240) loss: 1.722988\n",
      "(Iteration 15001 / 61240) loss: 1.239180\n",
      "(Iteration 15101 / 61240) loss: 1.519943\n",
      "(Iteration 15201 / 61240) loss: 1.243985\n",
      "(Iteration 15301 / 61240) loss: 1.364096\n",
      "(Epoch 10 / 40) train acc: 0.535000; val_acc: 0.512000\n",
      "(Iteration 15401 / 61240) loss: 1.434312\n",
      "(Iteration 15501 / 61240) loss: 1.500181\n",
      "(Iteration 15601 / 61240) loss: 1.546250\n",
      "(Iteration 15701 / 61240) loss: 1.321417\n",
      "(Iteration 15801 / 61240) loss: 1.718260\n",
      "(Iteration 15901 / 61240) loss: 1.352100\n",
      "(Iteration 16001 / 61240) loss: 1.254794\n",
      "(Iteration 16101 / 61240) loss: 1.567151\n",
      "(Iteration 16201 / 61240) loss: 1.376412\n",
      "(Iteration 16301 / 61240) loss: 1.534815\n",
      "(Iteration 16401 / 61240) loss: 1.727257\n",
      "(Iteration 16501 / 61240) loss: 1.415819\n",
      "(Iteration 16601 / 61240) loss: 1.480092\n",
      "(Iteration 16701 / 61240) loss: 1.324262\n",
      "(Iteration 16801 / 61240) loss: 1.256635\n",
      "(Epoch 11 / 40) train acc: 0.549000; val_acc: 0.509000\n",
      "(Iteration 16901 / 61240) loss: 1.393140\n",
      "(Iteration 17001 / 61240) loss: 1.292243\n",
      "(Iteration 17101 / 61240) loss: 1.636717\n",
      "(Iteration 17201 / 61240) loss: 1.493279\n",
      "(Iteration 17301 / 61240) loss: 1.308134\n",
      "(Iteration 17401 / 61240) loss: 1.326614\n",
      "(Iteration 17501 / 61240) loss: 1.260858\n",
      "(Iteration 17601 / 61240) loss: 1.771968\n",
      "(Iteration 17701 / 61240) loss: 1.482997\n",
      "(Iteration 17801 / 61240) loss: 1.556669\n",
      "(Iteration 17901 / 61240) loss: 1.189410\n",
      "(Iteration 18001 / 61240) loss: 1.232889\n",
      "(Iteration 18101 / 61240) loss: 1.403782\n",
      "(Iteration 18201 / 61240) loss: 1.657853\n",
      "(Iteration 18301 / 61240) loss: 1.185883\n",
      "(Epoch 12 / 40) train acc: 0.532000; val_acc: 0.519000\n",
      "(Iteration 18401 / 61240) loss: 1.516623\n",
      "(Iteration 18501 / 61240) loss: 1.335872\n",
      "(Iteration 18601 / 61240) loss: 1.404115\n",
      "(Iteration 18701 / 61240) loss: 1.470184\n",
      "(Iteration 18801 / 61240) loss: 1.072273\n",
      "(Iteration 18901 / 61240) loss: 1.186221\n",
      "(Iteration 19001 / 61240) loss: 1.254162\n",
      "(Iteration 19101 / 61240) loss: 1.326577\n",
      "(Iteration 19201 / 61240) loss: 1.705098\n",
      "(Iteration 19301 / 61240) loss: 1.357433\n",
      "(Iteration 19401 / 61240) loss: 1.395968\n",
      "(Iteration 19501 / 61240) loss: 1.483396\n",
      "(Iteration 19601 / 61240) loss: 1.325932\n",
      "(Iteration 19701 / 61240) loss: 1.278680\n",
      "(Iteration 19801 / 61240) loss: 1.365940\n",
      "(Iteration 19901 / 61240) loss: 1.237288\n",
      "(Epoch 13 / 40) train acc: 0.521000; val_acc: 0.513000\n",
      "(Iteration 20001 / 61240) loss: 1.790560\n",
      "(Iteration 20101 / 61240) loss: 1.689195\n",
      "(Iteration 20201 / 61240) loss: 1.671752\n",
      "(Iteration 20301 / 61240) loss: 1.437700\n",
      "(Iteration 20401 / 61240) loss: 1.511278\n",
      "(Iteration 20501 / 61240) loss: 1.546294\n",
      "(Iteration 20601 / 61240) loss: 1.694560\n",
      "(Iteration 20701 / 61240) loss: 1.421162\n",
      "(Iteration 20801 / 61240) loss: 1.431508\n",
      "(Iteration 20901 / 61240) loss: 1.478170\n",
      "(Iteration 21001 / 61240) loss: 1.581669\n",
      "(Iteration 21101 / 61240) loss: 1.495859\n",
      "(Iteration 21201 / 61240) loss: 1.180684\n",
      "(Iteration 21301 / 61240) loss: 1.538464\n",
      "(Iteration 21401 / 61240) loss: 1.752957\n",
      "(Epoch 14 / 40) train acc: 0.564000; val_acc: 0.504000\n",
      "(Iteration 21501 / 61240) loss: 1.476662\n",
      "(Iteration 21601 / 61240) loss: 1.091185\n",
      "(Iteration 21701 / 61240) loss: 1.533317\n",
      "(Iteration 21801 / 61240) loss: 1.506705\n",
      "(Iteration 21901 / 61240) loss: 1.934633\n",
      "(Iteration 22001 / 61240) loss: 1.475452\n",
      "(Iteration 22101 / 61240) loss: 1.613750\n",
      "(Iteration 22201 / 61240) loss: 1.906531\n",
      "(Iteration 22301 / 61240) loss: 1.327473\n",
      "(Iteration 22401 / 61240) loss: 1.308231\n",
      "(Iteration 22501 / 61240) loss: 1.516476\n",
      "(Iteration 22601 / 61240) loss: 1.155139\n",
      "(Iteration 22701 / 61240) loss: 1.255145\n",
      "(Iteration 22801 / 61240) loss: 1.603698\n",
      "(Iteration 22901 / 61240) loss: 1.395278\n",
      "(Epoch 15 / 40) train acc: 0.547000; val_acc: 0.509000\n",
      "(Iteration 23001 / 61240) loss: 1.577764\n",
      "(Iteration 23101 / 61240) loss: 1.141920\n",
      "(Iteration 23201 / 61240) loss: 1.622351\n",
      "(Iteration 23301 / 61240) loss: 1.195579\n",
      "(Iteration 23401 / 61240) loss: 1.144333\n",
      "(Iteration 23501 / 61240) loss: 1.268231\n",
      "(Iteration 23601 / 61240) loss: 1.629562\n",
      "(Iteration 23701 / 61240) loss: 1.342178\n",
      "(Iteration 23801 / 61240) loss: 1.500823\n",
      "(Iteration 23901 / 61240) loss: 1.418221\n",
      "(Iteration 24001 / 61240) loss: 1.341751\n",
      "(Iteration 24101 / 61240) loss: 1.443989\n",
      "(Iteration 24201 / 61240) loss: 1.153712\n",
      "(Iteration 24301 / 61240) loss: 1.442328\n",
      "(Iteration 24401 / 61240) loss: 1.376088\n",
      "(Epoch 16 / 40) train acc: 0.529000; val_acc: 0.515000\n",
      "(Iteration 24501 / 61240) loss: 1.381784\n",
      "(Iteration 24601 / 61240) loss: 1.477081\n",
      "(Iteration 24701 / 61240) loss: 1.747039\n",
      "(Iteration 24801 / 61240) loss: 1.345463\n",
      "(Iteration 24901 / 61240) loss: 1.604929\n",
      "(Iteration 25001 / 61240) loss: 1.250095\n",
      "(Iteration 25101 / 61240) loss: 1.558392\n",
      "(Iteration 25201 / 61240) loss: 1.600259\n",
      "(Iteration 25301 / 61240) loss: 1.758120\n",
      "(Iteration 25401 / 61240) loss: 1.046034\n",
      "(Iteration 25501 / 61240) loss: 1.350559\n",
      "(Iteration 25601 / 61240) loss: 1.424777\n",
      "(Iteration 25701 / 61240) loss: 1.448144\n",
      "(Iteration 25801 / 61240) loss: 1.384711\n",
      "(Iteration 25901 / 61240) loss: 1.609296\n",
      "(Iteration 26001 / 61240) loss: 1.588761\n",
      "(Epoch 17 / 40) train acc: 0.578000; val_acc: 0.512000\n",
      "(Iteration 26101 / 61240) loss: 1.503347\n",
      "(Iteration 26201 / 61240) loss: 1.375907\n",
      "(Iteration 26301 / 61240) loss: 1.527998\n",
      "(Iteration 26401 / 61240) loss: 1.424164\n",
      "(Iteration 26501 / 61240) loss: 1.305151\n",
      "(Iteration 26601 / 61240) loss: 1.471978\n",
      "(Iteration 26701 / 61240) loss: 1.362665\n",
      "(Iteration 26801 / 61240) loss: 1.455747\n",
      "(Iteration 26901 / 61240) loss: 1.259151\n",
      "(Iteration 27001 / 61240) loss: 1.493275\n",
      "(Iteration 27101 / 61240) loss: 1.421056\n",
      "(Iteration 27201 / 61240) loss: 1.035492\n",
      "(Iteration 27301 / 61240) loss: 1.367686\n",
      "(Iteration 27401 / 61240) loss: 1.048385\n",
      "(Iteration 27501 / 61240) loss: 1.296486\n",
      "(Epoch 18 / 40) train acc: 0.527000; val_acc: 0.515000\n",
      "(Iteration 27601 / 61240) loss: 1.562144\n",
      "(Iteration 27701 / 61240) loss: 1.346500\n",
      "(Iteration 27801 / 61240) loss: 1.029229\n",
      "(Iteration 27901 / 61240) loss: 1.488462\n",
      "(Iteration 28001 / 61240) loss: 1.204248\n",
      "(Iteration 28101 / 61240) loss: 1.282557\n",
      "(Iteration 28201 / 61240) loss: 1.070853\n",
      "(Iteration 28301 / 61240) loss: 1.503221\n",
      "(Iteration 28401 / 61240) loss: 1.416872\n",
      "(Iteration 28501 / 61240) loss: 1.497884\n",
      "(Iteration 28601 / 61240) loss: 1.308271\n",
      "(Iteration 28701 / 61240) loss: 1.622723\n",
      "(Iteration 28801 / 61240) loss: 1.286813\n",
      "(Iteration 28901 / 61240) loss: 1.897999\n",
      "(Iteration 29001 / 61240) loss: 1.816680\n",
      "(Epoch 19 / 40) train acc: 0.512000; val_acc: 0.514000\n",
      "(Iteration 29101 / 61240) loss: 1.306501\n",
      "(Iteration 29201 / 61240) loss: 1.453265\n",
      "(Iteration 29301 / 61240) loss: 1.608667\n",
      "(Iteration 29401 / 61240) loss: 1.477061\n",
      "(Iteration 29501 / 61240) loss: 1.759096\n",
      "(Iteration 29601 / 61240) loss: 1.311141\n",
      "(Iteration 29701 / 61240) loss: 1.611751\n",
      "(Iteration 29801 / 61240) loss: 1.489935\n",
      "(Iteration 29901 / 61240) loss: 1.333368\n",
      "(Iteration 30001 / 61240) loss: 1.528707\n",
      "(Iteration 30101 / 61240) loss: 1.330849\n",
      "(Iteration 30201 / 61240) loss: 1.431709\n",
      "(Iteration 30301 / 61240) loss: 1.246593\n",
      "(Iteration 30401 / 61240) loss: 1.478886\n",
      "(Iteration 30501 / 61240) loss: 1.409680\n",
      "(Iteration 30601 / 61240) loss: 1.567449\n",
      "(Epoch 20 / 40) train acc: 0.546000; val_acc: 0.517000\n",
      "(Iteration 30701 / 61240) loss: 1.718722\n",
      "(Iteration 30801 / 61240) loss: 1.414025\n",
      "(Iteration 30901 / 61240) loss: 1.511808\n",
      "(Iteration 31001 / 61240) loss: 1.447799\n",
      "(Iteration 31101 / 61240) loss: 1.310683\n",
      "(Iteration 31201 / 61240) loss: 1.272689\n",
      "(Iteration 31301 / 61240) loss: 1.341716\n",
      "(Iteration 31401 / 61240) loss: 1.416086\n",
      "(Iteration 31501 / 61240) loss: 1.226745\n",
      "(Iteration 31601 / 61240) loss: 1.428497\n",
      "(Iteration 31701 / 61240) loss: 1.854002\n",
      "(Iteration 31801 / 61240) loss: 1.454297\n",
      "(Iteration 31901 / 61240) loss: 1.408723\n",
      "(Iteration 32001 / 61240) loss: 1.866462\n",
      "(Iteration 32101 / 61240) loss: 1.253507\n",
      "(Epoch 21 / 40) train acc: 0.522000; val_acc: 0.515000\n",
      "(Iteration 32201 / 61240) loss: 1.619002\n",
      "(Iteration 32301 / 61240) loss: 1.537914\n",
      "(Iteration 32401 / 61240) loss: 1.403332\n",
      "(Iteration 32501 / 61240) loss: 1.683788\n",
      "(Iteration 32601 / 61240) loss: 1.557341\n",
      "(Iteration 32701 / 61240) loss: 1.298025\n",
      "(Iteration 32801 / 61240) loss: 1.535032\n",
      "(Iteration 32901 / 61240) loss: 1.570143\n",
      "(Iteration 33001 / 61240) loss: 1.379617\n",
      "(Iteration 33101 / 61240) loss: 1.324238\n",
      "(Iteration 33201 / 61240) loss: 1.673195\n",
      "(Iteration 33301 / 61240) loss: 1.410599\n",
      "(Iteration 33401 / 61240) loss: 1.268432\n",
      "(Iteration 33501 / 61240) loss: 1.498388\n",
      "(Iteration 33601 / 61240) loss: 1.446482\n",
      "(Epoch 22 / 40) train acc: 0.535000; val_acc: 0.515000\n",
      "(Iteration 33701 / 61240) loss: 1.384961\n",
      "(Iteration 33801 / 61240) loss: 1.602970\n",
      "(Iteration 33901 / 61240) loss: 1.353485\n",
      "(Iteration 34001 / 61240) loss: 1.263846\n",
      "(Iteration 34101 / 61240) loss: 1.265215\n",
      "(Iteration 34201 / 61240) loss: 1.535225\n",
      "(Iteration 34301 / 61240) loss: 1.569093\n",
      "(Iteration 34401 / 61240) loss: 1.733491\n",
      "(Iteration 34501 / 61240) loss: 1.421290\n",
      "(Iteration 34601 / 61240) loss: 1.236244\n",
      "(Iteration 34701 / 61240) loss: 1.156496\n",
      "(Iteration 34801 / 61240) loss: 1.458868\n",
      "(Iteration 34901 / 61240) loss: 1.599115\n",
      "(Iteration 35001 / 61240) loss: 1.268776\n",
      "(Iteration 35101 / 61240) loss: 1.336562\n",
      "(Iteration 35201 / 61240) loss: 1.684697\n",
      "(Epoch 23 / 40) train acc: 0.500000; val_acc: 0.517000\n",
      "(Iteration 35301 / 61240) loss: 1.378254\n",
      "(Iteration 35401 / 61240) loss: 1.169307\n",
      "(Iteration 35501 / 61240) loss: 1.442698\n",
      "(Iteration 35601 / 61240) loss: 1.554687\n",
      "(Iteration 35701 / 61240) loss: 1.700721\n",
      "(Iteration 35801 / 61240) loss: 1.719012\n",
      "(Iteration 35901 / 61240) loss: 1.486919\n",
      "(Iteration 36001 / 61240) loss: 1.607463\n",
      "(Iteration 36101 / 61240) loss: 1.483898\n",
      "(Iteration 36201 / 61240) loss: 1.193899\n",
      "(Iteration 36301 / 61240) loss: 1.463779\n",
      "(Iteration 36401 / 61240) loss: 1.684424\n",
      "(Iteration 36501 / 61240) loss: 1.882285\n",
      "(Iteration 36601 / 61240) loss: 1.422472\n",
      "(Iteration 36701 / 61240) loss: 1.483903\n",
      "(Epoch 24 / 40) train acc: 0.547000; val_acc: 0.522000\n",
      "(Iteration 36801 / 61240) loss: 1.278310\n",
      "(Iteration 36901 / 61240) loss: 1.551637\n",
      "(Iteration 37001 / 61240) loss: 1.332710\n",
      "(Iteration 37101 / 61240) loss: 1.666103\n",
      "(Iteration 37201 / 61240) loss: 1.263721\n",
      "(Iteration 37301 / 61240) loss: 1.389483\n",
      "(Iteration 37401 / 61240) loss: 1.770241\n",
      "(Iteration 37501 / 61240) loss: 1.294391\n",
      "(Iteration 37601 / 61240) loss: 1.775912\n",
      "(Iteration 37701 / 61240) loss: 1.247583\n",
      "(Iteration 37801 / 61240) loss: 1.707189\n",
      "(Iteration 37901 / 61240) loss: 1.170134\n",
      "(Iteration 38001 / 61240) loss: 1.221089\n",
      "(Iteration 38101 / 61240) loss: 1.387497\n",
      "(Iteration 38201 / 61240) loss: 1.317530\n",
      "(Epoch 25 / 40) train acc: 0.587000; val_acc: 0.518000\n",
      "(Iteration 38301 / 61240) loss: 1.657281\n",
      "(Iteration 38401 / 61240) loss: 1.338133\n",
      "(Iteration 38501 / 61240) loss: 1.435684\n",
      "(Iteration 38601 / 61240) loss: 1.598534\n",
      "(Iteration 38701 / 61240) loss: 1.749782\n",
      "(Iteration 38801 / 61240) loss: 1.011185\n",
      "(Iteration 38901 / 61240) loss: 1.293104\n",
      "(Iteration 39001 / 61240) loss: 1.516142\n",
      "(Iteration 39101 / 61240) loss: 1.500271\n",
      "(Iteration 39201 / 61240) loss: 1.480122\n",
      "(Iteration 39301 / 61240) loss: 1.199438\n",
      "(Iteration 39401 / 61240) loss: 1.413988\n",
      "(Iteration 39501 / 61240) loss: 1.139609\n",
      "(Iteration 39601 / 61240) loss: 1.318295\n",
      "(Iteration 39701 / 61240) loss: 1.443611\n",
      "(Iteration 39801 / 61240) loss: 1.454008\n",
      "(Epoch 26 / 40) train acc: 0.533000; val_acc: 0.520000\n",
      "(Iteration 39901 / 61240) loss: 1.442842\n",
      "(Iteration 40001 / 61240) loss: 1.465359\n",
      "(Iteration 40101 / 61240) loss: 1.544734\n",
      "(Iteration 40201 / 61240) loss: 1.342272\n",
      "(Iteration 40301 / 61240) loss: 1.240645\n",
      "(Iteration 40401 / 61240) loss: 1.489204\n",
      "(Iteration 40501 / 61240) loss: 1.243566\n",
      "(Iteration 40601 / 61240) loss: 1.696998\n",
      "(Iteration 40701 / 61240) loss: 1.235607\n",
      "(Iteration 40801 / 61240) loss: 1.539265\n",
      "(Iteration 40901 / 61240) loss: 1.452082\n",
      "(Iteration 41001 / 61240) loss: 1.339796\n",
      "(Iteration 41101 / 61240) loss: 1.443115\n",
      "(Iteration 41201 / 61240) loss: 1.648393\n",
      "(Iteration 41301 / 61240) loss: 1.804068\n",
      "(Epoch 27 / 40) train acc: 0.545000; val_acc: 0.524000\n",
      "(Iteration 41401 / 61240) loss: 1.467338\n",
      "(Iteration 41501 / 61240) loss: 1.514404\n",
      "(Iteration 41601 / 61240) loss: 1.362665\n",
      "(Iteration 41701 / 61240) loss: 1.357661\n",
      "(Iteration 41801 / 61240) loss: 1.401965\n",
      "(Iteration 41901 / 61240) loss: 1.376555\n",
      "(Iteration 42001 / 61240) loss: 1.517715\n",
      "(Iteration 42101 / 61240) loss: 1.523517\n",
      "(Iteration 42201 / 61240) loss: 1.522788\n",
      "(Iteration 42301 / 61240) loss: 1.289237\n",
      "(Iteration 42401 / 61240) loss: 1.516475\n",
      "(Iteration 42501 / 61240) loss: 1.496637\n",
      "(Iteration 42601 / 61240) loss: 1.898206\n",
      "(Iteration 42701 / 61240) loss: 1.611203\n",
      "(Iteration 42801 / 61240) loss: 1.378678\n",
      "(Epoch 28 / 40) train acc: 0.523000; val_acc: 0.521000\n",
      "(Iteration 42901 / 61240) loss: 1.462789\n",
      "(Iteration 43001 / 61240) loss: 1.259440\n",
      "(Iteration 43101 / 61240) loss: 1.272967\n",
      "(Iteration 43201 / 61240) loss: 1.844344\n",
      "(Iteration 43301 / 61240) loss: 1.634654\n",
      "(Iteration 43401 / 61240) loss: 1.168579\n",
      "(Iteration 43501 / 61240) loss: 1.584755\n",
      "(Iteration 43601 / 61240) loss: 1.425013\n",
      "(Iteration 43701 / 61240) loss: 1.739790\n",
      "(Iteration 43801 / 61240) loss: 1.076292\n",
      "(Iteration 43901 / 61240) loss: 1.643933\n",
      "(Iteration 44001 / 61240) loss: 1.122871\n",
      "(Iteration 44101 / 61240) loss: 1.369708\n",
      "(Iteration 44201 / 61240) loss: 1.126660\n",
      "(Iteration 44301 / 61240) loss: 1.525974\n",
      "(Epoch 29 / 40) train acc: 0.538000; val_acc: 0.521000\n",
      "(Iteration 44401 / 61240) loss: 1.145282\n",
      "(Iteration 44501 / 61240) loss: 1.150961\n",
      "(Iteration 44601 / 61240) loss: 1.274397\n",
      "(Iteration 44701 / 61240) loss: 1.782530\n",
      "(Iteration 44801 / 61240) loss: 1.486139\n",
      "(Iteration 44901 / 61240) loss: 1.270223\n",
      "(Iteration 45001 / 61240) loss: 1.653482\n",
      "(Iteration 45101 / 61240) loss: 1.555556\n",
      "(Iteration 45201 / 61240) loss: 1.293524\n",
      "(Iteration 45301 / 61240) loss: 1.738604\n",
      "(Iteration 45401 / 61240) loss: 1.318725\n",
      "(Iteration 45501 / 61240) loss: 1.303593\n",
      "(Iteration 45601 / 61240) loss: 1.234219\n",
      "(Iteration 45701 / 61240) loss: 1.434249\n",
      "(Iteration 45801 / 61240) loss: 1.230953\n",
      "(Iteration 45901 / 61240) loss: 1.836615\n",
      "(Epoch 30 / 40) train acc: 0.570000; val_acc: 0.524000\n",
      "(Iteration 46001 / 61240) loss: 1.539583\n",
      "(Iteration 46101 / 61240) loss: 1.364525\n",
      "(Iteration 46201 / 61240) loss: 1.528516\n",
      "(Iteration 46301 / 61240) loss: 1.560353\n",
      "(Iteration 46401 / 61240) loss: 1.325852\n",
      "(Iteration 46501 / 61240) loss: 1.501919\n",
      "(Iteration 46601 / 61240) loss: 1.480188\n",
      "(Iteration 46701 / 61240) loss: 1.488676\n",
      "(Iteration 46801 / 61240) loss: 1.656108\n",
      "(Iteration 46901 / 61240) loss: 1.562639\n",
      "(Iteration 47001 / 61240) loss: 1.414151\n",
      "(Iteration 47101 / 61240) loss: 1.538303\n",
      "(Iteration 47201 / 61240) loss: 1.341324\n",
      "(Iteration 47301 / 61240) loss: 1.383513\n",
      "(Iteration 47401 / 61240) loss: 1.186241\n",
      "(Epoch 31 / 40) train acc: 0.545000; val_acc: 0.521000\n",
      "(Iteration 47501 / 61240) loss: 1.289905\n",
      "(Iteration 47601 / 61240) loss: 1.494541\n",
      "(Iteration 47701 / 61240) loss: 1.609574\n",
      "(Iteration 47801 / 61240) loss: 1.327402\n",
      "(Iteration 47901 / 61240) loss: 1.583724\n",
      "(Iteration 48001 / 61240) loss: 1.518381\n",
      "(Iteration 48101 / 61240) loss: 1.462702\n",
      "(Iteration 48201 / 61240) loss: 1.668028\n",
      "(Iteration 48301 / 61240) loss: 1.174068\n",
      "(Iteration 48401 / 61240) loss: 1.492074\n",
      "(Iteration 48501 / 61240) loss: 1.360985\n",
      "(Iteration 48601 / 61240) loss: 1.381376\n",
      "(Iteration 48701 / 61240) loss: 1.283510\n",
      "(Iteration 48801 / 61240) loss: 1.747314\n",
      "(Iteration 48901 / 61240) loss: 1.163850\n",
      "(Epoch 32 / 40) train acc: 0.549000; val_acc: 0.519000\n",
      "(Iteration 49001 / 61240) loss: 1.273253\n",
      "(Iteration 49101 / 61240) loss: 1.363702\n",
      "(Iteration 49201 / 61240) loss: 1.382318\n",
      "(Iteration 49301 / 61240) loss: 1.351775\n",
      "(Iteration 49401 / 61240) loss: 1.244634\n",
      "(Iteration 49501 / 61240) loss: 1.347173\n",
      "(Iteration 49601 / 61240) loss: 1.176825\n",
      "(Iteration 49701 / 61240) loss: 1.561087\n",
      "(Iteration 49801 / 61240) loss: 1.274384\n",
      "(Iteration 49901 / 61240) loss: 1.409251\n",
      "(Iteration 50001 / 61240) loss: 1.217770\n",
      "(Iteration 50101 / 61240) loss: 1.424221\n",
      "(Iteration 50201 / 61240) loss: 1.302361\n",
      "(Iteration 50301 / 61240) loss: 1.358128\n",
      "(Iteration 50401 / 61240) loss: 1.292142\n",
      "(Iteration 50501 / 61240) loss: 1.437904\n",
      "(Epoch 33 / 40) train acc: 0.529000; val_acc: 0.521000\n",
      "(Iteration 50601 / 61240) loss: 1.051582\n",
      "(Iteration 50701 / 61240) loss: 1.307272\n",
      "(Iteration 50801 / 61240) loss: 1.555119\n",
      "(Iteration 50901 / 61240) loss: 1.414189\n",
      "(Iteration 51001 / 61240) loss: 1.321596\n",
      "(Iteration 51101 / 61240) loss: 1.522608\n",
      "(Iteration 51201 / 61240) loss: 1.700162\n",
      "(Iteration 51301 / 61240) loss: 1.332727\n",
      "(Iteration 51401 / 61240) loss: 1.554269\n",
      "(Iteration 51501 / 61240) loss: 1.499982\n",
      "(Iteration 51601 / 61240) loss: 1.248308\n",
      "(Iteration 51701 / 61240) loss: 1.653289\n",
      "(Iteration 51801 / 61240) loss: 1.485219\n",
      "(Iteration 51901 / 61240) loss: 1.481736\n",
      "(Iteration 52001 / 61240) loss: 1.622311\n",
      "(Epoch 34 / 40) train acc: 0.542000; val_acc: 0.521000\n",
      "(Iteration 52101 / 61240) loss: 1.852445\n",
      "(Iteration 52201 / 61240) loss: 1.699585\n",
      "(Iteration 52301 / 61240) loss: 1.425964\n",
      "(Iteration 52401 / 61240) loss: 1.705998\n",
      "(Iteration 52501 / 61240) loss: 1.352046\n",
      "(Iteration 52601 / 61240) loss: 1.707534\n",
      "(Iteration 52701 / 61240) loss: 1.594813\n",
      "(Iteration 52801 / 61240) loss: 1.240600\n",
      "(Iteration 52901 / 61240) loss: 1.568344\n",
      "(Iteration 53001 / 61240) loss: 1.507743\n",
      "(Iteration 53101 / 61240) loss: 1.505990\n",
      "(Iteration 53201 / 61240) loss: 1.562553\n",
      "(Iteration 53301 / 61240) loss: 1.429522\n",
      "(Iteration 53401 / 61240) loss: 1.469059\n",
      "(Iteration 53501 / 61240) loss: 1.592597\n",
      "(Epoch 35 / 40) train acc: 0.541000; val_acc: 0.521000\n",
      "(Iteration 53601 / 61240) loss: 1.417662\n",
      "(Iteration 53701 / 61240) loss: 1.228384\n",
      "(Iteration 53801 / 61240) loss: 1.532063\n",
      "(Iteration 53901 / 61240) loss: 1.257756\n",
      "(Iteration 54001 / 61240) loss: 1.536928\n",
      "(Iteration 54101 / 61240) loss: 1.357462\n",
      "(Iteration 54201 / 61240) loss: 1.715958\n",
      "(Iteration 54301 / 61240) loss: 1.302161\n",
      "(Iteration 54401 / 61240) loss: 1.412973\n",
      "(Iteration 54501 / 61240) loss: 1.578248\n",
      "(Iteration 54601 / 61240) loss: 1.333846\n",
      "(Iteration 54701 / 61240) loss: 1.531667\n",
      "(Iteration 54801 / 61240) loss: 1.624848\n",
      "(Iteration 54901 / 61240) loss: 1.432646\n",
      "(Iteration 55001 / 61240) loss: 1.269569\n",
      "(Iteration 55101 / 61240) loss: 1.402453\n",
      "(Epoch 36 / 40) train acc: 0.530000; val_acc: 0.520000\n",
      "(Iteration 55201 / 61240) loss: 1.450871\n",
      "(Iteration 55301 / 61240) loss: 1.002330\n",
      "(Iteration 55401 / 61240) loss: 1.360785\n",
      "(Iteration 55501 / 61240) loss: 1.652855\n",
      "(Iteration 55601 / 61240) loss: 1.356387\n",
      "(Iteration 55701 / 61240) loss: 1.442569\n",
      "(Iteration 55801 / 61240) loss: 1.456470\n",
      "(Iteration 55901 / 61240) loss: 1.576767\n",
      "(Iteration 56001 / 61240) loss: 1.430041\n",
      "(Iteration 56101 / 61240) loss: 1.651765\n",
      "(Iteration 56201 / 61240) loss: 1.344573\n",
      "(Iteration 56301 / 61240) loss: 1.370553\n",
      "(Iteration 56401 / 61240) loss: 1.335143\n",
      "(Iteration 56501 / 61240) loss: 1.503546\n",
      "(Iteration 56601 / 61240) loss: 1.580321\n",
      "(Epoch 37 / 40) train acc: 0.554000; val_acc: 0.523000\n",
      "(Iteration 56701 / 61240) loss: 1.500808\n",
      "(Iteration 56801 / 61240) loss: 1.299720\n",
      "(Iteration 56901 / 61240) loss: 1.533204\n",
      "(Iteration 57001 / 61240) loss: 1.389279\n",
      "(Iteration 57101 / 61240) loss: 1.553539\n",
      "(Iteration 57201 / 61240) loss: 1.292402\n",
      "(Iteration 57301 / 61240) loss: 1.492952\n",
      "(Iteration 57401 / 61240) loss: 1.112315\n",
      "(Iteration 57501 / 61240) loss: 1.213305\n",
      "(Iteration 57601 / 61240) loss: 1.427325\n",
      "(Iteration 57701 / 61240) loss: 1.493532\n",
      "(Iteration 57801 / 61240) loss: 1.329932\n",
      "(Iteration 57901 / 61240) loss: 1.388655\n",
      "(Iteration 58001 / 61240) loss: 1.258720\n",
      "(Iteration 58101 / 61240) loss: 1.511542\n",
      "(Epoch 38 / 40) train acc: 0.550000; val_acc: 0.523000\n",
      "(Iteration 58201 / 61240) loss: 1.153150\n",
      "(Iteration 58301 / 61240) loss: 1.430705\n",
      "(Iteration 58401 / 61240) loss: 1.650601\n",
      "(Iteration 58501 / 61240) loss: 1.336081\n",
      "(Iteration 58601 / 61240) loss: 1.129352\n",
      "(Iteration 58701 / 61240) loss: 1.520201\n",
      "(Iteration 58801 / 61240) loss: 1.272095\n",
      "(Iteration 58901 / 61240) loss: 1.647675\n",
      "(Iteration 59001 / 61240) loss: 1.626991\n",
      "(Iteration 59101 / 61240) loss: 1.391868\n",
      "(Iteration 59201 / 61240) loss: 1.586055\n",
      "(Iteration 59301 / 61240) loss: 1.320416\n",
      "(Iteration 59401 / 61240) loss: 1.353877\n",
      "(Iteration 59501 / 61240) loss: 1.356884\n",
      "(Iteration 59601 / 61240) loss: 1.436642\n",
      "(Iteration 59701 / 61240) loss: 1.274654\n",
      "(Epoch 39 / 40) train acc: 0.549000; val_acc: 0.523000\n",
      "(Iteration 59801 / 61240) loss: 1.723189\n",
      "(Iteration 59901 / 61240) loss: 1.403607\n",
      "(Iteration 60001 / 61240) loss: 1.453369\n",
      "(Iteration 60101 / 61240) loss: 1.351519\n",
      "(Iteration 60201 / 61240) loss: 1.568598\n",
      "(Iteration 60301 / 61240) loss: 1.483731\n",
      "(Iteration 60401 / 61240) loss: 1.463440\n",
      "(Iteration 60501 / 61240) loss: 1.348724\n",
      "(Iteration 60601 / 61240) loss: 1.379057\n",
      "(Iteration 60701 / 61240) loss: 1.578606\n",
      "(Iteration 60801 / 61240) loss: 1.609393\n",
      "(Iteration 60901 / 61240) loss: 1.412366\n",
      "(Iteration 61001 / 61240) loss: 1.500491\n",
      "(Iteration 61101 / 61240) loss: 1.265966\n",
      "(Iteration 61201 / 61240) loss: 1.537509\n",
      "(Epoch 40 / 40) train acc: 0.534000; val_acc: 0.524000\n",
      "New best model found with validation accuracy: 0.4966\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.001, 'num_epochs': 20, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 15300) loss: 2.308426\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 101 / 15300) loss: 2.308159\n",
      "(Iteration 201 / 15300) loss: 2.307966\n",
      "(Iteration 301 / 15300) loss: 2.308039\n",
      "(Iteration 401 / 15300) loss: 2.308012\n",
      "(Iteration 501 / 15300) loss: 2.307464\n",
      "(Iteration 601 / 15300) loss: 2.307809\n",
      "(Iteration 701 / 15300) loss: 2.307533\n",
      "(Epoch 1 / 20) train acc: 0.129000; val_acc: 0.115000\n",
      "(Iteration 801 / 15300) loss: 2.307579\n",
      "(Iteration 901 / 15300) loss: 2.307192\n",
      "(Iteration 1001 / 15300) loss: 2.307192\n",
      "(Iteration 1101 / 15300) loss: 2.307193\n",
      "(Iteration 1201 / 15300) loss: 2.306971\n",
      "(Iteration 1301 / 15300) loss: 2.306720\n",
      "(Iteration 1401 / 15300) loss: 2.306370\n",
      "(Iteration 1501 / 15300) loss: 2.306942\n",
      "(Epoch 2 / 20) train acc: 0.165000; val_acc: 0.158000\n",
      "(Iteration 1601 / 15300) loss: 2.306836\n",
      "(Iteration 1701 / 15300) loss: 2.306749\n",
      "(Iteration 1801 / 15300) loss: 2.306456\n",
      "(Iteration 1901 / 15300) loss: 2.306368\n",
      "(Iteration 2001 / 15300) loss: 2.306183\n",
      "(Iteration 2101 / 15300) loss: 2.306131\n",
      "(Iteration 2201 / 15300) loss: 2.305856\n",
      "(Epoch 3 / 20) train acc: 0.198000; val_acc: 0.159000\n",
      "(Iteration 2301 / 15300) loss: 2.306265\n",
      "(Iteration 2401 / 15300) loss: 2.306008\n",
      "(Iteration 2501 / 15300) loss: 2.305994\n",
      "(Iteration 2601 / 15300) loss: 2.305717\n",
      "(Iteration 2701 / 15300) loss: 2.305870\n",
      "(Iteration 2801 / 15300) loss: 2.305234\n",
      "(Iteration 2901 / 15300) loss: 2.305200\n",
      "(Iteration 3001 / 15300) loss: 2.305702\n",
      "(Epoch 4 / 20) train acc: 0.168000; val_acc: 0.144000\n",
      "(Iteration 3101 / 15300) loss: 2.305232\n",
      "(Iteration 3201 / 15300) loss: 2.305697\n",
      "(Iteration 3301 / 15300) loss: 2.305646\n",
      "(Iteration 3401 / 15300) loss: 2.305618\n",
      "(Iteration 3501 / 15300) loss: 2.305499\n",
      "(Iteration 3601 / 15300) loss: 2.305208\n",
      "(Iteration 3701 / 15300) loss: 2.305166\n",
      "(Iteration 3801 / 15300) loss: 2.305070\n",
      "(Epoch 5 / 20) train acc: 0.136000; val_acc: 0.106000\n",
      "(Iteration 3901 / 15300) loss: 2.304944\n",
      "(Iteration 4001 / 15300) loss: 2.304862\n",
      "(Iteration 4101 / 15300) loss: 2.304714\n",
      "(Iteration 4201 / 15300) loss: 2.305231\n",
      "(Iteration 4301 / 15300) loss: 2.305211\n",
      "(Iteration 4401 / 15300) loss: 2.304761\n",
      "(Iteration 4501 / 15300) loss: 2.304845\n",
      "(Epoch 6 / 20) train acc: 0.128000; val_acc: 0.107000\n",
      "(Iteration 4601 / 15300) loss: 2.304554\n",
      "(Iteration 4701 / 15300) loss: 2.304493\n",
      "(Iteration 4801 / 15300) loss: 2.304955\n",
      "(Iteration 4901 / 15300) loss: 2.304205\n",
      "(Iteration 5001 / 15300) loss: 2.304451\n",
      "(Iteration 5101 / 15300) loss: 2.304646\n",
      "(Iteration 5201 / 15300) loss: 2.304587\n",
      "(Iteration 5301 / 15300) loss: 2.304463\n",
      "(Epoch 7 / 20) train acc: 0.153000; val_acc: 0.132000\n",
      "(Iteration 5401 / 15300) loss: 2.304306\n",
      "(Iteration 5501 / 15300) loss: 2.304184\n",
      "(Iteration 5601 / 15300) loss: 2.304235\n",
      "(Iteration 5701 / 15300) loss: 2.304222\n",
      "(Iteration 5801 / 15300) loss: 2.303613\n",
      "(Iteration 5901 / 15300) loss: 2.304055\n",
      "(Iteration 6001 / 15300) loss: 2.304327\n",
      "(Iteration 6101 / 15300) loss: 2.304465\n",
      "(Epoch 8 / 20) train acc: 0.197000; val_acc: 0.169000\n",
      "(Iteration 6201 / 15300) loss: 2.303826\n",
      "(Iteration 6301 / 15300) loss: 2.303983\n",
      "(Iteration 6401 / 15300) loss: 2.303829\n",
      "(Iteration 6501 / 15300) loss: 2.303929\n",
      "(Iteration 6601 / 15300) loss: 2.303714\n",
      "(Iteration 6701 / 15300) loss: 2.303878\n",
      "(Iteration 6801 / 15300) loss: 2.304324\n",
      "(Epoch 9 / 20) train acc: 0.209000; val_acc: 0.179000\n",
      "(Iteration 6901 / 15300) loss: 2.303973\n",
      "(Iteration 7001 / 15300) loss: 2.303643\n",
      "(Iteration 7101 / 15300) loss: 2.304276\n",
      "(Iteration 7201 / 15300) loss: 2.303168\n",
      "(Iteration 7301 / 15300) loss: 2.304130\n",
      "(Iteration 7401 / 15300) loss: 2.303952\n",
      "(Iteration 7501 / 15300) loss: 2.303415\n",
      "(Iteration 7601 / 15300) loss: 2.303596\n",
      "(Epoch 10 / 20) train acc: 0.197000; val_acc: 0.186000\n",
      "(Iteration 7701 / 15300) loss: 2.303721\n",
      "(Iteration 7801 / 15300) loss: 2.303480\n",
      "(Iteration 7901 / 15300) loss: 2.303236\n",
      "(Iteration 8001 / 15300) loss: 2.303728\n",
      "(Iteration 8101 / 15300) loss: 2.303274\n",
      "(Iteration 8201 / 15300) loss: 2.303064\n",
      "(Iteration 8301 / 15300) loss: 2.302894\n",
      "(Iteration 8401 / 15300) loss: 2.304077\n",
      "(Epoch 11 / 20) train acc: 0.211000; val_acc: 0.200000\n",
      "(Iteration 8501 / 15300) loss: 2.303724\n",
      "(Iteration 8601 / 15300) loss: 2.303075\n",
      "(Iteration 8701 / 15300) loss: 2.303658\n",
      "(Iteration 8801 / 15300) loss: 2.303280\n",
      "(Iteration 8901 / 15300) loss: 2.303712\n",
      "(Iteration 9001 / 15300) loss: 2.303178\n",
      "(Iteration 9101 / 15300) loss: 2.303454\n",
      "(Epoch 12 / 20) train acc: 0.221000; val_acc: 0.197000\n",
      "(Iteration 9201 / 15300) loss: 2.302821\n",
      "(Iteration 9301 / 15300) loss: 2.302608\n",
      "(Iteration 9401 / 15300) loss: 2.302819\n",
      "(Iteration 9501 / 15300) loss: 2.303659\n",
      "(Iteration 9601 / 15300) loss: 2.302568\n",
      "(Iteration 9701 / 15300) loss: 2.302374\n",
      "(Iteration 9801 / 15300) loss: 2.303083\n",
      "(Iteration 9901 / 15300) loss: 2.302939\n",
      "(Epoch 13 / 20) train acc: 0.233000; val_acc: 0.203000\n",
      "(Iteration 10001 / 15300) loss: 2.302892\n",
      "(Iteration 10101 / 15300) loss: 2.303348\n",
      "(Iteration 10201 / 15300) loss: 2.302320\n",
      "(Iteration 10301 / 15300) loss: 2.302513\n",
      "(Iteration 10401 / 15300) loss: 2.302794\n",
      "(Iteration 10501 / 15300) loss: 2.303620\n",
      "(Iteration 10601 / 15300) loss: 2.303119\n",
      "(Iteration 10701 / 15300) loss: 2.302326\n",
      "(Epoch 14 / 20) train acc: 0.259000; val_acc: 0.233000\n",
      "(Iteration 10801 / 15300) loss: 2.303003\n",
      "(Iteration 10901 / 15300) loss: 2.302291\n",
      "(Iteration 11001 / 15300) loss: 2.302389\n",
      "(Iteration 11101 / 15300) loss: 2.301465\n",
      "(Iteration 11201 / 15300) loss: 2.302378\n",
      "(Iteration 11301 / 15300) loss: 2.302467\n",
      "(Iteration 11401 / 15300) loss: 2.302340\n",
      "(Epoch 15 / 20) train acc: 0.297000; val_acc: 0.242000\n",
      "(Iteration 11501 / 15300) loss: 2.302473\n",
      "(Iteration 11601 / 15300) loss: 2.302417\n",
      "(Iteration 11701 / 15300) loss: 2.303108\n",
      "(Iteration 11801 / 15300) loss: 2.302280\n",
      "(Iteration 11901 / 15300) loss: 2.302655\n",
      "(Iteration 12001 / 15300) loss: 2.302606\n",
      "(Iteration 12101 / 15300) loss: 2.302788\n",
      "(Iteration 12201 / 15300) loss: 2.302745\n",
      "(Epoch 16 / 20) train acc: 0.280000; val_acc: 0.243000\n",
      "(Iteration 12301 / 15300) loss: 2.302589\n",
      "(Iteration 12401 / 15300) loss: 2.301770\n",
      "(Iteration 12501 / 15300) loss: 2.301602\n",
      "(Iteration 12601 / 15300) loss: 2.302927\n",
      "(Iteration 12701 / 15300) loss: 2.302225\n",
      "(Iteration 12801 / 15300) loss: 2.302776\n",
      "(Iteration 12901 / 15300) loss: 2.302334\n",
      "(Iteration 13001 / 15300) loss: 2.302477\n",
      "(Epoch 17 / 20) train acc: 0.270000; val_acc: 0.244000\n",
      "(Iteration 13101 / 15300) loss: 2.302658\n",
      "(Iteration 13201 / 15300) loss: 2.302170\n",
      "(Iteration 13301 / 15300) loss: 2.301880\n",
      "(Iteration 13401 / 15300) loss: 2.302039\n",
      "(Iteration 13501 / 15300) loss: 2.302431\n",
      "(Iteration 13601 / 15300) loss: 2.302272\n",
      "(Iteration 13701 / 15300) loss: 2.301552\n",
      "(Epoch 18 / 20) train acc: 0.270000; val_acc: 0.244000\n",
      "(Iteration 13801 / 15300) loss: 2.301715\n",
      "(Iteration 13901 / 15300) loss: 2.302261\n",
      "(Iteration 14001 / 15300) loss: 2.301936\n",
      "(Iteration 14101 / 15300) loss: 2.301353\n",
      "(Iteration 14201 / 15300) loss: 2.302150\n",
      "(Iteration 14301 / 15300) loss: 2.300604\n",
      "(Iteration 14401 / 15300) loss: 2.300819\n",
      "(Iteration 14501 / 15300) loss: 2.302266\n",
      "(Epoch 19 / 20) train acc: 0.267000; val_acc: 0.246000\n",
      "(Iteration 14601 / 15300) loss: 2.302437\n",
      "(Iteration 14701 / 15300) loss: 2.301317\n",
      "(Iteration 14801 / 15300) loss: 2.301564\n",
      "(Iteration 14901 / 15300) loss: 2.301239\n",
      "(Iteration 15001 / 15300) loss: 2.303088\n",
      "(Iteration 15101 / 15300) loss: 2.301679\n",
      "(Iteration 15201 / 15300) loss: 2.301790\n",
      "(Epoch 20 / 20) train acc: 0.269000; val_acc: 0.245000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.001, 'num_epochs': 20, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 30620) loss: 2.308400\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.115000\n",
      "(Iteration 101 / 30620) loss: 2.308239\n",
      "(Iteration 201 / 30620) loss: 2.308156\n",
      "(Iteration 301 / 30620) loss: 2.307786\n",
      "(Iteration 401 / 30620) loss: 2.307847\n",
      "(Iteration 501 / 30620) loss: 2.307675\n",
      "(Iteration 601 / 30620) loss: 2.308104\n",
      "(Iteration 701 / 30620) loss: 2.307487\n",
      "(Iteration 801 / 30620) loss: 2.307518\n",
      "(Iteration 901 / 30620) loss: 2.307434\n",
      "(Iteration 1001 / 30620) loss: 2.307051\n",
      "(Iteration 1101 / 30620) loss: 2.306579\n",
      "(Iteration 1201 / 30620) loss: 2.306840\n",
      "(Iteration 1301 / 30620) loss: 2.306323\n",
      "(Iteration 1401 / 30620) loss: 2.306778\n",
      "(Iteration 1501 / 30620) loss: 2.306306\n",
      "(Epoch 1 / 20) train acc: 0.120000; val_acc: 0.110000\n",
      "(Iteration 1601 / 30620) loss: 2.306912\n",
      "(Iteration 1701 / 30620) loss: 2.306494\n",
      "(Iteration 1801 / 30620) loss: 2.306532\n",
      "(Iteration 1901 / 30620) loss: 2.306207\n",
      "(Iteration 2001 / 30620) loss: 2.306551\n",
      "(Iteration 2101 / 30620) loss: 2.305798\n",
      "(Iteration 2201 / 30620) loss: 2.305911\n",
      "(Iteration 2301 / 30620) loss: 2.305509\n",
      "(Iteration 2401 / 30620) loss: 2.305849\n",
      "(Iteration 2501 / 30620) loss: 2.305790\n",
      "(Iteration 2601 / 30620) loss: 2.305835\n",
      "(Iteration 2701 / 30620) loss: 2.305550\n",
      "(Iteration 2801 / 30620) loss: 2.305090\n",
      "(Iteration 2901 / 30620) loss: 2.305658\n",
      "(Iteration 3001 / 30620) loss: 2.305315\n",
      "(Epoch 2 / 20) train acc: 0.143000; val_acc: 0.136000\n",
      "(Iteration 3101 / 30620) loss: 2.305019\n",
      "(Iteration 3201 / 30620) loss: 2.305296\n",
      "(Iteration 3301 / 30620) loss: 2.304998\n",
      "(Iteration 3401 / 30620) loss: 2.305088\n",
      "(Iteration 3501 / 30620) loss: 2.304796\n",
      "(Iteration 3601 / 30620) loss: 2.304561\n",
      "(Iteration 3701 / 30620) loss: 2.305518\n",
      "(Iteration 3801 / 30620) loss: 2.304574\n",
      "(Iteration 3901 / 30620) loss: 2.305360\n",
      "(Iteration 4001 / 30620) loss: 2.305062\n",
      "(Iteration 4101 / 30620) loss: 2.304754\n",
      "(Iteration 4201 / 30620) loss: 2.304822\n",
      "(Iteration 4301 / 30620) loss: 2.304681\n",
      "(Iteration 4401 / 30620) loss: 2.304240\n",
      "(Iteration 4501 / 30620) loss: 2.304328\n",
      "(Epoch 3 / 20) train acc: 0.195000; val_acc: 0.183000\n",
      "(Iteration 4601 / 30620) loss: 2.303945\n",
      "(Iteration 4701 / 30620) loss: 2.303883\n",
      "(Iteration 4801 / 30620) loss: 2.304110\n",
      "(Iteration 4901 / 30620) loss: 2.304586\n",
      "(Iteration 5001 / 30620) loss: 2.303939\n",
      "(Iteration 5101 / 30620) loss: 2.305378\n",
      "(Iteration 5201 / 30620) loss: 2.304796\n",
      "(Iteration 5301 / 30620) loss: 2.303546\n",
      "(Iteration 5401 / 30620) loss: 2.303476\n",
      "(Iteration 5501 / 30620) loss: 2.303648\n",
      "(Iteration 5601 / 30620) loss: 2.303338\n",
      "(Iteration 5701 / 30620) loss: 2.302880\n",
      "(Iteration 5801 / 30620) loss: 2.303188\n",
      "(Iteration 5901 / 30620) loss: 2.304017\n",
      "(Iteration 6001 / 30620) loss: 2.303429\n",
      "(Iteration 6101 / 30620) loss: 2.303444\n",
      "(Epoch 4 / 20) train acc: 0.152000; val_acc: 0.119000\n",
      "(Iteration 6201 / 30620) loss: 2.304640\n",
      "(Iteration 6301 / 30620) loss: 2.303510\n",
      "(Iteration 6401 / 30620) loss: 2.302882\n",
      "(Iteration 6501 / 30620) loss: 2.303232\n",
      "(Iteration 6601 / 30620) loss: 2.302410\n",
      "(Iteration 6701 / 30620) loss: 2.302754\n",
      "(Iteration 6801 / 30620) loss: 2.302315\n",
      "(Iteration 6901 / 30620) loss: 2.302690\n",
      "(Iteration 7001 / 30620) loss: 2.303674\n",
      "(Iteration 7101 / 30620) loss: 2.302589\n",
      "(Iteration 7201 / 30620) loss: 2.303647\n",
      "(Iteration 7301 / 30620) loss: 2.304410\n",
      "(Iteration 7401 / 30620) loss: 2.303120\n",
      "(Iteration 7501 / 30620) loss: 2.301957\n",
      "(Iteration 7601 / 30620) loss: 2.301514\n",
      "(Epoch 5 / 20) train acc: 0.202000; val_acc: 0.161000\n",
      "(Iteration 7701 / 30620) loss: 2.301103\n",
      "(Iteration 7801 / 30620) loss: 2.304374\n",
      "(Iteration 7901 / 30620) loss: 2.302051\n",
      "(Iteration 8001 / 30620) loss: 2.300781\n",
      "(Iteration 8101 / 30620) loss: 2.300402\n",
      "(Iteration 8201 / 30620) loss: 2.302825\n",
      "(Iteration 8301 / 30620) loss: 2.302540\n",
      "(Iteration 8401 / 30620) loss: 2.301471\n",
      "(Iteration 8501 / 30620) loss: 2.301191\n",
      "(Iteration 8601 / 30620) loss: 2.303807\n",
      "(Iteration 8701 / 30620) loss: 2.302368\n",
      "(Iteration 8801 / 30620) loss: 2.303091\n",
      "(Iteration 8901 / 30620) loss: 2.302117\n",
      "(Iteration 9001 / 30620) loss: 2.300648\n",
      "(Iteration 9101 / 30620) loss: 2.300656\n",
      "(Epoch 6 / 20) train acc: 0.185000; val_acc: 0.157000\n",
      "(Iteration 9201 / 30620) loss: 2.300375\n",
      "(Iteration 9301 / 30620) loss: 2.297170\n",
      "(Iteration 9401 / 30620) loss: 2.302466\n",
      "(Iteration 9501 / 30620) loss: 2.300681\n",
      "(Iteration 9601 / 30620) loss: 2.301787\n",
      "(Iteration 9701 / 30620) loss: 2.302453\n",
      "(Iteration 9801 / 30620) loss: 2.299786\n",
      "(Iteration 9901 / 30620) loss: 2.298722\n",
      "(Iteration 10001 / 30620) loss: 2.299876\n",
      "(Iteration 10101 / 30620) loss: 2.297900\n",
      "(Iteration 10201 / 30620) loss: 2.299775\n",
      "(Iteration 10301 / 30620) loss: 2.299922\n",
      "(Iteration 10401 / 30620) loss: 2.297314\n",
      "(Iteration 10501 / 30620) loss: 2.299672\n",
      "(Iteration 10601 / 30620) loss: 2.301747\n",
      "(Iteration 10701 / 30620) loss: 2.302167\n",
      "(Epoch 7 / 20) train acc: 0.223000; val_acc: 0.194000\n",
      "(Iteration 10801 / 30620) loss: 2.301421\n",
      "(Iteration 10901 / 30620) loss: 2.298200\n",
      "(Iteration 11001 / 30620) loss: 2.299023\n",
      "(Iteration 11101 / 30620) loss: 2.298672\n",
      "(Iteration 11201 / 30620) loss: 2.300278\n",
      "(Iteration 11301 / 30620) loss: 2.299335\n",
      "(Iteration 11401 / 30620) loss: 2.297526\n",
      "(Iteration 11501 / 30620) loss: 2.298198\n",
      "(Iteration 11601 / 30620) loss: 2.301368\n",
      "(Iteration 11701 / 30620) loss: 2.299137\n",
      "(Iteration 11801 / 30620) loss: 2.299688\n",
      "(Iteration 11901 / 30620) loss: 2.298215\n",
      "(Iteration 12001 / 30620) loss: 2.298964\n",
      "(Iteration 12101 / 30620) loss: 2.298401\n",
      "(Iteration 12201 / 30620) loss: 2.299307\n",
      "(Epoch 8 / 20) train acc: 0.226000; val_acc: 0.209000\n",
      "(Iteration 12301 / 30620) loss: 2.292714\n",
      "(Iteration 12401 / 30620) loss: 2.298410\n",
      "(Iteration 12501 / 30620) loss: 2.295750\n",
      "(Iteration 12601 / 30620) loss: 2.296594\n",
      "(Iteration 12701 / 30620) loss: 2.296987\n",
      "(Iteration 12801 / 30620) loss: 2.299265\n",
      "(Iteration 12901 / 30620) loss: 2.298277\n",
      "(Iteration 13001 / 30620) loss: 2.291270\n",
      "(Iteration 13101 / 30620) loss: 2.298063\n",
      "(Iteration 13201 / 30620) loss: 2.294486\n",
      "(Iteration 13301 / 30620) loss: 2.299346\n",
      "(Iteration 13401 / 30620) loss: 2.293686\n",
      "(Iteration 13501 / 30620) loss: 2.302348\n",
      "(Iteration 13601 / 30620) loss: 2.297470\n",
      "(Iteration 13701 / 30620) loss: 2.295790\n",
      "(Epoch 9 / 20) train acc: 0.202000; val_acc: 0.207000\n",
      "(Iteration 13801 / 30620) loss: 2.299187\n",
      "(Iteration 13901 / 30620) loss: 2.296942\n",
      "(Iteration 14001 / 30620) loss: 2.297052\n",
      "(Iteration 14101 / 30620) loss: 2.297615\n",
      "(Iteration 14201 / 30620) loss: 2.299976\n",
      "(Iteration 14301 / 30620) loss: 2.292021\n",
      "(Iteration 14401 / 30620) loss: 2.298278\n",
      "(Iteration 14501 / 30620) loss: 2.286128\n",
      "(Iteration 14601 / 30620) loss: 2.293964\n",
      "(Iteration 14701 / 30620) loss: 2.299150\n",
      "(Iteration 14801 / 30620) loss: 2.290695\n",
      "(Iteration 14901 / 30620) loss: 2.294989\n",
      "(Iteration 15001 / 30620) loss: 2.286407\n",
      "(Iteration 15101 / 30620) loss: 2.293176\n",
      "(Iteration 15201 / 30620) loss: 2.287056\n",
      "(Iteration 15301 / 30620) loss: 2.294165\n",
      "(Epoch 10 / 20) train acc: 0.215000; val_acc: 0.210000\n",
      "(Iteration 15401 / 30620) loss: 2.298122\n",
      "(Iteration 15501 / 30620) loss: 2.296114\n",
      "(Iteration 15601 / 30620) loss: 2.286764\n",
      "(Iteration 15701 / 30620) loss: 2.295690\n",
      "(Iteration 15801 / 30620) loss: 2.297683\n",
      "(Iteration 15901 / 30620) loss: 2.288956\n",
      "(Iteration 16001 / 30620) loss: 2.298315\n",
      "(Iteration 16101 / 30620) loss: 2.295966\n",
      "(Iteration 16201 / 30620) loss: 2.286092\n",
      "(Iteration 16301 / 30620) loss: 2.293231\n",
      "(Iteration 16401 / 30620) loss: 2.286572\n",
      "(Iteration 16501 / 30620) loss: 2.281282\n",
      "(Iteration 16601 / 30620) loss: 2.288167\n",
      "(Iteration 16701 / 30620) loss: 2.289270\n",
      "(Iteration 16801 / 30620) loss: 2.290012\n",
      "(Epoch 11 / 20) train acc: 0.214000; val_acc: 0.208000\n",
      "(Iteration 16901 / 30620) loss: 2.285843\n",
      "(Iteration 17001 / 30620) loss: 2.293388\n",
      "(Iteration 17101 / 30620) loss: 2.285770\n",
      "(Iteration 17201 / 30620) loss: 2.286052\n",
      "(Iteration 17301 / 30620) loss: 2.291734\n",
      "(Iteration 17401 / 30620) loss: 2.291130\n",
      "(Iteration 17501 / 30620) loss: 2.290217\n",
      "(Iteration 17601 / 30620) loss: 2.282678\n",
      "(Iteration 17701 / 30620) loss: 2.287421\n",
      "(Iteration 17801 / 30620) loss: 2.294626\n",
      "(Iteration 17901 / 30620) loss: 2.293429\n",
      "(Iteration 18001 / 30620) loss: 2.281076\n",
      "(Iteration 18101 / 30620) loss: 2.285965\n",
      "(Iteration 18201 / 30620) loss: 2.297467\n",
      "(Iteration 18301 / 30620) loss: 2.285539\n",
      "(Epoch 12 / 20) train acc: 0.260000; val_acc: 0.224000\n",
      "(Iteration 18401 / 30620) loss: 2.302300\n",
      "(Iteration 18501 / 30620) loss: 2.284961\n",
      "(Iteration 18601 / 30620) loss: 2.288529\n",
      "(Iteration 18701 / 30620) loss: 2.289365\n",
      "(Iteration 18801 / 30620) loss: 2.289801\n",
      "(Iteration 18901 / 30620) loss: 2.292459\n",
      "(Iteration 19001 / 30620) loss: 2.284777\n",
      "(Iteration 19101 / 30620) loss: 2.282464\n",
      "(Iteration 19201 / 30620) loss: 2.291891\n",
      "(Iteration 19301 / 30620) loss: 2.275931\n",
      "(Iteration 19401 / 30620) loss: 2.277119\n",
      "(Iteration 19501 / 30620) loss: 2.275699\n",
      "(Iteration 19601 / 30620) loss: 2.282729\n",
      "(Iteration 19701 / 30620) loss: 2.292087\n",
      "(Iteration 19801 / 30620) loss: 2.290753\n",
      "(Iteration 19901 / 30620) loss: 2.289143\n",
      "(Epoch 13 / 20) train acc: 0.233000; val_acc: 0.232000\n",
      "(Iteration 20001 / 30620) loss: 2.288519\n",
      "(Iteration 20101 / 30620) loss: 2.288275\n",
      "(Iteration 20201 / 30620) loss: 2.281865\n",
      "(Iteration 20301 / 30620) loss: 2.290065\n",
      "(Iteration 20401 / 30620) loss: 2.303770\n",
      "(Iteration 20501 / 30620) loss: 2.261648\n",
      "(Iteration 20601 / 30620) loss: 2.274831\n",
      "(Iteration 20701 / 30620) loss: 2.281109\n",
      "(Iteration 20801 / 30620) loss: 2.292457\n",
      "(Iteration 20901 / 30620) loss: 2.275855\n",
      "(Iteration 21001 / 30620) loss: 2.275123\n",
      "(Iteration 21101 / 30620) loss: 2.289869\n",
      "(Iteration 21201 / 30620) loss: 2.260936\n",
      "(Iteration 21301 / 30620) loss: 2.290503\n",
      "(Iteration 21401 / 30620) loss: 2.267954\n",
      "(Epoch 14 / 20) train acc: 0.217000; val_acc: 0.227000\n",
      "(Iteration 21501 / 30620) loss: 2.286037\n",
      "(Iteration 21601 / 30620) loss: 2.291664\n",
      "(Iteration 21701 / 30620) loss: 2.263305\n",
      "(Iteration 21801 / 30620) loss: 2.278020\n",
      "(Iteration 21901 / 30620) loss: 2.277759\n",
      "(Iteration 22001 / 30620) loss: 2.291485\n",
      "(Iteration 22101 / 30620) loss: 2.283929\n",
      "(Iteration 22201 / 30620) loss: 2.286683\n",
      "(Iteration 22301 / 30620) loss: 2.273061\n",
      "(Iteration 22401 / 30620) loss: 2.269128\n",
      "(Iteration 22501 / 30620) loss: 2.284871\n",
      "(Iteration 22601 / 30620) loss: 2.256736\n",
      "(Iteration 22701 / 30620) loss: 2.278064\n",
      "(Iteration 22801 / 30620) loss: 2.274833\n",
      "(Iteration 22901 / 30620) loss: 2.270667\n",
      "(Epoch 15 / 20) train acc: 0.237000; val_acc: 0.235000\n",
      "(Iteration 23001 / 30620) loss: 2.252921\n",
      "(Iteration 23101 / 30620) loss: 2.271709\n",
      "(Iteration 23201 / 30620) loss: 2.278322\n",
      "(Iteration 23301 / 30620) loss: 2.274761\n",
      "(Iteration 23401 / 30620) loss: 2.277828\n",
      "(Iteration 23501 / 30620) loss: 2.271537\n",
      "(Iteration 23601 / 30620) loss: 2.267753\n",
      "(Iteration 23701 / 30620) loss: 2.240800\n",
      "(Iteration 23801 / 30620) loss: 2.281270\n",
      "(Iteration 23901 / 30620) loss: 2.265409\n",
      "(Iteration 24001 / 30620) loss: 2.260522\n",
      "(Iteration 24101 / 30620) loss: 2.257376\n",
      "(Iteration 24201 / 30620) loss: 2.294879\n",
      "(Iteration 24301 / 30620) loss: 2.252258\n",
      "(Iteration 24401 / 30620) loss: 2.278985\n",
      "(Epoch 16 / 20) train acc: 0.226000; val_acc: 0.238000\n",
      "(Iteration 24501 / 30620) loss: 2.288334\n",
      "(Iteration 24601 / 30620) loss: 2.261327\n",
      "(Iteration 24701 / 30620) loss: 2.267681\n",
      "(Iteration 24801 / 30620) loss: 2.262629\n",
      "(Iteration 24901 / 30620) loss: 2.290021\n",
      "(Iteration 25001 / 30620) loss: 2.272417\n",
      "(Iteration 25101 / 30620) loss: 2.246311\n",
      "(Iteration 25201 / 30620) loss: 2.269706\n",
      "(Iteration 25301 / 30620) loss: 2.263365\n",
      "(Iteration 25401 / 30620) loss: 2.240890\n",
      "(Iteration 25501 / 30620) loss: 2.256835\n",
      "(Iteration 25601 / 30620) loss: 2.248008\n",
      "(Iteration 25701 / 30620) loss: 2.275918\n",
      "(Iteration 25801 / 30620) loss: 2.229144\n",
      "(Iteration 25901 / 30620) loss: 2.256510\n",
      "(Iteration 26001 / 30620) loss: 2.279340\n",
      "(Epoch 17 / 20) train acc: 0.221000; val_acc: 0.236000\n",
      "(Iteration 26101 / 30620) loss: 2.262186\n",
      "(Iteration 26201 / 30620) loss: 2.270226\n",
      "(Iteration 26301 / 30620) loss: 2.289346\n",
      "(Iteration 26401 / 30620) loss: 2.295414\n",
      "(Iteration 26501 / 30620) loss: 2.283285\n",
      "(Iteration 26601 / 30620) loss: 2.268430\n",
      "(Iteration 26701 / 30620) loss: 2.304405\n",
      "(Iteration 26801 / 30620) loss: 2.271370\n",
      "(Iteration 26901 / 30620) loss: 2.227015\n",
      "(Iteration 27001 / 30620) loss: 2.238287\n",
      "(Iteration 27101 / 30620) loss: 2.303206\n",
      "(Iteration 27201 / 30620) loss: 2.296529\n",
      "(Iteration 27301 / 30620) loss: 2.255017\n",
      "(Iteration 27401 / 30620) loss: 2.250907\n",
      "(Iteration 27501 / 30620) loss: 2.301258\n",
      "(Epoch 18 / 20) train acc: 0.262000; val_acc: 0.236000\n",
      "(Iteration 27601 / 30620) loss: 2.250131\n",
      "(Iteration 27701 / 30620) loss: 2.289462\n",
      "(Iteration 27801 / 30620) loss: 2.278867\n",
      "(Iteration 27901 / 30620) loss: 2.256817\n",
      "(Iteration 28001 / 30620) loss: 2.257617\n",
      "(Iteration 28101 / 30620) loss: 2.257317\n",
      "(Iteration 28201 / 30620) loss: 2.275568\n",
      "(Iteration 28301 / 30620) loss: 2.275906\n",
      "(Iteration 28401 / 30620) loss: 2.257700\n",
      "(Iteration 28501 / 30620) loss: 2.252929\n",
      "(Iteration 28601 / 30620) loss: 2.243966\n",
      "(Iteration 28701 / 30620) loss: 2.265760\n",
      "(Iteration 28801 / 30620) loss: 2.283928\n",
      "(Iteration 28901 / 30620) loss: 2.269108\n",
      "(Iteration 29001 / 30620) loss: 2.279539\n",
      "(Epoch 19 / 20) train acc: 0.238000; val_acc: 0.236000\n",
      "(Iteration 29101 / 30620) loss: 2.238559\n",
      "(Iteration 29201 / 30620) loss: 2.269235\n",
      "(Iteration 29301 / 30620) loss: 2.240985\n",
      "(Iteration 29401 / 30620) loss: 2.270715\n",
      "(Iteration 29501 / 30620) loss: 2.261939\n",
      "(Iteration 29601 / 30620) loss: 2.295840\n",
      "(Iteration 29701 / 30620) loss: 2.241767\n",
      "(Iteration 29801 / 30620) loss: 2.257677\n",
      "(Iteration 29901 / 30620) loss: 2.232518\n",
      "(Iteration 30001 / 30620) loss: 2.213474\n",
      "(Iteration 30101 / 30620) loss: 2.254586\n",
      "(Iteration 30201 / 30620) loss: 2.249920\n",
      "(Iteration 30301 / 30620) loss: 2.255240\n",
      "(Iteration 30401 / 30620) loss: 2.245949\n",
      "(Iteration 30501 / 30620) loss: 2.243051\n",
      "(Iteration 30601 / 30620) loss: 2.277295\n",
      "(Epoch 20 / 20) train acc: 0.239000; val_acc: 0.236000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.001, 'num_epochs': 20, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 15300) loss: 2.303143\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.101000\n",
      "(Iteration 101 / 15300) loss: 2.303117\n",
      "(Iteration 201 / 15300) loss: 2.303104\n",
      "(Iteration 301 / 15300) loss: 2.302995\n",
      "(Iteration 401 / 15300) loss: 2.303025\n",
      "(Iteration 501 / 15300) loss: 2.303055\n",
      "(Iteration 601 / 15300) loss: 2.302862\n",
      "(Iteration 701 / 15300) loss: 2.302871\n",
      "(Epoch 1 / 20) train acc: 0.151000; val_acc: 0.131000\n",
      "(Iteration 801 / 15300) loss: 2.302751\n",
      "(Iteration 901 / 15300) loss: 2.302933\n",
      "(Iteration 1001 / 15300) loss: 2.302816\n",
      "(Iteration 1101 / 15300) loss: 2.302820\n",
      "(Iteration 1201 / 15300) loss: 2.302744\n",
      "(Iteration 1301 / 15300) loss: 2.302952\n",
      "(Iteration 1401 / 15300) loss: 2.302637\n",
      "(Iteration 1501 / 15300) loss: 2.302699\n",
      "(Epoch 2 / 20) train acc: 0.201000; val_acc: 0.187000\n",
      "(Iteration 1601 / 15300) loss: 2.302698\n",
      "(Iteration 1701 / 15300) loss: 2.302655\n",
      "(Iteration 1801 / 15300) loss: 2.302868\n",
      "(Iteration 1901 / 15300) loss: 2.302554\n",
      "(Iteration 2001 / 15300) loss: 2.302832\n",
      "(Iteration 2101 / 15300) loss: 2.302484\n",
      "(Iteration 2201 / 15300) loss: 2.302565\n",
      "(Epoch 3 / 20) train acc: 0.244000; val_acc: 0.233000\n",
      "(Iteration 2301 / 15300) loss: 2.302566\n",
      "(Iteration 2401 / 15300) loss: 2.302360\n",
      "(Iteration 2501 / 15300) loss: 2.302324\n",
      "(Iteration 2601 / 15300) loss: 2.302297\n",
      "(Iteration 2701 / 15300) loss: 2.302437\n",
      "(Iteration 2801 / 15300) loss: 2.302022\n",
      "(Iteration 2901 / 15300) loss: 2.302342\n",
      "(Iteration 3001 / 15300) loss: 2.302271\n",
      "(Epoch 4 / 20) train acc: 0.188000; val_acc: 0.179000\n",
      "(Iteration 3101 / 15300) loss: 2.302065\n",
      "(Iteration 3201 / 15300) loss: 2.302220\n",
      "(Iteration 3301 / 15300) loss: 2.302382\n",
      "(Iteration 3401 / 15300) loss: 2.302561\n",
      "(Iteration 3501 / 15300) loss: 2.302328\n",
      "(Iteration 3601 / 15300) loss: 2.301879\n",
      "(Iteration 3701 / 15300) loss: 2.302128\n",
      "(Iteration 3801 / 15300) loss: 2.301960\n",
      "(Epoch 5 / 20) train acc: 0.253000; val_acc: 0.217000\n",
      "(Iteration 3901 / 15300) loss: 2.301790\n",
      "(Iteration 4001 / 15300) loss: 2.302274\n",
      "(Iteration 4101 / 15300) loss: 2.301471\n",
      "(Iteration 4201 / 15300) loss: 2.301649\n",
      "(Iteration 4301 / 15300) loss: 2.301621\n",
      "(Iteration 4401 / 15300) loss: 2.301475\n",
      "(Iteration 4501 / 15300) loss: 2.301618\n",
      "(Epoch 6 / 20) train acc: 0.290000; val_acc: 0.273000\n",
      "(Iteration 4601 / 15300) loss: 2.301902\n",
      "(Iteration 4701 / 15300) loss: 2.301176\n",
      "(Iteration 4801 / 15300) loss: 2.301494\n",
      "(Iteration 4901 / 15300) loss: 2.302033\n",
      "(Iteration 5001 / 15300) loss: 2.301264\n",
      "(Iteration 5101 / 15300) loss: 2.301148\n",
      "(Iteration 5201 / 15300) loss: 2.300834\n",
      "(Iteration 5301 / 15300) loss: 2.301803\n",
      "(Epoch 7 / 20) train acc: 0.251000; val_acc: 0.235000\n",
      "(Iteration 5401 / 15300) loss: 2.301078\n",
      "(Iteration 5501 / 15300) loss: 2.300596\n",
      "(Iteration 5601 / 15300) loss: 2.300712\n",
      "(Iteration 5701 / 15300) loss: 2.301016\n",
      "(Iteration 5801 / 15300) loss: 2.300684\n",
      "(Iteration 5901 / 15300) loss: 2.301575\n",
      "(Iteration 6001 / 15300) loss: 2.300052\n",
      "(Iteration 6101 / 15300) loss: 2.300497\n",
      "(Epoch 8 / 20) train acc: 0.264000; val_acc: 0.239000\n",
      "(Iteration 6201 / 15300) loss: 2.301084\n",
      "(Iteration 6301 / 15300) loss: 2.300545\n",
      "(Iteration 6401 / 15300) loss: 2.300445\n",
      "(Iteration 6501 / 15300) loss: 2.300644\n",
      "(Iteration 6601 / 15300) loss: 2.299826\n",
      "(Iteration 6701 / 15300) loss: 2.300769\n",
      "(Iteration 6801 / 15300) loss: 2.300195\n",
      "(Epoch 9 / 20) train acc: 0.263000; val_acc: 0.253000\n",
      "(Iteration 6901 / 15300) loss: 2.301007\n",
      "(Iteration 7001 / 15300) loss: 2.299567\n",
      "(Iteration 7101 / 15300) loss: 2.300404\n",
      "(Iteration 7201 / 15300) loss: 2.300417\n",
      "(Iteration 7301 / 15300) loss: 2.300007\n",
      "(Iteration 7401 / 15300) loss: 2.300027\n",
      "(Iteration 7501 / 15300) loss: 2.299370\n",
      "(Iteration 7601 / 15300) loss: 2.299737\n",
      "(Epoch 10 / 20) train acc: 0.277000; val_acc: 0.245000\n",
      "(Iteration 7701 / 15300) loss: 2.300259\n",
      "(Iteration 7801 / 15300) loss: 2.299407\n",
      "(Iteration 7901 / 15300) loss: 2.299576\n",
      "(Iteration 8001 / 15300) loss: 2.299575\n",
      "(Iteration 8101 / 15300) loss: 2.298646\n",
      "(Iteration 8201 / 15300) loss: 2.298132\n",
      "(Iteration 8301 / 15300) loss: 2.299071\n",
      "(Iteration 8401 / 15300) loss: 2.298601\n",
      "(Epoch 11 / 20) train acc: 0.260000; val_acc: 0.261000\n",
      "(Iteration 8501 / 15300) loss: 2.299394\n",
      "(Iteration 8601 / 15300) loss: 2.300211\n",
      "(Iteration 8701 / 15300) loss: 2.299373\n",
      "(Iteration 8801 / 15300) loss: 2.298461\n",
      "(Iteration 8901 / 15300) loss: 2.299847\n",
      "(Iteration 9001 / 15300) loss: 2.297460\n",
      "(Iteration 9101 / 15300) loss: 2.298142\n",
      "(Epoch 12 / 20) train acc: 0.309000; val_acc: 0.274000\n",
      "(Iteration 9201 / 15300) loss: 2.298198\n",
      "(Iteration 9301 / 15300) loss: 2.297808\n",
      "(Iteration 9401 / 15300) loss: 2.298011\n",
      "(Iteration 9501 / 15300) loss: 2.299098\n",
      "(Iteration 9601 / 15300) loss: 2.297673\n",
      "(Iteration 9701 / 15300) loss: 2.297865\n",
      "(Iteration 9801 / 15300) loss: 2.298943\n",
      "(Iteration 9901 / 15300) loss: 2.296837\n",
      "(Epoch 13 / 20) train acc: 0.328000; val_acc: 0.284000\n",
      "(Iteration 10001 / 15300) loss: 2.299111\n",
      "(Iteration 10101 / 15300) loss: 2.297332\n",
      "(Iteration 10201 / 15300) loss: 2.297972\n",
      "(Iteration 10301 / 15300) loss: 2.296734\n",
      "(Iteration 10401 / 15300) loss: 2.297419\n",
      "(Iteration 10501 / 15300) loss: 2.296588\n",
      "(Iteration 10601 / 15300) loss: 2.297310\n",
      "(Iteration 10701 / 15300) loss: 2.296604\n",
      "(Epoch 14 / 20) train acc: 0.299000; val_acc: 0.304000\n",
      "(Iteration 10801 / 15300) loss: 2.294940\n",
      "(Iteration 10901 / 15300) loss: 2.295996\n",
      "(Iteration 11001 / 15300) loss: 2.297032\n",
      "(Iteration 11101 / 15300) loss: 2.296158\n",
      "(Iteration 11201 / 15300) loss: 2.296267\n",
      "(Iteration 11301 / 15300) loss: 2.293600\n",
      "(Iteration 11401 / 15300) loss: 2.298177\n",
      "(Epoch 15 / 20) train acc: 0.324000; val_acc: 0.305000\n",
      "(Iteration 11501 / 15300) loss: 2.298260\n",
      "(Iteration 11601 / 15300) loss: 2.296603\n",
      "(Iteration 11701 / 15300) loss: 2.295731\n",
      "(Iteration 11801 / 15300) loss: 2.295835\n",
      "(Iteration 11901 / 15300) loss: 2.294631\n",
      "(Iteration 12001 / 15300) loss: 2.294048\n",
      "(Iteration 12101 / 15300) loss: 2.295618\n",
      "(Iteration 12201 / 15300) loss: 2.295370\n",
      "(Epoch 16 / 20) train acc: 0.319000; val_acc: 0.292000\n",
      "(Iteration 12301 / 15300) loss: 2.298211\n",
      "(Iteration 12401 / 15300) loss: 2.295514\n",
      "(Iteration 12501 / 15300) loss: 2.294902\n",
      "(Iteration 12601 / 15300) loss: 2.294241\n",
      "(Iteration 12701 / 15300) loss: 2.295387\n",
      "(Iteration 12801 / 15300) loss: 2.296270\n",
      "(Iteration 12901 / 15300) loss: 2.296463\n",
      "(Iteration 13001 / 15300) loss: 2.294235\n",
      "(Epoch 17 / 20) train acc: 0.334000; val_acc: 0.296000\n",
      "(Iteration 13101 / 15300) loss: 2.298322\n",
      "(Iteration 13201 / 15300) loss: 2.295508\n",
      "(Iteration 13301 / 15300) loss: 2.296508\n",
      "(Iteration 13401 / 15300) loss: 2.295238\n",
      "(Iteration 13501 / 15300) loss: 2.295480\n",
      "(Iteration 13601 / 15300) loss: 2.296119\n",
      "(Iteration 13701 / 15300) loss: 2.292292\n",
      "(Epoch 18 / 20) train acc: 0.334000; val_acc: 0.298000\n",
      "(Iteration 13801 / 15300) loss: 2.290873\n",
      "(Iteration 13901 / 15300) loss: 2.295258\n",
      "(Iteration 14001 / 15300) loss: 2.293462\n",
      "(Iteration 14101 / 15300) loss: 2.295043\n",
      "(Iteration 14201 / 15300) loss: 2.294972\n",
      "(Iteration 14301 / 15300) loss: 2.290928\n",
      "(Iteration 14401 / 15300) loss: 2.298508\n",
      "(Iteration 14501 / 15300) loss: 2.294364\n",
      "(Epoch 19 / 20) train acc: 0.345000; val_acc: 0.297000\n",
      "(Iteration 14601 / 15300) loss: 2.294666\n",
      "(Iteration 14701 / 15300) loss: 2.289603\n",
      "(Iteration 14801 / 15300) loss: 2.291042\n",
      "(Iteration 14901 / 15300) loss: 2.292680\n",
      "(Iteration 15001 / 15300) loss: 2.290982\n",
      "(Iteration 15101 / 15300) loss: 2.290200\n",
      "(Iteration 15201 / 15300) loss: 2.291596\n",
      "(Epoch 20 / 20) train acc: 0.319000; val_acc: 0.298000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.001, 'num_epochs': 20, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 30620) loss: 2.303197\n",
      "(Epoch 0 / 20) train acc: 0.091000; val_acc: 0.075000\n",
      "(Iteration 101 / 30620) loss: 2.303034\n",
      "(Iteration 201 / 30620) loss: 2.303294\n",
      "(Iteration 301 / 30620) loss: 2.303163\n",
      "(Iteration 401 / 30620) loss: 2.303113\n",
      "(Iteration 501 / 30620) loss: 2.302863\n",
      "(Iteration 601 / 30620) loss: 2.303056\n",
      "(Iteration 701 / 30620) loss: 2.302892\n",
      "(Iteration 801 / 30620) loss: 2.302925\n",
      "(Iteration 901 / 30620) loss: 2.303160\n",
      "(Iteration 1001 / 30620) loss: 2.303072\n",
      "(Iteration 1101 / 30620) loss: 2.302958\n",
      "(Iteration 1201 / 30620) loss: 2.302772\n",
      "(Iteration 1301 / 30620) loss: 2.302542\n",
      "(Iteration 1401 / 30620) loss: 2.302649\n",
      "(Iteration 1501 / 30620) loss: 2.302448\n",
      "(Epoch 1 / 20) train acc: 0.134000; val_acc: 0.172000\n",
      "(Iteration 1601 / 30620) loss: 2.302367\n",
      "(Iteration 1701 / 30620) loss: 2.302385\n",
      "(Iteration 1801 / 30620) loss: 2.301828\n",
      "(Iteration 1901 / 30620) loss: 2.302619\n",
      "(Iteration 2001 / 30620) loss: 2.302784\n",
      "(Iteration 2101 / 30620) loss: 2.303540\n",
      "(Iteration 2201 / 30620) loss: 2.301833\n",
      "(Iteration 2301 / 30620) loss: 2.302494\n",
      "(Iteration 2401 / 30620) loss: 2.302645\n",
      "(Iteration 2501 / 30620) loss: 2.302756\n",
      "(Iteration 2601 / 30620) loss: 2.302839\n",
      "(Iteration 2701 / 30620) loss: 2.302012\n",
      "(Iteration 2801 / 30620) loss: 2.301380\n",
      "(Iteration 2901 / 30620) loss: 2.301693\n",
      "(Iteration 3001 / 30620) loss: 2.303099\n",
      "(Epoch 2 / 20) train acc: 0.156000; val_acc: 0.203000\n",
      "(Iteration 3101 / 30620) loss: 2.302493\n",
      "(Iteration 3201 / 30620) loss: 2.301998\n",
      "(Iteration 3301 / 30620) loss: 2.301621\n",
      "(Iteration 3401 / 30620) loss: 2.301944\n",
      "(Iteration 3501 / 30620) loss: 2.301328\n",
      "(Iteration 3601 / 30620) loss: 2.301425\n",
      "(Iteration 3701 / 30620) loss: 2.302195\n",
      "(Iteration 3801 / 30620) loss: 2.300873\n",
      "(Iteration 3901 / 30620) loss: 2.300856\n",
      "(Iteration 4001 / 30620) loss: 2.301717\n",
      "(Iteration 4101 / 30620) loss: 2.301132\n",
      "(Iteration 4201 / 30620) loss: 2.300777\n",
      "(Iteration 4301 / 30620) loss: 2.301469\n",
      "(Iteration 4401 / 30620) loss: 2.301612\n",
      "(Iteration 4501 / 30620) loss: 2.300115\n",
      "(Epoch 3 / 20) train acc: 0.229000; val_acc: 0.220000\n",
      "(Iteration 4601 / 30620) loss: 2.301059\n",
      "(Iteration 4701 / 30620) loss: 2.299901\n",
      "(Iteration 4801 / 30620) loss: 2.300265\n",
      "(Iteration 4901 / 30620) loss: 2.300597\n",
      "(Iteration 5001 / 30620) loss: 2.300400\n",
      "(Iteration 5101 / 30620) loss: 2.301649\n",
      "(Iteration 5201 / 30620) loss: 2.299907\n",
      "(Iteration 5301 / 30620) loss: 2.299483\n",
      "(Iteration 5401 / 30620) loss: 2.299454\n",
      "(Iteration 5501 / 30620) loss: 2.300500\n",
      "(Iteration 5601 / 30620) loss: 2.297775\n",
      "(Iteration 5701 / 30620) loss: 2.298219\n",
      "(Iteration 5801 / 30620) loss: 2.300140\n",
      "(Iteration 5901 / 30620) loss: 2.301037\n",
      "(Iteration 6001 / 30620) loss: 2.300196\n",
      "(Iteration 6101 / 30620) loss: 2.297915\n",
      "(Epoch 4 / 20) train acc: 0.268000; val_acc: 0.249000\n",
      "(Iteration 6201 / 30620) loss: 2.297512\n",
      "(Iteration 6301 / 30620) loss: 2.298225\n",
      "(Iteration 6401 / 30620) loss: 2.296075\n",
      "(Iteration 6501 / 30620) loss: 2.300298\n",
      "(Iteration 6601 / 30620) loss: 2.298678\n",
      "(Iteration 6701 / 30620) loss: 2.300290\n",
      "(Iteration 6801 / 30620) loss: 2.294327\n",
      "(Iteration 6901 / 30620) loss: 2.296486\n",
      "(Iteration 7001 / 30620) loss: 2.299290\n",
      "(Iteration 7101 / 30620) loss: 2.297554\n",
      "(Iteration 7201 / 30620) loss: 2.297259\n",
      "(Iteration 7301 / 30620) loss: 2.297328\n",
      "(Iteration 7401 / 30620) loss: 2.296727\n",
      "(Iteration 7501 / 30620) loss: 2.294753\n",
      "(Iteration 7601 / 30620) loss: 2.294585\n",
      "(Epoch 5 / 20) train acc: 0.276000; val_acc: 0.262000\n",
      "(Iteration 7701 / 30620) loss: 2.293805\n",
      "(Iteration 7801 / 30620) loss: 2.296060\n",
      "(Iteration 7901 / 30620) loss: 2.295653\n",
      "(Iteration 8001 / 30620) loss: 2.289210\n",
      "(Iteration 8101 / 30620) loss: 2.289887\n",
      "(Iteration 8201 / 30620) loss: 2.297783\n",
      "(Iteration 8301 / 30620) loss: 2.293905\n",
      "(Iteration 8401 / 30620) loss: 2.292391\n",
      "(Iteration 8501 / 30620) loss: 2.297310\n",
      "(Iteration 8601 / 30620) loss: 2.295469\n",
      "(Iteration 8701 / 30620) loss: 2.284949\n",
      "(Iteration 8801 / 30620) loss: 2.293156\n",
      "(Iteration 8901 / 30620) loss: 2.286809\n",
      "(Iteration 9001 / 30620) loss: 2.286781\n",
      "(Iteration 9101 / 30620) loss: 2.281767\n",
      "(Epoch 6 / 20) train acc: 0.260000; val_acc: 0.274000\n",
      "(Iteration 9201 / 30620) loss: 2.284928\n",
      "(Iteration 9301 / 30620) loss: 2.290738\n",
      "(Iteration 9401 / 30620) loss: 2.295541\n",
      "(Iteration 9501 / 30620) loss: 2.286599\n",
      "(Iteration 9601 / 30620) loss: 2.293863\n",
      "(Iteration 9701 / 30620) loss: 2.283390\n",
      "(Iteration 9801 / 30620) loss: 2.287699\n",
      "(Iteration 9901 / 30620) loss: 2.297259\n",
      "(Iteration 10001 / 30620) loss: 2.280768\n",
      "(Iteration 10101 / 30620) loss: 2.276146\n",
      "(Iteration 10201 / 30620) loss: 2.281746\n",
      "(Iteration 10301 / 30620) loss: 2.271558\n",
      "(Iteration 10401 / 30620) loss: 2.285799\n",
      "(Iteration 10501 / 30620) loss: 2.287218\n",
      "(Iteration 10601 / 30620) loss: 2.292670\n",
      "(Iteration 10701 / 30620) loss: 2.267907\n",
      "(Epoch 7 / 20) train acc: 0.280000; val_acc: 0.299000\n",
      "(Iteration 10801 / 30620) loss: 2.282244\n",
      "(Iteration 10901 / 30620) loss: 2.278803\n",
      "(Iteration 11001 / 30620) loss: 2.262292\n",
      "(Iteration 11101 / 30620) loss: 2.282847\n",
      "(Iteration 11201 / 30620) loss: 2.273021\n",
      "(Iteration 11301 / 30620) loss: 2.263994\n",
      "(Iteration 11401 / 30620) loss: 2.265011\n",
      "(Iteration 11501 / 30620) loss: 2.279193\n",
      "(Iteration 11601 / 30620) loss: 2.268545\n",
      "(Iteration 11701 / 30620) loss: 2.260489\n",
      "(Iteration 11801 / 30620) loss: 2.279581\n",
      "(Iteration 11901 / 30620) loss: 2.268736\n",
      "(Iteration 12001 / 30620) loss: 2.268554\n",
      "(Iteration 12101 / 30620) loss: 2.280430\n",
      "(Iteration 12201 / 30620) loss: 2.263401\n",
      "(Epoch 8 / 20) train acc: 0.271000; val_acc: 0.288000\n",
      "(Iteration 12301 / 30620) loss: 2.262808\n",
      "(Iteration 12401 / 30620) loss: 2.266043\n",
      "(Iteration 12501 / 30620) loss: 2.270799\n",
      "(Iteration 12601 / 30620) loss: 2.258129\n",
      "(Iteration 12701 / 30620) loss: 2.281074\n",
      "(Iteration 12801 / 30620) loss: 2.262100\n",
      "(Iteration 12901 / 30620) loss: 2.244508\n",
      "(Iteration 13001 / 30620) loss: 2.246190\n",
      "(Iteration 13101 / 30620) loss: 2.270353\n",
      "(Iteration 13201 / 30620) loss: 2.252950\n",
      "(Iteration 13301 / 30620) loss: 2.255899\n",
      "(Iteration 13401 / 30620) loss: 2.237044\n",
      "(Iteration 13501 / 30620) loss: 2.250240\n",
      "(Iteration 13601 / 30620) loss: 2.236685\n",
      "(Iteration 13701 / 30620) loss: 2.239039\n",
      "(Epoch 9 / 20) train acc: 0.239000; val_acc: 0.272000\n",
      "(Iteration 13801 / 30620) loss: 2.257549\n",
      "(Iteration 13901 / 30620) loss: 2.259550\n",
      "(Iteration 14001 / 30620) loss: 2.265640\n",
      "(Iteration 14101 / 30620) loss: 2.250832\n",
      "(Iteration 14201 / 30620) loss: 2.264675\n",
      "(Iteration 14301 / 30620) loss: 2.223432\n",
      "(Iteration 14401 / 30620) loss: 2.255497\n",
      "(Iteration 14501 / 30620) loss: 2.221222\n",
      "(Iteration 14601 / 30620) loss: 2.261874\n",
      "(Iteration 14701 / 30620) loss: 2.220778\n",
      "(Iteration 14801 / 30620) loss: 2.269275\n",
      "(Iteration 14901 / 30620) loss: 2.229027\n",
      "(Iteration 15001 / 30620) loss: 2.246321\n",
      "(Iteration 15101 / 30620) loss: 2.205345\n",
      "(Iteration 15201 / 30620) loss: 2.256443\n",
      "(Iteration 15301 / 30620) loss: 2.168712\n",
      "(Epoch 10 / 20) train acc: 0.232000; val_acc: 0.254000\n",
      "(Iteration 15401 / 30620) loss: 2.165633\n",
      "(Iteration 15501 / 30620) loss: 2.220827\n",
      "(Iteration 15601 / 30620) loss: 2.236614\n",
      "(Iteration 15701 / 30620) loss: 2.271219\n",
      "(Iteration 15801 / 30620) loss: 2.181735\n",
      "(Iteration 15901 / 30620) loss: 2.252146\n",
      "(Iteration 16001 / 30620) loss: 2.240326\n",
      "(Iteration 16101 / 30620) loss: 2.236019\n",
      "(Iteration 16201 / 30620) loss: 2.193940\n",
      "(Iteration 16301 / 30620) loss: 2.196673\n",
      "(Iteration 16401 / 30620) loss: 2.213722\n",
      "(Iteration 16501 / 30620) loss: 2.156437\n",
      "(Iteration 16601 / 30620) loss: 2.143933\n",
      "(Iteration 16701 / 30620) loss: 2.248990\n",
      "(Iteration 16801 / 30620) loss: 2.148566\n",
      "(Epoch 11 / 20) train acc: 0.272000; val_acc: 0.247000\n",
      "(Iteration 16901 / 30620) loss: 2.177067\n",
      "(Iteration 17001 / 30620) loss: 2.139875\n",
      "(Iteration 17101 / 30620) loss: 2.182992\n",
      "(Iteration 17201 / 30620) loss: 2.222589\n",
      "(Iteration 17301 / 30620) loss: 2.226741\n",
      "(Iteration 17401 / 30620) loss: 2.236198\n",
      "(Iteration 17501 / 30620) loss: 2.169045\n",
      "(Iteration 17601 / 30620) loss: 2.234779\n",
      "(Iteration 17701 / 30620) loss: 2.206956\n",
      "(Iteration 17801 / 30620) loss: 2.142954\n",
      "(Iteration 17901 / 30620) loss: 2.202366\n",
      "(Iteration 18001 / 30620) loss: 2.201949\n",
      "(Iteration 18101 / 30620) loss: 2.138629\n",
      "(Iteration 18201 / 30620) loss: 2.146593\n",
      "(Iteration 18301 / 30620) loss: 2.100716\n",
      "(Epoch 12 / 20) train acc: 0.252000; val_acc: 0.247000\n",
      "(Iteration 18401 / 30620) loss: 2.186695\n",
      "(Iteration 18501 / 30620) loss: 2.156188\n",
      "(Iteration 18601 / 30620) loss: 2.181611\n",
      "(Iteration 18701 / 30620) loss: 2.199311\n",
      "(Iteration 18801 / 30620) loss: 2.206561\n",
      "(Iteration 18901 / 30620) loss: 2.209827\n",
      "(Iteration 19001 / 30620) loss: 2.214605\n",
      "(Iteration 19101 / 30620) loss: 2.113872\n",
      "(Iteration 19201 / 30620) loss: 2.180800\n",
      "(Iteration 19301 / 30620) loss: 2.168792\n",
      "(Iteration 19401 / 30620) loss: 2.186198\n",
      "(Iteration 19501 / 30620) loss: 2.174453\n",
      "(Iteration 19601 / 30620) loss: 2.180028\n",
      "(Iteration 19701 / 30620) loss: 2.235666\n",
      "(Iteration 19801 / 30620) loss: 2.262744\n",
      "(Iteration 19901 / 30620) loss: 2.164767\n",
      "(Epoch 13 / 20) train acc: 0.234000; val_acc: 0.250000\n",
      "(Iteration 20001 / 30620) loss: 2.125713\n",
      "(Iteration 20101 / 30620) loss: 2.060464\n",
      "(Iteration 20201 / 30620) loss: 2.130673\n",
      "(Iteration 20301 / 30620) loss: 2.188200\n",
      "(Iteration 20401 / 30620) loss: 2.173243\n",
      "(Iteration 20501 / 30620) loss: 2.200281\n",
      "(Iteration 20601 / 30620) loss: 2.096904\n",
      "(Iteration 20701 / 30620) loss: 2.214682\n",
      "(Iteration 20801 / 30620) loss: 2.109771\n",
      "(Iteration 20901 / 30620) loss: 2.206178\n",
      "(Iteration 21001 / 30620) loss: 2.147629\n",
      "(Iteration 21101 / 30620) loss: 2.151109\n",
      "(Iteration 21201 / 30620) loss: 2.212663\n",
      "(Iteration 21301 / 30620) loss: 2.093474\n",
      "(Iteration 21401 / 30620) loss: 2.178660\n",
      "(Epoch 14 / 20) train acc: 0.232000; val_acc: 0.253000\n",
      "(Iteration 21501 / 30620) loss: 2.182419\n",
      "(Iteration 21601 / 30620) loss: 2.108200\n",
      "(Iteration 21701 / 30620) loss: 2.121545\n",
      "(Iteration 21801 / 30620) loss: 2.045587\n",
      "(Iteration 21901 / 30620) loss: 2.062780\n",
      "(Iteration 22001 / 30620) loss: 2.108243\n",
      "(Iteration 22101 / 30620) loss: 2.236470\n",
      "(Iteration 22201 / 30620) loss: 1.979156\n",
      "(Iteration 22301 / 30620) loss: 2.109543\n",
      "(Iteration 22401 / 30620) loss: 2.121100\n",
      "(Iteration 22501 / 30620) loss: 2.121586\n",
      "(Iteration 22601 / 30620) loss: 2.165521\n",
      "(Iteration 22701 / 30620) loss: 2.132400\n",
      "(Iteration 22801 / 30620) loss: 2.168823\n",
      "(Iteration 22901 / 30620) loss: 2.129570\n",
      "(Epoch 15 / 20) train acc: 0.243000; val_acc: 0.256000\n",
      "(Iteration 23001 / 30620) loss: 2.160892\n",
      "(Iteration 23101 / 30620) loss: 2.026868\n",
      "(Iteration 23201 / 30620) loss: 2.169707\n",
      "(Iteration 23301 / 30620) loss: 2.001589\n",
      "(Iteration 23401 / 30620) loss: 2.119626\n",
      "(Iteration 23501 / 30620) loss: 2.189011\n",
      "(Iteration 23601 / 30620) loss: 2.081785\n",
      "(Iteration 23701 / 30620) loss: 2.107722\n",
      "(Iteration 23801 / 30620) loss: 2.095209\n",
      "(Iteration 23901 / 30620) loss: 2.193312\n",
      "(Iteration 24001 / 30620) loss: 2.180424\n",
      "(Iteration 24101 / 30620) loss: 2.085846\n",
      "(Iteration 24201 / 30620) loss: 2.054475\n",
      "(Iteration 24301 / 30620) loss: 2.139779\n",
      "(Iteration 24401 / 30620) loss: 2.169746\n",
      "(Epoch 16 / 20) train acc: 0.231000; val_acc: 0.259000\n",
      "(Iteration 24501 / 30620) loss: 2.253157\n",
      "(Iteration 24601 / 30620) loss: 2.110872\n",
      "(Iteration 24701 / 30620) loss: 2.154835\n",
      "(Iteration 24801 / 30620) loss: 2.213246\n",
      "(Iteration 24901 / 30620) loss: 2.193302\n",
      "(Iteration 25001 / 30620) loss: 2.135595\n",
      "(Iteration 25101 / 30620) loss: 2.093771\n",
      "(Iteration 25201 / 30620) loss: 2.249208\n",
      "(Iteration 25301 / 30620) loss: 2.128983\n",
      "(Iteration 25401 / 30620) loss: 2.060388\n",
      "(Iteration 25501 / 30620) loss: 2.088609\n",
      "(Iteration 25601 / 30620) loss: 2.098886\n",
      "(Iteration 25701 / 30620) loss: 2.227614\n",
      "(Iteration 25801 / 30620) loss: 2.115163\n",
      "(Iteration 25901 / 30620) loss: 2.088828\n",
      "(Iteration 26001 / 30620) loss: 2.090236\n",
      "(Epoch 17 / 20) train acc: 0.271000; val_acc: 0.261000\n",
      "(Iteration 26101 / 30620) loss: 2.150489\n",
      "(Iteration 26201 / 30620) loss: 2.072266\n",
      "(Iteration 26301 / 30620) loss: 2.057518\n",
      "(Iteration 26401 / 30620) loss: 2.177630\n",
      "(Iteration 26501 / 30620) loss: 2.137828\n",
      "(Iteration 26601 / 30620) loss: 1.974586\n",
      "(Iteration 26701 / 30620) loss: 1.983817\n",
      "(Iteration 26801 / 30620) loss: 2.093764\n",
      "(Iteration 26901 / 30620) loss: 2.106113\n",
      "(Iteration 27001 / 30620) loss: 2.066833\n",
      "(Iteration 27101 / 30620) loss: 2.085975\n",
      "(Iteration 27201 / 30620) loss: 2.126999\n",
      "(Iteration 27301 / 30620) loss: 2.069508\n",
      "(Iteration 27401 / 30620) loss: 2.002114\n",
      "(Iteration 27501 / 30620) loss: 2.061785\n",
      "(Epoch 18 / 20) train acc: 0.244000; val_acc: 0.263000\n",
      "(Iteration 27601 / 30620) loss: 2.002789\n",
      "(Iteration 27701 / 30620) loss: 1.983096\n",
      "(Iteration 27801 / 30620) loss: 2.090116\n",
      "(Iteration 27901 / 30620) loss: 2.054264\n",
      "(Iteration 28001 / 30620) loss: 1.958472\n",
      "(Iteration 28101 / 30620) loss: 2.085088\n",
      "(Iteration 28201 / 30620) loss: 2.130047\n",
      "(Iteration 28301 / 30620) loss: 2.046125\n",
      "(Iteration 28401 / 30620) loss: 1.960855\n",
      "(Iteration 28501 / 30620) loss: 1.976390\n",
      "(Iteration 28601 / 30620) loss: 2.078788\n",
      "(Iteration 28701 / 30620) loss: 2.023263\n",
      "(Iteration 28801 / 30620) loss: 2.128895\n",
      "(Iteration 28901 / 30620) loss: 2.105874\n",
      "(Iteration 29001 / 30620) loss: 2.225770\n",
      "(Epoch 19 / 20) train acc: 0.255000; val_acc: 0.266000\n",
      "(Iteration 29101 / 30620) loss: 2.167162\n",
      "(Iteration 29201 / 30620) loss: 2.123857\n",
      "(Iteration 29301 / 30620) loss: 1.999088\n",
      "(Iteration 29401 / 30620) loss: 1.944558\n",
      "(Iteration 29501 / 30620) loss: 2.047999\n",
      "(Iteration 29601 / 30620) loss: 2.093169\n",
      "(Iteration 29701 / 30620) loss: 2.049947\n",
      "(Iteration 29801 / 30620) loss: 2.168853\n",
      "(Iteration 29901 / 30620) loss: 2.053575\n",
      "(Iteration 30001 / 30620) loss: 2.112627\n",
      "(Iteration 30101 / 30620) loss: 2.113120\n",
      "(Iteration 30201 / 30620) loss: 2.104288\n",
      "(Iteration 30301 / 30620) loss: 2.193548\n",
      "(Iteration 30401 / 30620) loss: 2.132649\n",
      "(Iteration 30501 / 30620) loss: 2.100740\n",
      "(Iteration 30601 / 30620) loss: 2.069352\n",
      "(Epoch 20 / 20) train acc: 0.279000; val_acc: 0.267000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.001, 'num_epochs': 30, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 22950) loss: 2.308373\n",
      "(Epoch 0 / 30) train acc: 0.125000; val_acc: 0.114000\n",
      "(Iteration 101 / 22950) loss: 2.308097\n",
      "(Iteration 201 / 22950) loss: 2.308033\n",
      "(Iteration 301 / 22950) loss: 2.307994\n",
      "(Iteration 401 / 22950) loss: 2.307742\n",
      "(Iteration 501 / 22950) loss: 2.307557\n",
      "(Iteration 601 / 22950) loss: 2.307391\n",
      "(Iteration 701 / 22950) loss: 2.307506\n",
      "(Epoch 1 / 30) train acc: 0.096000; val_acc: 0.081000\n",
      "(Iteration 801 / 22950) loss: 2.307444\n",
      "(Iteration 901 / 22950) loss: 2.307404\n",
      "(Iteration 1001 / 22950) loss: 2.307024\n",
      "(Iteration 1101 / 22950) loss: 2.307100\n",
      "(Iteration 1201 / 22950) loss: 2.306969\n",
      "(Iteration 1301 / 22950) loss: 2.306772\n",
      "(Iteration 1401 / 22950) loss: 2.306762\n",
      "(Iteration 1501 / 22950) loss: 2.306327\n",
      "(Epoch 2 / 30) train acc: 0.207000; val_acc: 0.192000\n",
      "(Iteration 1601 / 22950) loss: 2.306487\n",
      "(Iteration 1701 / 22950) loss: 2.306477\n",
      "(Iteration 1801 / 22950) loss: 2.306422\n",
      "(Iteration 1901 / 22950) loss: 2.306590\n",
      "(Iteration 2001 / 22950) loss: 2.306113\n",
      "(Iteration 2101 / 22950) loss: 2.306055\n",
      "(Iteration 2201 / 22950) loss: 2.305922\n",
      "(Epoch 3 / 30) train acc: 0.136000; val_acc: 0.126000\n",
      "(Iteration 2301 / 22950) loss: 2.305993\n",
      "(Iteration 2401 / 22950) loss: 2.306040\n",
      "(Iteration 2501 / 22950) loss: 2.305644\n",
      "(Iteration 2601 / 22950) loss: 2.305637\n",
      "(Iteration 2701 / 22950) loss: 2.305495\n",
      "(Iteration 2801 / 22950) loss: 2.305621\n",
      "(Iteration 2901 / 22950) loss: 2.305498\n",
      "(Iteration 3001 / 22950) loss: 2.305729\n",
      "(Epoch 4 / 30) train acc: 0.179000; val_acc: 0.144000\n",
      "(Iteration 3101 / 22950) loss: 2.306076\n",
      "(Iteration 3201 / 22950) loss: 2.305365\n",
      "(Iteration 3301 / 22950) loss: 2.305399\n",
      "(Iteration 3401 / 22950) loss: 2.305665\n",
      "(Iteration 3501 / 22950) loss: 2.305271\n",
      "(Iteration 3601 / 22950) loss: 2.305400\n",
      "(Iteration 3701 / 22950) loss: 2.305199\n",
      "(Iteration 3801 / 22950) loss: 2.305100\n",
      "(Epoch 5 / 30) train acc: 0.145000; val_acc: 0.116000\n",
      "(Iteration 3901 / 22950) loss: 2.305271\n",
      "(Iteration 4001 / 22950) loss: 2.305099\n",
      "(Iteration 4101 / 22950) loss: 2.304748\n",
      "(Iteration 4201 / 22950) loss: 2.304990\n",
      "(Iteration 4301 / 22950) loss: 2.305295\n",
      "(Iteration 4401 / 22950) loss: 2.304639\n",
      "(Iteration 4501 / 22950) loss: 2.304544\n",
      "(Epoch 6 / 30) train acc: 0.145000; val_acc: 0.149000\n",
      "(Iteration 4601 / 22950) loss: 2.304966\n",
      "(Iteration 4701 / 22950) loss: 2.304556\n",
      "(Iteration 4801 / 22950) loss: 2.304786\n",
      "(Iteration 4901 / 22950) loss: 2.304469\n",
      "(Iteration 5001 / 22950) loss: 2.304631\n",
      "(Iteration 5101 / 22950) loss: 2.304554\n",
      "(Iteration 5201 / 22950) loss: 2.303991\n",
      "(Iteration 5301 / 22950) loss: 2.303889\n",
      "(Epoch 7 / 30) train acc: 0.194000; val_acc: 0.168000\n",
      "(Iteration 5401 / 22950) loss: 2.304741\n",
      "(Iteration 5501 / 22950) loss: 2.304023\n",
      "(Iteration 5601 / 22950) loss: 2.304333\n",
      "(Iteration 5701 / 22950) loss: 2.304047\n",
      "(Iteration 5801 / 22950) loss: 2.303243\n",
      "(Iteration 5901 / 22950) loss: 2.304214\n",
      "(Iteration 6001 / 22950) loss: 2.304135\n",
      "(Iteration 6101 / 22950) loss: 2.303413\n",
      "(Epoch 8 / 30) train acc: 0.177000; val_acc: 0.150000\n",
      "(Iteration 6201 / 22950) loss: 2.304213\n",
      "(Iteration 6301 / 22950) loss: 2.304124\n",
      "(Iteration 6401 / 22950) loss: 2.303712\n",
      "(Iteration 6501 / 22950) loss: 2.304223\n",
      "(Iteration 6601 / 22950) loss: 2.303827\n",
      "(Iteration 6701 / 22950) loss: 2.304202\n",
      "(Iteration 6801 / 22950) loss: 2.304046\n",
      "(Epoch 9 / 30) train acc: 0.181000; val_acc: 0.166000\n",
      "(Iteration 6901 / 22950) loss: 2.303867\n",
      "(Iteration 7001 / 22950) loss: 2.303973\n",
      "(Iteration 7101 / 22950) loss: 2.304026\n",
      "(Iteration 7201 / 22950) loss: 2.303347\n",
      "(Iteration 7301 / 22950) loss: 2.303669\n",
      "(Iteration 7401 / 22950) loss: 2.303298\n",
      "(Iteration 7501 / 22950) loss: 2.303066\n",
      "(Iteration 7601 / 22950) loss: 2.303809\n",
      "(Epoch 10 / 30) train acc: 0.202000; val_acc: 0.179000\n",
      "(Iteration 7701 / 22950) loss: 2.302737\n",
      "(Iteration 7801 / 22950) loss: 2.303626\n",
      "(Iteration 7901 / 22950) loss: 2.304368\n",
      "(Iteration 8001 / 22950) loss: 2.303686\n",
      "(Iteration 8101 / 22950) loss: 2.302915\n",
      "(Iteration 8201 / 22950) loss: 2.302683\n",
      "(Iteration 8301 / 22950) loss: 2.303249\n",
      "(Iteration 8401 / 22950) loss: 2.302976\n",
      "(Epoch 11 / 30) train acc: 0.189000; val_acc: 0.175000\n",
      "(Iteration 8501 / 22950) loss: 2.302821\n",
      "(Iteration 8601 / 22950) loss: 2.303741\n",
      "(Iteration 8701 / 22950) loss: 2.303386\n",
      "(Iteration 8801 / 22950) loss: 2.303411\n",
      "(Iteration 8901 / 22950) loss: 2.303068\n",
      "(Iteration 9001 / 22950) loss: 2.302952\n",
      "(Iteration 9101 / 22950) loss: 2.303434\n",
      "(Epoch 12 / 30) train acc: 0.211000; val_acc: 0.174000\n",
      "(Iteration 9201 / 22950) loss: 2.303833\n",
      "(Iteration 9301 / 22950) loss: 2.303332\n",
      "(Iteration 9401 / 22950) loss: 2.303234\n",
      "(Iteration 9501 / 22950) loss: 2.303417\n",
      "(Iteration 9601 / 22950) loss: 2.302617\n",
      "(Iteration 9701 / 22950) loss: 2.302836\n",
      "(Iteration 9801 / 22950) loss: 2.302597\n",
      "(Iteration 9901 / 22950) loss: 2.302157\n",
      "(Epoch 13 / 30) train acc: 0.182000; val_acc: 0.171000\n",
      "(Iteration 10001 / 22950) loss: 2.303144\n",
      "(Iteration 10101 / 22950) loss: 2.303263\n",
      "(Iteration 10201 / 22950) loss: 2.302964\n",
      "(Iteration 10301 / 22950) loss: 2.303222\n",
      "(Iteration 10401 / 22950) loss: 2.301521\n",
      "(Iteration 10501 / 22950) loss: 2.301801\n",
      "(Iteration 10601 / 22950) loss: 2.302971\n",
      "(Iteration 10701 / 22950) loss: 2.302494\n",
      "(Epoch 14 / 30) train acc: 0.218000; val_acc: 0.188000\n",
      "(Iteration 10801 / 22950) loss: 2.302429\n",
      "(Iteration 10901 / 22950) loss: 2.302820\n",
      "(Iteration 11001 / 22950) loss: 2.302697\n",
      "(Iteration 11101 / 22950) loss: 2.302170\n",
      "(Iteration 11201 / 22950) loss: 2.301912\n",
      "(Iteration 11301 / 22950) loss: 2.303237\n",
      "(Iteration 11401 / 22950) loss: 2.302670\n",
      "(Epoch 15 / 30) train acc: 0.198000; val_acc: 0.172000\n",
      "(Iteration 11501 / 22950) loss: 2.303037\n",
      "(Iteration 11601 / 22950) loss: 2.302602\n",
      "(Iteration 11701 / 22950) loss: 2.303335\n",
      "(Iteration 11801 / 22950) loss: 2.301851\n",
      "(Iteration 11901 / 22950) loss: 2.302980\n",
      "(Iteration 12001 / 22950) loss: 2.302337\n",
      "(Iteration 12101 / 22950) loss: 2.302364\n",
      "(Iteration 12201 / 22950) loss: 2.302959\n",
      "(Epoch 16 / 30) train acc: 0.201000; val_acc: 0.176000\n",
      "(Iteration 12301 / 22950) loss: 2.302456\n",
      "(Iteration 12401 / 22950) loss: 2.301839\n",
      "(Iteration 12501 / 22950) loss: 2.302363\n",
      "(Iteration 12601 / 22950) loss: 2.302670\n",
      "(Iteration 12701 / 22950) loss: 2.301974\n",
      "(Iteration 12801 / 22950) loss: 2.302006\n",
      "(Iteration 12901 / 22950) loss: 2.302764\n",
      "(Iteration 13001 / 22950) loss: 2.300766\n",
      "(Epoch 17 / 30) train acc: 0.210000; val_acc: 0.171000\n",
      "(Iteration 13101 / 22950) loss: 2.302268\n",
      "(Iteration 13201 / 22950) loss: 2.302221\n",
      "(Iteration 13301 / 22950) loss: 2.302953\n",
      "(Iteration 13401 / 22950) loss: 2.301768\n",
      "(Iteration 13501 / 22950) loss: 2.302166\n",
      "(Iteration 13601 / 22950) loss: 2.302372\n",
      "(Iteration 13701 / 22950) loss: 2.302284\n",
      "(Epoch 18 / 30) train acc: 0.168000; val_acc: 0.174000\n",
      "(Iteration 13801 / 22950) loss: 2.301731\n",
      "(Iteration 13901 / 22950) loss: 2.302449\n",
      "(Iteration 14001 / 22950) loss: 2.301321\n",
      "(Iteration 14101 / 22950) loss: 2.301452\n",
      "(Iteration 14201 / 22950) loss: 2.302162\n",
      "(Iteration 14301 / 22950) loss: 2.301788\n",
      "(Iteration 14401 / 22950) loss: 2.301832\n",
      "(Iteration 14501 / 22950) loss: 2.300975\n",
      "(Epoch 19 / 30) train acc: 0.214000; val_acc: 0.176000\n",
      "(Iteration 14601 / 22950) loss: 2.301354\n",
      "(Iteration 14701 / 22950) loss: 2.302005\n",
      "(Iteration 14801 / 22950) loss: 2.301185\n",
      "(Iteration 14901 / 22950) loss: 2.301883\n",
      "(Iteration 15001 / 22950) loss: 2.300807\n",
      "(Iteration 15101 / 22950) loss: 2.300741\n",
      "(Iteration 15201 / 22950) loss: 2.302598\n",
      "(Epoch 20 / 30) train acc: 0.220000; val_acc: 0.179000\n",
      "(Iteration 15301 / 22950) loss: 2.301325\n",
      "(Iteration 15401 / 22950) loss: 2.301734\n",
      "(Iteration 15501 / 22950) loss: 2.301480\n",
      "(Iteration 15601 / 22950) loss: 2.302890\n",
      "(Iteration 15701 / 22950) loss: 2.300940\n",
      "(Iteration 15801 / 22950) loss: 2.302148\n",
      "(Iteration 15901 / 22950) loss: 2.301669\n",
      "(Iteration 16001 / 22950) loss: 2.301489\n",
      "(Epoch 21 / 30) train acc: 0.186000; val_acc: 0.178000\n",
      "(Iteration 16101 / 22950) loss: 2.302788\n",
      "(Iteration 16201 / 22950) loss: 2.302174\n",
      "(Iteration 16301 / 22950) loss: 2.302403\n",
      "(Iteration 16401 / 22950) loss: 2.301489\n",
      "(Iteration 16501 / 22950) loss: 2.300689\n",
      "(Iteration 16601 / 22950) loss: 2.300787\n",
      "(Iteration 16701 / 22950) loss: 2.301873\n",
      "(Iteration 16801 / 22950) loss: 2.301723\n",
      "(Epoch 22 / 30) train acc: 0.192000; val_acc: 0.176000\n",
      "(Iteration 16901 / 22950) loss: 2.301099\n",
      "(Iteration 17001 / 22950) loss: 2.301037\n",
      "(Iteration 17101 / 22950) loss: 2.302291\n",
      "(Iteration 17201 / 22950) loss: 2.302736\n",
      "(Iteration 17301 / 22950) loss: 2.301741\n",
      "(Iteration 17401 / 22950) loss: 2.299977\n",
      "(Iteration 17501 / 22950) loss: 2.300908\n",
      "(Epoch 23 / 30) train acc: 0.205000; val_acc: 0.183000\n",
      "(Iteration 17601 / 22950) loss: 2.301834\n",
      "(Iteration 17701 / 22950) loss: 2.299811\n",
      "(Iteration 17801 / 22950) loss: 2.301438\n",
      "(Iteration 17901 / 22950) loss: 2.299787\n",
      "(Iteration 18001 / 22950) loss: 2.300581\n",
      "(Iteration 18101 / 22950) loss: 2.300178\n",
      "(Iteration 18201 / 22950) loss: 2.302500\n",
      "(Iteration 18301 / 22950) loss: 2.301900\n",
      "(Epoch 24 / 30) train acc: 0.217000; val_acc: 0.186000\n",
      "(Iteration 18401 / 22950) loss: 2.300277\n",
      "(Iteration 18501 / 22950) loss: 2.300884\n",
      "(Iteration 18601 / 22950) loss: 2.301317\n",
      "(Iteration 18701 / 22950) loss: 2.300998\n",
      "(Iteration 18801 / 22950) loss: 2.301364\n",
      "(Iteration 18901 / 22950) loss: 2.300547\n",
      "(Iteration 19001 / 22950) loss: 2.299974\n",
      "(Iteration 19101 / 22950) loss: 2.301928\n",
      "(Epoch 25 / 30) train acc: 0.213000; val_acc: 0.185000\n",
      "(Iteration 19201 / 22950) loss: 2.300992\n",
      "(Iteration 19301 / 22950) loss: 2.301306\n",
      "(Iteration 19401 / 22950) loss: 2.300020\n",
      "(Iteration 19501 / 22950) loss: 2.300852\n",
      "(Iteration 19601 / 22950) loss: 2.300744\n",
      "(Iteration 19701 / 22950) loss: 2.301852\n",
      "(Iteration 19801 / 22950) loss: 2.300876\n",
      "(Epoch 26 / 30) train acc: 0.224000; val_acc: 0.187000\n",
      "(Iteration 19901 / 22950) loss: 2.301373\n",
      "(Iteration 20001 / 22950) loss: 2.300609\n",
      "(Iteration 20101 / 22950) loss: 2.301153\n",
      "(Iteration 20201 / 22950) loss: 2.300807\n",
      "(Iteration 20301 / 22950) loss: 2.300729\n",
      "(Iteration 20401 / 22950) loss: 2.300880\n",
      "(Iteration 20501 / 22950) loss: 2.299966\n",
      "(Iteration 20601 / 22950) loss: 2.300901\n",
      "(Epoch 27 / 30) train acc: 0.208000; val_acc: 0.189000\n",
      "(Iteration 20701 / 22950) loss: 2.302066\n",
      "(Iteration 20801 / 22950) loss: 2.301025\n",
      "(Iteration 20901 / 22950) loss: 2.300976\n",
      "(Iteration 21001 / 22950) loss: 2.300552\n",
      "(Iteration 21101 / 22950) loss: 2.301124\n",
      "(Iteration 21201 / 22950) loss: 2.302349\n",
      "(Iteration 21301 / 22950) loss: 2.301432\n",
      "(Iteration 21401 / 22950) loss: 2.300840\n",
      "(Epoch 28 / 30) train acc: 0.230000; val_acc: 0.191000\n",
      "(Iteration 21501 / 22950) loss: 2.300681\n",
      "(Iteration 21601 / 22950) loss: 2.301861\n",
      "(Iteration 21701 / 22950) loss: 2.301421\n",
      "(Iteration 21801 / 22950) loss: 2.302266\n",
      "(Iteration 21901 / 22950) loss: 2.300499\n",
      "(Iteration 22001 / 22950) loss: 2.300862\n",
      "(Iteration 22101 / 22950) loss: 2.299745\n",
      "(Epoch 29 / 30) train acc: 0.227000; val_acc: 0.191000\n",
      "(Iteration 22201 / 22950) loss: 2.302440\n",
      "(Iteration 22301 / 22950) loss: 2.301282\n",
      "(Iteration 22401 / 22950) loss: 2.300443\n",
      "(Iteration 22501 / 22950) loss: 2.300885\n",
      "(Iteration 22601 / 22950) loss: 2.301378\n",
      "(Iteration 22701 / 22950) loss: 2.300779\n",
      "(Iteration 22801 / 22950) loss: 2.302757\n",
      "(Iteration 22901 / 22950) loss: 2.300392\n",
      "(Epoch 30 / 30) train acc: 0.235000; val_acc: 0.192000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.001, 'num_epochs': 30, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 45930) loss: 2.308358\n",
      "(Epoch 0 / 30) train acc: 0.107000; val_acc: 0.107000\n",
      "(Iteration 101 / 45930) loss: 2.308153\n",
      "(Iteration 201 / 45930) loss: 2.308127\n",
      "(Iteration 301 / 45930) loss: 2.307738\n",
      "(Iteration 401 / 45930) loss: 2.307725\n",
      "(Iteration 501 / 45930) loss: 2.307547\n",
      "(Iteration 601 / 45930) loss: 2.307593\n",
      "(Iteration 701 / 45930) loss: 2.307204\n",
      "(Iteration 801 / 45930) loss: 2.307336\n",
      "(Iteration 901 / 45930) loss: 2.306823\n",
      "(Iteration 1001 / 45930) loss: 2.306723\n",
      "(Iteration 1101 / 45930) loss: 2.306685\n",
      "(Iteration 1201 / 45930) loss: 2.306676\n",
      "(Iteration 1301 / 45930) loss: 2.307221\n",
      "(Iteration 1401 / 45930) loss: 2.307176\n",
      "(Iteration 1501 / 45930) loss: 2.306802\n",
      "(Epoch 1 / 30) train acc: 0.198000; val_acc: 0.199000\n",
      "(Iteration 1601 / 45930) loss: 2.306458\n",
      "(Iteration 1701 / 45930) loss: 2.306221\n",
      "(Iteration 1801 / 45930) loss: 2.306392\n",
      "(Iteration 1901 / 45930) loss: 2.305224\n",
      "(Iteration 2001 / 45930) loss: 2.305463\n",
      "(Iteration 2101 / 45930) loss: 2.305828\n",
      "(Iteration 2201 / 45930) loss: 2.306130\n",
      "(Iteration 2301 / 45930) loss: 2.305501\n",
      "(Iteration 2401 / 45930) loss: 2.305453\n",
      "(Iteration 2501 / 45930) loss: 2.306154\n",
      "(Iteration 2601 / 45930) loss: 2.305882\n",
      "(Iteration 2701 / 45930) loss: 2.305668\n",
      "(Iteration 2801 / 45930) loss: 2.304564\n",
      "(Iteration 2901 / 45930) loss: 2.305321\n",
      "(Iteration 3001 / 45930) loss: 2.304455\n",
      "(Epoch 2 / 30) train acc: 0.134000; val_acc: 0.132000\n",
      "(Iteration 3101 / 45930) loss: 2.305163\n",
      "(Iteration 3201 / 45930) loss: 2.305132\n",
      "(Iteration 3301 / 45930) loss: 2.305763\n",
      "(Iteration 3401 / 45930) loss: 2.305419\n",
      "(Iteration 3501 / 45930) loss: 2.304898\n",
      "(Iteration 3601 / 45930) loss: 2.305212\n",
      "(Iteration 3701 / 45930) loss: 2.304989\n",
      "(Iteration 3801 / 45930) loss: 2.305118\n",
      "(Iteration 3901 / 45930) loss: 2.304900\n",
      "(Iteration 4001 / 45930) loss: 2.304443\n",
      "(Iteration 4101 / 45930) loss: 2.304228\n",
      "(Iteration 4201 / 45930) loss: 2.304821\n",
      "(Iteration 4301 / 45930) loss: 2.304539\n",
      "(Iteration 4401 / 45930) loss: 2.304311\n",
      "(Iteration 4501 / 45930) loss: 2.305068\n",
      "(Epoch 3 / 30) train acc: 0.217000; val_acc: 0.205000\n",
      "(Iteration 4601 / 45930) loss: 2.305085\n",
      "(Iteration 4701 / 45930) loss: 2.304952\n",
      "(Iteration 4801 / 45930) loss: 2.304802\n",
      "(Iteration 4901 / 45930) loss: 2.304292\n",
      "(Iteration 5001 / 45930) loss: 2.303857\n",
      "(Iteration 5101 / 45930) loss: 2.302792\n",
      "(Iteration 5201 / 45930) loss: 2.304238\n",
      "(Iteration 5301 / 45930) loss: 2.304421\n",
      "(Iteration 5401 / 45930) loss: 2.303594\n",
      "(Iteration 5501 / 45930) loss: 2.304841\n",
      "(Iteration 5601 / 45930) loss: 2.303558\n",
      "(Iteration 5701 / 45930) loss: 2.304288\n",
      "(Iteration 5801 / 45930) loss: 2.303128\n",
      "(Iteration 5901 / 45930) loss: 2.303562\n",
      "(Iteration 6001 / 45930) loss: 2.304052\n",
      "(Iteration 6101 / 45930) loss: 2.304177\n",
      "(Epoch 4 / 30) train acc: 0.199000; val_acc: 0.179000\n",
      "(Iteration 6201 / 45930) loss: 2.303335\n",
      "(Iteration 6301 / 45930) loss: 2.302981\n",
      "(Iteration 6401 / 45930) loss: 2.303558\n",
      "(Iteration 6501 / 45930) loss: 2.302772\n",
      "(Iteration 6601 / 45930) loss: 2.303148\n",
      "(Iteration 6701 / 45930) loss: 2.302114\n",
      "(Iteration 6801 / 45930) loss: 2.302910\n",
      "(Iteration 6901 / 45930) loss: 2.301550\n",
      "(Iteration 7001 / 45930) loss: 2.303923\n",
      "(Iteration 7101 / 45930) loss: 2.301653\n",
      "(Iteration 7201 / 45930) loss: 2.303161\n",
      "(Iteration 7301 / 45930) loss: 2.301724\n",
      "(Iteration 7401 / 45930) loss: 2.303233\n",
      "(Iteration 7501 / 45930) loss: 2.301975\n",
      "(Iteration 7601 / 45930) loss: 2.302079\n",
      "(Epoch 5 / 30) train acc: 0.198000; val_acc: 0.190000\n",
      "(Iteration 7701 / 45930) loss: 2.302589\n",
      "(Iteration 7801 / 45930) loss: 2.302966\n",
      "(Iteration 7901 / 45930) loss: 2.303040\n",
      "(Iteration 8001 / 45930) loss: 2.303225\n",
      "(Iteration 8101 / 45930) loss: 2.302134\n",
      "(Iteration 8201 / 45930) loss: 2.302473\n",
      "(Iteration 8301 / 45930) loss: 2.302151\n",
      "(Iteration 8401 / 45930) loss: 2.302548\n",
      "(Iteration 8501 / 45930) loss: 2.301920\n",
      "(Iteration 8601 / 45930) loss: 2.301217\n",
      "(Iteration 8701 / 45930) loss: 2.300939\n",
      "(Iteration 8801 / 45930) loss: 2.299306\n",
      "(Iteration 8901 / 45930) loss: 2.301756\n",
      "(Iteration 9001 / 45930) loss: 2.302301\n",
      "(Iteration 9101 / 45930) loss: 2.300326\n",
      "(Epoch 6 / 30) train acc: 0.245000; val_acc: 0.213000\n",
      "(Iteration 9201 / 45930) loss: 2.302208\n",
      "(Iteration 9301 / 45930) loss: 2.302485\n",
      "(Iteration 9401 / 45930) loss: 2.299412\n",
      "(Iteration 9501 / 45930) loss: 2.302312\n",
      "(Iteration 9601 / 45930) loss: 2.302543\n",
      "(Iteration 9701 / 45930) loss: 2.299850\n",
      "(Iteration 9801 / 45930) loss: 2.303103\n",
      "(Iteration 9901 / 45930) loss: 2.301429\n",
      "(Iteration 10001 / 45930) loss: 2.301967\n",
      "(Iteration 10101 / 45930) loss: 2.299147\n",
      "(Iteration 10201 / 45930) loss: 2.300246\n",
      "(Iteration 10301 / 45930) loss: 2.301393\n",
      "(Iteration 10401 / 45930) loss: 2.301762\n",
      "(Iteration 10501 / 45930) loss: 2.298684\n",
      "(Iteration 10601 / 45930) loss: 2.299030\n",
      "(Iteration 10701 / 45930) loss: 2.298081\n",
      "(Epoch 7 / 30) train acc: 0.263000; val_acc: 0.265000\n",
      "(Iteration 10801 / 45930) loss: 2.296124\n",
      "(Iteration 10901 / 45930) loss: 2.299812\n",
      "(Iteration 11001 / 45930) loss: 2.296829\n",
      "(Iteration 11101 / 45930) loss: 2.295291\n",
      "(Iteration 11201 / 45930) loss: 2.301632\n",
      "(Iteration 11301 / 45930) loss: 2.299250\n",
      "(Iteration 11401 / 45930) loss: 2.301227\n",
      "(Iteration 11501 / 45930) loss: 2.298799\n",
      "(Iteration 11601 / 45930) loss: 2.298537\n",
      "(Iteration 11701 / 45930) loss: 2.297555\n",
      "(Iteration 11801 / 45930) loss: 2.298644\n",
      "(Iteration 11901 / 45930) loss: 2.296623\n",
      "(Iteration 12001 / 45930) loss: 2.295001\n",
      "(Iteration 12101 / 45930) loss: 2.301192\n",
      "(Iteration 12201 / 45930) loss: 2.297954\n",
      "(Epoch 8 / 30) train acc: 0.288000; val_acc: 0.271000\n",
      "(Iteration 12301 / 45930) loss: 2.300160\n",
      "(Iteration 12401 / 45930) loss: 2.299352\n",
      "(Iteration 12501 / 45930) loss: 2.299440\n",
      "(Iteration 12601 / 45930) loss: 2.297798\n",
      "(Iteration 12701 / 45930) loss: 2.298035\n",
      "(Iteration 12801 / 45930) loss: 2.293061\n",
      "(Iteration 12901 / 45930) loss: 2.296031\n",
      "(Iteration 13001 / 45930) loss: 2.301774\n",
      "(Iteration 13101 / 45930) loss: 2.298264\n",
      "(Iteration 13201 / 45930) loss: 2.300582\n",
      "(Iteration 13301 / 45930) loss: 2.297785\n",
      "(Iteration 13401 / 45930) loss: 2.295925\n",
      "(Iteration 13501 / 45930) loss: 2.299401\n",
      "(Iteration 13601 / 45930) loss: 2.290192\n",
      "(Iteration 13701 / 45930) loss: 2.289783\n",
      "(Epoch 9 / 30) train acc: 0.246000; val_acc: 0.253000\n",
      "(Iteration 13801 / 45930) loss: 2.291178\n",
      "(Iteration 13901 / 45930) loss: 2.297262\n",
      "(Iteration 14001 / 45930) loss: 2.291753\n",
      "(Iteration 14101 / 45930) loss: 2.292282\n",
      "(Iteration 14201 / 45930) loss: 2.291578\n",
      "(Iteration 14301 / 45930) loss: 2.297088\n",
      "(Iteration 14401 / 45930) loss: 2.292683\n",
      "(Iteration 14501 / 45930) loss: 2.294942\n",
      "(Iteration 14601 / 45930) loss: 2.282933\n",
      "(Iteration 14701 / 45930) loss: 2.294030\n",
      "(Iteration 14801 / 45930) loss: 2.293192\n",
      "(Iteration 14901 / 45930) loss: 2.286275\n",
      "(Iteration 15001 / 45930) loss: 2.291704\n",
      "(Iteration 15101 / 45930) loss: 2.289446\n",
      "(Iteration 15201 / 45930) loss: 2.285502\n",
      "(Iteration 15301 / 45930) loss: 2.287651\n",
      "(Epoch 10 / 30) train acc: 0.263000; val_acc: 0.262000\n",
      "(Iteration 15401 / 45930) loss: 2.288944\n",
      "(Iteration 15501 / 45930) loss: 2.292447\n",
      "(Iteration 15601 / 45930) loss: 2.291029\n",
      "(Iteration 15701 / 45930) loss: 2.294296\n",
      "(Iteration 15801 / 45930) loss: 2.290152\n",
      "(Iteration 15901 / 45930) loss: 2.300035\n",
      "(Iteration 16001 / 45930) loss: 2.294404\n",
      "(Iteration 16101 / 45930) loss: 2.290936\n",
      "(Iteration 16201 / 45930) loss: 2.289152\n",
      "(Iteration 16301 / 45930) loss: 2.298118\n",
      "(Iteration 16401 / 45930) loss: 2.284370\n",
      "(Iteration 16501 / 45930) loss: 2.294383\n",
      "(Iteration 16601 / 45930) loss: 2.290180\n",
      "(Iteration 16701 / 45930) loss: 2.280550\n",
      "(Iteration 16801 / 45930) loss: 2.281060\n",
      "(Epoch 11 / 30) train acc: 0.270000; val_acc: 0.249000\n",
      "(Iteration 16901 / 45930) loss: 2.289024\n",
      "(Iteration 17001 / 45930) loss: 2.292632\n",
      "(Iteration 17101 / 45930) loss: 2.286422\n",
      "(Iteration 17201 / 45930) loss: 2.293719\n",
      "(Iteration 17301 / 45930) loss: 2.290427\n",
      "(Iteration 17401 / 45930) loss: 2.282122\n",
      "(Iteration 17501 / 45930) loss: 2.292120\n",
      "(Iteration 17601 / 45930) loss: 2.272637\n",
      "(Iteration 17701 / 45930) loss: 2.282785\n",
      "(Iteration 17801 / 45930) loss: 2.286779\n",
      "(Iteration 17901 / 45930) loss: 2.284029\n",
      "(Iteration 18001 / 45930) loss: 2.303988\n",
      "(Iteration 18101 / 45930) loss: 2.283154\n",
      "(Iteration 18201 / 45930) loss: 2.280116\n",
      "(Iteration 18301 / 45930) loss: 2.278351\n",
      "(Epoch 12 / 30) train acc: 0.221000; val_acc: 0.239000\n",
      "(Iteration 18401 / 45930) loss: 2.275365\n",
      "(Iteration 18501 / 45930) loss: 2.278011\n",
      "(Iteration 18601 / 45930) loss: 2.293811\n",
      "(Iteration 18701 / 45930) loss: 2.282597\n",
      "(Iteration 18801 / 45930) loss: 2.287554\n",
      "(Iteration 18901 / 45930) loss: 2.297488\n",
      "(Iteration 19001 / 45930) loss: 2.298824\n",
      "(Iteration 19101 / 45930) loss: 2.290194\n",
      "(Iteration 19201 / 45930) loss: 2.291242\n",
      "(Iteration 19301 / 45930) loss: 2.275501\n",
      "(Iteration 19401 / 45930) loss: 2.286405\n",
      "(Iteration 19501 / 45930) loss: 2.274848\n",
      "(Iteration 19601 / 45930) loss: 2.266927\n",
      "(Iteration 19701 / 45930) loss: 2.292885\n",
      "(Iteration 19801 / 45930) loss: 2.284820\n",
      "(Iteration 19901 / 45930) loss: 2.282527\n",
      "(Epoch 13 / 30) train acc: 0.264000; val_acc: 0.247000\n",
      "(Iteration 20001 / 45930) loss: 2.284849\n",
      "(Iteration 20101 / 45930) loss: 2.280569\n",
      "(Iteration 20201 / 45930) loss: 2.280742\n",
      "(Iteration 20301 / 45930) loss: 2.274357\n",
      "(Iteration 20401 / 45930) loss: 2.268872\n",
      "(Iteration 20501 / 45930) loss: 2.291980\n",
      "(Iteration 20601 / 45930) loss: 2.296570\n",
      "(Iteration 20701 / 45930) loss: 2.278146\n",
      "(Iteration 20801 / 45930) loss: 2.281118\n",
      "(Iteration 20901 / 45930) loss: 2.283102\n",
      "(Iteration 21001 / 45930) loss: 2.270622\n",
      "(Iteration 21101 / 45930) loss: 2.289234\n",
      "(Iteration 21201 / 45930) loss: 2.261028\n",
      "(Iteration 21301 / 45930) loss: 2.273722\n",
      "(Iteration 21401 / 45930) loss: 2.272741\n",
      "(Epoch 14 / 30) train acc: 0.246000; val_acc: 0.249000\n",
      "(Iteration 21501 / 45930) loss: 2.262342\n",
      "(Iteration 21601 / 45930) loss: 2.270225\n",
      "(Iteration 21701 / 45930) loss: 2.292180\n",
      "(Iteration 21801 / 45930) loss: 2.287612\n",
      "(Iteration 21901 / 45930) loss: 2.276696\n",
      "(Iteration 22001 / 45930) loss: 2.261560\n",
      "(Iteration 22101 / 45930) loss: 2.285660\n",
      "(Iteration 22201 / 45930) loss: 2.267669\n",
      "(Iteration 22301 / 45930) loss: 2.296167\n",
      "(Iteration 22401 / 45930) loss: 2.263168\n",
      "(Iteration 22501 / 45930) loss: 2.264531\n",
      "(Iteration 22601 / 45930) loss: 2.275904\n",
      "(Iteration 22701 / 45930) loss: 2.259972\n",
      "(Iteration 22801 / 45930) loss: 2.289735\n",
      "(Iteration 22901 / 45930) loss: 2.268560\n",
      "(Epoch 15 / 30) train acc: 0.250000; val_acc: 0.248000\n",
      "(Iteration 23001 / 45930) loss: 2.266346\n",
      "(Iteration 23101 / 45930) loss: 2.280047\n",
      "(Iteration 23201 / 45930) loss: 2.269657\n",
      "(Iteration 23301 / 45930) loss: 2.258009\n",
      "(Iteration 23401 / 45930) loss: 2.274906\n",
      "(Iteration 23501 / 45930) loss: 2.292637\n",
      "(Iteration 23601 / 45930) loss: 2.289640\n",
      "(Iteration 23701 / 45930) loss: 2.273558\n",
      "(Iteration 23801 / 45930) loss: 2.274997\n",
      "(Iteration 23901 / 45930) loss: 2.246004\n",
      "(Iteration 24001 / 45930) loss: 2.260954\n",
      "(Iteration 24101 / 45930) loss: 2.270638\n",
      "(Iteration 24201 / 45930) loss: 2.260873\n",
      "(Iteration 24301 / 45930) loss: 2.280959\n",
      "(Iteration 24401 / 45930) loss: 2.266512\n",
      "(Epoch 16 / 30) train acc: 0.254000; val_acc: 0.242000\n",
      "(Iteration 24501 / 45930) loss: 2.280536\n",
      "(Iteration 24601 / 45930) loss: 2.251903\n",
      "(Iteration 24701 / 45930) loss: 2.243381\n",
      "(Iteration 24801 / 45930) loss: 2.256845\n",
      "(Iteration 24901 / 45930) loss: 2.236736\n",
      "(Iteration 25001 / 45930) loss: 2.259391\n",
      "(Iteration 25101 / 45930) loss: 2.251853\n",
      "(Iteration 25201 / 45930) loss: 2.277633\n",
      "(Iteration 25301 / 45930) loss: 2.268981\n",
      "(Iteration 25401 / 45930) loss: 2.268019\n",
      "(Iteration 25501 / 45930) loss: 2.267868\n",
      "(Iteration 25601 / 45930) loss: 2.267915\n",
      "(Iteration 25701 / 45930) loss: 2.267348\n",
      "(Iteration 25801 / 45930) loss: 2.275594\n",
      "(Iteration 25901 / 45930) loss: 2.253955\n",
      "(Iteration 26001 / 45930) loss: 2.277298\n",
      "(Epoch 17 / 30) train acc: 0.235000; val_acc: 0.244000\n",
      "(Iteration 26101 / 45930) loss: 2.261797\n",
      "(Iteration 26201 / 45930) loss: 2.259879\n",
      "(Iteration 26301 / 45930) loss: 2.255784\n",
      "(Iteration 26401 / 45930) loss: 2.252410\n",
      "(Iteration 26501 / 45930) loss: 2.276261\n",
      "(Iteration 26601 / 45930) loss: 2.247401\n",
      "(Iteration 26701 / 45930) loss: 2.268205\n",
      "(Iteration 26801 / 45930) loss: 2.242132\n",
      "(Iteration 26901 / 45930) loss: 2.260087\n",
      "(Iteration 27001 / 45930) loss: 2.235348\n",
      "(Iteration 27101 / 45930) loss: 2.270710\n",
      "(Iteration 27201 / 45930) loss: 2.275530\n",
      "(Iteration 27301 / 45930) loss: 2.284730\n",
      "(Iteration 27401 / 45930) loss: 2.212993\n",
      "(Iteration 27501 / 45930) loss: 2.278209\n",
      "(Epoch 18 / 30) train acc: 0.253000; val_acc: 0.242000\n",
      "(Iteration 27601 / 45930) loss: 2.256862\n",
      "(Iteration 27701 / 45930) loss: 2.221623\n",
      "(Iteration 27801 / 45930) loss: 2.254196\n",
      "(Iteration 27901 / 45930) loss: 2.263375\n",
      "(Iteration 28001 / 45930) loss: 2.246897\n",
      "(Iteration 28101 / 45930) loss: 2.229275\n",
      "(Iteration 28201 / 45930) loss: 2.243152\n",
      "(Iteration 28301 / 45930) loss: 2.268503\n",
      "(Iteration 28401 / 45930) loss: 2.278473\n",
      "(Iteration 28501 / 45930) loss: 2.299133\n",
      "(Iteration 28601 / 45930) loss: 2.215520\n",
      "(Iteration 28701 / 45930) loss: 2.262580\n",
      "(Iteration 28801 / 45930) loss: 2.282221\n",
      "(Iteration 28901 / 45930) loss: 2.205237\n",
      "(Iteration 29001 / 45930) loss: 2.246827\n",
      "(Epoch 19 / 30) train acc: 0.260000; val_acc: 0.244000\n",
      "(Iteration 29101 / 45930) loss: 2.225070\n",
      "(Iteration 29201 / 45930) loss: 2.262417\n",
      "(Iteration 29301 / 45930) loss: 2.264587\n",
      "(Iteration 29401 / 45930) loss: 2.233628\n",
      "(Iteration 29501 / 45930) loss: 2.275002\n",
      "(Iteration 29601 / 45930) loss: 2.288838\n",
      "(Iteration 29701 / 45930) loss: 2.239646\n",
      "(Iteration 29801 / 45930) loss: 2.271200\n",
      "(Iteration 29901 / 45930) loss: 2.282857\n",
      "(Iteration 30001 / 45930) loss: 2.226771\n",
      "(Iteration 30101 / 45930) loss: 2.236102\n",
      "(Iteration 30201 / 45930) loss: 2.239259\n",
      "(Iteration 30301 / 45930) loss: 2.236193\n",
      "(Iteration 30401 / 45930) loss: 2.258729\n",
      "(Iteration 30501 / 45930) loss: 2.231474\n",
      "(Iteration 30601 / 45930) loss: 2.231766\n",
      "(Epoch 20 / 30) train acc: 0.263000; val_acc: 0.245000\n",
      "(Iteration 30701 / 45930) loss: 2.264799\n",
      "(Iteration 30801 / 45930) loss: 2.254128\n",
      "(Iteration 30901 / 45930) loss: 2.262359\n",
      "(Iteration 31001 / 45930) loss: 2.289689\n",
      "(Iteration 31101 / 45930) loss: 2.266604\n",
      "(Iteration 31201 / 45930) loss: 2.256423\n",
      "(Iteration 31301 / 45930) loss: 2.215089\n",
      "(Iteration 31401 / 45930) loss: 2.269529\n",
      "(Iteration 31501 / 45930) loss: 2.265096\n",
      "(Iteration 31601 / 45930) loss: 2.219580\n",
      "(Iteration 31701 / 45930) loss: 2.254972\n",
      "(Iteration 31801 / 45930) loss: 2.270514\n",
      "(Iteration 31901 / 45930) loss: 2.243156\n",
      "(Iteration 32001 / 45930) loss: 2.254774\n",
      "(Iteration 32101 / 45930) loss: 2.220603\n",
      "(Epoch 21 / 30) train acc: 0.241000; val_acc: 0.252000\n",
      "(Iteration 32201 / 45930) loss: 2.203595\n",
      "(Iteration 32301 / 45930) loss: 2.257192\n",
      "(Iteration 32401 / 45930) loss: 2.253739\n",
      "(Iteration 32501 / 45930) loss: 2.234796\n",
      "(Iteration 32601 / 45930) loss: 2.234741\n",
      "(Iteration 32701 / 45930) loss: 2.272157\n",
      "(Iteration 32801 / 45930) loss: 2.265924\n",
      "(Iteration 32901 / 45930) loss: 2.254568\n",
      "(Iteration 33001 / 45930) loss: 2.271553\n",
      "(Iteration 33101 / 45930) loss: 2.229975\n",
      "(Iteration 33201 / 45930) loss: 2.222646\n",
      "(Iteration 33301 / 45930) loss: 2.222144\n",
      "(Iteration 33401 / 45930) loss: 2.228612\n",
      "(Iteration 33501 / 45930) loss: 2.232105\n",
      "(Iteration 33601 / 45930) loss: 2.252432\n",
      "(Epoch 22 / 30) train acc: 0.253000; val_acc: 0.243000\n",
      "(Iteration 33701 / 45930) loss: 2.214700\n",
      "(Iteration 33801 / 45930) loss: 2.231264\n",
      "(Iteration 33901 / 45930) loss: 2.252618\n",
      "(Iteration 34001 / 45930) loss: 2.258163\n",
      "(Iteration 34101 / 45930) loss: 2.281028\n",
      "(Iteration 34201 / 45930) loss: 2.215291\n",
      "(Iteration 34301 / 45930) loss: 2.230991\n",
      "(Iteration 34401 / 45930) loss: 2.243583\n",
      "(Iteration 34501 / 45930) loss: 2.226273\n",
      "(Iteration 34601 / 45930) loss: 2.258893\n",
      "(Iteration 34701 / 45930) loss: 2.269250\n",
      "(Iteration 34801 / 45930) loss: 2.256477\n",
      "(Iteration 34901 / 45930) loss: 2.249391\n",
      "(Iteration 35001 / 45930) loss: 2.257302\n",
      "(Iteration 35101 / 45930) loss: 2.251939\n",
      "(Iteration 35201 / 45930) loss: 2.230028\n",
      "(Epoch 23 / 30) train acc: 0.268000; val_acc: 0.246000\n",
      "(Iteration 35301 / 45930) loss: 2.202578\n",
      "(Iteration 35401 / 45930) loss: 2.262584\n",
      "(Iteration 35501 / 45930) loss: 2.184510\n",
      "(Iteration 35601 / 45930) loss: 2.182013\n",
      "(Iteration 35701 / 45930) loss: 2.256306\n",
      "(Iteration 35801 / 45930) loss: 2.254125\n",
      "(Iteration 35901 / 45930) loss: 2.251557\n",
      "(Iteration 36001 / 45930) loss: 2.246502\n",
      "(Iteration 36101 / 45930) loss: 2.246060\n",
      "(Iteration 36201 / 45930) loss: 2.202720\n",
      "(Iteration 36301 / 45930) loss: 2.283043\n",
      "(Iteration 36401 / 45930) loss: 2.240342\n",
      "(Iteration 36501 / 45930) loss: 2.197673\n",
      "(Iteration 36601 / 45930) loss: 2.265942\n",
      "(Iteration 36701 / 45930) loss: 2.209611\n",
      "(Epoch 24 / 30) train acc: 0.259000; val_acc: 0.249000\n",
      "(Iteration 36801 / 45930) loss: 2.298357\n",
      "(Iteration 36901 / 45930) loss: 2.252976\n",
      "(Iteration 37001 / 45930) loss: 2.199554\n",
      "(Iteration 37101 / 45930) loss: 2.296325\n",
      "(Iteration 37201 / 45930) loss: 2.263882\n",
      "(Iteration 37301 / 45930) loss: 2.238974\n",
      "(Iteration 37401 / 45930) loss: 2.200129\n",
      "(Iteration 37501 / 45930) loss: 2.227441\n",
      "(Iteration 37601 / 45930) loss: 2.154454\n",
      "(Iteration 37701 / 45930) loss: 2.249857\n",
      "(Iteration 37801 / 45930) loss: 2.269895\n",
      "(Iteration 37901 / 45930) loss: 2.234150\n",
      "(Iteration 38001 / 45930) loss: 2.209866\n",
      "(Iteration 38101 / 45930) loss: 2.227731\n",
      "(Iteration 38201 / 45930) loss: 2.261784\n",
      "(Epoch 25 / 30) train acc: 0.243000; val_acc: 0.249000\n",
      "(Iteration 38301 / 45930) loss: 2.288310\n",
      "(Iteration 38401 / 45930) loss: 2.195827\n",
      "(Iteration 38501 / 45930) loss: 2.295284\n",
      "(Iteration 38601 / 45930) loss: 2.262108\n",
      "(Iteration 38701 / 45930) loss: 2.268236\n",
      "(Iteration 38801 / 45930) loss: 2.270131\n",
      "(Iteration 38901 / 45930) loss: 2.281230\n",
      "(Iteration 39001 / 45930) loss: 2.203663\n",
      "(Iteration 39101 / 45930) loss: 2.200841\n",
      "(Iteration 39201 / 45930) loss: 2.223739\n",
      "(Iteration 39301 / 45930) loss: 2.247171\n",
      "(Iteration 39401 / 45930) loss: 2.272057\n",
      "(Iteration 39501 / 45930) loss: 2.211246\n",
      "(Iteration 39601 / 45930) loss: 2.253094\n",
      "(Iteration 39701 / 45930) loss: 2.237076\n",
      "(Iteration 39801 / 45930) loss: 2.198097\n",
      "(Epoch 26 / 30) train acc: 0.243000; val_acc: 0.249000\n",
      "(Iteration 39901 / 45930) loss: 2.275236\n",
      "(Iteration 40001 / 45930) loss: 2.235859\n",
      "(Iteration 40101 / 45930) loss: 2.230035\n",
      "(Iteration 40201 / 45930) loss: 2.222771\n",
      "(Iteration 40301 / 45930) loss: 2.191084\n",
      "(Iteration 40401 / 45930) loss: 2.281588\n",
      "(Iteration 40501 / 45930) loss: 2.216754\n",
      "(Iteration 40601 / 45930) loss: 2.247902\n",
      "(Iteration 40701 / 45930) loss: 2.300965\n",
      "(Iteration 40801 / 45930) loss: 2.253361\n",
      "(Iteration 40901 / 45930) loss: 2.255058\n",
      "(Iteration 41001 / 45930) loss: 2.194131\n",
      "(Iteration 41101 / 45930) loss: 2.244977\n",
      "(Iteration 41201 / 45930) loss: 2.215390\n",
      "(Iteration 41301 / 45930) loss: 2.268393\n",
      "(Epoch 27 / 30) train acc: 0.275000; val_acc: 0.244000\n",
      "(Iteration 41401 / 45930) loss: 2.160620\n",
      "(Iteration 41501 / 45930) loss: 2.228502\n",
      "(Iteration 41601 / 45930) loss: 2.192943\n",
      "(Iteration 41701 / 45930) loss: 2.242414\n",
      "(Iteration 41801 / 45930) loss: 2.226976\n",
      "(Iteration 41901 / 45930) loss: 2.237823\n",
      "(Iteration 42001 / 45930) loss: 2.266839\n",
      "(Iteration 42101 / 45930) loss: 2.236842\n",
      "(Iteration 42201 / 45930) loss: 2.214744\n",
      "(Iteration 42301 / 45930) loss: 2.215378\n",
      "(Iteration 42401 / 45930) loss: 2.273131\n",
      "(Iteration 42501 / 45930) loss: 2.223813\n",
      "(Iteration 42601 / 45930) loss: 2.192401\n",
      "(Iteration 42701 / 45930) loss: 2.215529\n",
      "(Iteration 42801 / 45930) loss: 2.254201\n",
      "(Epoch 28 / 30) train acc: 0.250000; val_acc: 0.248000\n",
      "(Iteration 42901 / 45930) loss: 2.243431\n",
      "(Iteration 43001 / 45930) loss: 2.250502\n",
      "(Iteration 43101 / 45930) loss: 2.218406\n",
      "(Iteration 43201 / 45930) loss: 2.221246\n",
      "(Iteration 43301 / 45930) loss: 2.212557\n",
      "(Iteration 43401 / 45930) loss: 2.206432\n",
      "(Iteration 43501 / 45930) loss: 2.171584\n",
      "(Iteration 43601 / 45930) loss: 2.233569\n",
      "(Iteration 43701 / 45930) loss: 2.183310\n",
      "(Iteration 43801 / 45930) loss: 2.231031\n",
      "(Iteration 43901 / 45930) loss: 2.228617\n",
      "(Iteration 44001 / 45930) loss: 2.191540\n",
      "(Iteration 44101 / 45930) loss: 2.222403\n",
      "(Iteration 44201 / 45930) loss: 2.290177\n",
      "(Iteration 44301 / 45930) loss: 2.209616\n",
      "(Epoch 29 / 30) train acc: 0.248000; val_acc: 0.244000\n",
      "(Iteration 44401 / 45930) loss: 2.182788\n",
      "(Iteration 44501 / 45930) loss: 2.230739\n",
      "(Iteration 44601 / 45930) loss: 2.251561\n",
      "(Iteration 44701 / 45930) loss: 2.276215\n",
      "(Iteration 44801 / 45930) loss: 2.182633\n",
      "(Iteration 44901 / 45930) loss: 2.228605\n",
      "(Iteration 45001 / 45930) loss: 2.215166\n",
      "(Iteration 45101 / 45930) loss: 2.198815\n",
      "(Iteration 45201 / 45930) loss: 2.223432\n",
      "(Iteration 45301 / 45930) loss: 2.282071\n",
      "(Iteration 45401 / 45930) loss: 2.187669\n",
      "(Iteration 45501 / 45930) loss: 2.188936\n",
      "(Iteration 45601 / 45930) loss: 2.242275\n",
      "(Iteration 45701 / 45930) loss: 2.259701\n",
      "(Iteration 45801 / 45930) loss: 2.267443\n",
      "(Iteration 45901 / 45930) loss: 2.193590\n",
      "(Epoch 30 / 30) train acc: 0.232000; val_acc: 0.246000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.001, 'num_epochs': 30, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 22950) loss: 2.303210\n",
      "(Epoch 0 / 30) train acc: 0.101000; val_acc: 0.107000\n",
      "(Iteration 101 / 22950) loss: 2.303224\n",
      "(Iteration 201 / 22950) loss: 2.303141\n",
      "(Iteration 301 / 22950) loss: 2.303037\n",
      "(Iteration 401 / 22950) loss: 2.303000\n",
      "(Iteration 501 / 22950) loss: 2.303124\n",
      "(Iteration 601 / 22950) loss: 2.302998\n",
      "(Iteration 701 / 22950) loss: 2.302834\n",
      "(Epoch 1 / 30) train acc: 0.182000; val_acc: 0.166000\n",
      "(Iteration 801 / 22950) loss: 2.302939\n",
      "(Iteration 901 / 22950) loss: 2.302984\n",
      "(Iteration 1001 / 22950) loss: 2.302924\n",
      "(Iteration 1101 / 22950) loss: 2.302807\n",
      "(Iteration 1201 / 22950) loss: 2.302827\n",
      "(Iteration 1301 / 22950) loss: 2.302893\n",
      "(Iteration 1401 / 22950) loss: 2.302778\n",
      "(Iteration 1501 / 22950) loss: 2.302665\n",
      "(Epoch 2 / 30) train acc: 0.115000; val_acc: 0.113000\n",
      "(Iteration 1601 / 22950) loss: 2.302707\n",
      "(Iteration 1701 / 22950) loss: 2.302512\n",
      "(Iteration 1801 / 22950) loss: 2.302571\n",
      "(Iteration 1901 / 22950) loss: 2.302577\n",
      "(Iteration 2001 / 22950) loss: 2.302733\n",
      "(Iteration 2101 / 22950) loss: 2.302720\n",
      "(Iteration 2201 / 22950) loss: 2.302693\n",
      "(Epoch 3 / 30) train acc: 0.276000; val_acc: 0.251000\n",
      "(Iteration 2301 / 22950) loss: 2.302440\n",
      "(Iteration 2401 / 22950) loss: 2.302556\n",
      "(Iteration 2501 / 22950) loss: 2.302423\n",
      "(Iteration 2601 / 22950) loss: 2.302307\n",
      "(Iteration 2701 / 22950) loss: 2.302346\n",
      "(Iteration 2801 / 22950) loss: 2.302486\n",
      "(Iteration 2901 / 22950) loss: 2.302592\n",
      "(Iteration 3001 / 22950) loss: 2.302554\n",
      "(Epoch 4 / 30) train acc: 0.255000; val_acc: 0.228000\n",
      "(Iteration 3101 / 22950) loss: 2.302120\n",
      "(Iteration 3201 / 22950) loss: 2.301954\n",
      "(Iteration 3301 / 22950) loss: 2.302131\n",
      "(Iteration 3401 / 22950) loss: 2.302135\n",
      "(Iteration 3501 / 22950) loss: 2.302151\n",
      "(Iteration 3601 / 22950) loss: 2.302008\n",
      "(Iteration 3701 / 22950) loss: 2.302207\n",
      "(Iteration 3801 / 22950) loss: 2.301631\n",
      "(Epoch 5 / 30) train acc: 0.270000; val_acc: 0.260000\n",
      "(Iteration 3901 / 22950) loss: 2.301867\n",
      "(Iteration 4001 / 22950) loss: 2.301705\n",
      "(Iteration 4101 / 22950) loss: 2.301966\n",
      "(Iteration 4201 / 22950) loss: 2.301441\n",
      "(Iteration 4301 / 22950) loss: 2.301565\n",
      "(Iteration 4401 / 22950) loss: 2.301634\n",
      "(Iteration 4501 / 22950) loss: 2.301281\n",
      "(Epoch 6 / 30) train acc: 0.246000; val_acc: 0.245000\n",
      "(Iteration 4601 / 22950) loss: 2.301214\n",
      "(Iteration 4701 / 22950) loss: 2.301678\n",
      "(Iteration 4801 / 22950) loss: 2.301107\n",
      "(Iteration 4901 / 22950) loss: 2.301068\n",
      "(Iteration 5001 / 22950) loss: 2.301306\n",
      "(Iteration 5101 / 22950) loss: 2.301550\n",
      "(Iteration 5201 / 22950) loss: 2.301754\n",
      "(Iteration 5301 / 22950) loss: 2.300989\n",
      "(Epoch 7 / 30) train acc: 0.263000; val_acc: 0.273000\n",
      "(Iteration 5401 / 22950) loss: 2.301200\n",
      "(Iteration 5501 / 22950) loss: 2.301251\n",
      "(Iteration 5601 / 22950) loss: 2.300595\n",
      "(Iteration 5701 / 22950) loss: 2.300447\n",
      "(Iteration 5801 / 22950) loss: 2.302085\n",
      "(Iteration 5901 / 22950) loss: 2.301014\n",
      "(Iteration 6001 / 22950) loss: 2.300394\n",
      "(Iteration 6101 / 22950) loss: 2.300832\n",
      "(Epoch 8 / 30) train acc: 0.280000; val_acc: 0.274000\n",
      "(Iteration 6201 / 22950) loss: 2.300433\n",
      "(Iteration 6301 / 22950) loss: 2.301507\n",
      "(Iteration 6401 / 22950) loss: 2.300766\n",
      "(Iteration 6501 / 22950) loss: 2.300172\n",
      "(Iteration 6601 / 22950) loss: 2.300653\n",
      "(Iteration 6701 / 22950) loss: 2.300595\n",
      "(Iteration 6801 / 22950) loss: 2.300726\n",
      "(Epoch 9 / 30) train acc: 0.314000; val_acc: 0.310000\n",
      "(Iteration 6901 / 22950) loss: 2.300085\n",
      "(Iteration 7001 / 22950) loss: 2.300809\n",
      "(Iteration 7101 / 22950) loss: 2.299837\n",
      "(Iteration 7201 / 22950) loss: 2.299240\n",
      "(Iteration 7301 / 22950) loss: 2.300383\n",
      "(Iteration 7401 / 22950) loss: 2.300076\n",
      "(Iteration 7501 / 22950) loss: 2.299515\n",
      "(Iteration 7601 / 22950) loss: 2.300509\n",
      "(Epoch 10 / 30) train acc: 0.304000; val_acc: 0.294000\n",
      "(Iteration 7701 / 22950) loss: 2.299032\n",
      "(Iteration 7801 / 22950) loss: 2.298939\n",
      "(Iteration 7901 / 22950) loss: 2.299454\n",
      "(Iteration 8001 / 22950) loss: 2.299925\n",
      "(Iteration 8101 / 22950) loss: 2.299542\n",
      "(Iteration 8201 / 22950) loss: 2.298807\n",
      "(Iteration 8301 / 22950) loss: 2.299835\n",
      "(Iteration 8401 / 22950) loss: 2.298719\n",
      "(Epoch 11 / 30) train acc: 0.319000; val_acc: 0.295000\n",
      "(Iteration 8501 / 22950) loss: 2.298711\n",
      "(Iteration 8601 / 22950) loss: 2.299222\n",
      "(Iteration 8701 / 22950) loss: 2.298257\n",
      "(Iteration 8801 / 22950) loss: 2.298521\n",
      "(Iteration 8901 / 22950) loss: 2.297877\n",
      "(Iteration 9001 / 22950) loss: 2.299112\n",
      "(Iteration 9101 / 22950) loss: 2.298644\n",
      "(Epoch 12 / 30) train acc: 0.293000; val_acc: 0.289000\n",
      "(Iteration 9201 / 22950) loss: 2.297302\n",
      "(Iteration 9301 / 22950) loss: 2.297909\n",
      "(Iteration 9401 / 22950) loss: 2.296890\n",
      "(Iteration 9501 / 22950) loss: 2.299084\n",
      "(Iteration 9601 / 22950) loss: 2.296625\n",
      "(Iteration 9701 / 22950) loss: 2.296293\n",
      "(Iteration 9801 / 22950) loss: 2.298714\n",
      "(Iteration 9901 / 22950) loss: 2.298871\n",
      "(Epoch 13 / 30) train acc: 0.326000; val_acc: 0.307000\n",
      "(Iteration 10001 / 22950) loss: 2.298205\n",
      "(Iteration 10101 / 22950) loss: 2.298391\n",
      "(Iteration 10201 / 22950) loss: 2.298996\n",
      "(Iteration 10301 / 22950) loss: 2.297135\n",
      "(Iteration 10401 / 22950) loss: 2.298617\n",
      "(Iteration 10501 / 22950) loss: 2.296660\n",
      "(Iteration 10601 / 22950) loss: 2.296975\n",
      "(Iteration 10701 / 22950) loss: 2.295803\n",
      "(Epoch 14 / 30) train acc: 0.338000; val_acc: 0.310000\n",
      "(Iteration 10801 / 22950) loss: 2.298591\n",
      "(Iteration 10901 / 22950) loss: 2.296903\n",
      "(Iteration 11001 / 22950) loss: 2.298708\n",
      "(Iteration 11101 / 22950) loss: 2.297022\n",
      "(Iteration 11201 / 22950) loss: 2.297441\n",
      "(Iteration 11301 / 22950) loss: 2.297943\n",
      "(Iteration 11401 / 22950) loss: 2.298010\n",
      "(Epoch 15 / 30) train acc: 0.330000; val_acc: 0.309000\n",
      "(Iteration 11501 / 22950) loss: 2.297006\n",
      "(Iteration 11601 / 22950) loss: 2.296885\n",
      "(Iteration 11701 / 22950) loss: 2.295605\n",
      "(Iteration 11801 / 22950) loss: 2.297503\n",
      "(Iteration 11901 / 22950) loss: 2.296614\n",
      "(Iteration 12001 / 22950) loss: 2.295447\n",
      "(Iteration 12101 / 22950) loss: 2.294680\n",
      "(Iteration 12201 / 22950) loss: 2.296844\n",
      "(Epoch 16 / 30) train acc: 0.293000; val_acc: 0.304000\n",
      "(Iteration 12301 / 22950) loss: 2.295997\n",
      "(Iteration 12401 / 22950) loss: 2.298284\n",
      "(Iteration 12501 / 22950) loss: 2.294583\n",
      "(Iteration 12601 / 22950) loss: 2.293902\n",
      "(Iteration 12701 / 22950) loss: 2.296086\n",
      "(Iteration 12801 / 22950) loss: 2.295859\n",
      "(Iteration 12901 / 22950) loss: 2.297291\n",
      "(Iteration 13001 / 22950) loss: 2.295905\n",
      "(Epoch 17 / 30) train acc: 0.345000; val_acc: 0.300000\n",
      "(Iteration 13101 / 22950) loss: 2.295004\n",
      "(Iteration 13201 / 22950) loss: 2.294634\n",
      "(Iteration 13301 / 22950) loss: 2.293348\n",
      "(Iteration 13401 / 22950) loss: 2.296809\n",
      "(Iteration 13501 / 22950) loss: 2.293700\n",
      "(Iteration 13601 / 22950) loss: 2.295665\n",
      "(Iteration 13701 / 22950) loss: 2.295154\n",
      "(Epoch 18 / 30) train acc: 0.302000; val_acc: 0.294000\n",
      "(Iteration 13801 / 22950) loss: 2.294877\n",
      "(Iteration 13901 / 22950) loss: 2.295787\n",
      "(Iteration 14001 / 22950) loss: 2.291740\n",
      "(Iteration 14101 / 22950) loss: 2.295694\n",
      "(Iteration 14201 / 22950) loss: 2.295549\n",
      "(Iteration 14301 / 22950) loss: 2.295009\n",
      "(Iteration 14401 / 22950) loss: 2.294945\n",
      "(Iteration 14501 / 22950) loss: 2.293400\n",
      "(Epoch 19 / 30) train acc: 0.303000; val_acc: 0.287000\n",
      "(Iteration 14601 / 22950) loss: 2.294612\n",
      "(Iteration 14701 / 22950) loss: 2.292038\n",
      "(Iteration 14801 / 22950) loss: 2.292675\n",
      "(Iteration 14901 / 22950) loss: 2.294273\n",
      "(Iteration 15001 / 22950) loss: 2.294018\n",
      "(Iteration 15101 / 22950) loss: 2.294676\n",
      "(Iteration 15201 / 22950) loss: 2.291930\n",
      "(Epoch 20 / 30) train acc: 0.312000; val_acc: 0.287000\n",
      "(Iteration 15301 / 22950) loss: 2.291078\n",
      "(Iteration 15401 / 22950) loss: 2.294126\n",
      "(Iteration 15501 / 22950) loss: 2.295959\n",
      "(Iteration 15601 / 22950) loss: 2.295771\n",
      "(Iteration 15701 / 22950) loss: 2.293947\n",
      "(Iteration 15801 / 22950) loss: 2.292847\n",
      "(Iteration 15901 / 22950) loss: 2.290975\n",
      "(Iteration 16001 / 22950) loss: 2.291519\n",
      "(Epoch 21 / 30) train acc: 0.297000; val_acc: 0.287000\n",
      "(Iteration 16101 / 22950) loss: 2.292865\n",
      "(Iteration 16201 / 22950) loss: 2.296121\n",
      "(Iteration 16301 / 22950) loss: 2.295805\n",
      "(Iteration 16401 / 22950) loss: 2.295099\n",
      "(Iteration 16501 / 22950) loss: 2.288681\n",
      "(Iteration 16601 / 22950) loss: 2.294113\n",
      "(Iteration 16701 / 22950) loss: 2.292727\n",
      "(Iteration 16801 / 22950) loss: 2.290410\n",
      "(Epoch 22 / 30) train acc: 0.339000; val_acc: 0.287000\n",
      "(Iteration 16901 / 22950) loss: 2.292389\n",
      "(Iteration 17001 / 22950) loss: 2.292942\n",
      "(Iteration 17101 / 22950) loss: 2.288802\n",
      "(Iteration 17201 / 22950) loss: 2.292825\n",
      "(Iteration 17301 / 22950) loss: 2.290983\n",
      "(Iteration 17401 / 22950) loss: 2.290189\n",
      "(Iteration 17501 / 22950) loss: 2.293214\n",
      "(Epoch 23 / 30) train acc: 0.317000; val_acc: 0.290000\n",
      "(Iteration 17601 / 22950) loss: 2.289176\n",
      "(Iteration 17701 / 22950) loss: 2.291918\n",
      "(Iteration 17801 / 22950) loss: 2.294030\n",
      "(Iteration 17901 / 22950) loss: 2.290269\n",
      "(Iteration 18001 / 22950) loss: 2.290629\n",
      "(Iteration 18101 / 22950) loss: 2.289491\n",
      "(Iteration 18201 / 22950) loss: 2.288417\n",
      "(Iteration 18301 / 22950) loss: 2.290815\n",
      "(Epoch 24 / 30) train acc: 0.328000; val_acc: 0.294000\n",
      "(Iteration 18401 / 22950) loss: 2.291080\n",
      "(Iteration 18501 / 22950) loss: 2.291129\n",
      "(Iteration 18601 / 22950) loss: 2.292708\n",
      "(Iteration 18701 / 22950) loss: 2.291807\n",
      "(Iteration 18801 / 22950) loss: 2.291422\n",
      "(Iteration 18901 / 22950) loss: 2.291148\n",
      "(Iteration 19001 / 22950) loss: 2.293449\n",
      "(Iteration 19101 / 22950) loss: 2.292368\n",
      "(Epoch 25 / 30) train acc: 0.319000; val_acc: 0.291000\n",
      "(Iteration 19201 / 22950) loss: 2.296518\n",
      "(Iteration 19301 / 22950) loss: 2.292166\n",
      "(Iteration 19401 / 22950) loss: 2.287109\n",
      "(Iteration 19501 / 22950) loss: 2.289141\n",
      "(Iteration 19601 / 22950) loss: 2.294462\n",
      "(Iteration 19701 / 22950) loss: 2.291139\n",
      "(Iteration 19801 / 22950) loss: 2.288293\n",
      "(Epoch 26 / 30) train acc: 0.313000; val_acc: 0.294000\n",
      "(Iteration 19901 / 22950) loss: 2.291246\n",
      "(Iteration 20001 / 22950) loss: 2.291520\n",
      "(Iteration 20101 / 22950) loss: 2.290552\n",
      "(Iteration 20201 / 22950) loss: 2.283108\n",
      "(Iteration 20301 / 22950) loss: 2.282674\n",
      "(Iteration 20401 / 22950) loss: 2.287917\n",
      "(Iteration 20501 / 22950) loss: 2.287834\n",
      "(Iteration 20601 / 22950) loss: 2.290192\n",
      "(Epoch 27 / 30) train acc: 0.338000; val_acc: 0.293000\n",
      "(Iteration 20701 / 22950) loss: 2.292333\n",
      "(Iteration 20801 / 22950) loss: 2.287183\n",
      "(Iteration 20901 / 22950) loss: 2.289101\n",
      "(Iteration 21001 / 22950) loss: 2.288362\n",
      "(Iteration 21101 / 22950) loss: 2.290831\n",
      "(Iteration 21201 / 22950) loss: 2.283317\n",
      "(Iteration 21301 / 22950) loss: 2.285090\n",
      "(Iteration 21401 / 22950) loss: 2.289738\n",
      "(Epoch 28 / 30) train acc: 0.330000; val_acc: 0.290000\n",
      "(Iteration 21501 / 22950) loss: 2.287231\n",
      "(Iteration 21601 / 22950) loss: 2.286849\n",
      "(Iteration 21701 / 22950) loss: 2.289347\n",
      "(Iteration 21801 / 22950) loss: 2.287126\n",
      "(Iteration 21901 / 22950) loss: 2.289710\n",
      "(Iteration 22001 / 22950) loss: 2.291255\n",
      "(Iteration 22101 / 22950) loss: 2.291505\n",
      "(Epoch 29 / 30) train acc: 0.337000; val_acc: 0.293000\n",
      "(Iteration 22201 / 22950) loss: 2.288733\n",
      "(Iteration 22301 / 22950) loss: 2.293670\n",
      "(Iteration 22401 / 22950) loss: 2.289167\n",
      "(Iteration 22501 / 22950) loss: 2.286473\n",
      "(Iteration 22601 / 22950) loss: 2.288828\n",
      "(Iteration 22701 / 22950) loss: 2.289687\n",
      "(Iteration 22801 / 22950) loss: 2.289974\n",
      "(Iteration 22901 / 22950) loss: 2.289110\n",
      "(Epoch 30 / 30) train acc: 0.311000; val_acc: 0.292000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.001, 'num_epochs': 30, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 45930) loss: 2.303109\n",
      "(Epoch 0 / 30) train acc: 0.101000; val_acc: 0.093000\n",
      "(Iteration 101 / 45930) loss: 2.303015\n",
      "(Iteration 201 / 45930) loss: 2.303100\n",
      "(Iteration 301 / 45930) loss: 2.302735\n",
      "(Iteration 401 / 45930) loss: 2.303098\n",
      "(Iteration 501 / 45930) loss: 2.303454\n",
      "(Iteration 601 / 45930) loss: 2.303302\n",
      "(Iteration 701 / 45930) loss: 2.303022\n",
      "(Iteration 801 / 45930) loss: 2.303400\n",
      "(Iteration 901 / 45930) loss: 2.302574\n",
      "(Iteration 1001 / 45930) loss: 2.303017\n",
      "(Iteration 1101 / 45930) loss: 2.302837\n",
      "(Iteration 1201 / 45930) loss: 2.302724\n",
      "(Iteration 1301 / 45930) loss: 2.302951\n",
      "(Iteration 1401 / 45930) loss: 2.302994\n",
      "(Iteration 1501 / 45930) loss: 2.302678\n",
      "(Epoch 1 / 30) train acc: 0.123000; val_acc: 0.141000\n",
      "(Iteration 1601 / 45930) loss: 2.302959\n",
      "(Iteration 1701 / 45930) loss: 2.302526\n",
      "(Iteration 1801 / 45930) loss: 2.302655\n",
      "(Iteration 1901 / 45930) loss: 2.302900\n",
      "(Iteration 2001 / 45930) loss: 2.302598\n",
      "(Iteration 2101 / 45930) loss: 2.302247\n",
      "(Iteration 2201 / 45930) loss: 2.302539\n",
      "(Iteration 2301 / 45930) loss: 2.302535\n",
      "(Iteration 2401 / 45930) loss: 2.302551\n",
      "(Iteration 2501 / 45930) loss: 2.302056\n",
      "(Iteration 2601 / 45930) loss: 2.302447\n",
      "(Iteration 2701 / 45930) loss: 2.302755\n",
      "(Iteration 2801 / 45930) loss: 2.301557\n",
      "(Iteration 2901 / 45930) loss: 2.302161\n",
      "(Iteration 3001 / 45930) loss: 2.302193\n",
      "(Epoch 2 / 30) train acc: 0.164000; val_acc: 0.154000\n",
      "(Iteration 3101 / 45930) loss: 2.302352\n",
      "(Iteration 3201 / 45930) loss: 2.302334\n",
      "(Iteration 3301 / 45930) loss: 2.302146\n",
      "(Iteration 3401 / 45930) loss: 2.301180\n",
      "(Iteration 3501 / 45930) loss: 2.301305\n",
      "(Iteration 3601 / 45930) loss: 2.301887\n",
      "(Iteration 3701 / 45930) loss: 2.301290\n",
      "(Iteration 3801 / 45930) loss: 2.301456\n",
      "(Iteration 3901 / 45930) loss: 2.301061\n",
      "(Iteration 4001 / 45930) loss: 2.302117\n",
      "(Iteration 4101 / 45930) loss: 2.300846\n",
      "(Iteration 4201 / 45930) loss: 2.301236\n",
      "(Iteration 4301 / 45930) loss: 2.302211\n",
      "(Iteration 4401 / 45930) loss: 2.300424\n",
      "(Iteration 4501 / 45930) loss: 2.301700\n",
      "(Epoch 3 / 30) train acc: 0.224000; val_acc: 0.232000\n",
      "(Iteration 4601 / 45930) loss: 2.301129\n",
      "(Iteration 4701 / 45930) loss: 2.300901\n",
      "(Iteration 4801 / 45930) loss: 2.300948\n",
      "(Iteration 4901 / 45930) loss: 2.300721\n",
      "(Iteration 5001 / 45930) loss: 2.300067\n",
      "(Iteration 5101 / 45930) loss: 2.301381\n",
      "(Iteration 5201 / 45930) loss: 2.299022\n",
      "(Iteration 5301 / 45930) loss: 2.299658\n",
      "(Iteration 5401 / 45930) loss: 2.299234\n",
      "(Iteration 5501 / 45930) loss: 2.299967\n",
      "(Iteration 5601 / 45930) loss: 2.299342\n",
      "(Iteration 5701 / 45930) loss: 2.299794\n",
      "(Iteration 5801 / 45930) loss: 2.299965\n",
      "(Iteration 5901 / 45930) loss: 2.299269\n",
      "(Iteration 6001 / 45930) loss: 2.299283\n",
      "(Iteration 6101 / 45930) loss: 2.298979\n",
      "(Epoch 4 / 30) train acc: 0.260000; val_acc: 0.260000\n",
      "(Iteration 6201 / 45930) loss: 2.298745\n",
      "(Iteration 6301 / 45930) loss: 2.299164\n",
      "(Iteration 6401 / 45930) loss: 2.299592\n",
      "(Iteration 6501 / 45930) loss: 2.296664\n",
      "(Iteration 6601 / 45930) loss: 2.296569\n",
      "(Iteration 6701 / 45930) loss: 2.299847\n",
      "(Iteration 6801 / 45930) loss: 2.296815\n",
      "(Iteration 6901 / 45930) loss: 2.298913\n",
      "(Iteration 7001 / 45930) loss: 2.295789\n",
      "(Iteration 7101 / 45930) loss: 2.297468\n",
      "(Iteration 7201 / 45930) loss: 2.298772\n",
      "(Iteration 7301 / 45930) loss: 2.299382\n",
      "(Iteration 7401 / 45930) loss: 2.295235\n",
      "(Iteration 7501 / 45930) loss: 2.295606\n",
      "(Iteration 7601 / 45930) loss: 2.294014\n",
      "(Epoch 5 / 30) train acc: 0.295000; val_acc: 0.301000\n",
      "(Iteration 7701 / 45930) loss: 2.294070\n",
      "(Iteration 7801 / 45930) loss: 2.296262\n",
      "(Iteration 7901 / 45930) loss: 2.296150\n",
      "(Iteration 8001 / 45930) loss: 2.294546\n",
      "(Iteration 8101 / 45930) loss: 2.288391\n",
      "(Iteration 8201 / 45930) loss: 2.295263\n",
      "(Iteration 8301 / 45930) loss: 2.289066\n",
      "(Iteration 8401 / 45930) loss: 2.292136\n",
      "(Iteration 8501 / 45930) loss: 2.296839\n",
      "(Iteration 8601 / 45930) loss: 2.288257\n",
      "(Iteration 8701 / 45930) loss: 2.293218\n",
      "(Iteration 8801 / 45930) loss: 2.291234\n",
      "(Iteration 8901 / 45930) loss: 2.292978\n",
      "(Iteration 9001 / 45930) loss: 2.286964\n",
      "(Iteration 9101 / 45930) loss: 2.287250\n",
      "(Epoch 6 / 30) train acc: 0.311000; val_acc: 0.294000\n",
      "(Iteration 9201 / 45930) loss: 2.291115\n",
      "(Iteration 9301 / 45930) loss: 2.289851\n",
      "(Iteration 9401 / 45930) loss: 2.294146\n",
      "(Iteration 9501 / 45930) loss: 2.279394\n",
      "(Iteration 9601 / 45930) loss: 2.285260\n",
      "(Iteration 9701 / 45930) loss: 2.287976\n",
      "(Iteration 9801 / 45930) loss: 2.282900\n",
      "(Iteration 9901 / 45930) loss: 2.287395\n",
      "(Iteration 10001 / 45930) loss: 2.288198\n",
      "(Iteration 10101 / 45930) loss: 2.283688\n",
      "(Iteration 10201 / 45930) loss: 2.288443\n",
      "(Iteration 10301 / 45930) loss: 2.278301\n",
      "(Iteration 10401 / 45930) loss: 2.291230\n",
      "(Iteration 10501 / 45930) loss: 2.285069\n",
      "(Iteration 10601 / 45930) loss: 2.286432\n",
      "(Iteration 10701 / 45930) loss: 2.284771\n",
      "(Epoch 7 / 30) train acc: 0.283000; val_acc: 0.280000\n",
      "(Iteration 10801 / 45930) loss: 2.285176\n",
      "(Iteration 10901 / 45930) loss: 2.280293\n",
      "(Iteration 11001 / 45930) loss: 2.276017\n",
      "(Iteration 11101 / 45930) loss: 2.279280\n",
      "(Iteration 11201 / 45930) loss: 2.267309\n",
      "(Iteration 11301 / 45930) loss: 2.272299\n",
      "(Iteration 11401 / 45930) loss: 2.277097\n",
      "(Iteration 11501 / 45930) loss: 2.279010\n",
      "(Iteration 11601 / 45930) loss: 2.286205\n",
      "(Iteration 11701 / 45930) loss: 2.275044\n",
      "(Iteration 11801 / 45930) loss: 2.262193\n",
      "(Iteration 11901 / 45930) loss: 2.281886\n",
      "(Iteration 12001 / 45930) loss: 2.264989\n",
      "(Iteration 12101 / 45930) loss: 2.276312\n",
      "(Iteration 12201 / 45930) loss: 2.260152\n",
      "(Epoch 8 / 30) train acc: 0.237000; val_acc: 0.274000\n",
      "(Iteration 12301 / 45930) loss: 2.281392\n",
      "(Iteration 12401 / 45930) loss: 2.272904\n",
      "(Iteration 12501 / 45930) loss: 2.268571\n",
      "(Iteration 12601 / 45930) loss: 2.283191\n",
      "(Iteration 12701 / 45930) loss: 2.279252\n",
      "(Iteration 12801 / 45930) loss: 2.263146\n",
      "(Iteration 12901 / 45930) loss: 2.261548\n",
      "(Iteration 13001 / 45930) loss: 2.245695\n",
      "(Iteration 13101 / 45930) loss: 2.259488\n",
      "(Iteration 13201 / 45930) loss: 2.282018\n",
      "(Iteration 13301 / 45930) loss: 2.240147\n",
      "(Iteration 13401 / 45930) loss: 2.252424\n",
      "(Iteration 13501 / 45930) loss: 2.259279\n",
      "(Iteration 13601 / 45930) loss: 2.279055\n",
      "(Iteration 13701 / 45930) loss: 2.242837\n",
      "(Epoch 9 / 30) train acc: 0.251000; val_acc: 0.268000\n",
      "(Iteration 13801 / 45930) loss: 2.264871\n",
      "(Iteration 13901 / 45930) loss: 2.258328\n",
      "(Iteration 14001 / 45930) loss: 2.214607\n",
      "(Iteration 14101 / 45930) loss: 2.246698\n",
      "(Iteration 14201 / 45930) loss: 2.258440\n",
      "(Iteration 14301 / 45930) loss: 2.278505\n",
      "(Iteration 14401 / 45930) loss: 2.228094\n",
      "(Iteration 14501 / 45930) loss: 2.252745\n",
      "(Iteration 14601 / 45930) loss: 2.256005\n",
      "(Iteration 14701 / 45930) loss: 2.225735\n",
      "(Iteration 14801 / 45930) loss: 2.252357\n",
      "(Iteration 14901 / 45930) loss: 2.232965\n",
      "(Iteration 15001 / 45930) loss: 2.223721\n",
      "(Iteration 15101 / 45930) loss: 2.253222\n",
      "(Iteration 15201 / 45930) loss: 2.229101\n",
      "(Iteration 15301 / 45930) loss: 2.231049\n",
      "(Epoch 10 / 30) train acc: 0.245000; val_acc: 0.262000\n",
      "(Iteration 15401 / 45930) loss: 2.222573\n",
      "(Iteration 15501 / 45930) loss: 2.197805\n",
      "(Iteration 15601 / 45930) loss: 2.201286\n",
      "(Iteration 15701 / 45930) loss: 2.247453\n",
      "(Iteration 15801 / 45930) loss: 2.235374\n",
      "(Iteration 15901 / 45930) loss: 2.250381\n",
      "(Iteration 16001 / 45930) loss: 2.218458\n",
      "(Iteration 16101 / 45930) loss: 2.202152\n",
      "(Iteration 16201 / 45930) loss: 2.206141\n",
      "(Iteration 16301 / 45930) loss: 2.241707\n",
      "(Iteration 16401 / 45930) loss: 2.224752\n",
      "(Iteration 16501 / 45930) loss: 2.215497\n",
      "(Iteration 16601 / 45930) loss: 2.186261\n",
      "(Iteration 16701 / 45930) loss: 2.197082\n",
      "(Iteration 16801 / 45930) loss: 2.240880\n",
      "(Epoch 11 / 30) train acc: 0.239000; val_acc: 0.253000\n",
      "(Iteration 16901 / 45930) loss: 2.209827\n",
      "(Iteration 17001 / 45930) loss: 2.231271\n",
      "(Iteration 17101 / 45930) loss: 2.183536\n",
      "(Iteration 17201 / 45930) loss: 2.170220\n",
      "(Iteration 17301 / 45930) loss: 2.178132\n",
      "(Iteration 17401 / 45930) loss: 2.221647\n",
      "(Iteration 17501 / 45930) loss: 2.164463\n",
      "(Iteration 17601 / 45930) loss: 2.194745\n",
      "(Iteration 17701 / 45930) loss: 2.188721\n",
      "(Iteration 17801 / 45930) loss: 2.262669\n",
      "(Iteration 17901 / 45930) loss: 2.193194\n",
      "(Iteration 18001 / 45930) loss: 2.197141\n",
      "(Iteration 18101 / 45930) loss: 2.169716\n",
      "(Iteration 18201 / 45930) loss: 2.137463\n",
      "(Iteration 18301 / 45930) loss: 2.204427\n",
      "(Epoch 12 / 30) train acc: 0.225000; val_acc: 0.256000\n",
      "(Iteration 18401 / 45930) loss: 2.218090\n",
      "(Iteration 18501 / 45930) loss: 2.233034\n",
      "(Iteration 18601 / 45930) loss: 2.245137\n",
      "(Iteration 18701 / 45930) loss: 2.169967\n",
      "(Iteration 18801 / 45930) loss: 2.176370\n",
      "(Iteration 18901 / 45930) loss: 2.203307\n",
      "(Iteration 19001 / 45930) loss: 2.220132\n",
      "(Iteration 19101 / 45930) loss: 2.157505\n",
      "(Iteration 19201 / 45930) loss: 2.201816\n",
      "(Iteration 19301 / 45930) loss: 2.181474\n",
      "(Iteration 19401 / 45930) loss: 2.173825\n",
      "(Iteration 19501 / 45930) loss: 2.141377\n",
      "(Iteration 19601 / 45930) loss: 2.238327\n",
      "(Iteration 19701 / 45930) loss: 2.178966\n",
      "(Iteration 19801 / 45930) loss: 2.203140\n",
      "(Iteration 19901 / 45930) loss: 2.152457\n",
      "(Epoch 13 / 30) train acc: 0.225000; val_acc: 0.255000\n",
      "(Iteration 20001 / 45930) loss: 2.153874\n",
      "(Iteration 20101 / 45930) loss: 2.211076\n",
      "(Iteration 20201 / 45930) loss: 2.146902\n",
      "(Iteration 20301 / 45930) loss: 2.205645\n",
      "(Iteration 20401 / 45930) loss: 2.114032\n",
      "(Iteration 20501 / 45930) loss: 2.228611\n",
      "(Iteration 20601 / 45930) loss: 2.183116\n",
      "(Iteration 20701 / 45930) loss: 2.151832\n",
      "(Iteration 20801 / 45930) loss: 2.068612\n",
      "(Iteration 20901 / 45930) loss: 2.212276\n",
      "(Iteration 21001 / 45930) loss: 2.129413\n",
      "(Iteration 21101 / 45930) loss: 2.134920\n",
      "(Iteration 21201 / 45930) loss: 2.120539\n",
      "(Iteration 21301 / 45930) loss: 2.064055\n",
      "(Iteration 21401 / 45930) loss: 2.038413\n",
      "(Epoch 14 / 30) train acc: 0.230000; val_acc: 0.259000\n",
      "(Iteration 21501 / 45930) loss: 2.115744\n",
      "(Iteration 21601 / 45930) loss: 2.122087\n",
      "(Iteration 21701 / 45930) loss: 2.194732\n",
      "(Iteration 21801 / 45930) loss: 2.110546\n",
      "(Iteration 21901 / 45930) loss: 2.123465\n",
      "(Iteration 22001 / 45930) loss: 2.141304\n",
      "(Iteration 22101 / 45930) loss: 2.039503\n",
      "(Iteration 22201 / 45930) loss: 2.146517\n",
      "(Iteration 22301 / 45930) loss: 2.053031\n",
      "(Iteration 22401 / 45930) loss: 2.034569\n",
      "(Iteration 22501 / 45930) loss: 2.133124\n",
      "(Iteration 22601 / 45930) loss: 2.225248\n",
      "(Iteration 22701 / 45930) loss: 2.130499\n",
      "(Iteration 22801 / 45930) loss: 2.065384\n",
      "(Iteration 22901 / 45930) loss: 2.159483\n",
      "(Epoch 15 / 30) train acc: 0.245000; val_acc: 0.261000\n",
      "(Iteration 23001 / 45930) loss: 2.149492\n",
      "(Iteration 23101 / 45930) loss: 2.150825\n",
      "(Iteration 23201 / 45930) loss: 2.045435\n",
      "(Iteration 23301 / 45930) loss: 2.103199\n",
      "(Iteration 23401 / 45930) loss: 2.113229\n",
      "(Iteration 23501 / 45930) loss: 2.034158\n",
      "(Iteration 23601 / 45930) loss: 2.052145\n",
      "(Iteration 23701 / 45930) loss: 2.165457\n",
      "(Iteration 23801 / 45930) loss: 2.101480\n",
      "(Iteration 23901 / 45930) loss: 2.157696\n",
      "(Iteration 24001 / 45930) loss: 2.102403\n",
      "(Iteration 24101 / 45930) loss: 2.169465\n",
      "(Iteration 24201 / 45930) loss: 2.009866\n",
      "(Iteration 24301 / 45930) loss: 2.124327\n",
      "(Iteration 24401 / 45930) loss: 1.928555\n",
      "(Epoch 16 / 30) train acc: 0.263000; val_acc: 0.262000\n",
      "(Iteration 24501 / 45930) loss: 2.038204\n",
      "(Iteration 24601 / 45930) loss: 2.149155\n",
      "(Iteration 24701 / 45930) loss: 2.059448\n",
      "(Iteration 24801 / 45930) loss: 2.198601\n",
      "(Iteration 24901 / 45930) loss: 2.199801\n",
      "(Iteration 25001 / 45930) loss: 2.048674\n",
      "(Iteration 25101 / 45930) loss: 2.058230\n",
      "(Iteration 25201 / 45930) loss: 2.086520\n",
      "(Iteration 25301 / 45930) loss: 2.062380\n",
      "(Iteration 25401 / 45930) loss: 2.037347\n",
      "(Iteration 25501 / 45930) loss: 2.012008\n",
      "(Iteration 25601 / 45930) loss: 2.074185\n",
      "(Iteration 25701 / 45930) loss: 2.014120\n",
      "(Iteration 25801 / 45930) loss: 2.097585\n",
      "(Iteration 25901 / 45930) loss: 2.115852\n",
      "(Iteration 26001 / 45930) loss: 2.014218\n",
      "(Epoch 17 / 30) train acc: 0.252000; val_acc: 0.263000\n",
      "(Iteration 26101 / 45930) loss: 2.136436\n",
      "(Iteration 26201 / 45930) loss: 2.086852\n",
      "(Iteration 26301 / 45930) loss: 2.015694\n",
      "(Iteration 26401 / 45930) loss: 2.074193\n",
      "(Iteration 26501 / 45930) loss: 2.123441\n",
      "(Iteration 26601 / 45930) loss: 2.160391\n",
      "(Iteration 26701 / 45930) loss: 2.124024\n",
      "(Iteration 26801 / 45930) loss: 2.176983\n",
      "(Iteration 26901 / 45930) loss: 2.118498\n",
      "(Iteration 27001 / 45930) loss: 2.184753\n",
      "(Iteration 27101 / 45930) loss: 2.044068\n",
      "(Iteration 27201 / 45930) loss: 2.139490\n",
      "(Iteration 27301 / 45930) loss: 2.118118\n",
      "(Iteration 27401 / 45930) loss: 2.066964\n",
      "(Iteration 27501 / 45930) loss: 2.087814\n",
      "(Epoch 18 / 30) train acc: 0.265000; val_acc: 0.267000\n",
      "(Iteration 27601 / 45930) loss: 2.148602\n",
      "(Iteration 27701 / 45930) loss: 2.010061\n",
      "(Iteration 27801 / 45930) loss: 2.130505\n",
      "(Iteration 27901 / 45930) loss: 2.056246\n",
      "(Iteration 28001 / 45930) loss: 2.060679\n",
      "(Iteration 28101 / 45930) loss: 2.014756\n",
      "(Iteration 28201 / 45930) loss: 2.030894\n",
      "(Iteration 28301 / 45930) loss: 2.008798\n",
      "(Iteration 28401 / 45930) loss: 2.100445\n",
      "(Iteration 28501 / 45930) loss: 2.009346\n",
      "(Iteration 28601 / 45930) loss: 2.058025\n",
      "(Iteration 28701 / 45930) loss: 2.164173\n",
      "(Iteration 28801 / 45930) loss: 2.018334\n",
      "(Iteration 28901 / 45930) loss: 2.083701\n",
      "(Iteration 29001 / 45930) loss: 2.237818\n",
      "(Epoch 19 / 30) train acc: 0.263000; val_acc: 0.269000\n",
      "(Iteration 29101 / 45930) loss: 2.020035\n",
      "(Iteration 29201 / 45930) loss: 1.989500\n",
      "(Iteration 29301 / 45930) loss: 2.136174\n",
      "(Iteration 29401 / 45930) loss: 2.050589\n",
      "(Iteration 29501 / 45930) loss: 1.987355\n",
      "(Iteration 29601 / 45930) loss: 2.199922\n",
      "(Iteration 29701 / 45930) loss: 2.059223\n",
      "(Iteration 29801 / 45930) loss: 2.054438\n",
      "(Iteration 29901 / 45930) loss: 2.163148\n",
      "(Iteration 30001 / 45930) loss: 2.095055\n",
      "(Iteration 30101 / 45930) loss: 2.173968\n",
      "(Iteration 30201 / 45930) loss: 2.033029\n",
      "(Iteration 30301 / 45930) loss: 2.071414\n",
      "(Iteration 30401 / 45930) loss: 2.062415\n",
      "(Iteration 30501 / 45930) loss: 1.975240\n",
      "(Iteration 30601 / 45930) loss: 1.980906\n",
      "(Epoch 20 / 30) train acc: 0.295000; val_acc: 0.273000\n",
      "(Iteration 30701 / 45930) loss: 1.988252\n",
      "(Iteration 30801 / 45930) loss: 2.014603\n",
      "(Iteration 30901 / 45930) loss: 2.050573\n",
      "(Iteration 31001 / 45930) loss: 2.044874\n",
      "(Iteration 31101 / 45930) loss: 2.045102\n",
      "(Iteration 31201 / 45930) loss: 2.067849\n",
      "(Iteration 31301 / 45930) loss: 2.039618\n",
      "(Iteration 31401 / 45930) loss: 2.083053\n",
      "(Iteration 31501 / 45930) loss: 1.884775\n",
      "(Iteration 31601 / 45930) loss: 2.128844\n",
      "(Iteration 31701 / 45930) loss: 2.114235\n",
      "(Iteration 31801 / 45930) loss: 2.031917\n",
      "(Iteration 31901 / 45930) loss: 1.969949\n",
      "(Iteration 32001 / 45930) loss: 1.952322\n",
      "(Iteration 32101 / 45930) loss: 2.057723\n",
      "(Epoch 21 / 30) train acc: 0.255000; val_acc: 0.275000\n",
      "(Iteration 32201 / 45930) loss: 2.081725\n",
      "(Iteration 32301 / 45930) loss: 1.967917\n",
      "(Iteration 32401 / 45930) loss: 2.110196\n",
      "(Iteration 32501 / 45930) loss: 2.038566\n",
      "(Iteration 32601 / 45930) loss: 2.090918\n",
      "(Iteration 32701 / 45930) loss: 2.018336\n",
      "(Iteration 32801 / 45930) loss: 2.021411\n",
      "(Iteration 32901 / 45930) loss: 1.876943\n",
      "(Iteration 33001 / 45930) loss: 2.091829\n",
      "(Iteration 33101 / 45930) loss: 1.972088\n",
      "(Iteration 33201 / 45930) loss: 2.047243\n",
      "(Iteration 33301 / 45930) loss: 1.970335\n",
      "(Iteration 33401 / 45930) loss: 1.942539\n",
      "(Iteration 33501 / 45930) loss: 2.031252\n",
      "(Iteration 33601 / 45930) loss: 2.055280\n",
      "(Epoch 22 / 30) train acc: 0.260000; val_acc: 0.280000\n",
      "(Iteration 33701 / 45930) loss: 2.004372\n",
      "(Iteration 33801 / 45930) loss: 2.004584\n",
      "(Iteration 33901 / 45930) loss: 2.061135\n",
      "(Iteration 34001 / 45930) loss: 1.999190\n",
      "(Iteration 34101 / 45930) loss: 2.057716\n",
      "(Iteration 34201 / 45930) loss: 2.061774\n",
      "(Iteration 34301 / 45930) loss: 1.997699\n",
      "(Iteration 34401 / 45930) loss: 2.018081\n",
      "(Iteration 34501 / 45930) loss: 2.042065\n",
      "(Iteration 34601 / 45930) loss: 1.929803\n",
      "(Iteration 34701 / 45930) loss: 1.881661\n",
      "(Iteration 34801 / 45930) loss: 2.199648\n",
      "(Iteration 34901 / 45930) loss: 1.994715\n",
      "(Iteration 35001 / 45930) loss: 1.903087\n",
      "(Iteration 35101 / 45930) loss: 2.016025\n",
      "(Iteration 35201 / 45930) loss: 2.014284\n",
      "(Epoch 23 / 30) train acc: 0.270000; val_acc: 0.281000\n",
      "(Iteration 35301 / 45930) loss: 1.958012\n",
      "(Iteration 35401 / 45930) loss: 2.038356\n",
      "(Iteration 35501 / 45930) loss: 2.019793\n",
      "(Iteration 35601 / 45930) loss: 1.992060\n",
      "(Iteration 35701 / 45930) loss: 2.143435\n",
      "(Iteration 35801 / 45930) loss: 2.051026\n",
      "(Iteration 35901 / 45930) loss: 1.922819\n",
      "(Iteration 36001 / 45930) loss: 2.001046\n",
      "(Iteration 36101 / 45930) loss: 2.147111\n",
      "(Iteration 36201 / 45930) loss: 2.038794\n",
      "(Iteration 36301 / 45930) loss: 2.017036\n",
      "(Iteration 36401 / 45930) loss: 2.055567\n",
      "(Iteration 36501 / 45930) loss: 1.880929\n",
      "(Iteration 36601 / 45930) loss: 2.035780\n",
      "(Iteration 36701 / 45930) loss: 2.077270\n",
      "(Epoch 24 / 30) train acc: 0.263000; val_acc: 0.285000\n",
      "(Iteration 36801 / 45930) loss: 1.912084\n",
      "(Iteration 36901 / 45930) loss: 2.017540\n",
      "(Iteration 37001 / 45930) loss: 1.943319\n",
      "(Iteration 37101 / 45930) loss: 2.097962\n",
      "(Iteration 37201 / 45930) loss: 2.142485\n",
      "(Iteration 37301 / 45930) loss: 2.050394\n",
      "(Iteration 37401 / 45930) loss: 1.967866\n",
      "(Iteration 37501 / 45930) loss: 1.953900\n",
      "(Iteration 37601 / 45930) loss: 2.112747\n",
      "(Iteration 37701 / 45930) loss: 2.030478\n",
      "(Iteration 37801 / 45930) loss: 2.039687\n",
      "(Iteration 37901 / 45930) loss: 1.962509\n",
      "(Iteration 38001 / 45930) loss: 1.909662\n",
      "(Iteration 38101 / 45930) loss: 1.936804\n",
      "(Iteration 38201 / 45930) loss: 2.003021\n",
      "(Epoch 25 / 30) train acc: 0.281000; val_acc: 0.284000\n",
      "(Iteration 38301 / 45930) loss: 2.169562\n",
      "(Iteration 38401 / 45930) loss: 2.092172\n",
      "(Iteration 38501 / 45930) loss: 1.929035\n",
      "(Iteration 38601 / 45930) loss: 2.014847\n",
      "(Iteration 38701 / 45930) loss: 1.969079\n",
      "(Iteration 38801 / 45930) loss: 1.964774\n",
      "(Iteration 38901 / 45930) loss: 1.999423\n",
      "(Iteration 39001 / 45930) loss: 2.008297\n",
      "(Iteration 39101 / 45930) loss: 2.133112\n",
      "(Iteration 39201 / 45930) loss: 1.948160\n",
      "(Iteration 39301 / 45930) loss: 1.984201\n",
      "(Iteration 39401 / 45930) loss: 2.083724\n",
      "(Iteration 39501 / 45930) loss: 2.150261\n",
      "(Iteration 39601 / 45930) loss: 2.175205\n",
      "(Iteration 39701 / 45930) loss: 1.980970\n",
      "(Iteration 39801 / 45930) loss: 2.039693\n",
      "(Epoch 26 / 30) train acc: 0.250000; val_acc: 0.287000\n",
      "(Iteration 39901 / 45930) loss: 1.864079\n",
      "(Iteration 40001 / 45930) loss: 1.991918\n",
      "(Iteration 40101 / 45930) loss: 1.994331\n",
      "(Iteration 40201 / 45930) loss: 2.066781\n",
      "(Iteration 40301 / 45930) loss: 2.035036\n",
      "(Iteration 40401 / 45930) loss: 2.132366\n",
      "(Iteration 40501 / 45930) loss: 2.162089\n",
      "(Iteration 40601 / 45930) loss: 1.972600\n",
      "(Iteration 40701 / 45930) loss: 1.975438\n",
      "(Iteration 40801 / 45930) loss: 2.038146\n",
      "(Iteration 40901 / 45930) loss: 2.102446\n",
      "(Iteration 41001 / 45930) loss: 2.050102\n",
      "(Iteration 41101 / 45930) loss: 2.093032\n",
      "(Iteration 41201 / 45930) loss: 2.096581\n",
      "(Iteration 41301 / 45930) loss: 1.947164\n",
      "(Epoch 27 / 30) train acc: 0.276000; val_acc: 0.292000\n",
      "(Iteration 41401 / 45930) loss: 2.055293\n",
      "(Iteration 41501 / 45930) loss: 2.170220\n",
      "(Iteration 41601 / 45930) loss: 1.971288\n",
      "(Iteration 41701 / 45930) loss: 1.941839\n",
      "(Iteration 41801 / 45930) loss: 2.151704\n",
      "(Iteration 41901 / 45930) loss: 2.061689\n",
      "(Iteration 42001 / 45930) loss: 1.941608\n",
      "(Iteration 42101 / 45930) loss: 1.941240\n",
      "(Iteration 42201 / 45930) loss: 2.060041\n",
      "(Iteration 42301 / 45930) loss: 2.002375\n",
      "(Iteration 42401 / 45930) loss: 2.060329\n",
      "(Iteration 42501 / 45930) loss: 1.874940\n",
      "(Iteration 42601 / 45930) loss: 1.818679\n",
      "(Iteration 42701 / 45930) loss: 2.118162\n",
      "(Iteration 42801 / 45930) loss: 1.985027\n",
      "(Epoch 28 / 30) train acc: 0.265000; val_acc: 0.291000\n",
      "(Iteration 42901 / 45930) loss: 2.039597\n",
      "(Iteration 43001 / 45930) loss: 2.054047\n",
      "(Iteration 43101 / 45930) loss: 1.846968\n",
      "(Iteration 43201 / 45930) loss: 2.035406\n",
      "(Iteration 43301 / 45930) loss: 2.161181\n",
      "(Iteration 43401 / 45930) loss: 2.105014\n",
      "(Iteration 43501 / 45930) loss: 1.999723\n",
      "(Iteration 43601 / 45930) loss: 1.997038\n",
      "(Iteration 43701 / 45930) loss: 1.946881\n",
      "(Iteration 43801 / 45930) loss: 2.059774\n",
      "(Iteration 43901 / 45930) loss: 2.001430\n",
      "(Iteration 44001 / 45930) loss: 2.062955\n",
      "(Iteration 44101 / 45930) loss: 2.191146\n",
      "(Iteration 44201 / 45930) loss: 1.899994\n",
      "(Iteration 44301 / 45930) loss: 2.194852\n",
      "(Epoch 29 / 30) train acc: 0.291000; val_acc: 0.292000\n",
      "(Iteration 44401 / 45930) loss: 2.083437\n",
      "(Iteration 44501 / 45930) loss: 2.046025\n",
      "(Iteration 44601 / 45930) loss: 2.096277\n",
      "(Iteration 44701 / 45930) loss: 2.002870\n",
      "(Iteration 44801 / 45930) loss: 2.034583\n",
      "(Iteration 44901 / 45930) loss: 1.964535\n",
      "(Iteration 45001 / 45930) loss: 2.066855\n",
      "(Iteration 45101 / 45930) loss: 2.072073\n",
      "(Iteration 45201 / 45930) loss: 2.034939\n",
      "(Iteration 45301 / 45930) loss: 1.936245\n",
      "(Iteration 45401 / 45930) loss: 1.968042\n",
      "(Iteration 45501 / 45930) loss: 1.933944\n",
      "(Iteration 45601 / 45930) loss: 1.918492\n",
      "(Iteration 45701 / 45930) loss: 2.119210\n",
      "(Iteration 45801 / 45930) loss: 2.139490\n",
      "(Iteration 45901 / 45930) loss: 1.979913\n",
      "(Epoch 30 / 30) train acc: 0.272000; val_acc: 0.294000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.001, 'num_epochs': 40, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 30600) loss: 2.308360\n",
      "(Epoch 0 / 40) train acc: 0.069000; val_acc: 0.078000\n",
      "(Iteration 101 / 30600) loss: 2.308152\n",
      "(Iteration 201 / 30600) loss: 2.308011\n",
      "(Iteration 301 / 30600) loss: 2.307902\n",
      "(Iteration 401 / 30600) loss: 2.307893\n",
      "(Iteration 501 / 30600) loss: 2.307539\n",
      "(Iteration 601 / 30600) loss: 2.307711\n",
      "(Iteration 701 / 30600) loss: 2.307455\n",
      "(Epoch 1 / 40) train acc: 0.147000; val_acc: 0.153000\n",
      "(Iteration 801 / 30600) loss: 2.307444\n",
      "(Iteration 901 / 30600) loss: 2.307122\n",
      "(Iteration 1001 / 30600) loss: 2.307429\n",
      "(Iteration 1101 / 30600) loss: 2.307112\n",
      "(Iteration 1201 / 30600) loss: 2.306940\n",
      "(Iteration 1301 / 30600) loss: 2.306835\n",
      "(Iteration 1401 / 30600) loss: 2.307102\n",
      "(Iteration 1501 / 30600) loss: 2.306689\n",
      "(Epoch 2 / 40) train acc: 0.151000; val_acc: 0.169000\n",
      "(Iteration 1601 / 30600) loss: 2.306790\n",
      "(Iteration 1701 / 30600) loss: 2.306518\n",
      "(Iteration 1801 / 30600) loss: 2.306406\n",
      "(Iteration 1901 / 30600) loss: 2.306391\n",
      "(Iteration 2001 / 30600) loss: 2.306501\n",
      "(Iteration 2101 / 30600) loss: 2.306145\n",
      "(Iteration 2201 / 30600) loss: 2.306034\n",
      "(Epoch 3 / 40) train acc: 0.199000; val_acc: 0.196000\n",
      "(Iteration 2301 / 30600) loss: 2.306189\n",
      "(Iteration 2401 / 30600) loss: 2.306034\n",
      "(Iteration 2501 / 30600) loss: 2.305861\n",
      "(Iteration 2601 / 30600) loss: 2.306090\n",
      "(Iteration 2701 / 30600) loss: 2.306076\n",
      "(Iteration 2801 / 30600) loss: 2.305718\n",
      "(Iteration 2901 / 30600) loss: 2.305729\n",
      "(Iteration 3001 / 30600) loss: 2.305438\n",
      "(Epoch 4 / 40) train acc: 0.084000; val_acc: 0.078000\n",
      "(Iteration 3101 / 30600) loss: 2.305545\n",
      "(Iteration 3201 / 30600) loss: 2.305760\n",
      "(Iteration 3301 / 30600) loss: 2.305493\n",
      "(Iteration 3401 / 30600) loss: 2.305290\n",
      "(Iteration 3501 / 30600) loss: 2.305332\n",
      "(Iteration 3601 / 30600) loss: 2.305195\n",
      "(Iteration 3701 / 30600) loss: 2.305472\n",
      "(Iteration 3801 / 30600) loss: 2.305474\n",
      "(Epoch 5 / 40) train acc: 0.120000; val_acc: 0.079000\n",
      "(Iteration 3901 / 30600) loss: 2.304846\n",
      "(Iteration 4001 / 30600) loss: 2.305167\n",
      "(Iteration 4101 / 30600) loss: 2.305196\n",
      "(Iteration 4201 / 30600) loss: 2.305018\n",
      "(Iteration 4301 / 30600) loss: 2.304563\n",
      "(Iteration 4401 / 30600) loss: 2.304753\n",
      "(Iteration 4501 / 30600) loss: 2.305170\n",
      "(Epoch 6 / 40) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 4601 / 30600) loss: 2.304851\n",
      "(Iteration 4701 / 30600) loss: 2.304422\n",
      "(Iteration 4801 / 30600) loss: 2.304276\n",
      "(Iteration 4901 / 30600) loss: 2.304489\n",
      "(Iteration 5001 / 30600) loss: 2.304170\n",
      "(Iteration 5101 / 30600) loss: 2.304270\n",
      "(Iteration 5201 / 30600) loss: 2.304386\n",
      "(Iteration 5301 / 30600) loss: 2.304394\n",
      "(Epoch 7 / 40) train acc: 0.116000; val_acc: 0.083000\n",
      "(Iteration 5401 / 30600) loss: 2.303724\n",
      "(Iteration 5501 / 30600) loss: 2.304577\n",
      "(Iteration 5601 / 30600) loss: 2.304284\n",
      "(Iteration 5701 / 30600) loss: 2.304259\n",
      "(Iteration 5801 / 30600) loss: 2.304655\n",
      "(Iteration 5901 / 30600) loss: 2.304051\n",
      "(Iteration 6001 / 30600) loss: 2.303818\n",
      "(Iteration 6101 / 30600) loss: 2.304299\n",
      "(Epoch 8 / 40) train acc: 0.105000; val_acc: 0.091000\n",
      "(Iteration 6201 / 30600) loss: 2.304093\n",
      "(Iteration 6301 / 30600) loss: 2.303995\n",
      "(Iteration 6401 / 30600) loss: 2.304039\n",
      "(Iteration 6501 / 30600) loss: 2.303363\n",
      "(Iteration 6601 / 30600) loss: 2.304257\n",
      "(Iteration 6701 / 30600) loss: 2.303510\n",
      "(Iteration 6801 / 30600) loss: 2.303942\n",
      "(Epoch 9 / 40) train acc: 0.139000; val_acc: 0.107000\n",
      "(Iteration 6901 / 30600) loss: 2.303485\n",
      "(Iteration 7001 / 30600) loss: 2.303700\n",
      "(Iteration 7101 / 30600) loss: 2.303388\n",
      "(Iteration 7201 / 30600) loss: 2.303885\n",
      "(Iteration 7301 / 30600) loss: 2.303589\n",
      "(Iteration 7401 / 30600) loss: 2.303128\n",
      "(Iteration 7501 / 30600) loss: 2.304237\n",
      "(Iteration 7601 / 30600) loss: 2.303287\n",
      "(Epoch 10 / 40) train acc: 0.169000; val_acc: 0.131000\n",
      "(Iteration 7701 / 30600) loss: 2.304129\n",
      "(Iteration 7801 / 30600) loss: 2.303315\n",
      "(Iteration 7901 / 30600) loss: 2.303511\n",
      "(Iteration 8001 / 30600) loss: 2.302865\n",
      "(Iteration 8101 / 30600) loss: 2.304032\n",
      "(Iteration 8201 / 30600) loss: 2.303888\n",
      "(Iteration 8301 / 30600) loss: 2.303562\n",
      "(Iteration 8401 / 30600) loss: 2.303587\n",
      "(Epoch 11 / 40) train acc: 0.166000; val_acc: 0.139000\n",
      "(Iteration 8501 / 30600) loss: 2.302926\n",
      "(Iteration 8601 / 30600) loss: 2.303482\n",
      "(Iteration 8701 / 30600) loss: 2.303126\n",
      "(Iteration 8801 / 30600) loss: 2.303464\n",
      "(Iteration 8901 / 30600) loss: 2.303393\n",
      "(Iteration 9001 / 30600) loss: 2.303928\n",
      "(Iteration 9101 / 30600) loss: 2.301497\n",
      "(Epoch 12 / 40) train acc: 0.190000; val_acc: 0.149000\n",
      "(Iteration 9201 / 30600) loss: 2.302765\n",
      "(Iteration 9301 / 30600) loss: 2.303065\n",
      "(Iteration 9401 / 30600) loss: 2.303374\n",
      "(Iteration 9501 / 30600) loss: 2.302841\n",
      "(Iteration 9601 / 30600) loss: 2.302283\n",
      "(Iteration 9701 / 30600) loss: 2.302611\n",
      "(Iteration 9801 / 30600) loss: 2.303100\n",
      "(Iteration 9901 / 30600) loss: 2.303196\n",
      "(Epoch 13 / 40) train acc: 0.211000; val_acc: 0.153000\n",
      "(Iteration 10001 / 30600) loss: 2.303457\n",
      "(Iteration 10101 / 30600) loss: 2.303441\n",
      "(Iteration 10201 / 30600) loss: 2.302603\n",
      "(Iteration 10301 / 30600) loss: 2.302461\n",
      "(Iteration 10401 / 30600) loss: 2.302534\n",
      "(Iteration 10501 / 30600) loss: 2.303065\n",
      "(Iteration 10601 / 30600) loss: 2.302964\n",
      "(Iteration 10701 / 30600) loss: 2.302909\n",
      "(Epoch 14 / 40) train acc: 0.212000; val_acc: 0.171000\n",
      "(Iteration 10801 / 30600) loss: 2.302929\n",
      "(Iteration 10901 / 30600) loss: 2.302600\n",
      "(Iteration 11001 / 30600) loss: 2.302631\n",
      "(Iteration 11101 / 30600) loss: 2.303285\n",
      "(Iteration 11201 / 30600) loss: 2.302560\n",
      "(Iteration 11301 / 30600) loss: 2.302142\n",
      "(Iteration 11401 / 30600) loss: 2.302646\n",
      "(Epoch 15 / 40) train acc: 0.212000; val_acc: 0.184000\n",
      "(Iteration 11501 / 30600) loss: 2.301647\n",
      "(Iteration 11601 / 30600) loss: 2.302581\n",
      "(Iteration 11701 / 30600) loss: 2.302925\n",
      "(Iteration 11801 / 30600) loss: 2.302050\n",
      "(Iteration 11901 / 30600) loss: 2.303180\n",
      "(Iteration 12001 / 30600) loss: 2.303080\n",
      "(Iteration 12101 / 30600) loss: 2.302133\n",
      "(Iteration 12201 / 30600) loss: 2.302302\n",
      "(Epoch 16 / 40) train acc: 0.212000; val_acc: 0.196000\n",
      "(Iteration 12301 / 30600) loss: 2.302420\n",
      "(Iteration 12401 / 30600) loss: 2.302310\n",
      "(Iteration 12501 / 30600) loss: 2.302566\n",
      "(Iteration 12601 / 30600) loss: 2.302317\n",
      "(Iteration 12701 / 30600) loss: 2.301241\n",
      "(Iteration 12801 / 30600) loss: 2.301541\n",
      "(Iteration 12901 / 30600) loss: 2.301929\n",
      "(Iteration 13001 / 30600) loss: 2.301408\n",
      "(Epoch 17 / 40) train acc: 0.231000; val_acc: 0.207000\n",
      "(Iteration 13101 / 30600) loss: 2.301818\n",
      "(Iteration 13201 / 30600) loss: 2.301763\n",
      "(Iteration 13301 / 30600) loss: 2.300986\n",
      "(Iteration 13401 / 30600) loss: 2.301842\n",
      "(Iteration 13501 / 30600) loss: 2.302590\n",
      "(Iteration 13601 / 30600) loss: 2.302404\n",
      "(Iteration 13701 / 30600) loss: 2.301982\n",
      "(Epoch 18 / 40) train acc: 0.213000; val_acc: 0.216000\n",
      "(Iteration 13801 / 30600) loss: 2.301570\n",
      "(Iteration 13901 / 30600) loss: 2.301935\n",
      "(Iteration 14001 / 30600) loss: 2.301705\n",
      "(Iteration 14101 / 30600) loss: 2.301585\n",
      "(Iteration 14201 / 30600) loss: 2.302540\n",
      "(Iteration 14301 / 30600) loss: 2.300933\n",
      "(Iteration 14401 / 30600) loss: 2.301297\n",
      "(Iteration 14501 / 30600) loss: 2.301141\n",
      "(Epoch 19 / 40) train acc: 0.234000; val_acc: 0.216000\n",
      "(Iteration 14601 / 30600) loss: 2.301361\n",
      "(Iteration 14701 / 30600) loss: 2.302030\n",
      "(Iteration 14801 / 30600) loss: 2.301240\n",
      "(Iteration 14901 / 30600) loss: 2.301488\n",
      "(Iteration 15001 / 30600) loss: 2.301232\n",
      "(Iteration 15101 / 30600) loss: 2.301224\n",
      "(Iteration 15201 / 30600) loss: 2.301375\n",
      "(Epoch 20 / 40) train acc: 0.227000; val_acc: 0.217000\n",
      "(Iteration 15301 / 30600) loss: 2.302319\n",
      "(Iteration 15401 / 30600) loss: 2.302201\n",
      "(Iteration 15501 / 30600) loss: 2.301141\n",
      "(Iteration 15601 / 30600) loss: 2.300303\n",
      "(Iteration 15701 / 30600) loss: 2.300115\n",
      "(Iteration 15801 / 30600) loss: 2.301141\n",
      "(Iteration 15901 / 30600) loss: 2.300782\n",
      "(Iteration 16001 / 30600) loss: 2.302441\n",
      "(Epoch 21 / 40) train acc: 0.234000; val_acc: 0.220000\n",
      "(Iteration 16101 / 30600) loss: 2.300979\n",
      "(Iteration 16201 / 30600) loss: 2.299922\n",
      "(Iteration 16301 / 30600) loss: 2.301805\n",
      "(Iteration 16401 / 30600) loss: 2.301666\n",
      "(Iteration 16501 / 30600) loss: 2.302155\n",
      "(Iteration 16601 / 30600) loss: 2.301614\n",
      "(Iteration 16701 / 30600) loss: 2.301702\n",
      "(Iteration 16801 / 30600) loss: 2.300939\n",
      "(Epoch 22 / 40) train acc: 0.215000; val_acc: 0.224000\n",
      "(Iteration 16901 / 30600) loss: 2.301186\n",
      "(Iteration 17001 / 30600) loss: 2.302104\n",
      "(Iteration 17101 / 30600) loss: 2.302351\n",
      "(Iteration 17201 / 30600) loss: 2.300565\n",
      "(Iteration 17301 / 30600) loss: 2.301999\n",
      "(Iteration 17401 / 30600) loss: 2.300956\n",
      "(Iteration 17501 / 30600) loss: 2.301627\n",
      "(Epoch 23 / 40) train acc: 0.237000; val_acc: 0.235000\n",
      "(Iteration 17601 / 30600) loss: 2.302014\n",
      "(Iteration 17701 / 30600) loss: 2.301105\n",
      "(Iteration 17801 / 30600) loss: 2.301441\n",
      "(Iteration 17901 / 30600) loss: 2.301400\n",
      "(Iteration 18001 / 30600) loss: 2.301995\n",
      "(Iteration 18101 / 30600) loss: 2.301607\n",
      "(Iteration 18201 / 30600) loss: 2.301098\n",
      "(Iteration 18301 / 30600) loss: 2.300584\n",
      "(Epoch 24 / 40) train acc: 0.250000; val_acc: 0.241000\n",
      "(Iteration 18401 / 30600) loss: 2.300645\n",
      "(Iteration 18501 / 30600) loss: 2.300990\n",
      "(Iteration 18601 / 30600) loss: 2.301482\n",
      "(Iteration 18701 / 30600) loss: 2.301271\n",
      "(Iteration 18801 / 30600) loss: 2.301806\n",
      "(Iteration 18901 / 30600) loss: 2.300991\n",
      "(Iteration 19001 / 30600) loss: 2.301334\n",
      "(Iteration 19101 / 30600) loss: 2.301066\n",
      "(Epoch 25 / 40) train acc: 0.245000; val_acc: 0.236000\n",
      "(Iteration 19201 / 30600) loss: 2.300850\n",
      "(Iteration 19301 / 30600) loss: 2.301929\n",
      "(Iteration 19401 / 30600) loss: 2.299798\n",
      "(Iteration 19501 / 30600) loss: 2.301785\n",
      "(Iteration 19601 / 30600) loss: 2.301117\n",
      "(Iteration 19701 / 30600) loss: 2.302843\n",
      "(Iteration 19801 / 30600) loss: 2.300771\n",
      "(Epoch 26 / 40) train acc: 0.241000; val_acc: 0.238000\n",
      "(Iteration 19901 / 30600) loss: 2.301095\n",
      "(Iteration 20001 / 30600) loss: 2.301316\n",
      "(Iteration 20101 / 30600) loss: 2.301019\n",
      "(Iteration 20201 / 30600) loss: 2.299962\n",
      "(Iteration 20301 / 30600) loss: 2.301263\n",
      "(Iteration 20401 / 30600) loss: 2.299967\n",
      "(Iteration 20501 / 30600) loss: 2.299765\n",
      "(Iteration 20601 / 30600) loss: 2.301982\n",
      "(Epoch 27 / 40) train acc: 0.230000; val_acc: 0.236000\n",
      "(Iteration 20701 / 30600) loss: 2.300633\n",
      "(Iteration 20801 / 30600) loss: 2.300433\n",
      "(Iteration 20901 / 30600) loss: 2.300235\n",
      "(Iteration 21001 / 30600) loss: 2.299821\n",
      "(Iteration 21101 / 30600) loss: 2.301314\n",
      "(Iteration 21201 / 30600) loss: 2.301741\n",
      "(Iteration 21301 / 30600) loss: 2.301435\n",
      "(Iteration 21401 / 30600) loss: 2.300802\n",
      "(Epoch 28 / 40) train acc: 0.251000; val_acc: 0.237000\n",
      "(Iteration 21501 / 30600) loss: 2.299868\n",
      "(Iteration 21601 / 30600) loss: 2.300923\n",
      "(Iteration 21701 / 30600) loss: 2.300583\n",
      "(Iteration 21801 / 30600) loss: 2.300821\n",
      "(Iteration 21901 / 30600) loss: 2.301292\n",
      "(Iteration 22001 / 30600) loss: 2.302299\n",
      "(Iteration 22101 / 30600) loss: 2.301547\n",
      "(Epoch 29 / 40) train acc: 0.251000; val_acc: 0.240000\n",
      "(Iteration 22201 / 30600) loss: 2.301518\n",
      "(Iteration 22301 / 30600) loss: 2.299711\n",
      "(Iteration 22401 / 30600) loss: 2.302299\n",
      "(Iteration 22501 / 30600) loss: 2.300544\n",
      "(Iteration 22601 / 30600) loss: 2.300040\n",
      "(Iteration 22701 / 30600) loss: 2.301558\n",
      "(Iteration 22801 / 30600) loss: 2.300989\n",
      "(Iteration 22901 / 30600) loss: 2.299006\n",
      "(Epoch 30 / 40) train acc: 0.250000; val_acc: 0.249000\n",
      "(Iteration 23001 / 30600) loss: 2.300859\n",
      "(Iteration 23101 / 30600) loss: 2.299967\n",
      "(Iteration 23201 / 30600) loss: 2.300232\n",
      "(Iteration 23301 / 30600) loss: 2.301449\n",
      "(Iteration 23401 / 30600) loss: 2.299236\n",
      "(Iteration 23501 / 30600) loss: 2.301139\n",
      "(Iteration 23601 / 30600) loss: 2.300582\n",
      "(Iteration 23701 / 30600) loss: 2.301922\n",
      "(Epoch 31 / 40) train acc: 0.243000; val_acc: 0.245000\n",
      "(Iteration 23801 / 30600) loss: 2.300710\n",
      "(Iteration 23901 / 30600) loss: 2.300875\n",
      "(Iteration 24001 / 30600) loss: 2.299430\n",
      "(Iteration 24101 / 30600) loss: 2.301070\n",
      "(Iteration 24201 / 30600) loss: 2.300816\n",
      "(Iteration 24301 / 30600) loss: 2.300168\n",
      "(Iteration 24401 / 30600) loss: 2.300636\n",
      "(Epoch 32 / 40) train acc: 0.258000; val_acc: 0.246000\n",
      "(Iteration 24501 / 30600) loss: 2.300509\n",
      "(Iteration 24601 / 30600) loss: 2.299116\n",
      "(Iteration 24701 / 30600) loss: 2.301743\n",
      "(Iteration 24801 / 30600) loss: 2.299403\n",
      "(Iteration 24901 / 30600) loss: 2.300586\n",
      "(Iteration 25001 / 30600) loss: 2.300370\n",
      "(Iteration 25101 / 30600) loss: 2.299188\n",
      "(Iteration 25201 / 30600) loss: 2.300995\n",
      "(Epoch 33 / 40) train acc: 0.229000; val_acc: 0.243000\n",
      "(Iteration 25301 / 30600) loss: 2.299613\n",
      "(Iteration 25401 / 30600) loss: 2.300871\n",
      "(Iteration 25501 / 30600) loss: 2.300231\n",
      "(Iteration 25601 / 30600) loss: 2.300711\n",
      "(Iteration 25701 / 30600) loss: 2.299984\n",
      "(Iteration 25801 / 30600) loss: 2.301456\n",
      "(Iteration 25901 / 30600) loss: 2.299742\n",
      "(Iteration 26001 / 30600) loss: 2.301693\n",
      "(Epoch 34 / 40) train acc: 0.244000; val_acc: 0.248000\n",
      "(Iteration 26101 / 30600) loss: 2.300784\n",
      "(Iteration 26201 / 30600) loss: 2.300884\n",
      "(Iteration 26301 / 30600) loss: 2.298009\n",
      "(Iteration 26401 / 30600) loss: 2.300417\n",
      "(Iteration 26501 / 30600) loss: 2.300358\n",
      "(Iteration 26601 / 30600) loss: 2.300652\n",
      "(Iteration 26701 / 30600) loss: 2.297889\n",
      "(Epoch 35 / 40) train acc: 0.233000; val_acc: 0.248000\n",
      "(Iteration 26801 / 30600) loss: 2.300472\n",
      "(Iteration 26901 / 30600) loss: 2.301054\n",
      "(Iteration 27001 / 30600) loss: 2.300241\n",
      "(Iteration 27101 / 30600) loss: 2.301316\n",
      "(Iteration 27201 / 30600) loss: 2.302048\n",
      "(Iteration 27301 / 30600) loss: 2.301573\n",
      "(Iteration 27401 / 30600) loss: 2.301192\n",
      "(Iteration 27501 / 30600) loss: 2.301795\n",
      "(Epoch 36 / 40) train acc: 0.254000; val_acc: 0.250000\n",
      "(Iteration 27601 / 30600) loss: 2.301007\n",
      "(Iteration 27701 / 30600) loss: 2.300504\n",
      "(Iteration 27801 / 30600) loss: 2.299333\n",
      "(Iteration 27901 / 30600) loss: 2.300221\n",
      "(Iteration 28001 / 30600) loss: 2.302478\n",
      "(Iteration 28101 / 30600) loss: 2.299861\n",
      "(Iteration 28201 / 30600) loss: 2.299102\n",
      "(Iteration 28301 / 30600) loss: 2.297781\n",
      "(Epoch 37 / 40) train acc: 0.225000; val_acc: 0.252000\n",
      "(Iteration 28401 / 30600) loss: 2.300803\n",
      "(Iteration 28501 / 30600) loss: 2.300811\n",
      "(Iteration 28601 / 30600) loss: 2.299504\n",
      "(Iteration 28701 / 30600) loss: 2.300631\n",
      "(Iteration 28801 / 30600) loss: 2.299944\n",
      "(Iteration 28901 / 30600) loss: 2.301140\n",
      "(Iteration 29001 / 30600) loss: 2.301432\n",
      "(Epoch 38 / 40) train acc: 0.234000; val_acc: 0.253000\n",
      "(Iteration 29101 / 30600) loss: 2.299287\n",
      "(Iteration 29201 / 30600) loss: 2.299277\n",
      "(Iteration 29301 / 30600) loss: 2.300039\n",
      "(Iteration 29401 / 30600) loss: 2.301277\n",
      "(Iteration 29501 / 30600) loss: 2.300190\n",
      "(Iteration 29601 / 30600) loss: 2.301794\n",
      "(Iteration 29701 / 30600) loss: 2.299127\n",
      "(Iteration 29801 / 30600) loss: 2.301230\n",
      "(Epoch 39 / 40) train acc: 0.253000; val_acc: 0.253000\n",
      "(Iteration 29901 / 30600) loss: 2.300316\n",
      "(Iteration 30001 / 30600) loss: 2.298191\n",
      "(Iteration 30101 / 30600) loss: 2.302245\n",
      "(Iteration 30201 / 30600) loss: 2.301287\n",
      "(Iteration 30301 / 30600) loss: 2.298569\n",
      "(Iteration 30401 / 30600) loss: 2.300974\n",
      "(Iteration 30501 / 30600) loss: 2.301189\n",
      "(Epoch 40 / 40) train acc: 0.251000; val_acc: 0.253000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.001, 'num_epochs': 40, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 61240) loss: 2.308357\n",
      "(Epoch 0 / 40) train acc: 0.096000; val_acc: 0.125000\n",
      "(Iteration 101 / 61240) loss: 2.308154\n",
      "(Iteration 201 / 61240) loss: 2.307988\n",
      "(Iteration 301 / 61240) loss: 2.307910\n",
      "(Iteration 401 / 61240) loss: 2.307086\n",
      "(Iteration 501 / 61240) loss: 2.307646\n",
      "(Iteration 601 / 61240) loss: 2.307025\n",
      "(Iteration 701 / 61240) loss: 2.307287\n",
      "(Iteration 801 / 61240) loss: 2.307144\n",
      "(Iteration 901 / 61240) loss: 2.306757\n",
      "(Iteration 1001 / 61240) loss: 2.306950\n",
      "(Iteration 1101 / 61240) loss: 2.306909\n",
      "(Iteration 1201 / 61240) loss: 2.306311\n",
      "(Iteration 1301 / 61240) loss: 2.306441\n",
      "(Iteration 1401 / 61240) loss: 2.307357\n",
      "(Iteration 1501 / 61240) loss: 2.306187\n",
      "(Epoch 1 / 40) train acc: 0.160000; val_acc: 0.167000\n",
      "(Iteration 1601 / 61240) loss: 2.307875\n",
      "(Iteration 1701 / 61240) loss: 2.305978\n",
      "(Iteration 1801 / 61240) loss: 2.306578\n",
      "(Iteration 1901 / 61240) loss: 2.306340\n",
      "(Iteration 2001 / 61240) loss: 2.305975\n",
      "(Iteration 2101 / 61240) loss: 2.305965\n",
      "(Iteration 2201 / 61240) loss: 2.306758\n",
      "(Iteration 2301 / 61240) loss: 2.304288\n",
      "(Iteration 2401 / 61240) loss: 2.304617\n",
      "(Iteration 2501 / 61240) loss: 2.306151\n",
      "(Iteration 2601 / 61240) loss: 2.305921\n",
      "(Iteration 2701 / 61240) loss: 2.306720\n",
      "(Iteration 2801 / 61240) loss: 2.306716\n",
      "(Iteration 2901 / 61240) loss: 2.305953\n",
      "(Iteration 3001 / 61240) loss: 2.305690\n",
      "(Epoch 2 / 40) train acc: 0.169000; val_acc: 0.170000\n",
      "(Iteration 3101 / 61240) loss: 2.304304\n",
      "(Iteration 3201 / 61240) loss: 2.305364\n",
      "(Iteration 3301 / 61240) loss: 2.304007\n",
      "(Iteration 3401 / 61240) loss: 2.305853\n",
      "(Iteration 3501 / 61240) loss: 2.304781\n",
      "(Iteration 3601 / 61240) loss: 2.305244\n",
      "(Iteration 3701 / 61240) loss: 2.303963\n",
      "(Iteration 3801 / 61240) loss: 2.304116\n",
      "(Iteration 3901 / 61240) loss: 2.305034\n",
      "(Iteration 4001 / 61240) loss: 2.304557\n",
      "(Iteration 4101 / 61240) loss: 2.304607\n",
      "(Iteration 4201 / 61240) loss: 2.305232\n",
      "(Iteration 4301 / 61240) loss: 2.304375\n",
      "(Iteration 4401 / 61240) loss: 2.303828\n",
      "(Iteration 4501 / 61240) loss: 2.304596\n",
      "(Epoch 3 / 40) train acc: 0.134000; val_acc: 0.114000\n",
      "(Iteration 4601 / 61240) loss: 2.303881\n",
      "(Iteration 4701 / 61240) loss: 2.305282\n",
      "(Iteration 4801 / 61240) loss: 2.305540\n",
      "(Iteration 4901 / 61240) loss: 2.305073\n",
      "(Iteration 5001 / 61240) loss: 2.303393\n",
      "(Iteration 5101 / 61240) loss: 2.302443\n",
      "(Iteration 5201 / 61240) loss: 2.303557\n",
      "(Iteration 5301 / 61240) loss: 2.304617\n",
      "(Iteration 5401 / 61240) loss: 2.304600\n",
      "(Iteration 5501 / 61240) loss: 2.302984\n",
      "(Iteration 5601 / 61240) loss: 2.305093\n",
      "(Iteration 5701 / 61240) loss: 2.303360\n",
      "(Iteration 5801 / 61240) loss: 2.304041\n",
      "(Iteration 5901 / 61240) loss: 2.303534\n",
      "(Iteration 6001 / 61240) loss: 2.302587\n",
      "(Iteration 6101 / 61240) loss: 2.302316\n",
      "(Epoch 4 / 40) train acc: 0.158000; val_acc: 0.132000\n",
      "(Iteration 6201 / 61240) loss: 2.303795\n",
      "(Iteration 6301 / 61240) loss: 2.303279\n",
      "(Iteration 6401 / 61240) loss: 2.304695\n",
      "(Iteration 6501 / 61240) loss: 2.303511\n",
      "(Iteration 6601 / 61240) loss: 2.302871\n",
      "(Iteration 6701 / 61240) loss: 2.303034\n",
      "(Iteration 6801 / 61240) loss: 2.302607\n",
      "(Iteration 6901 / 61240) loss: 2.303588\n",
      "(Iteration 7001 / 61240) loss: 2.303148\n",
      "(Iteration 7101 / 61240) loss: 2.302519\n",
      "(Iteration 7201 / 61240) loss: 2.303663\n",
      "(Iteration 7301 / 61240) loss: 2.302518\n",
      "(Iteration 7401 / 61240) loss: 2.302729\n",
      "(Iteration 7501 / 61240) loss: 2.302273\n",
      "(Iteration 7601 / 61240) loss: 2.302727\n",
      "(Epoch 5 / 40) train acc: 0.192000; val_acc: 0.183000\n",
      "(Iteration 7701 / 61240) loss: 2.303227\n",
      "(Iteration 7801 / 61240) loss: 2.303850\n",
      "(Iteration 7901 / 61240) loss: 2.302243\n",
      "(Iteration 8001 / 61240) loss: 2.301048\n",
      "(Iteration 8101 / 61240) loss: 2.301827\n",
      "(Iteration 8201 / 61240) loss: 2.301709\n",
      "(Iteration 8301 / 61240) loss: 2.303493\n",
      "(Iteration 8401 / 61240) loss: 2.300605\n",
      "(Iteration 8501 / 61240) loss: 2.303339\n",
      "(Iteration 8601 / 61240) loss: 2.301859\n",
      "(Iteration 8701 / 61240) loss: 2.300506\n",
      "(Iteration 8801 / 61240) loss: 2.302712\n",
      "(Iteration 8901 / 61240) loss: 2.301458\n",
      "(Iteration 9001 / 61240) loss: 2.302313\n",
      "(Iteration 9101 / 61240) loss: 2.300600\n",
      "(Epoch 6 / 40) train acc: 0.230000; val_acc: 0.243000\n",
      "(Iteration 9201 / 61240) loss: 2.300923\n",
      "(Iteration 9301 / 61240) loss: 2.300545\n",
      "(Iteration 9401 / 61240) loss: 2.302637\n",
      "(Iteration 9501 / 61240) loss: 2.301421\n",
      "(Iteration 9601 / 61240) loss: 2.301470\n",
      "(Iteration 9701 / 61240) loss: 2.301054\n",
      "(Iteration 9801 / 61240) loss: 2.300594\n",
      "(Iteration 9901 / 61240) loss: 2.298761\n",
      "(Iteration 10001 / 61240) loss: 2.298227\n",
      "(Iteration 10101 / 61240) loss: 2.298797\n",
      "(Iteration 10201 / 61240) loss: 2.300309\n",
      "(Iteration 10301 / 61240) loss: 2.298225\n",
      "(Iteration 10401 / 61240) loss: 2.299510\n",
      "(Iteration 10501 / 61240) loss: 2.297384\n",
      "(Iteration 10601 / 61240) loss: 2.301817\n",
      "(Iteration 10701 / 61240) loss: 2.299347\n",
      "(Epoch 7 / 40) train acc: 0.239000; val_acc: 0.242000\n",
      "(Iteration 10801 / 61240) loss: 2.298727\n",
      "(Iteration 10901 / 61240) loss: 2.298993\n",
      "(Iteration 11001 / 61240) loss: 2.297185\n",
      "(Iteration 11101 / 61240) loss: 2.300132\n",
      "(Iteration 11201 / 61240) loss: 2.299192\n",
      "(Iteration 11301 / 61240) loss: 2.299440\n",
      "(Iteration 11401 / 61240) loss: 2.299270\n",
      "(Iteration 11501 / 61240) loss: 2.298052\n",
      "(Iteration 11601 / 61240) loss: 2.297919\n",
      "(Iteration 11701 / 61240) loss: 2.299390\n",
      "(Iteration 11801 / 61240) loss: 2.298216\n",
      "(Iteration 11901 / 61240) loss: 2.297827\n",
      "(Iteration 12001 / 61240) loss: 2.297279\n",
      "(Iteration 12101 / 61240) loss: 2.296633\n",
      "(Iteration 12201 / 61240) loss: 2.294847\n",
      "(Epoch 8 / 40) train acc: 0.245000; val_acc: 0.260000\n",
      "(Iteration 12301 / 61240) loss: 2.299687\n",
      "(Iteration 12401 / 61240) loss: 2.299977\n",
      "(Iteration 12501 / 61240) loss: 2.302398\n",
      "(Iteration 12601 / 61240) loss: 2.296712\n",
      "(Iteration 12701 / 61240) loss: 2.296844\n",
      "(Iteration 12801 / 61240) loss: 2.294988\n",
      "(Iteration 12901 / 61240) loss: 2.295521\n",
      "(Iteration 13001 / 61240) loss: 2.293934\n",
      "(Iteration 13101 / 61240) loss: 2.297432\n",
      "(Iteration 13201 / 61240) loss: 2.295414\n",
      "(Iteration 13301 / 61240) loss: 2.295984\n",
      "(Iteration 13401 / 61240) loss: 2.298491\n",
      "(Iteration 13501 / 61240) loss: 2.299355\n",
      "(Iteration 13601 / 61240) loss: 2.300689\n",
      "(Iteration 13701 / 61240) loss: 2.297196\n",
      "(Epoch 9 / 40) train acc: 0.254000; val_acc: 0.243000\n",
      "(Iteration 13801 / 61240) loss: 2.298110\n",
      "(Iteration 13901 / 61240) loss: 2.299464\n",
      "(Iteration 14001 / 61240) loss: 2.295785\n",
      "(Iteration 14101 / 61240) loss: 2.295263\n",
      "(Iteration 14201 / 61240) loss: 2.294633\n",
      "(Iteration 14301 / 61240) loss: 2.293172\n",
      "(Iteration 14401 / 61240) loss: 2.295905\n",
      "(Iteration 14501 / 61240) loss: 2.296125\n",
      "(Iteration 14601 / 61240) loss: 2.293478\n",
      "(Iteration 14701 / 61240) loss: 2.298707\n",
      "(Iteration 14801 / 61240) loss: 2.296561\n",
      "(Iteration 14901 / 61240) loss: 2.293035\n",
      "(Iteration 15001 / 61240) loss: 2.294369\n",
      "(Iteration 15101 / 61240) loss: 2.293941\n",
      "(Iteration 15201 / 61240) loss: 2.293215\n",
      "(Iteration 15301 / 61240) loss: 2.284224\n",
      "(Epoch 10 / 40) train acc: 0.265000; val_acc: 0.246000\n",
      "(Iteration 15401 / 61240) loss: 2.294689\n",
      "(Iteration 15501 / 61240) loss: 2.291242\n",
      "(Iteration 15601 / 61240) loss: 2.297807\n",
      "(Iteration 15701 / 61240) loss: 2.292515\n",
      "(Iteration 15801 / 61240) loss: 2.286415\n",
      "(Iteration 15901 / 61240) loss: 2.294285\n",
      "(Iteration 16001 / 61240) loss: 2.292633\n",
      "(Iteration 16101 / 61240) loss: 2.296443\n",
      "(Iteration 16201 / 61240) loss: 2.290961\n",
      "(Iteration 16301 / 61240) loss: 2.294103\n",
      "(Iteration 16401 / 61240) loss: 2.292981\n",
      "(Iteration 16501 / 61240) loss: 2.291426\n",
      "(Iteration 16601 / 61240) loss: 2.280927\n",
      "(Iteration 16701 / 61240) loss: 2.289177\n",
      "(Iteration 16801 / 61240) loss: 2.286849\n",
      "(Epoch 11 / 40) train acc: 0.267000; val_acc: 0.252000\n",
      "(Iteration 16901 / 61240) loss: 2.300941\n",
      "(Iteration 17001 / 61240) loss: 2.281248\n",
      "(Iteration 17101 / 61240) loss: 2.286410\n",
      "(Iteration 17201 / 61240) loss: 2.290087\n",
      "(Iteration 17301 / 61240) loss: 2.295788\n",
      "(Iteration 17401 / 61240) loss: 2.289361\n",
      "(Iteration 17501 / 61240) loss: 2.291007\n",
      "(Iteration 17601 / 61240) loss: 2.283778\n",
      "(Iteration 17701 / 61240) loss: 2.277614\n",
      "(Iteration 17801 / 61240) loss: 2.292881\n",
      "(Iteration 17901 / 61240) loss: 2.289293\n",
      "(Iteration 18001 / 61240) loss: 2.286902\n",
      "(Iteration 18101 / 61240) loss: 2.276723\n",
      "(Iteration 18201 / 61240) loss: 2.284016\n",
      "(Iteration 18301 / 61240) loss: 2.277842\n",
      "(Epoch 12 / 40) train acc: 0.255000; val_acc: 0.256000\n",
      "(Iteration 18401 / 61240) loss: 2.297775\n",
      "(Iteration 18501 / 61240) loss: 2.285632\n",
      "(Iteration 18601 / 61240) loss: 2.287499\n",
      "(Iteration 18701 / 61240) loss: 2.286740\n",
      "(Iteration 18801 / 61240) loss: 2.295022\n",
      "(Iteration 18901 / 61240) loss: 2.280479\n",
      "(Iteration 19001 / 61240) loss: 2.282643\n",
      "(Iteration 19101 / 61240) loss: 2.297035\n",
      "(Iteration 19201 / 61240) loss: 2.305125\n",
      "(Iteration 19301 / 61240) loss: 2.271691\n",
      "(Iteration 19401 / 61240) loss: 2.282721\n",
      "(Iteration 19501 / 61240) loss: 2.278126\n",
      "(Iteration 19601 / 61240) loss: 2.275171\n",
      "(Iteration 19701 / 61240) loss: 2.273767\n",
      "(Iteration 19801 / 61240) loss: 2.300641\n",
      "(Iteration 19901 / 61240) loss: 2.281256\n",
      "(Epoch 13 / 40) train acc: 0.268000; val_acc: 0.249000\n",
      "(Iteration 20001 / 61240) loss: 2.287602\n",
      "(Iteration 20101 / 61240) loss: 2.280724\n",
      "(Iteration 20201 / 61240) loss: 2.274937\n",
      "(Iteration 20301 / 61240) loss: 2.273708\n",
      "(Iteration 20401 / 61240) loss: 2.274300\n",
      "(Iteration 20501 / 61240) loss: 2.289968\n",
      "(Iteration 20601 / 61240) loss: 2.283477\n",
      "(Iteration 20701 / 61240) loss: 2.280381\n",
      "(Iteration 20801 / 61240) loss: 2.273079\n",
      "(Iteration 20901 / 61240) loss: 2.290764\n",
      "(Iteration 21001 / 61240) loss: 2.276632\n",
      "(Iteration 21101 / 61240) loss: 2.281970\n",
      "(Iteration 21201 / 61240) loss: 2.290508\n",
      "(Iteration 21301 / 61240) loss: 2.286015\n",
      "(Iteration 21401 / 61240) loss: 2.294808\n",
      "(Epoch 14 / 40) train acc: 0.278000; val_acc: 0.235000\n",
      "(Iteration 21501 / 61240) loss: 2.289715\n",
      "(Iteration 21601 / 61240) loss: 2.263376\n",
      "(Iteration 21701 / 61240) loss: 2.283838\n",
      "(Iteration 21801 / 61240) loss: 2.286733\n",
      "(Iteration 21901 / 61240) loss: 2.294749\n",
      "(Iteration 22001 / 61240) loss: 2.263137\n",
      "(Iteration 22101 / 61240) loss: 2.278000\n",
      "(Iteration 22201 / 61240) loss: 2.287799\n",
      "(Iteration 22301 / 61240) loss: 2.262659\n",
      "(Iteration 22401 / 61240) loss: 2.265943\n",
      "(Iteration 22501 / 61240) loss: 2.276536\n",
      "(Iteration 22601 / 61240) loss: 2.270238\n",
      "(Iteration 22701 / 61240) loss: 2.264909\n",
      "(Iteration 22801 / 61240) loss: 2.283961\n",
      "(Iteration 22901 / 61240) loss: 2.286472\n",
      "(Epoch 15 / 40) train acc: 0.241000; val_acc: 0.232000\n",
      "(Iteration 23001 / 61240) loss: 2.273979\n",
      "(Iteration 23101 / 61240) loss: 2.278262\n",
      "(Iteration 23201 / 61240) loss: 2.268770\n",
      "(Iteration 23301 / 61240) loss: 2.248175\n",
      "(Iteration 23401 / 61240) loss: 2.264234\n",
      "(Iteration 23501 / 61240) loss: 2.283112\n",
      "(Iteration 23601 / 61240) loss: 2.274706\n",
      "(Iteration 23701 / 61240) loss: 2.278717\n",
      "(Iteration 23801 / 61240) loss: 2.276300\n",
      "(Iteration 23901 / 61240) loss: 2.229086\n",
      "(Iteration 24001 / 61240) loss: 2.223442\n",
      "(Iteration 24101 / 61240) loss: 2.284794\n",
      "(Iteration 24201 / 61240) loss: 2.263811\n",
      "(Iteration 24301 / 61240) loss: 2.295239\n",
      "(Iteration 24401 / 61240) loss: 2.282739\n",
      "(Epoch 16 / 40) train acc: 0.241000; val_acc: 0.233000\n",
      "(Iteration 24501 / 61240) loss: 2.271984\n",
      "(Iteration 24601 / 61240) loss: 2.268906\n",
      "(Iteration 24701 / 61240) loss: 2.260704\n",
      "(Iteration 24801 / 61240) loss: 2.239674\n",
      "(Iteration 24901 / 61240) loss: 2.274256\n",
      "(Iteration 25001 / 61240) loss: 2.294748\n",
      "(Iteration 25101 / 61240) loss: 2.250381\n",
      "(Iteration 25201 / 61240) loss: 2.280089\n",
      "(Iteration 25301 / 61240) loss: 2.241313\n",
      "(Iteration 25401 / 61240) loss: 2.268262\n",
      "(Iteration 25501 / 61240) loss: 2.256822\n",
      "(Iteration 25601 / 61240) loss: 2.254804\n",
      "(Iteration 25701 / 61240) loss: 2.274751\n",
      "(Iteration 25801 / 61240) loss: 2.273117\n",
      "(Iteration 25901 / 61240) loss: 2.284213\n",
      "(Iteration 26001 / 61240) loss: 2.268566\n",
      "(Epoch 17 / 40) train acc: 0.259000; val_acc: 0.239000\n",
      "(Iteration 26101 / 61240) loss: 2.272845\n",
      "(Iteration 26201 / 61240) loss: 2.251487\n",
      "(Iteration 26301 / 61240) loss: 2.256687\n",
      "(Iteration 26401 / 61240) loss: 2.252554\n",
      "(Iteration 26501 / 61240) loss: 2.287819\n",
      "(Iteration 26601 / 61240) loss: 2.273811\n",
      "(Iteration 26701 / 61240) loss: 2.299356\n",
      "(Iteration 26801 / 61240) loss: 2.281471\n",
      "(Iteration 26901 / 61240) loss: 2.257689\n",
      "(Iteration 27001 / 61240) loss: 2.245958\n",
      "(Iteration 27101 / 61240) loss: 2.254418\n",
      "(Iteration 27201 / 61240) loss: 2.273840\n",
      "(Iteration 27301 / 61240) loss: 2.281635\n",
      "(Iteration 27401 / 61240) loss: 2.253197\n",
      "(Iteration 27501 / 61240) loss: 2.254733\n",
      "(Epoch 18 / 40) train acc: 0.252000; val_acc: 0.242000\n",
      "(Iteration 27601 / 61240) loss: 2.273182\n",
      "(Iteration 27701 / 61240) loss: 2.260534\n",
      "(Iteration 27801 / 61240) loss: 2.275843\n",
      "(Iteration 27901 / 61240) loss: 2.261134\n",
      "(Iteration 28001 / 61240) loss: 2.255214\n",
      "(Iteration 28101 / 61240) loss: 2.286128\n",
      "(Iteration 28201 / 61240) loss: 2.250690\n",
      "(Iteration 28301 / 61240) loss: 2.240678\n",
      "(Iteration 28401 / 61240) loss: 2.283108\n",
      "(Iteration 28501 / 61240) loss: 2.296008\n",
      "(Iteration 28601 / 61240) loss: 2.212649\n",
      "(Iteration 28701 / 61240) loss: 2.269513\n",
      "(Iteration 28801 / 61240) loss: 2.275030\n",
      "(Iteration 28901 / 61240) loss: 2.265089\n",
      "(Iteration 29001 / 61240) loss: 2.283202\n",
      "(Epoch 19 / 40) train acc: 0.221000; val_acc: 0.242000\n",
      "(Iteration 29101 / 61240) loss: 2.230135\n",
      "(Iteration 29201 / 61240) loss: 2.259193\n",
      "(Iteration 29301 / 61240) loss: 2.228477\n",
      "(Iteration 29401 / 61240) loss: 2.232097\n",
      "(Iteration 29501 / 61240) loss: 2.278786\n",
      "(Iteration 29601 / 61240) loss: 2.251062\n",
      "(Iteration 29701 / 61240) loss: 2.234595\n",
      "(Iteration 29801 / 61240) loss: 2.215707\n",
      "(Iteration 29901 / 61240) loss: 2.244291\n",
      "(Iteration 30001 / 61240) loss: 2.268807\n",
      "(Iteration 30101 / 61240) loss: 2.226002\n",
      "(Iteration 30201 / 61240) loss: 2.199377\n",
      "(Iteration 30301 / 61240) loss: 2.244844\n",
      "(Iteration 30401 / 61240) loss: 2.245058\n",
      "(Iteration 30501 / 61240) loss: 2.216924\n",
      "(Iteration 30601 / 61240) loss: 2.264492\n",
      "(Epoch 20 / 40) train acc: 0.257000; val_acc: 0.242000\n",
      "(Iteration 30701 / 61240) loss: 2.248145\n",
      "(Iteration 30801 / 61240) loss: 2.271896\n",
      "(Iteration 30901 / 61240) loss: 2.249442\n",
      "(Iteration 31001 / 61240) loss: 2.303225\n",
      "(Iteration 31101 / 61240) loss: 2.244522\n",
      "(Iteration 31201 / 61240) loss: 2.268252\n",
      "(Iteration 31301 / 61240) loss: 2.258912\n",
      "(Iteration 31401 / 61240) loss: 2.254858\n",
      "(Iteration 31501 / 61240) loss: 2.231799\n",
      "(Iteration 31601 / 61240) loss: 2.270335\n",
      "(Iteration 31701 / 61240) loss: 2.238682\n",
      "(Iteration 31801 / 61240) loss: 2.251282\n",
      "(Iteration 31901 / 61240) loss: 2.255865\n",
      "(Iteration 32001 / 61240) loss: 2.264496\n",
      "(Iteration 32101 / 61240) loss: 2.252709\n",
      "(Epoch 21 / 40) train acc: 0.220000; val_acc: 0.246000\n",
      "(Iteration 32201 / 61240) loss: 2.242660\n",
      "(Iteration 32301 / 61240) loss: 2.256830\n",
      "(Iteration 32401 / 61240) loss: 2.255470\n",
      "(Iteration 32501 / 61240) loss: 2.235942\n",
      "(Iteration 32601 / 61240) loss: 2.255861\n",
      "(Iteration 32701 / 61240) loss: 2.206586\n",
      "(Iteration 32801 / 61240) loss: 2.189850\n",
      "(Iteration 32901 / 61240) loss: 2.243908\n",
      "(Iteration 33001 / 61240) loss: 2.245001\n",
      "(Iteration 33101 / 61240) loss: 2.265536\n",
      "(Iteration 33201 / 61240) loss: 2.245957\n",
      "(Iteration 33301 / 61240) loss: 2.236864\n",
      "(Iteration 33401 / 61240) loss: 2.247403\n",
      "(Iteration 33501 / 61240) loss: 2.209150\n",
      "(Iteration 33601 / 61240) loss: 2.217117\n",
      "(Epoch 22 / 40) train acc: 0.226000; val_acc: 0.249000\n",
      "(Iteration 33701 / 61240) loss: 2.254896\n",
      "(Iteration 33801 / 61240) loss: 2.278169\n",
      "(Iteration 33901 / 61240) loss: 2.238011\n",
      "(Iteration 34001 / 61240) loss: 2.213607\n",
      "(Iteration 34101 / 61240) loss: 2.252503\n",
      "(Iteration 34201 / 61240) loss: 2.206593\n",
      "(Iteration 34301 / 61240) loss: 2.265518\n",
      "(Iteration 34401 / 61240) loss: 2.245843\n",
      "(Iteration 34501 / 61240) loss: 2.246303\n",
      "(Iteration 34601 / 61240) loss: 2.244077\n",
      "(Iteration 34701 / 61240) loss: 2.253759\n",
      "(Iteration 34801 / 61240) loss: 2.263781\n",
      "(Iteration 34901 / 61240) loss: 2.300795\n",
      "(Iteration 35001 / 61240) loss: 2.253829\n",
      "(Iteration 35101 / 61240) loss: 2.253728\n",
      "(Iteration 35201 / 61240) loss: 2.278859\n",
      "(Epoch 23 / 40) train acc: 0.238000; val_acc: 0.249000\n",
      "(Iteration 35301 / 61240) loss: 2.278255\n",
      "(Iteration 35401 / 61240) loss: 2.227287\n",
      "(Iteration 35501 / 61240) loss: 2.231655\n",
      "(Iteration 35601 / 61240) loss: 2.228467\n",
      "(Iteration 35701 / 61240) loss: 2.260550\n",
      "(Iteration 35801 / 61240) loss: 2.231022\n",
      "(Iteration 35901 / 61240) loss: 2.228506\n",
      "(Iteration 36001 / 61240) loss: 2.255675\n",
      "(Iteration 36101 / 61240) loss: 2.236901\n",
      "(Iteration 36201 / 61240) loss: 2.262852\n",
      "(Iteration 36301 / 61240) loss: 2.285262\n",
      "(Iteration 36401 / 61240) loss: 2.257285\n",
      "(Iteration 36501 / 61240) loss: 2.255399\n",
      "(Iteration 36601 / 61240) loss: 2.244212\n",
      "(Iteration 36701 / 61240) loss: 2.205683\n",
      "(Epoch 24 / 40) train acc: 0.218000; val_acc: 0.250000\n",
      "(Iteration 36801 / 61240) loss: 2.273901\n",
      "(Iteration 36901 / 61240) loss: 2.253634\n",
      "(Iteration 37001 / 61240) loss: 2.249511\n",
      "(Iteration 37101 / 61240) loss: 2.223822\n",
      "(Iteration 37201 / 61240) loss: 2.268025\n",
      "(Iteration 37301 / 61240) loss: 2.229304\n",
      "(Iteration 37401 / 61240) loss: 2.221683\n",
      "(Iteration 37501 / 61240) loss: 2.205891\n",
      "(Iteration 37601 / 61240) loss: 2.184741\n",
      "(Iteration 37701 / 61240) loss: 2.222506\n",
      "(Iteration 37801 / 61240) loss: 2.264066\n",
      "(Iteration 37901 / 61240) loss: 2.242756\n",
      "(Iteration 38001 / 61240) loss: 2.250448\n",
      "(Iteration 38101 / 61240) loss: 2.245790\n",
      "(Iteration 38201 / 61240) loss: 2.202600\n",
      "(Epoch 25 / 40) train acc: 0.234000; val_acc: 0.245000\n",
      "(Iteration 38301 / 61240) loss: 2.232065\n",
      "(Iteration 38401 / 61240) loss: 2.230006\n",
      "(Iteration 38501 / 61240) loss: 2.216150\n",
      "(Iteration 38601 / 61240) loss: 2.273097\n",
      "(Iteration 38701 / 61240) loss: 2.270636\n",
      "(Iteration 38801 / 61240) loss: 2.211352\n",
      "(Iteration 38901 / 61240) loss: 2.207959\n",
      "(Iteration 39001 / 61240) loss: 2.235642\n",
      "(Iteration 39101 / 61240) loss: 2.233238\n",
      "(Iteration 39201 / 61240) loss: 2.261410\n",
      "(Iteration 39301 / 61240) loss: 2.205855\n",
      "(Iteration 39401 / 61240) loss: 2.216242\n",
      "(Iteration 39501 / 61240) loss: 2.217267\n",
      "(Iteration 39601 / 61240) loss: 2.287036\n",
      "(Iteration 39701 / 61240) loss: 2.262923\n",
      "(Iteration 39801 / 61240) loss: 2.256454\n",
      "(Epoch 26 / 40) train acc: 0.252000; val_acc: 0.246000\n",
      "(Iteration 39901 / 61240) loss: 2.185669\n",
      "(Iteration 40001 / 61240) loss: 2.268954\n",
      "(Iteration 40101 / 61240) loss: 2.196373\n",
      "(Iteration 40201 / 61240) loss: 2.213932\n",
      "(Iteration 40301 / 61240) loss: 2.198194\n",
      "(Iteration 40401 / 61240) loss: 2.279373\n",
      "(Iteration 40501 / 61240) loss: 2.157891\n",
      "(Iteration 40601 / 61240) loss: 2.237400\n",
      "(Iteration 40701 / 61240) loss: 2.284765\n",
      "(Iteration 40801 / 61240) loss: 2.283074\n",
      "(Iteration 40901 / 61240) loss: 2.260517\n",
      "(Iteration 41001 / 61240) loss: 2.206768\n",
      "(Iteration 41101 / 61240) loss: 2.177544\n",
      "(Iteration 41201 / 61240) loss: 2.254933\n",
      "(Iteration 41301 / 61240) loss: 2.235063\n",
      "(Epoch 27 / 40) train acc: 0.230000; val_acc: 0.244000\n",
      "(Iteration 41401 / 61240) loss: 2.186385\n",
      "(Iteration 41501 / 61240) loss: 2.245931\n",
      "(Iteration 41601 / 61240) loss: 2.256968\n",
      "(Iteration 41701 / 61240) loss: 2.202348\n",
      "(Iteration 41801 / 61240) loss: 2.260147\n",
      "(Iteration 41901 / 61240) loss: 2.239640\n",
      "(Iteration 42001 / 61240) loss: 2.220773\n",
      "(Iteration 42101 / 61240) loss: 2.222953\n",
      "(Iteration 42201 / 61240) loss: 2.232719\n",
      "(Iteration 42301 / 61240) loss: 2.249332\n",
      "(Iteration 42401 / 61240) loss: 2.294126\n",
      "(Iteration 42501 / 61240) loss: 2.188513\n",
      "(Iteration 42601 / 61240) loss: 2.197472\n",
      "(Iteration 42701 / 61240) loss: 2.259802\n",
      "(Iteration 42801 / 61240) loss: 2.278258\n",
      "(Epoch 28 / 40) train acc: 0.245000; val_acc: 0.246000\n",
      "(Iteration 42901 / 61240) loss: 2.168953\n",
      "(Iteration 43001 / 61240) loss: 2.190344\n",
      "(Iteration 43101 / 61240) loss: 2.281433\n",
      "(Iteration 43201 / 61240) loss: 2.269975\n",
      "(Iteration 43301 / 61240) loss: 2.235005\n",
      "(Iteration 43401 / 61240) loss: 2.231136\n",
      "(Iteration 43501 / 61240) loss: 2.238212\n",
      "(Iteration 43601 / 61240) loss: 2.269244\n",
      "(Iteration 43701 / 61240) loss: 2.217896\n",
      "(Iteration 43801 / 61240) loss: 2.239712\n",
      "(Iteration 43901 / 61240) loss: 2.294952\n",
      "(Iteration 44001 / 61240) loss: 2.244224\n",
      "(Iteration 44101 / 61240) loss: 2.233598\n",
      "(Iteration 44201 / 61240) loss: 2.224103\n",
      "(Iteration 44301 / 61240) loss: 2.272970\n",
      "(Epoch 29 / 40) train acc: 0.245000; val_acc: 0.247000\n",
      "(Iteration 44401 / 61240) loss: 2.184875\n",
      "(Iteration 44501 / 61240) loss: 2.216662\n",
      "(Iteration 44601 / 61240) loss: 2.261964\n",
      "(Iteration 44701 / 61240) loss: 2.265365\n",
      "(Iteration 44801 / 61240) loss: 2.204138\n",
      "(Iteration 44901 / 61240) loss: 2.259642\n",
      "(Iteration 45001 / 61240) loss: 2.186824\n",
      "(Iteration 45101 / 61240) loss: 2.267142\n",
      "(Iteration 45201 / 61240) loss: 2.218780\n",
      "(Iteration 45301 / 61240) loss: 2.221772\n",
      "(Iteration 45401 / 61240) loss: 2.228357\n",
      "(Iteration 45501 / 61240) loss: 2.212888\n",
      "(Iteration 45601 / 61240) loss: 2.196361\n",
      "(Iteration 45701 / 61240) loss: 2.244128\n",
      "(Iteration 45801 / 61240) loss: 2.255301\n",
      "(Iteration 45901 / 61240) loss: 2.225899\n",
      "(Epoch 30 / 40) train acc: 0.226000; val_acc: 0.248000\n",
      "(Iteration 46001 / 61240) loss: 2.249596\n",
      "(Iteration 46101 / 61240) loss: 2.226420\n",
      "(Iteration 46201 / 61240) loss: 2.281908\n",
      "(Iteration 46301 / 61240) loss: 2.219427\n",
      "(Iteration 46401 / 61240) loss: 2.261179\n",
      "(Iteration 46501 / 61240) loss: 2.260721\n",
      "(Iteration 46601 / 61240) loss: 2.253379\n",
      "(Iteration 46701 / 61240) loss: 2.245878\n",
      "(Iteration 46801 / 61240) loss: 2.262286\n",
      "(Iteration 46901 / 61240) loss: 2.231952\n",
      "(Iteration 47001 / 61240) loss: 2.256025\n",
      "(Iteration 47101 / 61240) loss: 2.130574\n",
      "(Iteration 47201 / 61240) loss: 2.228279\n",
      "(Iteration 47301 / 61240) loss: 2.190269\n",
      "(Iteration 47401 / 61240) loss: 2.217526\n",
      "(Epoch 31 / 40) train acc: 0.225000; val_acc: 0.247000\n",
      "(Iteration 47501 / 61240) loss: 2.233737\n",
      "(Iteration 47601 / 61240) loss: 2.253367\n",
      "(Iteration 47701 / 61240) loss: 2.173670\n",
      "(Iteration 47801 / 61240) loss: 2.244851\n",
      "(Iteration 47901 / 61240) loss: 2.229251\n",
      "(Iteration 48001 / 61240) loss: 2.247818\n",
      "(Iteration 48101 / 61240) loss: 2.272283\n",
      "(Iteration 48201 / 61240) loss: 2.189643\n",
      "(Iteration 48301 / 61240) loss: 2.242394\n",
      "(Iteration 48401 / 61240) loss: 2.203731\n",
      "(Iteration 48501 / 61240) loss: 2.248154\n",
      "(Iteration 48601 / 61240) loss: 2.249048\n",
      "(Iteration 48701 / 61240) loss: 2.172491\n",
      "(Iteration 48801 / 61240) loss: 2.217787\n",
      "(Iteration 48901 / 61240) loss: 2.188101\n",
      "(Epoch 32 / 40) train acc: 0.261000; val_acc: 0.246000\n",
      "(Iteration 49001 / 61240) loss: 2.289868\n",
      "(Iteration 49101 / 61240) loss: 2.281505\n",
      "(Iteration 49201 / 61240) loss: 2.238237\n",
      "(Iteration 49301 / 61240) loss: 2.169463\n",
      "(Iteration 49401 / 61240) loss: 2.230381\n",
      "(Iteration 49501 / 61240) loss: 2.243431\n",
      "(Iteration 49601 / 61240) loss: 2.189177\n",
      "(Iteration 49701 / 61240) loss: 2.207966\n",
      "(Iteration 49801 / 61240) loss: 2.234922\n",
      "(Iteration 49901 / 61240) loss: 2.155484\n",
      "(Iteration 50001 / 61240) loss: 2.206417\n",
      "(Iteration 50101 / 61240) loss: 2.194052\n",
      "(Iteration 50201 / 61240) loss: 2.234998\n",
      "(Iteration 50301 / 61240) loss: 2.237688\n",
      "(Iteration 50401 / 61240) loss: 2.150576\n",
      "(Iteration 50501 / 61240) loss: 2.229268\n",
      "(Epoch 33 / 40) train acc: 0.226000; val_acc: 0.247000\n",
      "(Iteration 50601 / 61240) loss: 2.207081\n",
      "(Iteration 50701 / 61240) loss: 2.182207\n",
      "(Iteration 50801 / 61240) loss: 2.172240\n",
      "(Iteration 50901 / 61240) loss: 2.170115\n",
      "(Iteration 51001 / 61240) loss: 2.207574\n",
      "(Iteration 51101 / 61240) loss: 2.226213\n",
      "(Iteration 51201 / 61240) loss: 2.197599\n",
      "(Iteration 51301 / 61240) loss: 2.224336\n",
      "(Iteration 51401 / 61240) loss: 2.215265\n",
      "(Iteration 51501 / 61240) loss: 2.249075\n",
      "(Iteration 51601 / 61240) loss: 2.211698\n",
      "(Iteration 51701 / 61240) loss: 2.225673\n",
      "(Iteration 51801 / 61240) loss: 2.186884\n",
      "(Iteration 51901 / 61240) loss: 2.196269\n",
      "(Iteration 52001 / 61240) loss: 2.233935\n",
      "(Epoch 34 / 40) train acc: 0.227000; val_acc: 0.248000\n",
      "(Iteration 52101 / 61240) loss: 2.236365\n",
      "(Iteration 52201 / 61240) loss: 2.222891\n",
      "(Iteration 52301 / 61240) loss: 2.248770\n",
      "(Iteration 52401 / 61240) loss: 2.258663\n",
      "(Iteration 52501 / 61240) loss: 2.249704\n",
      "(Iteration 52601 / 61240) loss: 2.223977\n",
      "(Iteration 52701 / 61240) loss: 2.249945\n",
      "(Iteration 52801 / 61240) loss: 2.211704\n",
      "(Iteration 52901 / 61240) loss: 2.236122\n",
      "(Iteration 53001 / 61240) loss: 2.200834\n",
      "(Iteration 53101 / 61240) loss: 2.108404\n",
      "(Iteration 53201 / 61240) loss: 2.263978\n",
      "(Iteration 53301 / 61240) loss: 2.240694\n",
      "(Iteration 53401 / 61240) loss: 2.240533\n",
      "(Iteration 53501 / 61240) loss: 2.267790\n",
      "(Epoch 35 / 40) train acc: 0.242000; val_acc: 0.247000\n",
      "(Iteration 53601 / 61240) loss: 2.309599\n",
      "(Iteration 53701 / 61240) loss: 2.256648\n",
      "(Iteration 53801 / 61240) loss: 2.211454\n",
      "(Iteration 53901 / 61240) loss: 2.205057\n",
      "(Iteration 54001 / 61240) loss: 2.204357\n",
      "(Iteration 54101 / 61240) loss: 2.221229\n",
      "(Iteration 54201 / 61240) loss: 2.232818\n",
      "(Iteration 54301 / 61240) loss: 2.248875\n",
      "(Iteration 54401 / 61240) loss: 2.223078\n",
      "(Iteration 54501 / 61240) loss: 2.270474\n",
      "(Iteration 54601 / 61240) loss: 2.170939\n",
      "(Iteration 54701 / 61240) loss: 2.224879\n",
      "(Iteration 54801 / 61240) loss: 2.235699\n",
      "(Iteration 54901 / 61240) loss: 2.249215\n",
      "(Iteration 55001 / 61240) loss: 2.265138\n",
      "(Iteration 55101 / 61240) loss: 2.195829\n",
      "(Epoch 36 / 40) train acc: 0.236000; val_acc: 0.247000\n",
      "(Iteration 55201 / 61240) loss: 2.152190\n",
      "(Iteration 55301 / 61240) loss: 2.208595\n",
      "(Iteration 55401 / 61240) loss: 2.276226\n",
      "(Iteration 55501 / 61240) loss: 2.216465\n",
      "(Iteration 55601 / 61240) loss: 2.186156\n",
      "(Iteration 55701 / 61240) loss: 2.249703\n",
      "(Iteration 55801 / 61240) loss: 2.258781\n",
      "(Iteration 55901 / 61240) loss: 2.244262\n",
      "(Iteration 56001 / 61240) loss: 2.204068\n",
      "(Iteration 56101 / 61240) loss: 2.218555\n",
      "(Iteration 56201 / 61240) loss: 2.222578\n",
      "(Iteration 56301 / 61240) loss: 2.279849\n",
      "(Iteration 56401 / 61240) loss: 2.209692\n",
      "(Iteration 56501 / 61240) loss: 2.194978\n",
      "(Iteration 56601 / 61240) loss: 2.204063\n",
      "(Epoch 37 / 40) train acc: 0.236000; val_acc: 0.247000\n",
      "(Iteration 56701 / 61240) loss: 2.214864\n",
      "(Iteration 56801 / 61240) loss: 2.188035\n",
      "(Iteration 56901 / 61240) loss: 2.231450\n",
      "(Iteration 57001 / 61240) loss: 2.247693\n",
      "(Iteration 57101 / 61240) loss: 2.219902\n",
      "(Iteration 57201 / 61240) loss: 2.232393\n",
      "(Iteration 57301 / 61240) loss: 2.206076\n",
      "(Iteration 57401 / 61240) loss: 2.174485\n",
      "(Iteration 57501 / 61240) loss: 2.251193\n",
      "(Iteration 57601 / 61240) loss: 2.247498\n",
      "(Iteration 57701 / 61240) loss: 2.197052\n",
      "(Iteration 57801 / 61240) loss: 2.246318\n",
      "(Iteration 57901 / 61240) loss: 2.281435\n",
      "(Iteration 58001 / 61240) loss: 2.133013\n",
      "(Iteration 58101 / 61240) loss: 2.186394\n",
      "(Epoch 38 / 40) train acc: 0.228000; val_acc: 0.246000\n",
      "(Iteration 58201 / 61240) loss: 2.244626\n",
      "(Iteration 58301 / 61240) loss: 2.237955\n",
      "(Iteration 58401 / 61240) loss: 2.188741\n",
      "(Iteration 58501 / 61240) loss: 2.229120\n",
      "(Iteration 58601 / 61240) loss: 2.196614\n",
      "(Iteration 58701 / 61240) loss: 2.249975\n",
      "(Iteration 58801 / 61240) loss: 2.212648\n",
      "(Iteration 58901 / 61240) loss: 2.260625\n",
      "(Iteration 59001 / 61240) loss: 2.255725\n",
      "(Iteration 59101 / 61240) loss: 2.160811\n",
      "(Iteration 59201 / 61240) loss: 2.233896\n",
      "(Iteration 59301 / 61240) loss: 2.227515\n",
      "(Iteration 59401 / 61240) loss: 2.257579\n",
      "(Iteration 59501 / 61240) loss: 2.219210\n",
      "(Iteration 59601 / 61240) loss: 2.272635\n",
      "(Iteration 59701 / 61240) loss: 2.211135\n",
      "(Epoch 39 / 40) train acc: 0.235000; val_acc: 0.245000\n",
      "(Iteration 59801 / 61240) loss: 2.182575\n",
      "(Iteration 59901 / 61240) loss: 2.193684\n",
      "(Iteration 60001 / 61240) loss: 2.193166\n",
      "(Iteration 60101 / 61240) loss: 2.211617\n",
      "(Iteration 60201 / 61240) loss: 2.272652\n",
      "(Iteration 60301 / 61240) loss: 2.252757\n",
      "(Iteration 60401 / 61240) loss: 2.273719\n",
      "(Iteration 60501 / 61240) loss: 2.146893\n",
      "(Iteration 60601 / 61240) loss: 2.108667\n",
      "(Iteration 60701 / 61240) loss: 2.166655\n",
      "(Iteration 60801 / 61240) loss: 2.181384\n",
      "(Iteration 60901 / 61240) loss: 2.230000\n",
      "(Iteration 61001 / 61240) loss: 2.239862\n",
      "(Iteration 61101 / 61240) loss: 2.145888\n",
      "(Iteration 61201 / 61240) loss: 2.214804\n",
      "(Epoch 40 / 40) train acc: 0.206000; val_acc: 0.245000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.001, 'num_epochs': 40, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 30600) loss: 2.303120\n",
      "(Epoch 0 / 40) train acc: 0.123000; val_acc: 0.111000\n",
      "(Iteration 101 / 30600) loss: 2.303190\n",
      "(Iteration 201 / 30600) loss: 2.303163\n",
      "(Iteration 301 / 30600) loss: 2.303069\n",
      "(Iteration 401 / 30600) loss: 2.303235\n",
      "(Iteration 501 / 30600) loss: 2.303014\n",
      "(Iteration 601 / 30600) loss: 2.302803\n",
      "(Iteration 701 / 30600) loss: 2.303013\n",
      "(Epoch 1 / 40) train acc: 0.117000; val_acc: 0.107000\n",
      "(Iteration 801 / 30600) loss: 2.302939\n",
      "(Iteration 901 / 30600) loss: 2.302778\n",
      "(Iteration 1001 / 30600) loss: 2.302949\n",
      "(Iteration 1101 / 30600) loss: 2.302767\n",
      "(Iteration 1201 / 30600) loss: 2.302893\n",
      "(Iteration 1301 / 30600) loss: 2.302988\n",
      "(Iteration 1401 / 30600) loss: 2.302777\n",
      "(Iteration 1501 / 30600) loss: 2.302344\n",
      "(Epoch 2 / 40) train acc: 0.089000; val_acc: 0.107000\n",
      "(Iteration 1601 / 30600) loss: 2.302881\n",
      "(Iteration 1701 / 30600) loss: 2.302389\n",
      "(Iteration 1801 / 30600) loss: 2.302819\n",
      "(Iteration 1901 / 30600) loss: 2.302711\n",
      "(Iteration 2001 / 30600) loss: 2.302878\n",
      "(Iteration 2101 / 30600) loss: 2.302512\n",
      "(Iteration 2201 / 30600) loss: 2.302377\n",
      "(Epoch 3 / 40) train acc: 0.175000; val_acc: 0.187000\n",
      "(Iteration 2301 / 30600) loss: 2.302588\n",
      "(Iteration 2401 / 30600) loss: 2.302167\n",
      "(Iteration 2501 / 30600) loss: 2.302451\n",
      "(Iteration 2601 / 30600) loss: 2.301834\n",
      "(Iteration 2701 / 30600) loss: 2.302235\n",
      "(Iteration 2801 / 30600) loss: 2.302223\n",
      "(Iteration 2901 / 30600) loss: 2.302006\n",
      "(Iteration 3001 / 30600) loss: 2.302536\n",
      "(Epoch 4 / 40) train acc: 0.264000; val_acc: 0.257000\n",
      "(Iteration 3101 / 30600) loss: 2.302146\n",
      "(Iteration 3201 / 30600) loss: 2.302383\n",
      "(Iteration 3301 / 30600) loss: 2.301835\n",
      "(Iteration 3401 / 30600) loss: 2.301953\n",
      "(Iteration 3501 / 30600) loss: 2.302076\n",
      "(Iteration 3601 / 30600) loss: 2.301521\n",
      "(Iteration 3701 / 30600) loss: 2.302152\n",
      "(Iteration 3801 / 30600) loss: 2.302018\n",
      "(Epoch 5 / 40) train acc: 0.230000; val_acc: 0.223000\n",
      "(Iteration 3901 / 30600) loss: 2.301441\n",
      "(Iteration 4001 / 30600) loss: 2.302325\n",
      "(Iteration 4101 / 30600) loss: 2.301662\n",
      "(Iteration 4201 / 30600) loss: 2.301403\n",
      "(Iteration 4301 / 30600) loss: 2.301921\n",
      "(Iteration 4401 / 30600) loss: 2.302462\n",
      "(Iteration 4501 / 30600) loss: 2.301279\n",
      "(Epoch 6 / 40) train acc: 0.217000; val_acc: 0.228000\n",
      "(Iteration 4601 / 30600) loss: 2.301671\n",
      "(Iteration 4701 / 30600) loss: 2.301389\n",
      "(Iteration 4801 / 30600) loss: 2.300808\n",
      "(Iteration 4901 / 30600) loss: 2.301563\n",
      "(Iteration 5001 / 30600) loss: 2.301641\n",
      "(Iteration 5101 / 30600) loss: 2.301750\n",
      "(Iteration 5201 / 30600) loss: 2.300673\n",
      "(Iteration 5301 / 30600) loss: 2.301406\n",
      "(Epoch 7 / 40) train acc: 0.226000; val_acc: 0.227000\n",
      "(Iteration 5401 / 30600) loss: 2.301157\n",
      "(Iteration 5501 / 30600) loss: 2.301229\n",
      "(Iteration 5601 / 30600) loss: 2.301122\n",
      "(Iteration 5701 / 30600) loss: 2.301352\n",
      "(Iteration 5801 / 30600) loss: 2.300705\n",
      "(Iteration 5901 / 30600) loss: 2.301009\n",
      "(Iteration 6001 / 30600) loss: 2.301128\n",
      "(Iteration 6101 / 30600) loss: 2.301003\n",
      "(Epoch 8 / 40) train acc: 0.270000; val_acc: 0.249000\n",
      "(Iteration 6201 / 30600) loss: 2.300806\n",
      "(Iteration 6301 / 30600) loss: 2.300395\n",
      "(Iteration 6401 / 30600) loss: 2.299833\n",
      "(Iteration 6501 / 30600) loss: 2.299623\n",
      "(Iteration 6601 / 30600) loss: 2.300302\n",
      "(Iteration 6701 / 30600) loss: 2.299756\n",
      "(Iteration 6801 / 30600) loss: 2.299841\n",
      "(Epoch 9 / 40) train acc: 0.269000; val_acc: 0.261000\n",
      "(Iteration 6901 / 30600) loss: 2.299269\n",
      "(Iteration 7001 / 30600) loss: 2.299767\n",
      "(Iteration 7101 / 30600) loss: 2.300192\n",
      "(Iteration 7201 / 30600) loss: 2.299937\n",
      "(Iteration 7301 / 30600) loss: 2.298156\n",
      "(Iteration 7401 / 30600) loss: 2.299097\n",
      "(Iteration 7501 / 30600) loss: 2.299298\n",
      "(Iteration 7601 / 30600) loss: 2.299209\n",
      "(Epoch 10 / 40) train acc: 0.253000; val_acc: 0.274000\n",
      "(Iteration 7701 / 30600) loss: 2.299800\n",
      "(Iteration 7801 / 30600) loss: 2.300003\n",
      "(Iteration 7901 / 30600) loss: 2.298488\n",
      "(Iteration 8001 / 30600) loss: 2.299330\n",
      "(Iteration 8101 / 30600) loss: 2.297458\n",
      "(Iteration 8201 / 30600) loss: 2.300057\n",
      "(Iteration 8301 / 30600) loss: 2.299689\n",
      "(Iteration 8401 / 30600) loss: 2.298427\n",
      "(Epoch 11 / 40) train acc: 0.274000; val_acc: 0.290000\n",
      "(Iteration 8501 / 30600) loss: 2.299127\n",
      "(Iteration 8601 / 30600) loss: 2.299507\n",
      "(Iteration 8701 / 30600) loss: 2.298758\n",
      "(Iteration 8801 / 30600) loss: 2.297936\n",
      "(Iteration 8901 / 30600) loss: 2.298753\n",
      "(Iteration 9001 / 30600) loss: 2.299054\n",
      "(Iteration 9101 / 30600) loss: 2.298066\n",
      "(Epoch 12 / 40) train acc: 0.314000; val_acc: 0.303000\n",
      "(Iteration 9201 / 30600) loss: 2.299265\n",
      "(Iteration 9301 / 30600) loss: 2.296935\n",
      "(Iteration 9401 / 30600) loss: 2.297496\n",
      "(Iteration 9501 / 30600) loss: 2.298053\n",
      "(Iteration 9601 / 30600) loss: 2.296615\n",
      "(Iteration 9701 / 30600) loss: 2.297660\n",
      "(Iteration 9801 / 30600) loss: 2.298644\n",
      "(Iteration 9901 / 30600) loss: 2.297088\n",
      "(Epoch 13 / 40) train acc: 0.293000; val_acc: 0.297000\n",
      "(Iteration 10001 / 30600) loss: 2.297262\n",
      "(Iteration 10101 / 30600) loss: 2.298068\n",
      "(Iteration 10201 / 30600) loss: 2.295883\n",
      "(Iteration 10301 / 30600) loss: 2.296858\n",
      "(Iteration 10401 / 30600) loss: 2.296808\n",
      "(Iteration 10501 / 30600) loss: 2.294494\n",
      "(Iteration 10601 / 30600) loss: 2.293865\n",
      "(Iteration 10701 / 30600) loss: 2.297447\n",
      "(Epoch 14 / 40) train acc: 0.316000; val_acc: 0.297000\n",
      "(Iteration 10801 / 30600) loss: 2.297149\n",
      "(Iteration 10901 / 30600) loss: 2.297317\n",
      "(Iteration 11001 / 30600) loss: 2.295516\n",
      "(Iteration 11101 / 30600) loss: 2.297160\n",
      "(Iteration 11201 / 30600) loss: 2.296641\n",
      "(Iteration 11301 / 30600) loss: 2.296695\n",
      "(Iteration 11401 / 30600) loss: 2.296987\n",
      "(Epoch 15 / 40) train acc: 0.295000; val_acc: 0.290000\n",
      "(Iteration 11501 / 30600) loss: 2.295020\n",
      "(Iteration 11601 / 30600) loss: 2.296181\n",
      "(Iteration 11701 / 30600) loss: 2.296295\n",
      "(Iteration 11801 / 30600) loss: 2.296054\n",
      "(Iteration 11901 / 30600) loss: 2.294135\n",
      "(Iteration 12001 / 30600) loss: 2.293512\n",
      "(Iteration 12101 / 30600) loss: 2.296267\n",
      "(Iteration 12201 / 30600) loss: 2.297564\n",
      "(Epoch 16 / 40) train acc: 0.331000; val_acc: 0.294000\n",
      "(Iteration 12301 / 30600) loss: 2.297015\n",
      "(Iteration 12401 / 30600) loss: 2.292790\n",
      "(Iteration 12501 / 30600) loss: 2.295073\n",
      "(Iteration 12601 / 30600) loss: 2.295701\n",
      "(Iteration 12701 / 30600) loss: 2.295599\n",
      "(Iteration 12801 / 30600) loss: 2.293110\n",
      "(Iteration 12901 / 30600) loss: 2.293192\n",
      "(Iteration 13001 / 30600) loss: 2.294402\n",
      "(Epoch 17 / 40) train acc: 0.274000; val_acc: 0.294000\n",
      "(Iteration 13101 / 30600) loss: 2.293931\n",
      "(Iteration 13201 / 30600) loss: 2.295742\n",
      "(Iteration 13301 / 30600) loss: 2.293074\n",
      "(Iteration 13401 / 30600) loss: 2.295733\n",
      "(Iteration 13501 / 30600) loss: 2.292006\n",
      "(Iteration 13601 / 30600) loss: 2.293509\n",
      "(Iteration 13701 / 30600) loss: 2.293128\n",
      "(Epoch 18 / 40) train acc: 0.326000; val_acc: 0.302000\n",
      "(Iteration 13801 / 30600) loss: 2.291596\n",
      "(Iteration 13901 / 30600) loss: 2.294531\n",
      "(Iteration 14001 / 30600) loss: 2.290944\n",
      "(Iteration 14101 / 30600) loss: 2.293750\n",
      "(Iteration 14201 / 30600) loss: 2.293522\n",
      "(Iteration 14301 / 30600) loss: 2.289442\n",
      "(Iteration 14401 / 30600) loss: 2.290303\n",
      "(Iteration 14501 / 30600) loss: 2.292728\n",
      "(Epoch 19 / 40) train acc: 0.317000; val_acc: 0.300000\n",
      "(Iteration 14601 / 30600) loss: 2.294000\n",
      "(Iteration 14701 / 30600) loss: 2.293190\n",
      "(Iteration 14801 / 30600) loss: 2.293371\n",
      "(Iteration 14901 / 30600) loss: 2.294847\n",
      "(Iteration 15001 / 30600) loss: 2.291297\n",
      "(Iteration 15101 / 30600) loss: 2.291061\n",
      "(Iteration 15201 / 30600) loss: 2.294771\n",
      "(Epoch 20 / 40) train acc: 0.297000; val_acc: 0.306000\n",
      "(Iteration 15301 / 30600) loss: 2.294935\n",
      "(Iteration 15401 / 30600) loss: 2.295758\n",
      "(Iteration 15501 / 30600) loss: 2.288247\n",
      "(Iteration 15601 / 30600) loss: 2.296358\n",
      "(Iteration 15701 / 30600) loss: 2.293783\n",
      "(Iteration 15801 / 30600) loss: 2.290804\n",
      "(Iteration 15901 / 30600) loss: 2.293759\n",
      "(Iteration 16001 / 30600) loss: 2.288124\n",
      "(Epoch 21 / 40) train acc: 0.306000; val_acc: 0.308000\n",
      "(Iteration 16101 / 30600) loss: 2.293620\n",
      "(Iteration 16201 / 30600) loss: 2.292511\n",
      "(Iteration 16301 / 30600) loss: 2.291925\n",
      "(Iteration 16401 / 30600) loss: 2.288137\n",
      "(Iteration 16501 / 30600) loss: 2.290365\n",
      "(Iteration 16601 / 30600) loss: 2.288911\n",
      "(Iteration 16701 / 30600) loss: 2.289016\n",
      "(Iteration 16801 / 30600) loss: 2.295308\n",
      "(Epoch 22 / 40) train acc: 0.328000; val_acc: 0.307000\n",
      "(Iteration 16901 / 30600) loss: 2.290714\n",
      "(Iteration 17001 / 30600) loss: 2.295664\n",
      "(Iteration 17101 / 30600) loss: 2.290226\n",
      "(Iteration 17201 / 30600) loss: 2.290494\n",
      "(Iteration 17301 / 30600) loss: 2.290098\n",
      "(Iteration 17401 / 30600) loss: 2.290547\n",
      "(Iteration 17501 / 30600) loss: 2.286684\n",
      "(Epoch 23 / 40) train acc: 0.319000; val_acc: 0.299000\n",
      "(Iteration 17601 / 30600) loss: 2.293003\n",
      "(Iteration 17701 / 30600) loss: 2.290631\n",
      "(Iteration 17801 / 30600) loss: 2.292214\n",
      "(Iteration 17901 / 30600) loss: 2.288859\n",
      "(Iteration 18001 / 30600) loss: 2.290146\n",
      "(Iteration 18101 / 30600) loss: 2.293561\n",
      "(Iteration 18201 / 30600) loss: 2.292013\n",
      "(Iteration 18301 / 30600) loss: 2.286638\n",
      "(Epoch 24 / 40) train acc: 0.292000; val_acc: 0.298000\n",
      "(Iteration 18401 / 30600) loss: 2.293331\n",
      "(Iteration 18501 / 30600) loss: 2.294296\n",
      "(Iteration 18601 / 30600) loss: 2.288998\n",
      "(Iteration 18701 / 30600) loss: 2.290913\n",
      "(Iteration 18801 / 30600) loss: 2.289879\n",
      "(Iteration 18901 / 30600) loss: 2.287636\n",
      "(Iteration 19001 / 30600) loss: 2.290702\n",
      "(Iteration 19101 / 30600) loss: 2.293588\n",
      "(Epoch 25 / 40) train acc: 0.341000; val_acc: 0.298000\n",
      "(Iteration 19201 / 30600) loss: 2.284878\n",
      "(Iteration 19301 / 30600) loss: 2.288510\n",
      "(Iteration 19401 / 30600) loss: 2.290412\n",
      "(Iteration 19501 / 30600) loss: 2.283232\n",
      "(Iteration 19601 / 30600) loss: 2.290645\n",
      "(Iteration 19701 / 30600) loss: 2.287767\n",
      "(Iteration 19801 / 30600) loss: 2.286405\n",
      "(Epoch 26 / 40) train acc: 0.306000; val_acc: 0.301000\n",
      "(Iteration 19901 / 30600) loss: 2.283524\n",
      "(Iteration 20001 / 30600) loss: 2.288935\n",
      "(Iteration 20101 / 30600) loss: 2.282219\n",
      "(Iteration 20201 / 30600) loss: 2.291960\n",
      "(Iteration 20301 / 30600) loss: 2.295133\n",
      "(Iteration 20401 / 30600) loss: 2.294790\n",
      "(Iteration 20501 / 30600) loss: 2.287309\n",
      "(Iteration 20601 / 30600) loss: 2.278260\n",
      "(Epoch 27 / 40) train acc: 0.309000; val_acc: 0.300000\n",
      "(Iteration 20701 / 30600) loss: 2.293230\n",
      "(Iteration 20801 / 30600) loss: 2.293234\n",
      "(Iteration 20901 / 30600) loss: 2.290121\n",
      "(Iteration 21001 / 30600) loss: 2.285851\n",
      "(Iteration 21101 / 30600) loss: 2.284459\n",
      "(Iteration 21201 / 30600) loss: 2.289109\n",
      "(Iteration 21301 / 30600) loss: 2.283908\n",
      "(Iteration 21401 / 30600) loss: 2.286777\n",
      "(Epoch 28 / 40) train acc: 0.285000; val_acc: 0.297000\n",
      "(Iteration 21501 / 30600) loss: 2.285378\n",
      "(Iteration 21601 / 30600) loss: 2.288647\n",
      "(Iteration 21701 / 30600) loss: 2.290577\n",
      "(Iteration 21801 / 30600) loss: 2.289711\n",
      "(Iteration 21901 / 30600) loss: 2.293045\n",
      "(Iteration 22001 / 30600) loss: 2.293135\n",
      "(Iteration 22101 / 30600) loss: 2.290393\n",
      "(Epoch 29 / 40) train acc: 0.314000; val_acc: 0.297000\n",
      "(Iteration 22201 / 30600) loss: 2.291392\n",
      "(Iteration 22301 / 30600) loss: 2.287811\n",
      "(Iteration 22401 / 30600) loss: 2.284377\n",
      "(Iteration 22501 / 30600) loss: 2.285010\n",
      "(Iteration 22601 / 30600) loss: 2.292862\n",
      "(Iteration 22701 / 30600) loss: 2.288806\n",
      "(Iteration 22801 / 30600) loss: 2.288117\n",
      "(Iteration 22901 / 30600) loss: 2.290067\n",
      "(Epoch 30 / 40) train acc: 0.302000; val_acc: 0.299000\n",
      "(Iteration 23001 / 30600) loss: 2.281369\n",
      "(Iteration 23101 / 30600) loss: 2.284786\n",
      "(Iteration 23201 / 30600) loss: 2.283960\n",
      "(Iteration 23301 / 30600) loss: 2.290866\n",
      "(Iteration 23401 / 30600) loss: 2.283890\n",
      "(Iteration 23501 / 30600) loss: 2.286644\n",
      "(Iteration 23601 / 30600) loss: 2.290442\n",
      "(Iteration 23701 / 30600) loss: 2.282150\n",
      "(Epoch 31 / 40) train acc: 0.307000; val_acc: 0.297000\n",
      "(Iteration 23801 / 30600) loss: 2.289431\n",
      "(Iteration 23901 / 30600) loss: 2.289023\n",
      "(Iteration 24001 / 30600) loss: 2.285212\n",
      "(Iteration 24101 / 30600) loss: 2.280417\n",
      "(Iteration 24201 / 30600) loss: 2.282741\n",
      "(Iteration 24301 / 30600) loss: 2.289852\n",
      "(Iteration 24401 / 30600) loss: 2.289123\n",
      "(Epoch 32 / 40) train acc: 0.318000; val_acc: 0.297000\n",
      "(Iteration 24501 / 30600) loss: 2.282938\n",
      "(Iteration 24601 / 30600) loss: 2.284950\n",
      "(Iteration 24701 / 30600) loss: 2.288884\n",
      "(Iteration 24801 / 30600) loss: 2.288316\n",
      "(Iteration 24901 / 30600) loss: 2.289755\n",
      "(Iteration 25001 / 30600) loss: 2.285979\n",
      "(Iteration 25101 / 30600) loss: 2.289718\n",
      "(Iteration 25201 / 30600) loss: 2.287939\n",
      "(Epoch 33 / 40) train acc: 0.334000; val_acc: 0.297000\n",
      "(Iteration 25301 / 30600) loss: 2.283528\n",
      "(Iteration 25401 / 30600) loss: 2.298779\n",
      "(Iteration 25501 / 30600) loss: 2.284125\n",
      "(Iteration 25601 / 30600) loss: 2.282699\n",
      "(Iteration 25701 / 30600) loss: 2.290810\n",
      "(Iteration 25801 / 30600) loss: 2.289748\n",
      "(Iteration 25901 / 30600) loss: 2.280789\n",
      "(Iteration 26001 / 30600) loss: 2.284340\n",
      "(Epoch 34 / 40) train acc: 0.318000; val_acc: 0.293000\n",
      "(Iteration 26101 / 30600) loss: 2.286007\n",
      "(Iteration 26201 / 30600) loss: 2.286873\n",
      "(Iteration 26301 / 30600) loss: 2.286163\n",
      "(Iteration 26401 / 30600) loss: 2.290694\n",
      "(Iteration 26501 / 30600) loss: 2.287766\n",
      "(Iteration 26601 / 30600) loss: 2.292989\n",
      "(Iteration 26701 / 30600) loss: 2.291323\n",
      "(Epoch 35 / 40) train acc: 0.320000; val_acc: 0.294000\n",
      "(Iteration 26801 / 30600) loss: 2.288676\n",
      "(Iteration 26901 / 30600) loss: 2.286086\n",
      "(Iteration 27001 / 30600) loss: 2.285191\n",
      "(Iteration 27101 / 30600) loss: 2.286504\n",
      "(Iteration 27201 / 30600) loss: 2.285851\n",
      "(Iteration 27301 / 30600) loss: 2.287247\n",
      "(Iteration 27401 / 30600) loss: 2.287606\n",
      "(Iteration 27501 / 30600) loss: 2.290810\n",
      "(Epoch 36 / 40) train acc: 0.324000; val_acc: 0.293000\n",
      "(Iteration 27601 / 30600) loss: 2.276117\n",
      "(Iteration 27701 / 30600) loss: 2.290283\n",
      "(Iteration 27801 / 30600) loss: 2.285462\n",
      "(Iteration 27901 / 30600) loss: 2.284678\n",
      "(Iteration 28001 / 30600) loss: 2.284550\n",
      "(Iteration 28101 / 30600) loss: 2.287491\n",
      "(Iteration 28201 / 30600) loss: 2.283112\n",
      "(Iteration 28301 / 30600) loss: 2.280253\n",
      "(Epoch 37 / 40) train acc: 0.286000; val_acc: 0.294000\n",
      "(Iteration 28401 / 30600) loss: 2.280733\n",
      "(Iteration 28501 / 30600) loss: 2.284093\n",
      "(Iteration 28601 / 30600) loss: 2.281058\n",
      "(Iteration 28701 / 30600) loss: 2.284381\n",
      "(Iteration 28801 / 30600) loss: 2.285888\n",
      "(Iteration 28901 / 30600) loss: 2.290293\n",
      "(Iteration 29001 / 30600) loss: 2.277832\n",
      "(Epoch 38 / 40) train acc: 0.284000; val_acc: 0.294000\n",
      "(Iteration 29101 / 30600) loss: 2.283041\n",
      "(Iteration 29201 / 30600) loss: 2.289520\n",
      "(Iteration 29301 / 30600) loss: 2.287138\n",
      "(Iteration 29401 / 30600) loss: 2.282720\n",
      "(Iteration 29501 / 30600) loss: 2.285269\n",
      "(Iteration 29601 / 30600) loss: 2.285104\n",
      "(Iteration 29701 / 30600) loss: 2.278791\n",
      "(Iteration 29801 / 30600) loss: 2.281868\n",
      "(Epoch 39 / 40) train acc: 0.333000; val_acc: 0.293000\n",
      "(Iteration 29901 / 30600) loss: 2.288382\n",
      "(Iteration 30001 / 30600) loss: 2.288018\n",
      "(Iteration 30101 / 30600) loss: 2.289588\n",
      "(Iteration 30201 / 30600) loss: 2.287385\n",
      "(Iteration 30301 / 30600) loss: 2.281201\n",
      "(Iteration 30401 / 30600) loss: 2.282317\n",
      "(Iteration 30501 / 30600) loss: 2.287195\n",
      "(Epoch 40 / 40) train acc: 0.297000; val_acc: 0.293000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.001, 'num_epochs': 40, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 61240) loss: 2.303259\n",
      "(Epoch 0 / 40) train acc: 0.124000; val_acc: 0.127000\n",
      "(Iteration 101 / 61240) loss: 2.303188\n",
      "(Iteration 201 / 61240) loss: 2.302879\n",
      "(Iteration 301 / 61240) loss: 2.303148\n",
      "(Iteration 401 / 61240) loss: 2.303254\n",
      "(Iteration 501 / 61240) loss: 2.303064\n",
      "(Iteration 601 / 61240) loss: 2.303131\n",
      "(Iteration 701 / 61240) loss: 2.302972\n",
      "(Iteration 801 / 61240) loss: 2.302439\n",
      "(Iteration 901 / 61240) loss: 2.302571\n",
      "(Iteration 1001 / 61240) loss: 2.303063\n",
      "(Iteration 1101 / 61240) loss: 2.302675\n",
      "(Iteration 1201 / 61240) loss: 2.302985\n",
      "(Iteration 1301 / 61240) loss: 2.302887\n",
      "(Iteration 1401 / 61240) loss: 2.302706\n",
      "(Iteration 1501 / 61240) loss: 2.302617\n",
      "(Epoch 1 / 40) train acc: 0.140000; val_acc: 0.164000\n",
      "(Iteration 1601 / 61240) loss: 2.302914\n",
      "(Iteration 1701 / 61240) loss: 2.302610\n",
      "(Iteration 1801 / 61240) loss: 2.302771\n",
      "(Iteration 1901 / 61240) loss: 2.302281\n",
      "(Iteration 2001 / 61240) loss: 2.302775\n",
      "(Iteration 2101 / 61240) loss: 2.302435\n",
      "(Iteration 2201 / 61240) loss: 2.302231\n",
      "(Iteration 2301 / 61240) loss: 2.302260\n",
      "(Iteration 2401 / 61240) loss: 2.302115\n",
      "(Iteration 2501 / 61240) loss: 2.301350\n",
      "(Iteration 2601 / 61240) loss: 2.301364\n",
      "(Iteration 2701 / 61240) loss: 2.301068\n",
      "(Iteration 2801 / 61240) loss: 2.302093\n",
      "(Iteration 2901 / 61240) loss: 2.302347\n",
      "(Iteration 3001 / 61240) loss: 2.301928\n",
      "(Epoch 2 / 40) train acc: 0.291000; val_acc: 0.286000\n",
      "(Iteration 3101 / 61240) loss: 2.301773\n",
      "(Iteration 3201 / 61240) loss: 2.302190\n",
      "(Iteration 3301 / 61240) loss: 2.302892\n",
      "(Iteration 3401 / 61240) loss: 2.302679\n",
      "(Iteration 3501 / 61240) loss: 2.302326\n",
      "(Iteration 3601 / 61240) loss: 2.301273\n",
      "(Iteration 3701 / 61240) loss: 2.300934\n",
      "(Iteration 3801 / 61240) loss: 2.302404\n",
      "(Iteration 3901 / 61240) loss: 2.301841\n",
      "(Iteration 4001 / 61240) loss: 2.302508\n",
      "(Iteration 4101 / 61240) loss: 2.302031\n",
      "(Iteration 4201 / 61240) loss: 2.301713\n",
      "(Iteration 4301 / 61240) loss: 2.301649\n",
      "(Iteration 4401 / 61240) loss: 2.300284\n",
      "(Iteration 4501 / 61240) loss: 2.301324\n",
      "(Epoch 3 / 40) train acc: 0.201000; val_acc: 0.240000\n",
      "(Iteration 4601 / 61240) loss: 2.300482\n",
      "(Iteration 4701 / 61240) loss: 2.300030\n",
      "(Iteration 4801 / 61240) loss: 2.299278\n",
      "(Iteration 4901 / 61240) loss: 2.300630\n",
      "(Iteration 5001 / 61240) loss: 2.301268\n",
      "(Iteration 5101 / 61240) loss: 2.301424\n",
      "(Iteration 5201 / 61240) loss: 2.300053\n",
      "(Iteration 5301 / 61240) loss: 2.298733\n",
      "(Iteration 5401 / 61240) loss: 2.300061\n",
      "(Iteration 5501 / 61240) loss: 2.299092\n",
      "(Iteration 5601 / 61240) loss: 2.300267\n",
      "(Iteration 5701 / 61240) loss: 2.296581\n",
      "(Iteration 5801 / 61240) loss: 2.299591\n",
      "(Iteration 5901 / 61240) loss: 2.295589\n",
      "(Iteration 6001 / 61240) loss: 2.298379\n",
      "(Iteration 6101 / 61240) loss: 2.299844\n",
      "(Epoch 4 / 40) train acc: 0.233000; val_acc: 0.255000\n",
      "(Iteration 6201 / 61240) loss: 2.299036\n",
      "(Iteration 6301 / 61240) loss: 2.298953\n",
      "(Iteration 6401 / 61240) loss: 2.298193\n",
      "(Iteration 6501 / 61240) loss: 2.298331\n",
      "(Iteration 6601 / 61240) loss: 2.297133\n",
      "(Iteration 6701 / 61240) loss: 2.297017\n",
      "(Iteration 6801 / 61240) loss: 2.299522\n",
      "(Iteration 6901 / 61240) loss: 2.291376\n",
      "(Iteration 7001 / 61240) loss: 2.296257\n",
      "(Iteration 7101 / 61240) loss: 2.295795\n",
      "(Iteration 7201 / 61240) loss: 2.294053\n",
      "(Iteration 7301 / 61240) loss: 2.297909\n",
      "(Iteration 7401 / 61240) loss: 2.294311\n",
      "(Iteration 7501 / 61240) loss: 2.293576\n",
      "(Iteration 7601 / 61240) loss: 2.296441\n",
      "(Epoch 5 / 40) train acc: 0.241000; val_acc: 0.252000\n",
      "(Iteration 7701 / 61240) loss: 2.291230\n",
      "(Iteration 7801 / 61240) loss: 2.297920\n",
      "(Iteration 7901 / 61240) loss: 2.293267\n",
      "(Iteration 8001 / 61240) loss: 2.292787\n",
      "(Iteration 8101 / 61240) loss: 2.292914\n",
      "(Iteration 8201 / 61240) loss: 2.296231\n",
      "(Iteration 8301 / 61240) loss: 2.290970\n",
      "(Iteration 8401 / 61240) loss: 2.290923\n",
      "(Iteration 8501 / 61240) loss: 2.293884\n",
      "(Iteration 8601 / 61240) loss: 2.289365\n",
      "(Iteration 8701 / 61240) loss: 2.295745\n",
      "(Iteration 8801 / 61240) loss: 2.289690\n",
      "(Iteration 8901 / 61240) loss: 2.286561\n",
      "(Iteration 9001 / 61240) loss: 2.291343\n",
      "(Iteration 9101 / 61240) loss: 2.289322\n",
      "(Epoch 6 / 40) train acc: 0.264000; val_acc: 0.278000\n",
      "(Iteration 9201 / 61240) loss: 2.287080\n",
      "(Iteration 9301 / 61240) loss: 2.289898\n",
      "(Iteration 9401 / 61240) loss: 2.294386\n",
      "(Iteration 9501 / 61240) loss: 2.294371\n",
      "(Iteration 9601 / 61240) loss: 2.288395\n",
      "(Iteration 9701 / 61240) loss: 2.287749\n",
      "(Iteration 9801 / 61240) loss: 2.293967\n",
      "(Iteration 9901 / 61240) loss: 2.283891\n",
      "(Iteration 10001 / 61240) loss: 2.285957\n",
      "(Iteration 10101 / 61240) loss: 2.291939\n",
      "(Iteration 10201 / 61240) loss: 2.271592\n",
      "(Iteration 10301 / 61240) loss: 2.288326\n",
      "(Iteration 10401 / 61240) loss: 2.283477\n",
      "(Iteration 10501 / 61240) loss: 2.279649\n",
      "(Iteration 10601 / 61240) loss: 2.277994\n",
      "(Iteration 10701 / 61240) loss: 2.291057\n",
      "(Epoch 7 / 40) train acc: 0.305000; val_acc: 0.298000\n",
      "(Iteration 10801 / 61240) loss: 2.268977\n",
      "(Iteration 10901 / 61240) loss: 2.291586\n",
      "(Iteration 11001 / 61240) loss: 2.281740\n",
      "(Iteration 11101 / 61240) loss: 2.286010\n",
      "(Iteration 11201 / 61240) loss: 2.280593\n",
      "(Iteration 11301 / 61240) loss: 2.272628\n",
      "(Iteration 11401 / 61240) loss: 2.261425\n",
      "(Iteration 11501 / 61240) loss: 2.280435\n",
      "(Iteration 11601 / 61240) loss: 2.277609\n",
      "(Iteration 11701 / 61240) loss: 2.273089\n",
      "(Iteration 11801 / 61240) loss: 2.266230\n",
      "(Iteration 11901 / 61240) loss: 2.270402\n",
      "(Iteration 12001 / 61240) loss: 2.279939\n",
      "(Iteration 12101 / 61240) loss: 2.265427\n",
      "(Iteration 12201 / 61240) loss: 2.283431\n",
      "(Epoch 8 / 40) train acc: 0.282000; val_acc: 0.281000\n",
      "(Iteration 12301 / 61240) loss: 2.290174\n",
      "(Iteration 12401 / 61240) loss: 2.284459\n",
      "(Iteration 12501 / 61240) loss: 2.237905\n",
      "(Iteration 12601 / 61240) loss: 2.260650\n",
      "(Iteration 12701 / 61240) loss: 2.259204\n",
      "(Iteration 12801 / 61240) loss: 2.261277\n",
      "(Iteration 12901 / 61240) loss: 2.280834\n",
      "(Iteration 13001 / 61240) loss: 2.247918\n",
      "(Iteration 13101 / 61240) loss: 2.248202\n",
      "(Iteration 13201 / 61240) loss: 2.252007\n",
      "(Iteration 13301 / 61240) loss: 2.250153\n",
      "(Iteration 13401 / 61240) loss: 2.246346\n",
      "(Iteration 13501 / 61240) loss: 2.258146\n",
      "(Iteration 13601 / 61240) loss: 2.226888\n",
      "(Iteration 13701 / 61240) loss: 2.237494\n",
      "(Epoch 9 / 40) train acc: 0.270000; val_acc: 0.284000\n",
      "(Iteration 13801 / 61240) loss: 2.240706\n",
      "(Iteration 13901 / 61240) loss: 2.247148\n",
      "(Iteration 14001 / 61240) loss: 2.257221\n",
      "(Iteration 14101 / 61240) loss: 2.217015\n",
      "(Iteration 14201 / 61240) loss: 2.256121\n",
      "(Iteration 14301 / 61240) loss: 2.227216\n",
      "(Iteration 14401 / 61240) loss: 2.245041\n",
      "(Iteration 14501 / 61240) loss: 2.259672\n",
      "(Iteration 14601 / 61240) loss: 2.213901\n",
      "(Iteration 14701 / 61240) loss: 2.227574\n",
      "(Iteration 14801 / 61240) loss: 2.207504\n",
      "(Iteration 14901 / 61240) loss: 2.268168\n",
      "(Iteration 15001 / 61240) loss: 2.232147\n",
      "(Iteration 15101 / 61240) loss: 2.211167\n",
      "(Iteration 15201 / 61240) loss: 2.259895\n",
      "(Iteration 15301 / 61240) loss: 2.252981\n",
      "(Epoch 10 / 40) train acc: 0.290000; val_acc: 0.283000\n",
      "(Iteration 15401 / 61240) loss: 2.240949\n",
      "(Iteration 15501 / 61240) loss: 2.257137\n",
      "(Iteration 15601 / 61240) loss: 2.270125\n",
      "(Iteration 15701 / 61240) loss: 2.187833\n",
      "(Iteration 15801 / 61240) loss: 2.246742\n",
      "(Iteration 15901 / 61240) loss: 2.203455\n",
      "(Iteration 16001 / 61240) loss: 2.236368\n",
      "(Iteration 16101 / 61240) loss: 2.245433\n",
      "(Iteration 16201 / 61240) loss: 2.220433\n",
      "(Iteration 16301 / 61240) loss: 2.188118\n",
      "(Iteration 16401 / 61240) loss: 2.235477\n",
      "(Iteration 16501 / 61240) loss: 2.195157\n",
      "(Iteration 16601 / 61240) loss: 2.222355\n",
      "(Iteration 16701 / 61240) loss: 2.235436\n",
      "(Iteration 16801 / 61240) loss: 2.187870\n",
      "(Epoch 11 / 40) train acc: 0.282000; val_acc: 0.282000\n",
      "(Iteration 16901 / 61240) loss: 2.237456\n",
      "(Iteration 17001 / 61240) loss: 2.243783\n",
      "(Iteration 17101 / 61240) loss: 2.220994\n",
      "(Iteration 17201 / 61240) loss: 2.221342\n",
      "(Iteration 17301 / 61240) loss: 2.225439\n",
      "(Iteration 17401 / 61240) loss: 2.188343\n",
      "(Iteration 17501 / 61240) loss: 2.187339\n",
      "(Iteration 17601 / 61240) loss: 2.215715\n",
      "(Iteration 17701 / 61240) loss: 2.112994\n",
      "(Iteration 17801 / 61240) loss: 2.183194\n",
      "(Iteration 17901 / 61240) loss: 2.267520\n",
      "(Iteration 18001 / 61240) loss: 2.220480\n",
      "(Iteration 18101 / 61240) loss: 2.169486\n",
      "(Iteration 18201 / 61240) loss: 2.241083\n",
      "(Iteration 18301 / 61240) loss: 2.234412\n",
      "(Epoch 12 / 40) train acc: 0.284000; val_acc: 0.278000\n",
      "(Iteration 18401 / 61240) loss: 2.207046\n",
      "(Iteration 18501 / 61240) loss: 2.189323\n",
      "(Iteration 18601 / 61240) loss: 2.173535\n",
      "(Iteration 18701 / 61240) loss: 2.164855\n",
      "(Iteration 18801 / 61240) loss: 2.187222\n",
      "(Iteration 18901 / 61240) loss: 2.113748\n",
      "(Iteration 19001 / 61240) loss: 2.226243\n",
      "(Iteration 19101 / 61240) loss: 2.203838\n",
      "(Iteration 19201 / 61240) loss: 2.191462\n",
      "(Iteration 19301 / 61240) loss: 2.171592\n",
      "(Iteration 19401 / 61240) loss: 2.167646\n",
      "(Iteration 19501 / 61240) loss: 2.111919\n",
      "(Iteration 19601 / 61240) loss: 2.132195\n",
      "(Iteration 19701 / 61240) loss: 2.173981\n",
      "(Iteration 19801 / 61240) loss: 2.194575\n",
      "(Iteration 19901 / 61240) loss: 2.161178\n",
      "(Epoch 13 / 40) train acc: 0.254000; val_acc: 0.280000\n",
      "(Iteration 20001 / 61240) loss: 2.229345\n",
      "(Iteration 20101 / 61240) loss: 2.096276\n",
      "(Iteration 20201 / 61240) loss: 2.095084\n",
      "(Iteration 20301 / 61240) loss: 2.129523\n",
      "(Iteration 20401 / 61240) loss: 2.115909\n",
      "(Iteration 20501 / 61240) loss: 2.074290\n",
      "(Iteration 20601 / 61240) loss: 2.113550\n",
      "(Iteration 20701 / 61240) loss: 2.127295\n",
      "(Iteration 20801 / 61240) loss: 2.154927\n",
      "(Iteration 20901 / 61240) loss: 2.128391\n",
      "(Iteration 21001 / 61240) loss: 2.050571\n",
      "(Iteration 21101 / 61240) loss: 2.069994\n",
      "(Iteration 21201 / 61240) loss: 2.075766\n",
      "(Iteration 21301 / 61240) loss: 2.123433\n",
      "(Iteration 21401 / 61240) loss: 2.075031\n",
      "(Epoch 14 / 40) train acc: 0.252000; val_acc: 0.283000\n",
      "(Iteration 21501 / 61240) loss: 2.126860\n",
      "(Iteration 21601 / 61240) loss: 2.160231\n",
      "(Iteration 21701 / 61240) loss: 2.149891\n",
      "(Iteration 21801 / 61240) loss: 2.133864\n",
      "(Iteration 21901 / 61240) loss: 2.146927\n",
      "(Iteration 22001 / 61240) loss: 2.114299\n",
      "(Iteration 22101 / 61240) loss: 2.136703\n",
      "(Iteration 22201 / 61240) loss: 2.102650\n",
      "(Iteration 22301 / 61240) loss: 2.099448\n",
      "(Iteration 22401 / 61240) loss: 2.040185\n",
      "(Iteration 22501 / 61240) loss: 2.137078\n",
      "(Iteration 22601 / 61240) loss: 2.073672\n",
      "(Iteration 22701 / 61240) loss: 2.255053\n",
      "(Iteration 22801 / 61240) loss: 2.116778\n",
      "(Iteration 22901 / 61240) loss: 2.159266\n",
      "(Epoch 15 / 40) train acc: 0.258000; val_acc: 0.281000\n",
      "(Iteration 23001 / 61240) loss: 2.151851\n",
      "(Iteration 23101 / 61240) loss: 2.147481\n",
      "(Iteration 23201 / 61240) loss: 2.089649\n",
      "(Iteration 23301 / 61240) loss: 2.080269\n",
      "(Iteration 23401 / 61240) loss: 2.139817\n",
      "(Iteration 23501 / 61240) loss: 2.187421\n",
      "(Iteration 23601 / 61240) loss: 2.066821\n",
      "(Iteration 23701 / 61240) loss: 2.141312\n",
      "(Iteration 23801 / 61240) loss: 2.108096\n",
      "(Iteration 23901 / 61240) loss: 2.037815\n",
      "(Iteration 24001 / 61240) loss: 2.028800\n",
      "(Iteration 24101 / 61240) loss: 2.107179\n",
      "(Iteration 24201 / 61240) loss: 2.081009\n",
      "(Iteration 24301 / 61240) loss: 2.169906\n",
      "(Iteration 24401 / 61240) loss: 2.176415\n",
      "(Epoch 16 / 40) train acc: 0.247000; val_acc: 0.287000\n",
      "(Iteration 24501 / 61240) loss: 2.060737\n",
      "(Iteration 24601 / 61240) loss: 2.087775\n",
      "(Iteration 24701 / 61240) loss: 2.121681\n",
      "(Iteration 24801 / 61240) loss: 2.207102\n",
      "(Iteration 24901 / 61240) loss: 2.123495\n",
      "(Iteration 25001 / 61240) loss: 1.987756\n",
      "(Iteration 25101 / 61240) loss: 2.048458\n",
      "(Iteration 25201 / 61240) loss: 2.045363\n",
      "(Iteration 25301 / 61240) loss: 2.011556\n",
      "(Iteration 25401 / 61240) loss: 2.090090\n",
      "(Iteration 25501 / 61240) loss: 2.117124\n",
      "(Iteration 25601 / 61240) loss: 2.159818\n",
      "(Iteration 25701 / 61240) loss: 2.197998\n",
      "(Iteration 25801 / 61240) loss: 2.118139\n",
      "(Iteration 25901 / 61240) loss: 2.104508\n",
      "(Iteration 26001 / 61240) loss: 2.098107\n",
      "(Epoch 17 / 40) train acc: 0.285000; val_acc: 0.279000\n",
      "(Iteration 26101 / 61240) loss: 2.129055\n",
      "(Iteration 26201 / 61240) loss: 2.064977\n",
      "(Iteration 26301 / 61240) loss: 2.063826\n",
      "(Iteration 26401 / 61240) loss: 2.027125\n",
      "(Iteration 26501 / 61240) loss: 2.117832\n",
      "(Iteration 26601 / 61240) loss: 1.978920\n",
      "(Iteration 26701 / 61240) loss: 2.087590\n",
      "(Iteration 26801 / 61240) loss: 2.110552\n",
      "(Iteration 26901 / 61240) loss: 2.104618\n",
      "(Iteration 27001 / 61240) loss: 2.178603\n",
      "(Iteration 27101 / 61240) loss: 2.160146\n",
      "(Iteration 27201 / 61240) loss: 2.101015\n",
      "(Iteration 27301 / 61240) loss: 1.960204\n",
      "(Iteration 27401 / 61240) loss: 2.006554\n",
      "(Iteration 27501 / 61240) loss: 2.107623\n",
      "(Epoch 18 / 40) train acc: 0.257000; val_acc: 0.278000\n",
      "(Iteration 27601 / 61240) loss: 2.037349\n",
      "(Iteration 27701 / 61240) loss: 2.126259\n",
      "(Iteration 27801 / 61240) loss: 1.937616\n",
      "(Iteration 27901 / 61240) loss: 2.096644\n",
      "(Iteration 28001 / 61240) loss: 2.081966\n",
      "(Iteration 28101 / 61240) loss: 2.073687\n",
      "(Iteration 28201 / 61240) loss: 2.143828\n",
      "(Iteration 28301 / 61240) loss: 2.023366\n",
      "(Iteration 28401 / 61240) loss: 2.082552\n",
      "(Iteration 28501 / 61240) loss: 2.071279\n",
      "(Iteration 28601 / 61240) loss: 2.121380\n",
      "(Iteration 28701 / 61240) loss: 2.126800\n",
      "(Iteration 28801 / 61240) loss: 2.103098\n",
      "(Iteration 28901 / 61240) loss: 2.064700\n",
      "(Iteration 29001 / 61240) loss: 2.147191\n",
      "(Epoch 19 / 40) train acc: 0.263000; val_acc: 0.277000\n",
      "(Iteration 29101 / 61240) loss: 2.040362\n",
      "(Iteration 29201 / 61240) loss: 2.044496\n",
      "(Iteration 29301 / 61240) loss: 2.064388\n",
      "(Iteration 29401 / 61240) loss: 2.095783\n",
      "(Iteration 29501 / 61240) loss: 2.062403\n",
      "(Iteration 29601 / 61240) loss: 2.051575\n",
      "(Iteration 29701 / 61240) loss: 2.048011\n",
      "(Iteration 29801 / 61240) loss: 2.063443\n",
      "(Iteration 29901 / 61240) loss: 2.066358\n",
      "(Iteration 30001 / 61240) loss: 2.075576\n",
      "(Iteration 30101 / 61240) loss: 2.169230\n",
      "(Iteration 30201 / 61240) loss: 2.090557\n",
      "(Iteration 30301 / 61240) loss: 2.045686\n",
      "(Iteration 30401 / 61240) loss: 2.167511\n",
      "(Iteration 30501 / 61240) loss: 1.988426\n",
      "(Iteration 30601 / 61240) loss: 2.189206\n",
      "(Epoch 20 / 40) train acc: 0.275000; val_acc: 0.281000\n",
      "(Iteration 30701 / 61240) loss: 2.060843\n",
      "(Iteration 30801 / 61240) loss: 2.053301\n",
      "(Iteration 30901 / 61240) loss: 2.068590\n",
      "(Iteration 31001 / 61240) loss: 2.122498\n",
      "(Iteration 31101 / 61240) loss: 1.952042\n",
      "(Iteration 31201 / 61240) loss: 2.068182\n",
      "(Iteration 31301 / 61240) loss: 2.150358\n",
      "(Iteration 31401 / 61240) loss: 1.997106\n",
      "(Iteration 31501 / 61240) loss: 2.091683\n",
      "(Iteration 31601 / 61240) loss: 2.106323\n",
      "(Iteration 31701 / 61240) loss: 2.042737\n",
      "(Iteration 31801 / 61240) loss: 2.056670\n",
      "(Iteration 31901 / 61240) loss: 2.147755\n",
      "(Iteration 32001 / 61240) loss: 2.066010\n",
      "(Iteration 32101 / 61240) loss: 2.085536\n",
      "(Epoch 21 / 40) train acc: 0.279000; val_acc: 0.284000\n",
      "(Iteration 32201 / 61240) loss: 1.920367\n",
      "(Iteration 32301 / 61240) loss: 1.925192\n",
      "(Iteration 32401 / 61240) loss: 2.193203\n",
      "(Iteration 32501 / 61240) loss: 2.126057\n",
      "(Iteration 32601 / 61240) loss: 2.121067\n",
      "(Iteration 32701 / 61240) loss: 2.024291\n",
      "(Iteration 32801 / 61240) loss: 2.141390\n",
      "(Iteration 32901 / 61240) loss: 1.931067\n",
      "(Iteration 33001 / 61240) loss: 2.050030\n",
      "(Iteration 33101 / 61240) loss: 1.989120\n",
      "(Iteration 33201 / 61240) loss: 2.065519\n",
      "(Iteration 33301 / 61240) loss: 2.060057\n",
      "(Iteration 33401 / 61240) loss: 2.118197\n",
      "(Iteration 33501 / 61240) loss: 2.015733\n",
      "(Iteration 33601 / 61240) loss: 1.910568\n",
      "(Epoch 22 / 40) train acc: 0.264000; val_acc: 0.288000\n",
      "(Iteration 33701 / 61240) loss: 2.042683\n",
      "(Iteration 33801 / 61240) loss: 2.116308\n",
      "(Iteration 33901 / 61240) loss: 2.035401\n",
      "(Iteration 34001 / 61240) loss: 1.906140\n",
      "(Iteration 34101 / 61240) loss: 2.052713\n",
      "(Iteration 34201 / 61240) loss: 2.108470\n",
      "(Iteration 34301 / 61240) loss: 2.043734\n",
      "(Iteration 34401 / 61240) loss: 2.120367\n",
      "(Iteration 34501 / 61240) loss: 1.987432\n",
      "(Iteration 34601 / 61240) loss: 2.004030\n",
      "(Iteration 34701 / 61240) loss: 2.126912\n",
      "(Iteration 34801 / 61240) loss: 1.845303\n",
      "(Iteration 34901 / 61240) loss: 2.109830\n",
      "(Iteration 35001 / 61240) loss: 2.037115\n",
      "(Iteration 35101 / 61240) loss: 2.076618\n",
      "(Iteration 35201 / 61240) loss: 1.931417\n",
      "(Epoch 23 / 40) train acc: 0.264000; val_acc: 0.291000\n",
      "(Iteration 35301 / 61240) loss: 1.994856\n",
      "(Iteration 35401 / 61240) loss: 2.030909\n",
      "(Iteration 35501 / 61240) loss: 1.904886\n",
      "(Iteration 35601 / 61240) loss: 2.146108\n",
      "(Iteration 35701 / 61240) loss: 2.049694\n",
      "(Iteration 35801 / 61240) loss: 2.043434\n",
      "(Iteration 35901 / 61240) loss: 1.919551\n",
      "(Iteration 36001 / 61240) loss: 1.944269\n",
      "(Iteration 36101 / 61240) loss: 2.041138\n",
      "(Iteration 36201 / 61240) loss: 1.902836\n",
      "(Iteration 36301 / 61240) loss: 2.064863\n",
      "(Iteration 36401 / 61240) loss: 1.946630\n",
      "(Iteration 36501 / 61240) loss: 2.025051\n",
      "(Iteration 36601 / 61240) loss: 2.020306\n",
      "(Iteration 36701 / 61240) loss: 2.039658\n",
      "(Epoch 24 / 40) train acc: 0.265000; val_acc: 0.293000\n",
      "(Iteration 36801 / 61240) loss: 2.030644\n",
      "(Iteration 36901 / 61240) loss: 1.997956\n",
      "(Iteration 37001 / 61240) loss: 1.891416\n",
      "(Iteration 37101 / 61240) loss: 2.193088\n",
      "(Iteration 37201 / 61240) loss: 2.065868\n",
      "(Iteration 37301 / 61240) loss: 1.855232\n",
      "(Iteration 37401 / 61240) loss: 2.017552\n",
      "(Iteration 37501 / 61240) loss: 1.941103\n",
      "(Iteration 37601 / 61240) loss: 2.007244\n",
      "(Iteration 37701 / 61240) loss: 2.143583\n",
      "(Iteration 37801 / 61240) loss: 2.043951\n",
      "(Iteration 37901 / 61240) loss: 2.136587\n",
      "(Iteration 38001 / 61240) loss: 2.011591\n",
      "(Iteration 38101 / 61240) loss: 1.919911\n",
      "(Iteration 38201 / 61240) loss: 2.081998\n",
      "(Epoch 25 / 40) train acc: 0.285000; val_acc: 0.293000\n",
      "(Iteration 38301 / 61240) loss: 1.908671\n",
      "(Iteration 38401 / 61240) loss: 2.104439\n",
      "(Iteration 38501 / 61240) loss: 1.850565\n",
      "(Iteration 38601 / 61240) loss: 2.047249\n",
      "(Iteration 38701 / 61240) loss: 2.092681\n",
      "(Iteration 38801 / 61240) loss: 1.904352\n",
      "(Iteration 38901 / 61240) loss: 2.082577\n",
      "(Iteration 39001 / 61240) loss: 2.042931\n",
      "(Iteration 39101 / 61240) loss: 2.082667\n",
      "(Iteration 39201 / 61240) loss: 2.057796\n",
      "(Iteration 39301 / 61240) loss: 1.969752\n",
      "(Iteration 39401 / 61240) loss: 1.929737\n",
      "(Iteration 39501 / 61240) loss: 2.019769\n",
      "(Iteration 39601 / 61240) loss: 2.049725\n",
      "(Iteration 39701 / 61240) loss: 2.004732\n",
      "(Iteration 39801 / 61240) loss: 1.992148\n",
      "(Epoch 26 / 40) train acc: 0.294000; val_acc: 0.296000\n",
      "(Iteration 39901 / 61240) loss: 2.032305\n",
      "(Iteration 40001 / 61240) loss: 2.070563\n",
      "(Iteration 40101 / 61240) loss: 2.040152\n",
      "(Iteration 40201 / 61240) loss: 2.143015\n",
      "(Iteration 40301 / 61240) loss: 2.034051\n",
      "(Iteration 40401 / 61240) loss: 2.003784\n",
      "(Iteration 40501 / 61240) loss: 2.056814\n",
      "(Iteration 40601 / 61240) loss: 1.918525\n",
      "(Iteration 40701 / 61240) loss: 2.066925\n",
      "(Iteration 40801 / 61240) loss: 2.142617\n",
      "(Iteration 40901 / 61240) loss: 1.870682\n",
      "(Iteration 41001 / 61240) loss: 1.939868\n",
      "(Iteration 41101 / 61240) loss: 1.916850\n",
      "(Iteration 41201 / 61240) loss: 1.991043\n",
      "(Iteration 41301 / 61240) loss: 2.028110\n",
      "(Epoch 27 / 40) train acc: 0.280000; val_acc: 0.298000\n",
      "(Iteration 41401 / 61240) loss: 2.067661\n",
      "(Iteration 41501 / 61240) loss: 1.963387\n",
      "(Iteration 41601 / 61240) loss: 1.951404\n",
      "(Iteration 41701 / 61240) loss: 2.037296\n",
      "(Iteration 41801 / 61240) loss: 2.030651\n",
      "(Iteration 41901 / 61240) loss: 2.129043\n",
      "(Iteration 42001 / 61240) loss: 2.049098\n",
      "(Iteration 42101 / 61240) loss: 2.007318\n",
      "(Iteration 42201 / 61240) loss: 1.998956\n",
      "(Iteration 42301 / 61240) loss: 1.957886\n",
      "(Iteration 42401 / 61240) loss: 2.066744\n",
      "(Iteration 42501 / 61240) loss: 1.999823\n",
      "(Iteration 42601 / 61240) loss: 1.897880\n",
      "(Iteration 42701 / 61240) loss: 2.086203\n",
      "(Iteration 42801 / 61240) loss: 1.927313\n",
      "(Epoch 28 / 40) train acc: 0.270000; val_acc: 0.297000\n",
      "(Iteration 42901 / 61240) loss: 1.966612\n",
      "(Iteration 43001 / 61240) loss: 1.975363\n",
      "(Iteration 43101 / 61240) loss: 2.042490\n",
      "(Iteration 43201 / 61240) loss: 1.815652\n",
      "(Iteration 43301 / 61240) loss: 2.100338\n",
      "(Iteration 43401 / 61240) loss: 1.979132\n",
      "(Iteration 43501 / 61240) loss: 1.924020\n",
      "(Iteration 43601 / 61240) loss: 1.990625\n",
      "(Iteration 43701 / 61240) loss: 1.977682\n",
      "(Iteration 43801 / 61240) loss: 1.928642\n",
      "(Iteration 43901 / 61240) loss: 2.030913\n",
      "(Iteration 44001 / 61240) loss: 1.921350\n",
      "(Iteration 44101 / 61240) loss: 1.996807\n",
      "(Iteration 44201 / 61240) loss: 1.906524\n",
      "(Iteration 44301 / 61240) loss: 2.117423\n",
      "(Epoch 29 / 40) train acc: 0.292000; val_acc: 0.297000\n",
      "(Iteration 44401 / 61240) loss: 1.826002\n",
      "(Iteration 44501 / 61240) loss: 1.973389\n",
      "(Iteration 44601 / 61240) loss: 2.001538\n",
      "(Iteration 44701 / 61240) loss: 2.018483\n",
      "(Iteration 44801 / 61240) loss: 1.847650\n",
      "(Iteration 44901 / 61240) loss: 1.890531\n",
      "(Iteration 45001 / 61240) loss: 1.957576\n",
      "(Iteration 45101 / 61240) loss: 1.996828\n",
      "(Iteration 45201 / 61240) loss: 2.051533\n",
      "(Iteration 45301 / 61240) loss: 2.091557\n",
      "(Iteration 45401 / 61240) loss: 2.116927\n",
      "(Iteration 45501 / 61240) loss: 1.980937\n",
      "(Iteration 45601 / 61240) loss: 1.948569\n",
      "(Iteration 45701 / 61240) loss: 1.925258\n",
      "(Iteration 45801 / 61240) loss: 2.156885\n",
      "(Iteration 45901 / 61240) loss: 2.040076\n",
      "(Epoch 30 / 40) train acc: 0.286000; val_acc: 0.297000\n",
      "(Iteration 46001 / 61240) loss: 1.956104\n",
      "(Iteration 46101 / 61240) loss: 2.127761\n",
      "(Iteration 46201 / 61240) loss: 2.023674\n",
      "(Iteration 46301 / 61240) loss: 2.227578\n",
      "(Iteration 46401 / 61240) loss: 1.893195\n",
      "(Iteration 46501 / 61240) loss: 1.827666\n",
      "(Iteration 46601 / 61240) loss: 2.036001\n",
      "(Iteration 46701 / 61240) loss: 1.840400\n",
      "(Iteration 46801 / 61240) loss: 2.128963\n",
      "(Iteration 46901 / 61240) loss: 1.921563\n",
      "(Iteration 47001 / 61240) loss: 1.816636\n",
      "(Iteration 47101 / 61240) loss: 1.977233\n",
      "(Iteration 47201 / 61240) loss: 2.033574\n",
      "(Iteration 47301 / 61240) loss: 1.945975\n",
      "(Iteration 47401 / 61240) loss: 2.112644\n",
      "(Epoch 31 / 40) train acc: 0.260000; val_acc: 0.297000\n",
      "(Iteration 47501 / 61240) loss: 2.043072\n",
      "(Iteration 47601 / 61240) loss: 2.021755\n",
      "(Iteration 47701 / 61240) loss: 2.045785\n",
      "(Iteration 47801 / 61240) loss: 1.942906\n",
      "(Iteration 47901 / 61240) loss: 2.101645\n",
      "(Iteration 48001 / 61240) loss: 1.844091\n",
      "(Iteration 48101 / 61240) loss: 1.877387\n",
      "(Iteration 48201 / 61240) loss: 2.067420\n",
      "(Iteration 48301 / 61240) loss: 2.049917\n",
      "(Iteration 48401 / 61240) loss: 1.890722\n",
      "(Iteration 48501 / 61240) loss: 2.022502\n",
      "(Iteration 48601 / 61240) loss: 1.781727\n",
      "(Iteration 48701 / 61240) loss: 1.987616\n",
      "(Iteration 48801 / 61240) loss: 2.037021\n",
      "(Iteration 48901 / 61240) loss: 1.939532\n",
      "(Epoch 32 / 40) train acc: 0.287000; val_acc: 0.296000\n",
      "(Iteration 49001 / 61240) loss: 1.998370\n",
      "(Iteration 49101 / 61240) loss: 2.088246\n",
      "(Iteration 49201 / 61240) loss: 1.999501\n",
      "(Iteration 49301 / 61240) loss: 2.029536\n",
      "(Iteration 49401 / 61240) loss: 2.080639\n",
      "(Iteration 49501 / 61240) loss: 1.851977\n",
      "(Iteration 49601 / 61240) loss: 1.833650\n",
      "(Iteration 49701 / 61240) loss: 1.937285\n",
      "(Iteration 49801 / 61240) loss: 2.079692\n",
      "(Iteration 49901 / 61240) loss: 1.835592\n",
      "(Iteration 50001 / 61240) loss: 2.084046\n",
      "(Iteration 50101 / 61240) loss: 1.873048\n",
      "(Iteration 50201 / 61240) loss: 2.123963\n",
      "(Iteration 50301 / 61240) loss: 1.882346\n",
      "(Iteration 50401 / 61240) loss: 1.889596\n",
      "(Iteration 50501 / 61240) loss: 2.087200\n",
      "(Epoch 33 / 40) train acc: 0.305000; val_acc: 0.298000\n",
      "(Iteration 50601 / 61240) loss: 1.910791\n",
      "(Iteration 50701 / 61240) loss: 1.961329\n",
      "(Iteration 50801 / 61240) loss: 1.951258\n",
      "(Iteration 50901 / 61240) loss: 2.128269\n",
      "(Iteration 51001 / 61240) loss: 2.064216\n",
      "(Iteration 51101 / 61240) loss: 1.981698\n",
      "(Iteration 51201 / 61240) loss: 1.959934\n",
      "(Iteration 51301 / 61240) loss: 1.995102\n",
      "(Iteration 51401 / 61240) loss: 1.995877\n",
      "(Iteration 51501 / 61240) loss: 2.021409\n",
      "(Iteration 51601 / 61240) loss: 1.825338\n",
      "(Iteration 51701 / 61240) loss: 1.958356\n",
      "(Iteration 51801 / 61240) loss: 2.004791\n",
      "(Iteration 51901 / 61240) loss: 2.038226\n",
      "(Iteration 52001 / 61240) loss: 1.940233\n",
      "(Epoch 34 / 40) train acc: 0.288000; val_acc: 0.300000\n",
      "(Iteration 52101 / 61240) loss: 1.955158\n",
      "(Iteration 52201 / 61240) loss: 1.878197\n",
      "(Iteration 52301 / 61240) loss: 2.119244\n",
      "(Iteration 52401 / 61240) loss: 1.916243\n",
      "(Iteration 52501 / 61240) loss: 2.056633\n",
      "(Iteration 52601 / 61240) loss: 1.996899\n",
      "(Iteration 52701 / 61240) loss: 1.925070\n",
      "(Iteration 52801 / 61240) loss: 2.095657\n",
      "(Iteration 52901 / 61240) loss: 1.957980\n",
      "(Iteration 53001 / 61240) loss: 1.960447\n",
      "(Iteration 53101 / 61240) loss: 2.198800\n",
      "(Iteration 53201 / 61240) loss: 2.107717\n",
      "(Iteration 53301 / 61240) loss: 1.983812\n",
      "(Iteration 53401 / 61240) loss: 2.001366\n",
      "(Iteration 53501 / 61240) loss: 1.967958\n",
      "(Epoch 35 / 40) train acc: 0.289000; val_acc: 0.299000\n",
      "(Iteration 53601 / 61240) loss: 2.106472\n",
      "(Iteration 53701 / 61240) loss: 2.058235\n",
      "(Iteration 53801 / 61240) loss: 1.915199\n",
      "(Iteration 53901 / 61240) loss: 1.903372\n",
      "(Iteration 54001 / 61240) loss: 2.021618\n",
      "(Iteration 54101 / 61240) loss: 1.948046\n",
      "(Iteration 54201 / 61240) loss: 1.984871\n",
      "(Iteration 54301 / 61240) loss: 1.922204\n",
      "(Iteration 54401 / 61240) loss: 2.073425\n",
      "(Iteration 54501 / 61240) loss: 1.886940\n",
      "(Iteration 54601 / 61240) loss: 1.910758\n",
      "(Iteration 54701 / 61240) loss: 2.060148\n",
      "(Iteration 54801 / 61240) loss: 1.898141\n",
      "(Iteration 54901 / 61240) loss: 2.007744\n",
      "(Iteration 55001 / 61240) loss: 1.904182\n",
      "(Iteration 55101 / 61240) loss: 1.861045\n",
      "(Epoch 36 / 40) train acc: 0.276000; val_acc: 0.300000\n",
      "(Iteration 55201 / 61240) loss: 1.997917\n",
      "(Iteration 55301 / 61240) loss: 2.104689\n",
      "(Iteration 55401 / 61240) loss: 1.965926\n",
      "(Iteration 55501 / 61240) loss: 1.915888\n",
      "(Iteration 55601 / 61240) loss: 1.870354\n",
      "(Iteration 55701 / 61240) loss: 2.062403\n",
      "(Iteration 55801 / 61240) loss: 2.092551\n",
      "(Iteration 55901 / 61240) loss: 1.887382\n",
      "(Iteration 56001 / 61240) loss: 2.058100\n",
      "(Iteration 56101 / 61240) loss: 2.040748\n",
      "(Iteration 56201 / 61240) loss: 2.001378\n",
      "(Iteration 56301 / 61240) loss: 1.970617\n",
      "(Iteration 56401 / 61240) loss: 2.033420\n",
      "(Iteration 56501 / 61240) loss: 1.929696\n",
      "(Iteration 56601 / 61240) loss: 1.939612\n",
      "(Epoch 37 / 40) train acc: 0.289000; val_acc: 0.301000\n",
      "(Iteration 56701 / 61240) loss: 1.949553\n",
      "(Iteration 56801 / 61240) loss: 2.030293\n",
      "(Iteration 56901 / 61240) loss: 1.870003\n",
      "(Iteration 57001 / 61240) loss: 2.080287\n",
      "(Iteration 57101 / 61240) loss: 1.964257\n",
      "(Iteration 57201 / 61240) loss: 2.009145\n",
      "(Iteration 57301 / 61240) loss: 2.030502\n",
      "(Iteration 57401 / 61240) loss: 1.871822\n",
      "(Iteration 57501 / 61240) loss: 2.026686\n",
      "(Iteration 57601 / 61240) loss: 2.017779\n",
      "(Iteration 57701 / 61240) loss: 1.973547\n",
      "(Iteration 57801 / 61240) loss: 2.002731\n",
      "(Iteration 57901 / 61240) loss: 1.973772\n",
      "(Iteration 58001 / 61240) loss: 2.040005\n",
      "(Iteration 58101 / 61240) loss: 2.056555\n",
      "(Epoch 38 / 40) train acc: 0.287000; val_acc: 0.300000\n",
      "(Iteration 58201 / 61240) loss: 2.070001\n",
      "(Iteration 58301 / 61240) loss: 1.936238\n",
      "(Iteration 58401 / 61240) loss: 1.915555\n",
      "(Iteration 58501 / 61240) loss: 2.043601\n",
      "(Iteration 58601 / 61240) loss: 2.001257\n",
      "(Iteration 58701 / 61240) loss: 2.037020\n",
      "(Iteration 58801 / 61240) loss: 2.082297\n",
      "(Iteration 58901 / 61240) loss: 1.963071\n",
      "(Iteration 59001 / 61240) loss: 1.946088\n",
      "(Iteration 59101 / 61240) loss: 2.045590\n",
      "(Iteration 59201 / 61240) loss: 1.899959\n",
      "(Iteration 59301 / 61240) loss: 1.951241\n",
      "(Iteration 59401 / 61240) loss: 1.860212\n",
      "(Iteration 59501 / 61240) loss: 1.819812\n",
      "(Iteration 59601 / 61240) loss: 1.892621\n",
      "(Iteration 59701 / 61240) loss: 1.913007\n",
      "(Epoch 39 / 40) train acc: 0.285000; val_acc: 0.299000\n",
      "(Iteration 59801 / 61240) loss: 1.912187\n",
      "(Iteration 59901 / 61240) loss: 1.952147\n",
      "(Iteration 60001 / 61240) loss: 2.064071\n",
      "(Iteration 60101 / 61240) loss: 1.821623\n",
      "(Iteration 60201 / 61240) loss: 1.849243\n",
      "(Iteration 60301 / 61240) loss: 2.015947\n",
      "(Iteration 60401 / 61240) loss: 2.029159\n",
      "(Iteration 60501 / 61240) loss: 2.045512\n",
      "(Iteration 60601 / 61240) loss: 2.014125\n",
      "(Iteration 60701 / 61240) loss: 1.966014\n",
      "(Iteration 60801 / 61240) loss: 2.028027\n",
      "(Iteration 60901 / 61240) loss: 1.941803\n",
      "(Iteration 61001 / 61240) loss: 2.072144\n",
      "(Iteration 61101 / 61240) loss: 1.964268\n",
      "(Iteration 61201 / 61240) loss: 2.029203\n",
      "(Epoch 40 / 40) train acc: 0.324000; val_acc: 0.299000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.01, 'num_epochs': 20, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 15300) loss: 2.308299\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.106000\n",
      "(Iteration 101 / 15300) loss: 2.306723\n",
      "(Iteration 201 / 15300) loss: 2.305962\n",
      "(Iteration 301 / 15300) loss: 2.305884\n",
      "(Iteration 401 / 15300) loss: 2.305139\n",
      "(Iteration 501 / 15300) loss: 2.303937\n",
      "(Iteration 601 / 15300) loss: 2.301216\n",
      "(Iteration 701 / 15300) loss: 2.302398\n",
      "(Epoch 1 / 20) train acc: 0.139000; val_acc: 0.125000\n",
      "(Iteration 801 / 15300) loss: 2.299224\n",
      "(Iteration 901 / 15300) loss: 2.296546\n",
      "(Iteration 1001 / 15300) loss: 2.291237\n",
      "(Iteration 1101 / 15300) loss: 2.288695\n",
      "(Iteration 1201 / 15300) loss: 2.274853\n",
      "(Iteration 1301 / 15300) loss: 2.255384\n",
      "(Iteration 1401 / 15300) loss: 2.273762\n",
      "(Iteration 1501 / 15300) loss: 2.216495\n",
      "(Epoch 2 / 20) train acc: 0.247000; val_acc: 0.248000\n",
      "(Iteration 1601 / 15300) loss: 2.203634\n",
      "(Iteration 1701 / 15300) loss: 2.217102\n",
      "(Iteration 1801 / 15300) loss: 2.179961\n",
      "(Iteration 1901 / 15300) loss: 2.222485\n",
      "(Iteration 2001 / 15300) loss: 2.222296\n",
      "(Iteration 2101 / 15300) loss: 2.089729\n",
      "(Iteration 2201 / 15300) loss: 2.090854\n",
      "(Epoch 3 / 20) train acc: 0.256000; val_acc: 0.289000\n",
      "(Iteration 2301 / 15300) loss: 2.126372\n",
      "(Iteration 2401 / 15300) loss: 2.127658\n",
      "(Iteration 2501 / 15300) loss: 2.052340\n",
      "(Iteration 2601 / 15300) loss: 2.141550\n",
      "(Iteration 2701 / 15300) loss: 2.019949\n",
      "(Iteration 2801 / 15300) loss: 1.947933\n",
      "(Iteration 2901 / 15300) loss: 2.098285\n",
      "(Iteration 3001 / 15300) loss: 2.056036\n",
      "(Epoch 4 / 20) train acc: 0.285000; val_acc: 0.306000\n",
      "(Iteration 3101 / 15300) loss: 2.068887\n",
      "(Iteration 3201 / 15300) loss: 2.061898\n",
      "(Iteration 3301 / 15300) loss: 2.020748\n",
      "(Iteration 3401 / 15300) loss: 2.080182\n",
      "(Iteration 3501 / 15300) loss: 2.087717\n",
      "(Iteration 3601 / 15300) loss: 2.070408\n",
      "(Iteration 3701 / 15300) loss: 2.072143\n",
      "(Iteration 3801 / 15300) loss: 2.092309\n",
      "(Epoch 5 / 20) train acc: 0.323000; val_acc: 0.333000\n",
      "(Iteration 3901 / 15300) loss: 2.066447\n",
      "(Iteration 4001 / 15300) loss: 2.096853\n",
      "(Iteration 4101 / 15300) loss: 2.048542\n",
      "(Iteration 4201 / 15300) loss: 2.008277\n",
      "(Iteration 4301 / 15300) loss: 2.075540\n",
      "(Iteration 4401 / 15300) loss: 2.050627\n",
      "(Iteration 4501 / 15300) loss: 2.136412\n",
      "(Epoch 6 / 20) train acc: 0.363000; val_acc: 0.342000\n",
      "(Iteration 4601 / 15300) loss: 2.033271\n",
      "(Iteration 4701 / 15300) loss: 1.993705\n",
      "(Iteration 4801 / 15300) loss: 1.979641\n",
      "(Iteration 4901 / 15300) loss: 2.132193\n",
      "(Iteration 5001 / 15300) loss: 2.025006\n",
      "(Iteration 5101 / 15300) loss: 2.073640\n",
      "(Iteration 5201 / 15300) loss: 2.104862\n",
      "(Iteration 5301 / 15300) loss: 1.960516\n",
      "(Epoch 7 / 20) train acc: 0.375000; val_acc: 0.368000\n",
      "(Iteration 5401 / 15300) loss: 2.096047\n",
      "(Iteration 5501 / 15300) loss: 1.918814\n",
      "(Iteration 5601 / 15300) loss: 2.066120\n",
      "(Iteration 5701 / 15300) loss: 1.947377\n",
      "(Iteration 5801 / 15300) loss: 2.024147\n",
      "(Iteration 5901 / 15300) loss: 2.027088\n",
      "(Iteration 6001 / 15300) loss: 2.030319\n",
      "(Iteration 6101 / 15300) loss: 2.002651\n",
      "(Epoch 8 / 20) train acc: 0.390000; val_acc: 0.389000\n",
      "(Iteration 6201 / 15300) loss: 2.068838\n",
      "(Iteration 6301 / 15300) loss: 1.965306\n",
      "(Iteration 6401 / 15300) loss: 1.979661\n",
      "(Iteration 6501 / 15300) loss: 2.029903\n",
      "(Iteration 6601 / 15300) loss: 1.906458\n",
      "(Iteration 6701 / 15300) loss: 1.936538\n",
      "(Iteration 6801 / 15300) loss: 1.946553\n",
      "(Epoch 9 / 20) train acc: 0.383000; val_acc: 0.404000\n",
      "(Iteration 6901 / 15300) loss: 2.030282\n",
      "(Iteration 7001 / 15300) loss: 1.912582\n",
      "(Iteration 7101 / 15300) loss: 2.056990\n",
      "(Iteration 7201 / 15300) loss: 1.892572\n",
      "(Iteration 7301 / 15300) loss: 2.021508\n",
      "(Iteration 7401 / 15300) loss: 2.020783\n",
      "(Iteration 7501 / 15300) loss: 2.072309\n",
      "(Iteration 7601 / 15300) loss: 2.043218\n",
      "(Epoch 10 / 20) train acc: 0.400000; val_acc: 0.405000\n",
      "(Iteration 7701 / 15300) loss: 1.903423\n",
      "(Iteration 7801 / 15300) loss: 1.986041\n",
      "(Iteration 7901 / 15300) loss: 2.062700\n",
      "(Iteration 8001 / 15300) loss: 2.016057\n",
      "(Iteration 8101 / 15300) loss: 1.953746\n",
      "(Iteration 8201 / 15300) loss: 2.010121\n",
      "(Iteration 8301 / 15300) loss: 1.939992\n",
      "(Iteration 8401 / 15300) loss: 2.020557\n",
      "(Epoch 11 / 20) train acc: 0.432000; val_acc: 0.395000\n",
      "(Iteration 8501 / 15300) loss: 2.029261\n",
      "(Iteration 8601 / 15300) loss: 2.075101\n",
      "(Iteration 8701 / 15300) loss: 2.020109\n",
      "(Iteration 8801 / 15300) loss: 1.996005\n",
      "(Iteration 8901 / 15300) loss: 2.040937\n",
      "(Iteration 9001 / 15300) loss: 2.018217\n",
      "(Iteration 9101 / 15300) loss: 1.945162\n",
      "(Epoch 12 / 20) train acc: 0.413000; val_acc: 0.414000\n",
      "(Iteration 9201 / 15300) loss: 2.054058\n",
      "(Iteration 9301 / 15300) loss: 1.878280\n",
      "(Iteration 9401 / 15300) loss: 2.041337\n",
      "(Iteration 9501 / 15300) loss: 2.049858\n",
      "(Iteration 9601 / 15300) loss: 2.039050\n",
      "(Iteration 9701 / 15300) loss: 2.111598\n",
      "(Iteration 9801 / 15300) loss: 1.893893\n",
      "(Iteration 9901 / 15300) loss: 2.084183\n",
      "(Epoch 13 / 20) train acc: 0.418000; val_acc: 0.402000\n",
      "(Iteration 10001 / 15300) loss: 2.064695\n",
      "(Iteration 10101 / 15300) loss: 2.080726\n",
      "(Iteration 10201 / 15300) loss: 1.923073\n",
      "(Iteration 10301 / 15300) loss: 2.037838\n",
      "(Iteration 10401 / 15300) loss: 2.042663\n",
      "(Iteration 10501 / 15300) loss: 2.039796\n",
      "(Iteration 10601 / 15300) loss: 1.922572\n",
      "(Iteration 10701 / 15300) loss: 1.929077\n",
      "(Epoch 14 / 20) train acc: 0.436000; val_acc: 0.410000\n",
      "(Iteration 10801 / 15300) loss: 2.016152\n",
      "(Iteration 10901 / 15300) loss: 1.952932\n",
      "(Iteration 11001 / 15300) loss: 2.043437\n",
      "(Iteration 11101 / 15300) loss: 2.008122\n",
      "(Iteration 11201 / 15300) loss: 2.006406\n",
      "(Iteration 11301 / 15300) loss: 2.058177\n",
      "(Iteration 11401 / 15300) loss: 1.965292\n",
      "(Epoch 15 / 20) train acc: 0.405000; val_acc: 0.409000\n",
      "(Iteration 11501 / 15300) loss: 2.066492\n",
      "(Iteration 11601 / 15300) loss: 2.019257\n",
      "(Iteration 11701 / 15300) loss: 2.019295\n",
      "(Iteration 11801 / 15300) loss: 1.992598\n",
      "(Iteration 11901 / 15300) loss: 1.881984\n",
      "(Iteration 12001 / 15300) loss: 2.015817\n",
      "(Iteration 12101 / 15300) loss: 2.032834\n",
      "(Iteration 12201 / 15300) loss: 2.105655\n",
      "(Epoch 16 / 20) train acc: 0.424000; val_acc: 0.416000\n",
      "(Iteration 12301 / 15300) loss: 1.929629\n",
      "(Iteration 12401 / 15300) loss: 2.018142\n",
      "(Iteration 12501 / 15300) loss: 2.000639\n",
      "(Iteration 12601 / 15300) loss: 1.841794\n",
      "(Iteration 12701 / 15300) loss: 1.915461\n",
      "(Iteration 12801 / 15300) loss: 2.030991\n",
      "(Iteration 12901 / 15300) loss: 2.000572\n",
      "(Iteration 13001 / 15300) loss: 1.998273\n",
      "(Epoch 17 / 20) train acc: 0.430000; val_acc: 0.413000\n",
      "(Iteration 13101 / 15300) loss: 2.045157\n",
      "(Iteration 13201 / 15300) loss: 1.942745\n",
      "(Iteration 13301 / 15300) loss: 1.935099\n",
      "(Iteration 13401 / 15300) loss: 1.994052\n",
      "(Iteration 13501 / 15300) loss: 2.056633\n",
      "(Iteration 13601 / 15300) loss: 1.983509\n",
      "(Iteration 13701 / 15300) loss: 1.983129\n",
      "(Epoch 18 / 20) train acc: 0.428000; val_acc: 0.406000\n",
      "(Iteration 13801 / 15300) loss: 1.950603\n",
      "(Iteration 13901 / 15300) loss: 1.914735\n",
      "(Iteration 14001 / 15300) loss: 1.865964\n",
      "(Iteration 14101 / 15300) loss: 1.914747\n",
      "(Iteration 14201 / 15300) loss: 2.061487\n",
      "(Iteration 14301 / 15300) loss: 2.085761\n",
      "(Iteration 14401 / 15300) loss: 1.946777\n",
      "(Iteration 14501 / 15300) loss: 2.065896\n",
      "(Epoch 19 / 20) train acc: 0.451000; val_acc: 0.415000\n",
      "(Iteration 14601 / 15300) loss: 1.970939\n",
      "(Iteration 14701 / 15300) loss: 2.060246\n",
      "(Iteration 14801 / 15300) loss: 1.997449\n",
      "(Iteration 14901 / 15300) loss: 2.002027\n",
      "(Iteration 15001 / 15300) loss: 2.024725\n",
      "(Iteration 15101 / 15300) loss: 1.847515\n",
      "(Iteration 15201 / 15300) loss: 2.062936\n",
      "(Epoch 20 / 20) train acc: 0.417000; val_acc: 0.420000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.01, 'num_epochs': 20, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 30620) loss: 2.308316\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.110000\n",
      "(Iteration 101 / 30620) loss: 2.308395\n",
      "(Iteration 201 / 30620) loss: 2.307016\n",
      "(Iteration 301 / 30620) loss: 2.306992\n",
      "(Iteration 401 / 30620) loss: 2.304936\n",
      "(Iteration 501 / 30620) loss: 2.303019\n",
      "(Iteration 601 / 30620) loss: 2.303647\n",
      "(Iteration 701 / 30620) loss: 2.301775\n",
      "(Iteration 801 / 30620) loss: 2.303095\n",
      "(Iteration 901 / 30620) loss: 2.299605\n",
      "(Iteration 1001 / 30620) loss: 2.300353\n",
      "(Iteration 1101 / 30620) loss: 2.297253\n",
      "(Iteration 1201 / 30620) loss: 2.285112\n",
      "(Iteration 1301 / 30620) loss: 2.272512\n",
      "(Iteration 1401 / 30620) loss: 2.290761\n",
      "(Iteration 1501 / 30620) loss: 2.250433\n",
      "(Epoch 1 / 20) train acc: 0.215000; val_acc: 0.232000\n",
      "(Iteration 1601 / 30620) loss: 2.224185\n",
      "(Iteration 1701 / 30620) loss: 2.195110\n",
      "(Iteration 1801 / 30620) loss: 2.051146\n",
      "(Iteration 1901 / 30620) loss: 2.073407\n",
      "(Iteration 2001 / 30620) loss: 2.057438\n",
      "(Iteration 2101 / 30620) loss: 2.119892\n",
      "(Iteration 2201 / 30620) loss: 2.186186\n",
      "(Iteration 2301 / 30620) loss: 2.076309\n",
      "(Iteration 2401 / 30620) loss: 2.005781\n",
      "(Iteration 2501 / 30620) loss: 2.208377\n",
      "(Iteration 2601 / 30620) loss: 2.224575\n",
      "(Iteration 2701 / 30620) loss: 2.120797\n",
      "(Iteration 2801 / 30620) loss: 2.073662\n",
      "(Iteration 2901 / 30620) loss: 2.035227\n",
      "(Iteration 3001 / 30620) loss: 2.179206\n",
      "(Epoch 2 / 20) train acc: 0.334000; val_acc: 0.338000\n",
      "(Iteration 3101 / 30620) loss: 2.091736\n",
      "(Iteration 3201 / 30620) loss: 2.176730\n",
      "(Iteration 3301 / 30620) loss: 1.998056\n",
      "(Iteration 3401 / 30620) loss: 2.050628\n",
      "(Iteration 3501 / 30620) loss: 2.068721\n",
      "(Iteration 3601 / 30620) loss: 2.174235\n",
      "(Iteration 3701 / 30620) loss: 2.040355\n",
      "(Iteration 3801 / 30620) loss: 1.997429\n",
      "(Iteration 3901 / 30620) loss: 1.961881\n",
      "(Iteration 4001 / 30620) loss: 2.107258\n",
      "(Iteration 4101 / 30620) loss: 2.117155\n",
      "(Iteration 4201 / 30620) loss: 2.121359\n",
      "(Iteration 4301 / 30620) loss: 1.909392\n",
      "(Iteration 4401 / 30620) loss: 1.979141\n",
      "(Iteration 4501 / 30620) loss: 2.033896\n",
      "(Epoch 3 / 20) train acc: 0.408000; val_acc: 0.361000\n",
      "(Iteration 4601 / 30620) loss: 1.816685\n",
      "(Iteration 4701 / 30620) loss: 1.904049\n",
      "(Iteration 4801 / 30620) loss: 1.970014\n",
      "(Iteration 4901 / 30620) loss: 2.088537\n",
      "(Iteration 5001 / 30620) loss: 2.089819\n",
      "(Iteration 5101 / 30620) loss: 1.968282\n",
      "(Iteration 5201 / 30620) loss: 2.034939\n",
      "(Iteration 5301 / 30620) loss: 2.154091\n",
      "(Iteration 5401 / 30620) loss: 1.876359\n",
      "(Iteration 5501 / 30620) loss: 2.099633\n",
      "(Iteration 5601 / 30620) loss: 1.888843\n",
      "(Iteration 5701 / 30620) loss: 1.987924\n",
      "(Iteration 5801 / 30620) loss: 1.928160\n",
      "(Iteration 5901 / 30620) loss: 2.164096\n",
      "(Iteration 6001 / 30620) loss: 1.975826\n",
      "(Iteration 6101 / 30620) loss: 1.941088\n",
      "(Epoch 4 / 20) train acc: 0.389000; val_acc: 0.399000\n",
      "(Iteration 6201 / 30620) loss: 2.183602\n",
      "(Iteration 6301 / 30620) loss: 1.949671\n",
      "(Iteration 6401 / 30620) loss: 1.911967\n",
      "(Iteration 6501 / 30620) loss: 2.203263\n",
      "(Iteration 6601 / 30620) loss: 1.978864\n",
      "(Iteration 6701 / 30620) loss: 1.945148\n",
      "(Iteration 6801 / 30620) loss: 2.079904\n",
      "(Iteration 6901 / 30620) loss: 2.056339\n",
      "(Iteration 7001 / 30620) loss: 1.822243\n",
      "(Iteration 7101 / 30620) loss: 2.061026\n",
      "(Iteration 7201 / 30620) loss: 1.893342\n",
      "(Iteration 7301 / 30620) loss: 1.773174\n",
      "(Iteration 7401 / 30620) loss: 1.874353\n",
      "(Iteration 7501 / 30620) loss: 2.009134\n",
      "(Iteration 7601 / 30620) loss: 1.989237\n",
      "(Epoch 5 / 20) train acc: 0.457000; val_acc: 0.404000\n",
      "(Iteration 7701 / 30620) loss: 1.970809\n",
      "(Iteration 7801 / 30620) loss: 1.860855\n",
      "(Iteration 7901 / 30620) loss: 2.033320\n",
      "(Iteration 8001 / 30620) loss: 2.037760\n",
      "(Iteration 8101 / 30620) loss: 2.008502\n",
      "(Iteration 8201 / 30620) loss: 2.040220\n",
      "(Iteration 8301 / 30620) loss: 1.944929\n",
      "(Iteration 8401 / 30620) loss: 1.998983\n",
      "(Iteration 8501 / 30620) loss: 1.992793\n",
      "(Iteration 8601 / 30620) loss: 1.865755\n",
      "(Iteration 8701 / 30620) loss: 2.078367\n",
      "(Iteration 8801 / 30620) loss: 1.982107\n",
      "(Iteration 8901 / 30620) loss: 1.959619\n",
      "(Iteration 9001 / 30620) loss: 1.882085\n",
      "(Iteration 9101 / 30620) loss: 2.065570\n",
      "(Epoch 6 / 20) train acc: 0.446000; val_acc: 0.421000\n",
      "(Iteration 9201 / 30620) loss: 1.929758\n",
      "(Iteration 9301 / 30620) loss: 2.061372\n",
      "(Iteration 9401 / 30620) loss: 1.968131\n",
      "(Iteration 9501 / 30620) loss: 1.913548\n",
      "(Iteration 9601 / 30620) loss: 2.060715\n",
      "(Iteration 9701 / 30620) loss: 2.018322\n",
      "(Iteration 9801 / 30620) loss: 2.036890\n",
      "(Iteration 9901 / 30620) loss: 2.046582\n",
      "(Iteration 10001 / 30620) loss: 2.258336\n",
      "(Iteration 10101 / 30620) loss: 1.917206\n",
      "(Iteration 10201 / 30620) loss: 2.141105\n",
      "(Iteration 10301 / 30620) loss: 1.958053\n",
      "(Iteration 10401 / 30620) loss: 1.869439\n",
      "(Iteration 10501 / 30620) loss: 1.944566\n",
      "(Iteration 10601 / 30620) loss: 1.962177\n",
      "(Iteration 10701 / 30620) loss: 2.070209\n",
      "(Epoch 7 / 20) train acc: 0.452000; val_acc: 0.429000\n",
      "(Iteration 10801 / 30620) loss: 1.987534\n",
      "(Iteration 10901 / 30620) loss: 1.911051\n",
      "(Iteration 11001 / 30620) loss: 1.949335\n",
      "(Iteration 11101 / 30620) loss: 1.919117\n",
      "(Iteration 11201 / 30620) loss: 1.939155\n",
      "(Iteration 11301 / 30620) loss: 2.010576\n",
      "(Iteration 11401 / 30620) loss: 2.029208\n",
      "(Iteration 11501 / 30620) loss: 2.011256\n",
      "(Iteration 11601 / 30620) loss: 1.985502\n",
      "(Iteration 11701 / 30620) loss: 1.936125\n",
      "(Iteration 11801 / 30620) loss: 2.054793\n",
      "(Iteration 11901 / 30620) loss: 2.131699\n",
      "(Iteration 12001 / 30620) loss: 2.153409\n",
      "(Iteration 12101 / 30620) loss: 1.883324\n",
      "(Iteration 12201 / 30620) loss: 1.944476\n",
      "(Epoch 8 / 20) train acc: 0.445000; val_acc: 0.433000\n",
      "(Iteration 12301 / 30620) loss: 2.099932\n",
      "(Iteration 12401 / 30620) loss: 2.136794\n",
      "(Iteration 12501 / 30620) loss: 1.864748\n",
      "(Iteration 12601 / 30620) loss: 1.889996\n",
      "(Iteration 12701 / 30620) loss: 1.834622\n",
      "(Iteration 12801 / 30620) loss: 2.062679\n",
      "(Iteration 12901 / 30620) loss: 1.964015\n",
      "(Iteration 13001 / 30620) loss: 1.976429\n",
      "(Iteration 13101 / 30620) loss: 1.967066\n",
      "(Iteration 13201 / 30620) loss: 1.961114\n",
      "(Iteration 13301 / 30620) loss: 1.871582\n",
      "(Iteration 13401 / 30620) loss: 1.902068\n",
      "(Iteration 13501 / 30620) loss: 1.961121\n",
      "(Iteration 13601 / 30620) loss: 1.966086\n",
      "(Iteration 13701 / 30620) loss: 1.920812\n",
      "(Epoch 9 / 20) train acc: 0.425000; val_acc: 0.418000\n",
      "(Iteration 13801 / 30620) loss: 2.023494\n",
      "(Iteration 13901 / 30620) loss: 1.825067\n",
      "(Iteration 14001 / 30620) loss: 2.055871\n",
      "(Iteration 14101 / 30620) loss: 2.028284\n",
      "(Iteration 14201 / 30620) loss: 1.951142\n",
      "(Iteration 14301 / 30620) loss: 1.977424\n",
      "(Iteration 14401 / 30620) loss: 2.058151\n",
      "(Iteration 14501 / 30620) loss: 1.994851\n",
      "(Iteration 14601 / 30620) loss: 1.999032\n",
      "(Iteration 14701 / 30620) loss: 1.950317\n",
      "(Iteration 14801 / 30620) loss: 1.868838\n",
      "(Iteration 14901 / 30620) loss: 1.958244\n",
      "(Iteration 15001 / 30620) loss: 1.917972\n",
      "(Iteration 15101 / 30620) loss: 2.078011\n",
      "(Iteration 15201 / 30620) loss: 1.967176\n",
      "(Iteration 15301 / 30620) loss: 1.995579\n",
      "(Epoch 10 / 20) train acc: 0.442000; val_acc: 0.431000\n",
      "(Iteration 15401 / 30620) loss: 1.894247\n",
      "(Iteration 15501 / 30620) loss: 1.934397\n",
      "(Iteration 15601 / 30620) loss: 2.035725\n",
      "(Iteration 15701 / 30620) loss: 1.992883\n",
      "(Iteration 15801 / 30620) loss: 1.753600\n",
      "(Iteration 15901 / 30620) loss: 1.804144\n",
      "(Iteration 16001 / 30620) loss: 2.039434\n",
      "(Iteration 16101 / 30620) loss: 2.003035\n",
      "(Iteration 16201 / 30620) loss: 1.921883\n",
      "(Iteration 16301 / 30620) loss: 2.059632\n",
      "(Iteration 16401 / 30620) loss: 1.974281\n",
      "(Iteration 16501 / 30620) loss: 2.030200\n",
      "(Iteration 16601 / 30620) loss: 2.040945\n",
      "(Iteration 16701 / 30620) loss: 1.996653\n",
      "(Iteration 16801 / 30620) loss: 1.970417\n",
      "(Epoch 11 / 20) train acc: 0.443000; val_acc: 0.430000\n",
      "(Iteration 16901 / 30620) loss: 1.878526\n",
      "(Iteration 17001 / 30620) loss: 1.932099\n",
      "(Iteration 17101 / 30620) loss: 2.050908\n",
      "(Iteration 17201 / 30620) loss: 1.626707\n",
      "(Iteration 17301 / 30620) loss: 1.835960\n",
      "(Iteration 17401 / 30620) loss: 1.866252\n",
      "(Iteration 17501 / 30620) loss: 1.854292\n",
      "(Iteration 17601 / 30620) loss: 1.809783\n",
      "(Iteration 17701 / 30620) loss: 1.941235\n",
      "(Iteration 17801 / 30620) loss: 2.074864\n",
      "(Iteration 17901 / 30620) loss: 2.079773\n",
      "(Iteration 18001 / 30620) loss: 1.921618\n",
      "(Iteration 18101 / 30620) loss: 1.764689\n",
      "(Iteration 18201 / 30620) loss: 1.861917\n",
      "(Iteration 18301 / 30620) loss: 1.940490\n",
      "(Epoch 12 / 20) train acc: 0.447000; val_acc: 0.434000\n",
      "(Iteration 18401 / 30620) loss: 1.871231\n",
      "(Iteration 18501 / 30620) loss: 1.898999\n",
      "(Iteration 18601 / 30620) loss: 2.131359\n",
      "(Iteration 18701 / 30620) loss: 1.824196\n",
      "(Iteration 18801 / 30620) loss: 1.886244\n",
      "(Iteration 18901 / 30620) loss: 1.827224\n",
      "(Iteration 19001 / 30620) loss: 1.940249\n",
      "(Iteration 19101 / 30620) loss: 1.978100\n",
      "(Iteration 19201 / 30620) loss: 1.921793\n",
      "(Iteration 19301 / 30620) loss: 1.872625\n",
      "(Iteration 19401 / 30620) loss: 1.976247\n",
      "(Iteration 19501 / 30620) loss: 1.969268\n",
      "(Iteration 19601 / 30620) loss: 1.927478\n",
      "(Iteration 19701 / 30620) loss: 1.910798\n",
      "(Iteration 19801 / 30620) loss: 1.967222\n",
      "(Iteration 19901 / 30620) loss: 1.815500\n",
      "(Epoch 13 / 20) train acc: 0.458000; val_acc: 0.438000\n",
      "(Iteration 20001 / 30620) loss: 1.933802\n",
      "(Iteration 20101 / 30620) loss: 2.023451\n",
      "(Iteration 20201 / 30620) loss: 1.993539\n",
      "(Iteration 20301 / 30620) loss: 2.072289\n",
      "(Iteration 20401 / 30620) loss: 2.009028\n",
      "(Iteration 20501 / 30620) loss: 1.934985\n",
      "(Iteration 20601 / 30620) loss: 2.025544\n",
      "(Iteration 20701 / 30620) loss: 1.811179\n",
      "(Iteration 20801 / 30620) loss: 1.781509\n",
      "(Iteration 20901 / 30620) loss: 2.032330\n",
      "(Iteration 21001 / 30620) loss: 2.094240\n",
      "(Iteration 21101 / 30620) loss: 2.053250\n",
      "(Iteration 21201 / 30620) loss: 2.199068\n",
      "(Iteration 21301 / 30620) loss: 1.798749\n",
      "(Iteration 21401 / 30620) loss: 2.060865\n",
      "(Epoch 14 / 20) train acc: 0.445000; val_acc: 0.431000\n",
      "(Iteration 21501 / 30620) loss: 1.932058\n",
      "(Iteration 21601 / 30620) loss: 2.085070\n",
      "(Iteration 21701 / 30620) loss: 1.945466\n",
      "(Iteration 21801 / 30620) loss: 2.096707\n",
      "(Iteration 21901 / 30620) loss: 2.090291\n",
      "(Iteration 22001 / 30620) loss: 1.997917\n",
      "(Iteration 22101 / 30620) loss: 2.064994\n",
      "(Iteration 22201 / 30620) loss: 1.975105\n",
      "(Iteration 22301 / 30620) loss: 2.015263\n",
      "(Iteration 22401 / 30620) loss: 2.038230\n",
      "(Iteration 22501 / 30620) loss: 1.874511\n",
      "(Iteration 22601 / 30620) loss: 2.089818\n",
      "(Iteration 22701 / 30620) loss: 1.910065\n",
      "(Iteration 22801 / 30620) loss: 1.970899\n",
      "(Iteration 22901 / 30620) loss: 1.878460\n",
      "(Epoch 15 / 20) train acc: 0.426000; val_acc: 0.433000\n",
      "(Iteration 23001 / 30620) loss: 1.859750\n",
      "(Iteration 23101 / 30620) loss: 2.055161\n",
      "(Iteration 23201 / 30620) loss: 2.051942\n",
      "(Iteration 23301 / 30620) loss: 1.795745\n",
      "(Iteration 23401 / 30620) loss: 2.193197\n",
      "(Iteration 23501 / 30620) loss: 1.974073\n",
      "(Iteration 23601 / 30620) loss: 1.872967\n",
      "(Iteration 23701 / 30620) loss: 1.937765\n",
      "(Iteration 23801 / 30620) loss: 1.624272\n",
      "(Iteration 23901 / 30620) loss: 1.930364\n",
      "(Iteration 24001 / 30620) loss: 2.053132\n",
      "(Iteration 24101 / 30620) loss: 1.888282\n",
      "(Iteration 24201 / 30620) loss: 1.894024\n",
      "(Iteration 24301 / 30620) loss: 1.975178\n",
      "(Iteration 24401 / 30620) loss: 1.927884\n",
      "(Epoch 16 / 20) train acc: 0.429000; val_acc: 0.435000\n",
      "(Iteration 24501 / 30620) loss: 1.808071\n",
      "(Iteration 24601 / 30620) loss: 2.039300\n",
      "(Iteration 24701 / 30620) loss: 1.805298\n",
      "(Iteration 24801 / 30620) loss: 1.851416\n",
      "(Iteration 24901 / 30620) loss: 1.775248\n",
      "(Iteration 25001 / 30620) loss: 1.834033\n",
      "(Iteration 25101 / 30620) loss: 1.882604\n",
      "(Iteration 25201 / 30620) loss: 2.068580\n",
      "(Iteration 25301 / 30620) loss: 1.859433\n",
      "(Iteration 25401 / 30620) loss: 1.919231\n",
      "(Iteration 25501 / 30620) loss: 2.036472\n",
      "(Iteration 25601 / 30620) loss: 2.095586\n",
      "(Iteration 25701 / 30620) loss: 1.856136\n",
      "(Iteration 25801 / 30620) loss: 2.036003\n",
      "(Iteration 25901 / 30620) loss: 1.938349\n",
      "(Iteration 26001 / 30620) loss: 1.892973\n",
      "(Epoch 17 / 20) train acc: 0.452000; val_acc: 0.434000\n",
      "(Iteration 26101 / 30620) loss: 1.990278\n",
      "(Iteration 26201 / 30620) loss: 1.808234\n",
      "(Iteration 26301 / 30620) loss: 1.996848\n",
      "(Iteration 26401 / 30620) loss: 1.937268\n",
      "(Iteration 26501 / 30620) loss: 2.062348\n",
      "(Iteration 26601 / 30620) loss: 1.900584\n",
      "(Iteration 26701 / 30620) loss: 1.853540\n",
      "(Iteration 26801 / 30620) loss: 1.792734\n",
      "(Iteration 26901 / 30620) loss: 2.056962\n",
      "(Iteration 27001 / 30620) loss: 1.911277\n",
      "(Iteration 27101 / 30620) loss: 1.827426\n",
      "(Iteration 27201 / 30620) loss: 1.892806\n",
      "(Iteration 27301 / 30620) loss: 1.891407\n",
      "(Iteration 27401 / 30620) loss: 1.832350\n",
      "(Iteration 27501 / 30620) loss: 1.959690\n",
      "(Epoch 18 / 20) train acc: 0.426000; val_acc: 0.434000\n",
      "(Iteration 27601 / 30620) loss: 1.966189\n",
      "(Iteration 27701 / 30620) loss: 2.090230\n",
      "(Iteration 27801 / 30620) loss: 2.003161\n",
      "(Iteration 27901 / 30620) loss: 1.755932\n",
      "(Iteration 28001 / 30620) loss: 1.973379\n",
      "(Iteration 28101 / 30620) loss: 1.854061\n",
      "(Iteration 28201 / 30620) loss: 1.901116\n",
      "(Iteration 28301 / 30620) loss: 1.929433\n",
      "(Iteration 28401 / 30620) loss: 1.997729\n",
      "(Iteration 28501 / 30620) loss: 1.926314\n",
      "(Iteration 28601 / 30620) loss: 1.873643\n",
      "(Iteration 28701 / 30620) loss: 2.004037\n",
      "(Iteration 28801 / 30620) loss: 1.872770\n",
      "(Iteration 28901 / 30620) loss: 2.017483\n",
      "(Iteration 29001 / 30620) loss: 1.861115\n",
      "(Epoch 19 / 20) train acc: 0.430000; val_acc: 0.432000\n",
      "(Iteration 29101 / 30620) loss: 1.963744\n",
      "(Iteration 29201 / 30620) loss: 1.992834\n",
      "(Iteration 29301 / 30620) loss: 1.860684\n",
      "(Iteration 29401 / 30620) loss: 1.914651\n",
      "(Iteration 29501 / 30620) loss: 2.111323\n",
      "(Iteration 29601 / 30620) loss: 1.662842\n",
      "(Iteration 29701 / 30620) loss: 2.135790\n",
      "(Iteration 29801 / 30620) loss: 1.948666\n",
      "(Iteration 29901 / 30620) loss: 1.857317\n",
      "(Iteration 30001 / 30620) loss: 2.039819\n",
      "(Iteration 30101 / 30620) loss: 1.913129\n",
      "(Iteration 30201 / 30620) loss: 1.811404\n",
      "(Iteration 30301 / 30620) loss: 2.062350\n",
      "(Iteration 30401 / 30620) loss: 2.004535\n",
      "(Iteration 30501 / 30620) loss: 1.895133\n",
      "(Iteration 30601 / 30620) loss: 1.924036\n",
      "(Epoch 20 / 20) train acc: 0.430000; val_acc: 0.436000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.01, 'num_epochs': 20, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 15300) loss: 2.303211\n",
      "(Epoch 0 / 20) train acc: 0.080000; val_acc: 0.099000\n",
      "(Iteration 101 / 15300) loss: 2.302396\n",
      "(Iteration 201 / 15300) loss: 2.302198\n",
      "(Iteration 301 / 15300) loss: 2.302537\n",
      "(Iteration 401 / 15300) loss: 2.299617\n",
      "(Iteration 501 / 15300) loss: 2.300659\n",
      "(Iteration 601 / 15300) loss: 2.296575\n",
      "(Iteration 701 / 15300) loss: 2.291753\n",
      "(Epoch 1 / 20) train acc: 0.245000; val_acc: 0.230000\n",
      "(Iteration 801 / 15300) loss: 2.276277\n",
      "(Iteration 901 / 15300) loss: 2.271133\n",
      "(Iteration 1001 / 15300) loss: 2.226999\n",
      "(Iteration 1101 / 15300) loss: 2.173639\n",
      "(Iteration 1201 / 15300) loss: 2.184578\n",
      "(Iteration 1301 / 15300) loss: 2.133479\n",
      "(Iteration 1401 / 15300) loss: 2.048534\n",
      "(Iteration 1501 / 15300) loss: 2.042204\n",
      "(Epoch 2 / 20) train acc: 0.282000; val_acc: 0.305000\n",
      "(Iteration 1601 / 15300) loss: 1.973760\n",
      "(Iteration 1701 / 15300) loss: 1.869511\n",
      "(Iteration 1801 / 15300) loss: 1.843977\n",
      "(Iteration 1901 / 15300) loss: 1.785967\n",
      "(Iteration 2001 / 15300) loss: 1.789118\n",
      "(Iteration 2101 / 15300) loss: 1.858879\n",
      "(Iteration 2201 / 15300) loss: 1.734785\n",
      "(Epoch 3 / 20) train acc: 0.385000; val_acc: 0.378000\n",
      "(Iteration 2301 / 15300) loss: 1.716410\n",
      "(Iteration 2401 / 15300) loss: 1.811626\n",
      "(Iteration 2501 / 15300) loss: 1.793047\n",
      "(Iteration 2601 / 15300) loss: 1.680277\n",
      "(Iteration 2701 / 15300) loss: 1.685829\n",
      "(Iteration 2801 / 15300) loss: 1.590045\n",
      "(Iteration 2901 / 15300) loss: 1.745595\n",
      "(Iteration 3001 / 15300) loss: 1.626828\n",
      "(Epoch 4 / 20) train acc: 0.427000; val_acc: 0.402000\n",
      "(Iteration 3101 / 15300) loss: 1.718407\n",
      "(Iteration 3201 / 15300) loss: 1.640657\n",
      "(Iteration 3301 / 15300) loss: 1.658782\n",
      "(Iteration 3401 / 15300) loss: 1.538471\n",
      "(Iteration 3501 / 15300) loss: 1.621633\n",
      "(Iteration 3601 / 15300) loss: 1.850456\n",
      "(Iteration 3701 / 15300) loss: 1.513580\n",
      "(Iteration 3801 / 15300) loss: 1.601279\n",
      "(Epoch 5 / 20) train acc: 0.425000; val_acc: 0.440000\n",
      "(Iteration 3901 / 15300) loss: 1.612015\n",
      "(Iteration 4001 / 15300) loss: 1.653508\n",
      "(Iteration 4101 / 15300) loss: 1.493244\n",
      "(Iteration 4201 / 15300) loss: 1.586479\n",
      "(Iteration 4301 / 15300) loss: 1.339510\n",
      "(Iteration 4401 / 15300) loss: 1.526051\n",
      "(Iteration 4501 / 15300) loss: 1.667432\n",
      "(Epoch 6 / 20) train acc: 0.477000; val_acc: 0.452000\n",
      "(Iteration 4601 / 15300) loss: 1.598128\n",
      "(Iteration 4701 / 15300) loss: 1.663806\n",
      "(Iteration 4801 / 15300) loss: 1.649849\n",
      "(Iteration 4901 / 15300) loss: 1.608358\n",
      "(Iteration 5001 / 15300) loss: 1.554471\n",
      "(Iteration 5101 / 15300) loss: 1.685016\n",
      "(Iteration 5201 / 15300) loss: 1.692593\n",
      "(Iteration 5301 / 15300) loss: 1.559372\n",
      "(Epoch 7 / 20) train acc: 0.489000; val_acc: 0.480000\n",
      "(Iteration 5401 / 15300) loss: 1.642848\n",
      "(Iteration 5501 / 15300) loss: 1.541061\n",
      "(Iteration 5601 / 15300) loss: 1.522477\n",
      "(Iteration 5701 / 15300) loss: 1.570685\n",
      "(Iteration 5801 / 15300) loss: 1.417812\n",
      "(Iteration 5901 / 15300) loss: 1.378711\n",
      "(Iteration 6001 / 15300) loss: 1.489475\n",
      "(Iteration 6101 / 15300) loss: 1.431979\n",
      "(Epoch 8 / 20) train acc: 0.503000; val_acc: 0.488000\n",
      "(Iteration 6201 / 15300) loss: 1.507373\n",
      "(Iteration 6301 / 15300) loss: 1.400209\n",
      "(Iteration 6401 / 15300) loss: 1.516603\n",
      "(Iteration 6501 / 15300) loss: 1.510795\n",
      "(Iteration 6601 / 15300) loss: 1.610474\n",
      "(Iteration 6701 / 15300) loss: 1.592960\n",
      "(Iteration 6801 / 15300) loss: 1.498636\n",
      "(Epoch 9 / 20) train acc: 0.484000; val_acc: 0.482000\n",
      "(Iteration 6901 / 15300) loss: 1.467082\n",
      "(Iteration 7001 / 15300) loss: 1.605718\n",
      "(Iteration 7101 / 15300) loss: 1.286702\n",
      "(Iteration 7201 / 15300) loss: 1.600338\n",
      "(Iteration 7301 / 15300) loss: 1.679887\n",
      "(Iteration 7401 / 15300) loss: 1.389297\n",
      "(Iteration 7501 / 15300) loss: 1.796471\n",
      "(Iteration 7601 / 15300) loss: 1.386987\n",
      "(Epoch 10 / 20) train acc: 0.478000; val_acc: 0.495000\n",
      "(Iteration 7701 / 15300) loss: 1.343426\n",
      "(Iteration 7801 / 15300) loss: 1.534298\n",
      "(Iteration 7901 / 15300) loss: 1.451590\n",
      "(Iteration 8001 / 15300) loss: 1.466681\n",
      "(Iteration 8101 / 15300) loss: 1.506473\n",
      "(Iteration 8201 / 15300) loss: 1.463778\n",
      "(Iteration 8301 / 15300) loss: 1.616030\n",
      "(Iteration 8401 / 15300) loss: 1.424730\n",
      "(Epoch 11 / 20) train acc: 0.504000; val_acc: 0.501000\n",
      "(Iteration 8501 / 15300) loss: 1.456567\n",
      "(Iteration 8601 / 15300) loss: 1.309606\n",
      "(Iteration 8701 / 15300) loss: 1.309030\n",
      "(Iteration 8801 / 15300) loss: 1.468667\n",
      "(Iteration 8901 / 15300) loss: 1.545852\n",
      "(Iteration 9001 / 15300) loss: 1.570928\n",
      "(Iteration 9101 / 15300) loss: 1.525049\n",
      "(Epoch 12 / 20) train acc: 0.509000; val_acc: 0.506000\n",
      "(Iteration 9201 / 15300) loss: 1.476610\n",
      "(Iteration 9301 / 15300) loss: 1.631343\n",
      "(Iteration 9401 / 15300) loss: 1.521488\n",
      "(Iteration 9501 / 15300) loss: 1.498241\n",
      "(Iteration 9601 / 15300) loss: 1.306245\n",
      "(Iteration 9701 / 15300) loss: 1.375144\n",
      "(Iteration 9801 / 15300) loss: 1.574606\n",
      "(Iteration 9901 / 15300) loss: 1.636424\n",
      "(Epoch 13 / 20) train acc: 0.527000; val_acc: 0.506000\n",
      "(Iteration 10001 / 15300) loss: 1.341845\n",
      "(Iteration 10101 / 15300) loss: 1.481705\n",
      "(Iteration 10201 / 15300) loss: 1.557130\n",
      "(Iteration 10301 / 15300) loss: 1.605518\n",
      "(Iteration 10401 / 15300) loss: 1.528136\n",
      "(Iteration 10501 / 15300) loss: 1.517249\n",
      "(Iteration 10601 / 15300) loss: 1.576145\n",
      "(Iteration 10701 / 15300) loss: 1.512872\n",
      "(Epoch 14 / 20) train acc: 0.505000; val_acc: 0.511000\n",
      "(Iteration 10801 / 15300) loss: 1.380533\n",
      "(Iteration 10901 / 15300) loss: 1.437118\n",
      "(Iteration 11001 / 15300) loss: 1.394351\n",
      "(Iteration 11101 / 15300) loss: 1.636450\n",
      "(Iteration 11201 / 15300) loss: 1.457723\n",
      "(Iteration 11301 / 15300) loss: 1.468372\n",
      "(Iteration 11401 / 15300) loss: 1.508550\n",
      "(Epoch 15 / 20) train acc: 0.544000; val_acc: 0.512000\n",
      "(Iteration 11501 / 15300) loss: 1.453490\n",
      "(Iteration 11601 / 15300) loss: 1.408395\n",
      "(Iteration 11701 / 15300) loss: 1.506996\n",
      "(Iteration 11801 / 15300) loss: 1.408157\n",
      "(Iteration 11901 / 15300) loss: 1.325335\n",
      "(Iteration 12001 / 15300) loss: 1.441981\n",
      "(Iteration 12101 / 15300) loss: 1.428005\n",
      "(Iteration 12201 / 15300) loss: 1.276575\n",
      "(Epoch 16 / 20) train acc: 0.522000; val_acc: 0.512000\n",
      "(Iteration 12301 / 15300) loss: 1.287344\n",
      "(Iteration 12401 / 15300) loss: 1.379747\n",
      "(Iteration 12501 / 15300) loss: 1.533403\n",
      "(Iteration 12601 / 15300) loss: 1.298385\n",
      "(Iteration 12701 / 15300) loss: 1.458440\n",
      "(Iteration 12801 / 15300) loss: 1.410760\n",
      "(Iteration 12901 / 15300) loss: 1.375364\n",
      "(Iteration 13001 / 15300) loss: 1.526593\n",
      "(Epoch 17 / 20) train acc: 0.496000; val_acc: 0.513000\n",
      "(Iteration 13101 / 15300) loss: 1.640986\n",
      "(Iteration 13201 / 15300) loss: 1.564802\n",
      "(Iteration 13301 / 15300) loss: 1.311680\n",
      "(Iteration 13401 / 15300) loss: 1.501440\n",
      "(Iteration 13501 / 15300) loss: 1.288290\n",
      "(Iteration 13601 / 15300) loss: 1.385718\n",
      "(Iteration 13701 / 15300) loss: 1.471451\n",
      "(Epoch 18 / 20) train acc: 0.527000; val_acc: 0.510000\n",
      "(Iteration 13801 / 15300) loss: 1.608930\n",
      "(Iteration 13901 / 15300) loss: 1.336934\n",
      "(Iteration 14001 / 15300) loss: 1.538022\n",
      "(Iteration 14101 / 15300) loss: 1.172929\n",
      "(Iteration 14201 / 15300) loss: 1.249270\n",
      "(Iteration 14301 / 15300) loss: 1.304086\n",
      "(Iteration 14401 / 15300) loss: 1.478426\n",
      "(Iteration 14501 / 15300) loss: 1.527126\n",
      "(Epoch 19 / 20) train acc: 0.519000; val_acc: 0.509000\n",
      "(Iteration 14601 / 15300) loss: 1.526005\n",
      "(Iteration 14701 / 15300) loss: 1.416639\n",
      "(Iteration 14801 / 15300) loss: 1.316499\n",
      "(Iteration 14901 / 15300) loss: 1.329396\n",
      "(Iteration 15001 / 15300) loss: 1.408984\n",
      "(Iteration 15101 / 15300) loss: 1.431585\n",
      "(Iteration 15201 / 15300) loss: 1.421251\n",
      "(Epoch 20 / 20) train acc: 0.532000; val_acc: 0.511000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.01, 'num_epochs': 20, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 30620) loss: 2.303189\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.101000\n",
      "(Iteration 101 / 30620) loss: 2.301778\n",
      "(Iteration 201 / 30620) loss: 2.303493\n",
      "(Iteration 301 / 30620) loss: 2.301926\n",
      "(Iteration 401 / 30620) loss: 2.300797\n",
      "(Iteration 501 / 30620) loss: 2.302356\n",
      "(Iteration 601 / 30620) loss: 2.298215\n",
      "(Iteration 701 / 30620) loss: 2.283648\n",
      "(Iteration 801 / 30620) loss: 2.281015\n",
      "(Iteration 901 / 30620) loss: 2.256130\n",
      "(Iteration 1001 / 30620) loss: 2.212927\n",
      "(Iteration 1101 / 30620) loss: 2.224240\n",
      "(Iteration 1201 / 30620) loss: 2.230554\n",
      "(Iteration 1301 / 30620) loss: 2.024568\n",
      "(Iteration 1401 / 30620) loss: 2.144582\n",
      "(Iteration 1501 / 30620) loss: 1.884533\n",
      "(Epoch 1 / 20) train acc: 0.299000; val_acc: 0.306000\n",
      "(Iteration 1601 / 30620) loss: 1.963865\n",
      "(Iteration 1701 / 30620) loss: 2.050528\n",
      "(Iteration 1801 / 30620) loss: 1.959225\n",
      "(Iteration 1901 / 30620) loss: 1.941271\n",
      "(Iteration 2001 / 30620) loss: 1.785796\n",
      "(Iteration 2101 / 30620) loss: 1.667902\n",
      "(Iteration 2201 / 30620) loss: 1.608478\n",
      "(Iteration 2301 / 30620) loss: 2.035750\n",
      "(Iteration 2401 / 30620) loss: 1.701741\n",
      "(Iteration 2501 / 30620) loss: 1.521615\n",
      "(Iteration 2601 / 30620) loss: 1.444265\n",
      "(Iteration 2701 / 30620) loss: 1.678074\n",
      "(Iteration 2801 / 30620) loss: 1.719121\n",
      "(Iteration 2901 / 30620) loss: 1.578330\n",
      "(Iteration 3001 / 30620) loss: 1.546835\n",
      "(Epoch 2 / 20) train acc: 0.453000; val_acc: 0.427000\n",
      "(Iteration 3101 / 30620) loss: 1.482163\n",
      "(Iteration 3201 / 30620) loss: 1.577048\n",
      "(Iteration 3301 / 30620) loss: 1.500440\n",
      "(Iteration 3401 / 30620) loss: 1.627564\n",
      "(Iteration 3501 / 30620) loss: 1.788976\n",
      "(Iteration 3601 / 30620) loss: 1.495293\n",
      "(Iteration 3701 / 30620) loss: 1.544961\n",
      "(Iteration 3801 / 30620) loss: 1.806703\n",
      "(Iteration 3901 / 30620) loss: 1.438561\n",
      "(Iteration 4001 / 30620) loss: 1.564205\n",
      "(Iteration 4101 / 30620) loss: 1.836274\n",
      "(Iteration 4201 / 30620) loss: 1.786662\n",
      "(Iteration 4301 / 30620) loss: 1.374745\n",
      "(Iteration 4401 / 30620) loss: 1.462018\n",
      "(Iteration 4501 / 30620) loss: 1.679191\n",
      "(Epoch 3 / 20) train acc: 0.500000; val_acc: 0.480000\n",
      "(Iteration 4601 / 30620) loss: 1.406276\n",
      "(Iteration 4701 / 30620) loss: 1.634143\n",
      "(Iteration 4801 / 30620) loss: 1.409136\n",
      "(Iteration 4901 / 30620) loss: 1.347721\n",
      "(Iteration 5001 / 30620) loss: 1.165046\n",
      "(Iteration 5101 / 30620) loss: 1.570945\n",
      "(Iteration 5201 / 30620) loss: 1.166619\n",
      "(Iteration 5301 / 30620) loss: 1.508038\n",
      "(Iteration 5401 / 30620) loss: 1.462124\n",
      "(Iteration 5501 / 30620) loss: 1.730476\n",
      "(Iteration 5601 / 30620) loss: 1.586096\n",
      "(Iteration 5701 / 30620) loss: 1.675072\n",
      "(Iteration 5801 / 30620) loss: 1.583245\n",
      "(Iteration 5901 / 30620) loss: 1.599251\n",
      "(Iteration 6001 / 30620) loss: 1.607987\n",
      "(Iteration 6101 / 30620) loss: 1.770160\n",
      "(Epoch 4 / 20) train acc: 0.473000; val_acc: 0.501000\n",
      "(Iteration 6201 / 30620) loss: 1.544681\n",
      "(Iteration 6301 / 30620) loss: 1.357436\n",
      "(Iteration 6401 / 30620) loss: 1.589878\n",
      "(Iteration 6501 / 30620) loss: 1.354984\n",
      "(Iteration 6601 / 30620) loss: 1.586378\n",
      "(Iteration 6701 / 30620) loss: 1.159321\n",
      "(Iteration 6801 / 30620) loss: 1.478651\n",
      "(Iteration 6901 / 30620) loss: 1.716820\n",
      "(Iteration 7001 / 30620) loss: 1.383492\n",
      "(Iteration 7101 / 30620) loss: 1.451067\n",
      "(Iteration 7201 / 30620) loss: 1.472860\n",
      "(Iteration 7301 / 30620) loss: 1.612740\n",
      "(Iteration 7401 / 30620) loss: 1.335542\n",
      "(Iteration 7501 / 30620) loss: 1.644283\n",
      "(Iteration 7601 / 30620) loss: 1.108578\n",
      "(Epoch 5 / 20) train acc: 0.544000; val_acc: 0.512000\n",
      "(Iteration 7701 / 30620) loss: 1.488809\n",
      "(Iteration 7801 / 30620) loss: 1.572412\n",
      "(Iteration 7901 / 30620) loss: 1.347962\n",
      "(Iteration 8001 / 30620) loss: 1.371634\n",
      "(Iteration 8101 / 30620) loss: 1.646030\n",
      "(Iteration 8201 / 30620) loss: 1.717838\n",
      "(Iteration 8301 / 30620) loss: 1.632113\n",
      "(Iteration 8401 / 30620) loss: 1.540422\n",
      "(Iteration 8501 / 30620) loss: 1.478701\n",
      "(Iteration 8601 / 30620) loss: 1.337772\n",
      "(Iteration 8701 / 30620) loss: 1.494745\n",
      "(Iteration 8801 / 30620) loss: 1.526589\n",
      "(Iteration 8901 / 30620) loss: 1.467546\n",
      "(Iteration 9001 / 30620) loss: 1.310593\n",
      "(Iteration 9101 / 30620) loss: 1.306399\n",
      "(Epoch 6 / 20) train acc: 0.507000; val_acc: 0.506000\n",
      "(Iteration 9201 / 30620) loss: 1.566917\n",
      "(Iteration 9301 / 30620) loss: 1.555455\n",
      "(Iteration 9401 / 30620) loss: 1.525328\n",
      "(Iteration 9501 / 30620) loss: 1.528462\n",
      "(Iteration 9601 / 30620) loss: 1.377482\n",
      "(Iteration 9701 / 30620) loss: 1.412113\n",
      "(Iteration 9801 / 30620) loss: 1.445674\n",
      "(Iteration 9901 / 30620) loss: 1.564916\n",
      "(Iteration 10001 / 30620) loss: 1.254996\n",
      "(Iteration 10101 / 30620) loss: 1.231889\n",
      "(Iteration 10201 / 30620) loss: 1.555888\n",
      "(Iteration 10301 / 30620) loss: 1.452370\n",
      "(Iteration 10401 / 30620) loss: 1.227668\n",
      "(Iteration 10501 / 30620) loss: 1.661750\n",
      "(Iteration 10601 / 30620) loss: 1.682843\n",
      "(Iteration 10701 / 30620) loss: 1.449310\n",
      "(Epoch 7 / 20) train acc: 0.526000; val_acc: 0.516000\n",
      "(Iteration 10801 / 30620) loss: 1.375203\n",
      "(Iteration 10901 / 30620) loss: 1.363272\n",
      "(Iteration 11001 / 30620) loss: 1.509884\n",
      "(Iteration 11101 / 30620) loss: 1.311252\n",
      "(Iteration 11201 / 30620) loss: 1.259740\n",
      "(Iteration 11301 / 30620) loss: 1.464979\n",
      "(Iteration 11401 / 30620) loss: 1.689363\n",
      "(Iteration 11501 / 30620) loss: 1.673732\n",
      "(Iteration 11601 / 30620) loss: 1.268447\n",
      "(Iteration 11701 / 30620) loss: 1.606546\n",
      "(Iteration 11801 / 30620) loss: 1.761898\n",
      "(Iteration 11901 / 30620) loss: 1.575548\n",
      "(Iteration 12001 / 30620) loss: 1.714030\n",
      "(Iteration 12101 / 30620) loss: 1.113602\n",
      "(Iteration 12201 / 30620) loss: 1.114318\n",
      "(Epoch 8 / 20) train acc: 0.533000; val_acc: 0.510000\n",
      "(Iteration 12301 / 30620) loss: 1.753707\n",
      "(Iteration 12401 / 30620) loss: 1.758077\n",
      "(Iteration 12501 / 30620) loss: 1.528733\n",
      "(Iteration 12601 / 30620) loss: 1.430610\n",
      "(Iteration 12701 / 30620) loss: 1.577960\n",
      "(Iteration 12801 / 30620) loss: 1.642934\n",
      "(Iteration 12901 / 30620) loss: 1.352834\n",
      "(Iteration 13001 / 30620) loss: 1.643754\n",
      "(Iteration 13101 / 30620) loss: 1.486890\n",
      "(Iteration 13201 / 30620) loss: 1.345694\n",
      "(Iteration 13301 / 30620) loss: 1.449404\n",
      "(Iteration 13401 / 30620) loss: 1.505734\n",
      "(Iteration 13501 / 30620) loss: 1.580252\n",
      "(Iteration 13601 / 30620) loss: 1.407054\n",
      "(Iteration 13701 / 30620) loss: 1.704298\n",
      "(Epoch 9 / 20) train acc: 0.504000; val_acc: 0.520000\n",
      "(Iteration 13801 / 30620) loss: 1.334743\n",
      "(Iteration 13901 / 30620) loss: 1.482941\n",
      "(Iteration 14001 / 30620) loss: 1.583823\n",
      "(Iteration 14101 / 30620) loss: 1.490537\n",
      "(Iteration 14201 / 30620) loss: 1.533720\n",
      "(Iteration 14301 / 30620) loss: 1.547984\n",
      "(Iteration 14401 / 30620) loss: 1.503459\n",
      "(Iteration 14501 / 30620) loss: 1.217539\n",
      "(Iteration 14601 / 30620) loss: 1.160330\n",
      "(Iteration 14701 / 30620) loss: 1.219607\n",
      "(Iteration 14801 / 30620) loss: 1.288178\n",
      "(Iteration 14901 / 30620) loss: 1.383335\n",
      "(Iteration 15001 / 30620) loss: 1.448518\n",
      "(Iteration 15101 / 30620) loss: 1.559584\n",
      "(Iteration 15201 / 30620) loss: 1.676275\n",
      "(Iteration 15301 / 30620) loss: 1.320214\n",
      "(Epoch 10 / 20) train acc: 0.537000; val_acc: 0.520000\n",
      "(Iteration 15401 / 30620) loss: 1.528328\n",
      "(Iteration 15501 / 30620) loss: 1.525016\n",
      "(Iteration 15601 / 30620) loss: 1.460908\n",
      "(Iteration 15701 / 30620) loss: 1.759299\n",
      "(Iteration 15801 / 30620) loss: 1.708273\n",
      "(Iteration 15901 / 30620) loss: 1.563757\n",
      "(Iteration 16001 / 30620) loss: 1.438185\n",
      "(Iteration 16101 / 30620) loss: 1.623101\n",
      "(Iteration 16201 / 30620) loss: 1.607596\n",
      "(Iteration 16301 / 30620) loss: 1.753311\n",
      "(Iteration 16401 / 30620) loss: 1.369611\n",
      "(Iteration 16501 / 30620) loss: 1.237535\n",
      "(Iteration 16601 / 30620) loss: 1.225019\n",
      "(Iteration 16701 / 30620) loss: 1.436341\n",
      "(Iteration 16801 / 30620) loss: 1.365504\n",
      "(Epoch 11 / 20) train acc: 0.544000; val_acc: 0.524000\n",
      "(Iteration 16901 / 30620) loss: 1.622739\n",
      "(Iteration 17001 / 30620) loss: 1.555255\n",
      "(Iteration 17101 / 30620) loss: 1.762044\n",
      "(Iteration 17201 / 30620) loss: 1.250881\n",
      "(Iteration 17301 / 30620) loss: 1.330733\n",
      "(Iteration 17401 / 30620) loss: 1.484838\n",
      "(Iteration 17501 / 30620) loss: 1.409890\n",
      "(Iteration 17601 / 30620) loss: 1.582860\n",
      "(Iteration 17701 / 30620) loss: 1.288740\n",
      "(Iteration 17801 / 30620) loss: 1.559083\n",
      "(Iteration 17901 / 30620) loss: 1.381015\n",
      "(Iteration 18001 / 30620) loss: 1.619571\n",
      "(Iteration 18101 / 30620) loss: 1.173787\n",
      "(Iteration 18201 / 30620) loss: 1.454504\n",
      "(Iteration 18301 / 30620) loss: 1.492315\n",
      "(Epoch 12 / 20) train acc: 0.543000; val_acc: 0.516000\n",
      "(Iteration 18401 / 30620) loss: 1.273667\n",
      "(Iteration 18501 / 30620) loss: 1.520636\n",
      "(Iteration 18601 / 30620) loss: 1.596424\n",
      "(Iteration 18701 / 30620) loss: 1.396793\n",
      "(Iteration 18801 / 30620) loss: 1.350077\n",
      "(Iteration 18901 / 30620) loss: 1.312706\n",
      "(Iteration 19001 / 30620) loss: 1.426505\n",
      "(Iteration 19101 / 30620) loss: 1.303405\n",
      "(Iteration 19201 / 30620) loss: 1.389474\n",
      "(Iteration 19301 / 30620) loss: 1.381434\n",
      "(Iteration 19401 / 30620) loss: 1.751686\n",
      "(Iteration 19501 / 30620) loss: 1.192326\n",
      "(Iteration 19601 / 30620) loss: 1.461071\n",
      "(Iteration 19701 / 30620) loss: 1.313135\n",
      "(Iteration 19801 / 30620) loss: 1.273628\n",
      "(Iteration 19901 / 30620) loss: 1.405715\n",
      "(Epoch 13 / 20) train acc: 0.548000; val_acc: 0.519000\n",
      "(Iteration 20001 / 30620) loss: 1.475002\n",
      "(Iteration 20101 / 30620) loss: 1.661327\n",
      "(Iteration 20201 / 30620) loss: 1.373506\n",
      "(Iteration 20301 / 30620) loss: 1.212938\n",
      "(Iteration 20401 / 30620) loss: 1.377777\n",
      "(Iteration 20501 / 30620) loss: 1.277624\n",
      "(Iteration 20601 / 30620) loss: 1.667193\n",
      "(Iteration 20701 / 30620) loss: 1.566455\n",
      "(Iteration 20801 / 30620) loss: 1.677498\n",
      "(Iteration 20901 / 30620) loss: 1.617998\n",
      "(Iteration 21001 / 30620) loss: 1.664296\n",
      "(Iteration 21101 / 30620) loss: 1.542762\n",
      "(Iteration 21201 / 30620) loss: 1.369174\n",
      "(Iteration 21301 / 30620) loss: 1.335672\n",
      "(Iteration 21401 / 30620) loss: 1.389102\n",
      "(Epoch 14 / 20) train acc: 0.553000; val_acc: 0.522000\n",
      "(Iteration 21501 / 30620) loss: 1.280660\n",
      "(Iteration 21601 / 30620) loss: 1.460570\n",
      "(Iteration 21701 / 30620) loss: 1.277201\n",
      "(Iteration 21801 / 30620) loss: 1.342291\n",
      "(Iteration 21901 / 30620) loss: 1.447615\n",
      "(Iteration 22001 / 30620) loss: 1.800101\n",
      "(Iteration 22101 / 30620) loss: 1.493269\n",
      "(Iteration 22201 / 30620) loss: 1.483331\n",
      "(Iteration 22301 / 30620) loss: 1.407465\n",
      "(Iteration 22401 / 30620) loss: 1.717877\n",
      "(Iteration 22501 / 30620) loss: 1.324310\n",
      "(Iteration 22601 / 30620) loss: 1.408308\n",
      "(Iteration 22701 / 30620) loss: 1.559135\n",
      "(Iteration 22801 / 30620) loss: 1.202193\n",
      "(Iteration 22901 / 30620) loss: 1.572825\n",
      "(Epoch 15 / 20) train acc: 0.564000; val_acc: 0.517000\n",
      "(Iteration 23001 / 30620) loss: 1.639635\n",
      "(Iteration 23101 / 30620) loss: 1.066365\n",
      "(Iteration 23201 / 30620) loss: 1.600322\n",
      "(Iteration 23301 / 30620) loss: 1.366482\n",
      "(Iteration 23401 / 30620) loss: 1.613370\n",
      "(Iteration 23501 / 30620) loss: 1.423060\n",
      "(Iteration 23601 / 30620) loss: 1.213427\n",
      "(Iteration 23701 / 30620) loss: 1.113340\n",
      "(Iteration 23801 / 30620) loss: 1.660369\n",
      "(Iteration 23901 / 30620) loss: 1.527106\n",
      "(Iteration 24001 / 30620) loss: 1.465457\n",
      "(Iteration 24101 / 30620) loss: 1.519037\n",
      "(Iteration 24201 / 30620) loss: 1.379818\n",
      "(Iteration 24301 / 30620) loss: 1.243081\n",
      "(Iteration 24401 / 30620) loss: 1.687384\n",
      "(Epoch 16 / 20) train acc: 0.500000; val_acc: 0.515000\n",
      "(Iteration 24501 / 30620) loss: 1.454876\n",
      "(Iteration 24601 / 30620) loss: 1.521610\n",
      "(Iteration 24701 / 30620) loss: 1.487613\n",
      "(Iteration 24801 / 30620) loss: 1.492553\n",
      "(Iteration 24901 / 30620) loss: 1.519257\n",
      "(Iteration 25001 / 30620) loss: 1.388764\n",
      "(Iteration 25101 / 30620) loss: 1.244920\n",
      "(Iteration 25201 / 30620) loss: 1.603059\n",
      "(Iteration 25301 / 30620) loss: 1.342574\n",
      "(Iteration 25401 / 30620) loss: 1.591918\n",
      "(Iteration 25501 / 30620) loss: 1.634390\n",
      "(Iteration 25601 / 30620) loss: 1.721821\n",
      "(Iteration 25701 / 30620) loss: 1.560839\n",
      "(Iteration 25801 / 30620) loss: 1.159337\n",
      "(Iteration 25901 / 30620) loss: 1.348132\n",
      "(Iteration 26001 / 30620) loss: 1.304227\n",
      "(Epoch 17 / 20) train acc: 0.562000; val_acc: 0.513000\n",
      "(Iteration 26101 / 30620) loss: 1.326976\n",
      "(Iteration 26201 / 30620) loss: 1.370381\n",
      "(Iteration 26301 / 30620) loss: 1.108637\n",
      "(Iteration 26401 / 30620) loss: 1.546937\n",
      "(Iteration 26501 / 30620) loss: 1.470831\n",
      "(Iteration 26601 / 30620) loss: 1.207261\n",
      "(Iteration 26701 / 30620) loss: 1.344905\n",
      "(Iteration 26801 / 30620) loss: 1.476394\n",
      "(Iteration 26901 / 30620) loss: 1.522101\n",
      "(Iteration 27001 / 30620) loss: 1.487086\n",
      "(Iteration 27101 / 30620) loss: 1.527817\n",
      "(Iteration 27201 / 30620) loss: 1.418269\n",
      "(Iteration 27301 / 30620) loss: 1.465401\n",
      "(Iteration 27401 / 30620) loss: 1.620354\n",
      "(Iteration 27501 / 30620) loss: 1.314573\n",
      "(Epoch 18 / 20) train acc: 0.502000; val_acc: 0.513000\n",
      "(Iteration 27601 / 30620) loss: 1.326851\n",
      "(Iteration 27701 / 30620) loss: 1.113257\n",
      "(Iteration 27801 / 30620) loss: 1.616152\n",
      "(Iteration 27901 / 30620) loss: 1.500519\n",
      "(Iteration 28001 / 30620) loss: 1.388163\n",
      "(Iteration 28101 / 30620) loss: 1.442636\n",
      "(Iteration 28201 / 30620) loss: 1.631777\n",
      "(Iteration 28301 / 30620) loss: 1.520204\n",
      "(Iteration 28401 / 30620) loss: 1.424044\n",
      "(Iteration 28501 / 30620) loss: 1.501157\n",
      "(Iteration 28601 / 30620) loss: 1.476329\n",
      "(Iteration 28701 / 30620) loss: 1.507049\n",
      "(Iteration 28801 / 30620) loss: 1.313603\n",
      "(Iteration 28901 / 30620) loss: 1.526533\n",
      "(Iteration 29001 / 30620) loss: 1.778409\n",
      "(Epoch 19 / 20) train acc: 0.541000; val_acc: 0.510000\n",
      "(Iteration 29101 / 30620) loss: 1.374444\n",
      "(Iteration 29201 / 30620) loss: 1.275202\n",
      "(Iteration 29301 / 30620) loss: 1.650984\n",
      "(Iteration 29401 / 30620) loss: 1.463560\n",
      "(Iteration 29501 / 30620) loss: 1.536254\n",
      "(Iteration 29601 / 30620) loss: 1.354809\n",
      "(Iteration 29701 / 30620) loss: 1.442591\n",
      "(Iteration 29801 / 30620) loss: 1.659084\n",
      "(Iteration 29901 / 30620) loss: 1.094600\n",
      "(Iteration 30001 / 30620) loss: 1.242445\n",
      "(Iteration 30101 / 30620) loss: 1.488425\n",
      "(Iteration 30201 / 30620) loss: 1.563560\n",
      "(Iteration 30301 / 30620) loss: 1.210424\n",
      "(Iteration 30401 / 30620) loss: 1.210447\n",
      "(Iteration 30501 / 30620) loss: 1.592866\n",
      "(Iteration 30601 / 30620) loss: 1.467083\n",
      "(Epoch 20 / 20) train acc: 0.537000; val_acc: 0.513000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.01, 'num_epochs': 30, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 22950) loss: 2.308335\n",
      "(Epoch 0 / 30) train acc: 0.111000; val_acc: 0.104000\n",
      "(Iteration 101 / 22950) loss: 2.307552\n",
      "(Iteration 201 / 22950) loss: 2.306289\n",
      "(Iteration 301 / 22950) loss: 2.306063\n",
      "(Iteration 401 / 22950) loss: 2.303597\n",
      "(Iteration 501 / 22950) loss: 2.303256\n",
      "(Iteration 601 / 22950) loss: 2.303582\n",
      "(Iteration 701 / 22950) loss: 2.300859\n",
      "(Epoch 1 / 30) train acc: 0.188000; val_acc: 0.202000\n",
      "(Iteration 801 / 22950) loss: 2.299446\n",
      "(Iteration 901 / 22950) loss: 2.297465\n",
      "(Iteration 1001 / 22950) loss: 2.294316\n",
      "(Iteration 1101 / 22950) loss: 2.289355\n",
      "(Iteration 1201 / 22950) loss: 2.297163\n",
      "(Iteration 1301 / 22950) loss: 2.265874\n",
      "(Iteration 1401 / 22950) loss: 2.262029\n",
      "(Iteration 1501 / 22950) loss: 2.266196\n",
      "(Epoch 2 / 30) train acc: 0.211000; val_acc: 0.227000\n",
      "(Iteration 1601 / 22950) loss: 2.201772\n",
      "(Iteration 1701 / 22950) loss: 2.222557\n",
      "(Iteration 1801 / 22950) loss: 2.227344\n",
      "(Iteration 1901 / 22950) loss: 2.220311\n",
      "(Iteration 2001 / 22950) loss: 2.226310\n",
      "(Iteration 2101 / 22950) loss: 2.085939\n",
      "(Iteration 2201 / 22950) loss: 2.116449\n",
      "(Epoch 3 / 30) train acc: 0.252000; val_acc: 0.278000\n",
      "(Iteration 2301 / 22950) loss: 2.030909\n",
      "(Iteration 2401 / 22950) loss: 2.125261\n",
      "(Iteration 2501 / 22950) loss: 2.152495\n",
      "(Iteration 2601 / 22950) loss: 2.114937\n",
      "(Iteration 2701 / 22950) loss: 2.065021\n",
      "(Iteration 2801 / 22950) loss: 2.139238\n",
      "(Iteration 2901 / 22950) loss: 2.039222\n",
      "(Iteration 3001 / 22950) loss: 2.075515\n",
      "(Epoch 4 / 30) train acc: 0.317000; val_acc: 0.299000\n",
      "(Iteration 3101 / 22950) loss: 2.011520\n",
      "(Iteration 3201 / 22950) loss: 2.033737\n",
      "(Iteration 3301 / 22950) loss: 2.045775\n",
      "(Iteration 3401 / 22950) loss: 2.103083\n",
      "(Iteration 3501 / 22950) loss: 1.912729\n",
      "(Iteration 3601 / 22950) loss: 2.046575\n",
      "(Iteration 3701 / 22950) loss: 1.953008\n",
      "(Iteration 3801 / 22950) loss: 1.931693\n",
      "(Epoch 5 / 30) train acc: 0.321000; val_acc: 0.319000\n",
      "(Iteration 3901 / 22950) loss: 2.027791\n",
      "(Iteration 4001 / 22950) loss: 2.101097\n",
      "(Iteration 4101 / 22950) loss: 2.028364\n",
      "(Iteration 4201 / 22950) loss: 2.008320\n",
      "(Iteration 4301 / 22950) loss: 2.026219\n",
      "(Iteration 4401 / 22950) loss: 2.058163\n",
      "(Iteration 4501 / 22950) loss: 2.083983\n",
      "(Epoch 6 / 30) train acc: 0.347000; val_acc: 0.355000\n",
      "(Iteration 4601 / 22950) loss: 2.151955\n",
      "(Iteration 4701 / 22950) loss: 1.972998\n",
      "(Iteration 4801 / 22950) loss: 2.134331\n",
      "(Iteration 4901 / 22950) loss: 2.040257\n",
      "(Iteration 5001 / 22950) loss: 2.057087\n",
      "(Iteration 5101 / 22950) loss: 1.976793\n",
      "(Iteration 5201 / 22950) loss: 2.076432\n",
      "(Iteration 5301 / 22950) loss: 2.074373\n",
      "(Epoch 7 / 30) train acc: 0.385000; val_acc: 0.370000\n",
      "(Iteration 5401 / 22950) loss: 2.034795\n",
      "(Iteration 5501 / 22950) loss: 1.900570\n",
      "(Iteration 5601 / 22950) loss: 2.041366\n",
      "(Iteration 5701 / 22950) loss: 1.922747\n",
      "(Iteration 5801 / 22950) loss: 2.003180\n",
      "(Iteration 5901 / 22950) loss: 1.923860\n",
      "(Iteration 6001 / 22950) loss: 1.952373\n",
      "(Iteration 6101 / 22950) loss: 2.034051\n",
      "(Epoch 8 / 30) train acc: 0.406000; val_acc: 0.383000\n",
      "(Iteration 6201 / 22950) loss: 2.019781\n",
      "(Iteration 6301 / 22950) loss: 1.910310\n",
      "(Iteration 6401 / 22950) loss: 2.080088\n",
      "(Iteration 6501 / 22950) loss: 2.042200\n",
      "(Iteration 6601 / 22950) loss: 2.005558\n",
      "(Iteration 6701 / 22950) loss: 2.075814\n",
      "(Iteration 6801 / 22950) loss: 2.001103\n",
      "(Epoch 9 / 30) train acc: 0.394000; val_acc: 0.398000\n",
      "(Iteration 6901 / 22950) loss: 1.969028\n",
      "(Iteration 7001 / 22950) loss: 2.141362\n",
      "(Iteration 7101 / 22950) loss: 2.040168\n",
      "(Iteration 7201 / 22950) loss: 1.942888\n",
      "(Iteration 7301 / 22950) loss: 1.980317\n",
      "(Iteration 7401 / 22950) loss: 2.054040\n",
      "(Iteration 7501 / 22950) loss: 1.999047\n",
      "(Iteration 7601 / 22950) loss: 1.977358\n",
      "(Epoch 10 / 30) train acc: 0.422000; val_acc: 0.409000\n",
      "(Iteration 7701 / 22950) loss: 2.044436\n",
      "(Iteration 7801 / 22950) loss: 2.018547\n",
      "(Iteration 7901 / 22950) loss: 1.961648\n",
      "(Iteration 8001 / 22950) loss: 1.934942\n",
      "(Iteration 8101 / 22950) loss: 2.041754\n",
      "(Iteration 8201 / 22950) loss: 1.963216\n",
      "(Iteration 8301 / 22950) loss: 1.981526\n",
      "(Iteration 8401 / 22950) loss: 1.879845\n",
      "(Epoch 11 / 30) train acc: 0.418000; val_acc: 0.398000\n",
      "(Iteration 8501 / 22950) loss: 2.012105\n",
      "(Iteration 8601 / 22950) loss: 2.019349\n",
      "(Iteration 8701 / 22950) loss: 1.989604\n",
      "(Iteration 8801 / 22950) loss: 2.037589\n",
      "(Iteration 8901 / 22950) loss: 2.018423\n",
      "(Iteration 9001 / 22950) loss: 1.895886\n",
      "(Iteration 9101 / 22950) loss: 1.991275\n",
      "(Epoch 12 / 30) train acc: 0.428000; val_acc: 0.408000\n",
      "(Iteration 9201 / 22950) loss: 2.032731\n",
      "(Iteration 9301 / 22950) loss: 1.945438\n",
      "(Iteration 9401 / 22950) loss: 2.147858\n",
      "(Iteration 9501 / 22950) loss: 1.956231\n",
      "(Iteration 9601 / 22950) loss: 2.163315\n",
      "(Iteration 9701 / 22950) loss: 1.994156\n",
      "(Iteration 9801 / 22950) loss: 2.013083\n",
      "(Iteration 9901 / 22950) loss: 1.967254\n",
      "(Epoch 13 / 30) train acc: 0.408000; val_acc: 0.412000\n",
      "(Iteration 10001 / 22950) loss: 2.035492\n",
      "(Iteration 10101 / 22950) loss: 1.964953\n",
      "(Iteration 10201 / 22950) loss: 1.867757\n",
      "(Iteration 10301 / 22950) loss: 2.142674\n",
      "(Iteration 10401 / 22950) loss: 2.006565\n",
      "(Iteration 10501 / 22950) loss: 1.975676\n",
      "(Iteration 10601 / 22950) loss: 1.884875\n",
      "(Iteration 10701 / 22950) loss: 1.916891\n",
      "(Epoch 14 / 30) train acc: 0.423000; val_acc: 0.412000\n",
      "(Iteration 10801 / 22950) loss: 1.963873\n",
      "(Iteration 10901 / 22950) loss: 1.965817\n",
      "(Iteration 11001 / 22950) loss: 2.031693\n",
      "(Iteration 11101 / 22950) loss: 2.115554\n",
      "(Iteration 11201 / 22950) loss: 1.970260\n",
      "(Iteration 11301 / 22950) loss: 1.963872\n",
      "(Iteration 11401 / 22950) loss: 2.044503\n",
      "(Epoch 15 / 30) train acc: 0.432000; val_acc: 0.408000\n",
      "(Iteration 11501 / 22950) loss: 2.084239\n",
      "(Iteration 11601 / 22950) loss: 2.058142\n",
      "(Iteration 11701 / 22950) loss: 2.023449\n",
      "(Iteration 11801 / 22950) loss: 2.079336\n",
      "(Iteration 11901 / 22950) loss: 1.969429\n",
      "(Iteration 12001 / 22950) loss: 2.049729\n",
      "(Iteration 12101 / 22950) loss: 2.007611\n",
      "(Iteration 12201 / 22950) loss: 2.041397\n",
      "(Epoch 16 / 30) train acc: 0.419000; val_acc: 0.407000\n",
      "(Iteration 12301 / 22950) loss: 1.911005\n",
      "(Iteration 12401 / 22950) loss: 1.999061\n",
      "(Iteration 12501 / 22950) loss: 1.966660\n",
      "(Iteration 12601 / 22950) loss: 2.093062\n",
      "(Iteration 12701 / 22950) loss: 1.944892\n",
      "(Iteration 12801 / 22950) loss: 1.996241\n",
      "(Iteration 12901 / 22950) loss: 1.983188\n",
      "(Iteration 13001 / 22950) loss: 1.927371\n",
      "(Epoch 17 / 30) train acc: 0.392000; val_acc: 0.415000\n",
      "(Iteration 13101 / 22950) loss: 1.994474\n",
      "(Iteration 13201 / 22950) loss: 1.874862\n",
      "(Iteration 13301 / 22950) loss: 1.956250\n",
      "(Iteration 13401 / 22950) loss: 2.029061\n",
      "(Iteration 13501 / 22950) loss: 1.940177\n",
      "(Iteration 13601 / 22950) loss: 1.989166\n",
      "(Iteration 13701 / 22950) loss: 1.899955\n",
      "(Epoch 18 / 30) train acc: 0.416000; val_acc: 0.416000\n",
      "(Iteration 13801 / 22950) loss: 1.923965\n",
      "(Iteration 13901 / 22950) loss: 1.939882\n",
      "(Iteration 14001 / 22950) loss: 1.840896\n",
      "(Iteration 14101 / 22950) loss: 2.018601\n",
      "(Iteration 14201 / 22950) loss: 1.925190\n",
      "(Iteration 14301 / 22950) loss: 2.091241\n",
      "(Iteration 14401 / 22950) loss: 1.995715\n",
      "(Iteration 14501 / 22950) loss: 1.962905\n",
      "(Epoch 19 / 30) train acc: 0.403000; val_acc: 0.417000\n",
      "(Iteration 14601 / 22950) loss: 2.012380\n",
      "(Iteration 14701 / 22950) loss: 1.946932\n",
      "(Iteration 14801 / 22950) loss: 2.015305\n",
      "(Iteration 14901 / 22950) loss: 1.928805\n",
      "(Iteration 15001 / 22950) loss: 1.979656\n",
      "(Iteration 15101 / 22950) loss: 1.880897\n",
      "(Iteration 15201 / 22950) loss: 1.966101\n",
      "(Epoch 20 / 30) train acc: 0.420000; val_acc: 0.412000\n",
      "(Iteration 15301 / 22950) loss: 1.864943\n",
      "(Iteration 15401 / 22950) loss: 1.916586\n",
      "(Iteration 15501 / 22950) loss: 2.037488\n",
      "(Iteration 15601 / 22950) loss: 2.026507\n",
      "(Iteration 15701 / 22950) loss: 1.858703\n",
      "(Iteration 15801 / 22950) loss: 1.928802\n",
      "(Iteration 15901 / 22950) loss: 2.047451\n",
      "(Iteration 16001 / 22950) loss: 1.997318\n",
      "(Epoch 21 / 30) train acc: 0.424000; val_acc: 0.414000\n",
      "(Iteration 16101 / 22950) loss: 1.953598\n",
      "(Iteration 16201 / 22950) loss: 1.956879\n",
      "(Iteration 16301 / 22950) loss: 2.004803\n",
      "(Iteration 16401 / 22950) loss: 2.032635\n",
      "(Iteration 16501 / 22950) loss: 1.879581\n",
      "(Iteration 16601 / 22950) loss: 1.882720\n",
      "(Iteration 16701 / 22950) loss: 2.024967\n",
      "(Iteration 16801 / 22950) loss: 1.881498\n",
      "(Epoch 22 / 30) train acc: 0.445000; val_acc: 0.418000\n",
      "(Iteration 16901 / 22950) loss: 1.891113\n",
      "(Iteration 17001 / 22950) loss: 1.999040\n",
      "(Iteration 17101 / 22950) loss: 1.986318\n",
      "(Iteration 17201 / 22950) loss: 1.917330\n",
      "(Iteration 17301 / 22950) loss: 2.099240\n",
      "(Iteration 17401 / 22950) loss: 1.965957\n",
      "(Iteration 17501 / 22950) loss: 2.077563\n",
      "(Epoch 23 / 30) train acc: 0.442000; val_acc: 0.417000\n",
      "(Iteration 17601 / 22950) loss: 1.959951\n",
      "(Iteration 17701 / 22950) loss: 1.959196\n",
      "(Iteration 17801 / 22950) loss: 1.988206\n",
      "(Iteration 17901 / 22950) loss: 1.996430\n",
      "(Iteration 18001 / 22950) loss: 1.930809\n",
      "(Iteration 18101 / 22950) loss: 1.945939\n",
      "(Iteration 18201 / 22950) loss: 1.928761\n",
      "(Iteration 18301 / 22950) loss: 2.025985\n",
      "(Epoch 24 / 30) train acc: 0.423000; val_acc: 0.418000\n",
      "(Iteration 18401 / 22950) loss: 1.956204\n",
      "(Iteration 18501 / 22950) loss: 1.903689\n",
      "(Iteration 18601 / 22950) loss: 2.046666\n",
      "(Iteration 18701 / 22950) loss: 1.951107\n",
      "(Iteration 18801 / 22950) loss: 1.883206\n",
      "(Iteration 18901 / 22950) loss: 1.964700\n",
      "(Iteration 19001 / 22950) loss: 2.037127\n",
      "(Iteration 19101 / 22950) loss: 1.917902\n",
      "(Epoch 25 / 30) train acc: 0.394000; val_acc: 0.420000\n",
      "(Iteration 19201 / 22950) loss: 1.928421\n",
      "(Iteration 19301 / 22950) loss: 2.081743\n",
      "(Iteration 19401 / 22950) loss: 1.907846\n",
      "(Iteration 19501 / 22950) loss: 1.964796\n",
      "(Iteration 19601 / 22950) loss: 2.028448\n",
      "(Iteration 19701 / 22950) loss: 1.897500\n",
      "(Iteration 19801 / 22950) loss: 2.087114\n",
      "(Epoch 26 / 30) train acc: 0.423000; val_acc: 0.423000\n",
      "(Iteration 19901 / 22950) loss: 2.070169\n",
      "(Iteration 20001 / 22950) loss: 1.910227\n",
      "(Iteration 20101 / 22950) loss: 1.992952\n",
      "(Iteration 20201 / 22950) loss: 1.975725\n",
      "(Iteration 20301 / 22950) loss: 1.961613\n",
      "(Iteration 20401 / 22950) loss: 1.946612\n",
      "(Iteration 20501 / 22950) loss: 1.999653\n",
      "(Iteration 20601 / 22950) loss: 1.958543\n",
      "(Epoch 27 / 30) train acc: 0.425000; val_acc: 0.429000\n",
      "(Iteration 20701 / 22950) loss: 1.869623\n",
      "(Iteration 20801 / 22950) loss: 1.909360\n",
      "(Iteration 20901 / 22950) loss: 2.001203\n",
      "(Iteration 21001 / 22950) loss: 2.002170\n",
      "(Iteration 21101 / 22950) loss: 2.000005\n",
      "(Iteration 21201 / 22950) loss: 1.951859\n",
      "(Iteration 21301 / 22950) loss: 2.035196\n",
      "(Iteration 21401 / 22950) loss: 1.946324\n",
      "(Epoch 28 / 30) train acc: 0.424000; val_acc: 0.423000\n",
      "(Iteration 21501 / 22950) loss: 1.862630\n",
      "(Iteration 21601 / 22950) loss: 1.847177\n",
      "(Iteration 21701 / 22950) loss: 2.061535\n",
      "(Iteration 21801 / 22950) loss: 2.019937\n",
      "(Iteration 21901 / 22950) loss: 1.997804\n",
      "(Iteration 22001 / 22950) loss: 2.062082\n",
      "(Iteration 22101 / 22950) loss: 2.051060\n",
      "(Epoch 29 / 30) train acc: 0.442000; val_acc: 0.427000\n",
      "(Iteration 22201 / 22950) loss: 2.187097\n",
      "(Iteration 22301 / 22950) loss: 2.053407\n",
      "(Iteration 22401 / 22950) loss: 2.046521\n",
      "(Iteration 22501 / 22950) loss: 1.880722\n",
      "(Iteration 22601 / 22950) loss: 2.172849\n",
      "(Iteration 22701 / 22950) loss: 1.978675\n",
      "(Iteration 22801 / 22950) loss: 1.949089\n",
      "(Iteration 22901 / 22950) loss: 2.024612\n",
      "(Epoch 30 / 30) train acc: 0.407000; val_acc: 0.421000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.01, 'num_epochs': 30, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 45930) loss: 2.308337\n",
      "(Epoch 0 / 30) train acc: 0.088000; val_acc: 0.116000\n",
      "(Iteration 101 / 45930) loss: 2.306798\n",
      "(Iteration 201 / 45930) loss: 2.307621\n",
      "(Iteration 301 / 45930) loss: 2.304695\n",
      "(Iteration 401 / 45930) loss: 2.303762\n",
      "(Iteration 501 / 45930) loss: 2.302113\n",
      "(Iteration 601 / 45930) loss: 2.303670\n",
      "(Iteration 701 / 45930) loss: 2.297400\n",
      "(Iteration 801 / 45930) loss: 2.295580\n",
      "(Iteration 901 / 45930) loss: 2.295423\n",
      "(Iteration 1001 / 45930) loss: 2.292082\n",
      "(Iteration 1101 / 45930) loss: 2.291046\n",
      "(Iteration 1201 / 45930) loss: 2.281858\n",
      "(Iteration 1301 / 45930) loss: 2.294948\n",
      "(Iteration 1401 / 45930) loss: 2.269932\n",
      "(Iteration 1501 / 45930) loss: 2.212979\n",
      "(Epoch 1 / 30) train acc: 0.215000; val_acc: 0.241000\n",
      "(Iteration 1601 / 45930) loss: 2.177255\n",
      "(Iteration 1701 / 45930) loss: 2.192770\n",
      "(Iteration 1801 / 45930) loss: 2.169066\n",
      "(Iteration 1901 / 45930) loss: 2.224614\n",
      "(Iteration 2001 / 45930) loss: 2.238363\n",
      "(Iteration 2101 / 45930) loss: 2.103238\n",
      "(Iteration 2201 / 45930) loss: 1.964406\n",
      "(Iteration 2301 / 45930) loss: 2.138289\n",
      "(Iteration 2401 / 45930) loss: 2.228648\n",
      "(Iteration 2501 / 45930) loss: 2.199063\n",
      "(Iteration 2601 / 45930) loss: 2.055254\n",
      "(Iteration 2701 / 45930) loss: 2.202689\n",
      "(Iteration 2801 / 45930) loss: 2.220094\n",
      "(Iteration 2901 / 45930) loss: 2.098348\n",
      "(Iteration 3001 / 45930) loss: 2.120660\n",
      "(Epoch 2 / 30) train acc: 0.292000; val_acc: 0.302000\n",
      "(Iteration 3101 / 45930) loss: 2.113261\n",
      "(Iteration 3201 / 45930) loss: 2.066576\n",
      "(Iteration 3301 / 45930) loss: 2.003800\n",
      "(Iteration 3401 / 45930) loss: 2.126033\n",
      "(Iteration 3501 / 45930) loss: 2.063463\n",
      "(Iteration 3601 / 45930) loss: 2.030605\n",
      "(Iteration 3701 / 45930) loss: 1.947386\n",
      "(Iteration 3801 / 45930) loss: 2.225218\n",
      "(Iteration 3901 / 45930) loss: 2.102914\n",
      "(Iteration 4001 / 45930) loss: 2.099142\n",
      "(Iteration 4101 / 45930) loss: 2.103119\n",
      "(Iteration 4201 / 45930) loss: 2.068547\n",
      "(Iteration 4301 / 45930) loss: 2.178792\n",
      "(Iteration 4401 / 45930) loss: 1.982382\n",
      "(Iteration 4501 / 45930) loss: 2.027702\n",
      "(Epoch 3 / 30) train acc: 0.365000; val_acc: 0.381000\n",
      "(Iteration 4601 / 45930) loss: 2.104144\n",
      "(Iteration 4701 / 45930) loss: 2.036582\n",
      "(Iteration 4801 / 45930) loss: 2.059295\n",
      "(Iteration 4901 / 45930) loss: 2.222719\n",
      "(Iteration 5001 / 45930) loss: 2.050449\n",
      "(Iteration 5101 / 45930) loss: 2.058456\n",
      "(Iteration 5201 / 45930) loss: 2.102381\n",
      "(Iteration 5301 / 45930) loss: 2.017736\n",
      "(Iteration 5401 / 45930) loss: 2.151327\n",
      "(Iteration 5501 / 45930) loss: 1.922308\n",
      "(Iteration 5601 / 45930) loss: 1.973211\n",
      "(Iteration 5701 / 45930) loss: 1.955256\n",
      "(Iteration 5801 / 45930) loss: 1.950003\n",
      "(Iteration 5901 / 45930) loss: 1.875113\n",
      "(Iteration 6001 / 45930) loss: 2.138582\n",
      "(Iteration 6101 / 45930) loss: 1.880918\n",
      "(Epoch 4 / 30) train acc: 0.371000; val_acc: 0.400000\n",
      "(Iteration 6201 / 45930) loss: 2.135052\n",
      "(Iteration 6301 / 45930) loss: 2.096998\n",
      "(Iteration 6401 / 45930) loss: 1.825207\n",
      "(Iteration 6501 / 45930) loss: 2.101036\n",
      "(Iteration 6601 / 45930) loss: 1.831353\n",
      "(Iteration 6701 / 45930) loss: 1.981108\n",
      "(Iteration 6801 / 45930) loss: 1.931702\n",
      "(Iteration 6901 / 45930) loss: 1.976281\n",
      "(Iteration 7001 / 45930) loss: 1.940730\n",
      "(Iteration 7101 / 45930) loss: 2.058155\n",
      "(Iteration 7201 / 45930) loss: 1.926712\n",
      "(Iteration 7301 / 45930) loss: 1.945844\n",
      "(Iteration 7401 / 45930) loss: 1.991155\n",
      "(Iteration 7501 / 45930) loss: 1.964854\n",
      "(Iteration 7601 / 45930) loss: 1.993368\n",
      "(Epoch 5 / 30) train acc: 0.421000; val_acc: 0.425000\n",
      "(Iteration 7701 / 45930) loss: 2.077009\n",
      "(Iteration 7801 / 45930) loss: 1.842538\n",
      "(Iteration 7901 / 45930) loss: 1.870524\n",
      "(Iteration 8001 / 45930) loss: 1.921343\n",
      "(Iteration 8101 / 45930) loss: 1.946089\n",
      "(Iteration 8201 / 45930) loss: 1.843344\n",
      "(Iteration 8301 / 45930) loss: 1.973270\n",
      "(Iteration 8401 / 45930) loss: 1.997611\n",
      "(Iteration 8501 / 45930) loss: 2.018296\n",
      "(Iteration 8601 / 45930) loss: 1.828657\n",
      "(Iteration 8701 / 45930) loss: 2.107609\n",
      "(Iteration 8801 / 45930) loss: 1.837120\n",
      "(Iteration 8901 / 45930) loss: 1.982643\n",
      "(Iteration 9001 / 45930) loss: 2.068044\n",
      "(Iteration 9101 / 45930) loss: 2.044252\n",
      "(Epoch 6 / 30) train acc: 0.446000; val_acc: 0.409000\n",
      "(Iteration 9201 / 45930) loss: 1.800422\n",
      "(Iteration 9301 / 45930) loss: 1.944344\n",
      "(Iteration 9401 / 45930) loss: 2.122393\n",
      "(Iteration 9501 / 45930) loss: 1.913252\n",
      "(Iteration 9601 / 45930) loss: 1.961332\n",
      "(Iteration 9701 / 45930) loss: 2.043201\n",
      "(Iteration 9801 / 45930) loss: 2.039015\n",
      "(Iteration 9901 / 45930) loss: 1.842628\n",
      "(Iteration 10001 / 45930) loss: 1.896818\n",
      "(Iteration 10101 / 45930) loss: 1.928192\n",
      "(Iteration 10201 / 45930) loss: 2.056732\n",
      "(Iteration 10301 / 45930) loss: 2.050094\n",
      "(Iteration 10401 / 45930) loss: 1.849754\n",
      "(Iteration 10501 / 45930) loss: 1.837591\n",
      "(Iteration 10601 / 45930) loss: 1.946933\n",
      "(Iteration 10701 / 45930) loss: 2.093528\n",
      "(Epoch 7 / 30) train acc: 0.425000; val_acc: 0.430000\n",
      "(Iteration 10801 / 45930) loss: 1.935711\n",
      "(Iteration 10901 / 45930) loss: 1.899922\n",
      "(Iteration 11001 / 45930) loss: 2.074732\n",
      "(Iteration 11101 / 45930) loss: 2.094569\n",
      "(Iteration 11201 / 45930) loss: 2.009732\n",
      "(Iteration 11301 / 45930) loss: 1.990867\n",
      "(Iteration 11401 / 45930) loss: 1.871306\n",
      "(Iteration 11501 / 45930) loss: 1.923010\n",
      "(Iteration 11601 / 45930) loss: 1.968377\n",
      "(Iteration 11701 / 45930) loss: 1.900883\n",
      "(Iteration 11801 / 45930) loss: 1.985376\n",
      "(Iteration 11901 / 45930) loss: 2.122364\n",
      "(Iteration 12001 / 45930) loss: 1.958385\n",
      "(Iteration 12101 / 45930) loss: 1.998616\n",
      "(Iteration 12201 / 45930) loss: 1.997761\n",
      "(Epoch 8 / 30) train acc: 0.456000; val_acc: 0.428000\n",
      "(Iteration 12301 / 45930) loss: 1.842880\n",
      "(Iteration 12401 / 45930) loss: 1.998219\n",
      "(Iteration 12501 / 45930) loss: 2.108177\n",
      "(Iteration 12601 / 45930) loss: 2.095582\n",
      "(Iteration 12701 / 45930) loss: 2.272507\n",
      "(Iteration 12801 / 45930) loss: 1.820418\n",
      "(Iteration 12901 / 45930) loss: 1.914086\n",
      "(Iteration 13001 / 45930) loss: 1.963249\n",
      "(Iteration 13101 / 45930) loss: 1.997398\n",
      "(Iteration 13201 / 45930) loss: 2.080839\n",
      "(Iteration 13301 / 45930) loss: 2.016770\n",
      "(Iteration 13401 / 45930) loss: 2.215790\n",
      "(Iteration 13501 / 45930) loss: 1.905153\n",
      "(Iteration 13601 / 45930) loss: 2.128816\n",
      "(Iteration 13701 / 45930) loss: 1.850487\n",
      "(Epoch 9 / 30) train acc: 0.439000; val_acc: 0.429000\n",
      "(Iteration 13801 / 45930) loss: 2.127413\n",
      "(Iteration 13901 / 45930) loss: 1.860143\n",
      "(Iteration 14001 / 45930) loss: 1.945499\n",
      "(Iteration 14101 / 45930) loss: 1.858715\n",
      "(Iteration 14201 / 45930) loss: 1.961485\n",
      "(Iteration 14301 / 45930) loss: 1.900428\n",
      "(Iteration 14401 / 45930) loss: 2.086361\n",
      "(Iteration 14501 / 45930) loss: 1.881173\n",
      "(Iteration 14601 / 45930) loss: 1.926487\n",
      "(Iteration 14701 / 45930) loss: 2.011379\n",
      "(Iteration 14801 / 45930) loss: 2.074614\n",
      "(Iteration 14901 / 45930) loss: 1.926060\n",
      "(Iteration 15001 / 45930) loss: 1.812600\n",
      "(Iteration 15101 / 45930) loss: 1.893289\n",
      "(Iteration 15201 / 45930) loss: 1.915225\n",
      "(Iteration 15301 / 45930) loss: 1.863507\n",
      "(Epoch 10 / 30) train acc: 0.479000; val_acc: 0.429000\n",
      "(Iteration 15401 / 45930) loss: 1.950664\n",
      "(Iteration 15501 / 45930) loss: 1.894147\n",
      "(Iteration 15601 / 45930) loss: 2.131288\n",
      "(Iteration 15701 / 45930) loss: 1.859202\n",
      "(Iteration 15801 / 45930) loss: 1.955389\n",
      "(Iteration 15901 / 45930) loss: 1.832742\n",
      "(Iteration 16001 / 45930) loss: 1.924231\n",
      "(Iteration 16101 / 45930) loss: 1.995837\n",
      "(Iteration 16201 / 45930) loss: 1.772171\n",
      "(Iteration 16301 / 45930) loss: 1.985237\n",
      "(Iteration 16401 / 45930) loss: 2.049285\n",
      "(Iteration 16501 / 45930) loss: 1.874883\n",
      "(Iteration 16601 / 45930) loss: 2.067493\n",
      "(Iteration 16701 / 45930) loss: 2.055816\n",
      "(Iteration 16801 / 45930) loss: 1.858073\n",
      "(Epoch 11 / 30) train acc: 0.405000; val_acc: 0.420000\n",
      "(Iteration 16901 / 45930) loss: 2.064589\n",
      "(Iteration 17001 / 45930) loss: 1.978213\n",
      "(Iteration 17101 / 45930) loss: 1.891018\n",
      "(Iteration 17201 / 45930) loss: 1.955921\n",
      "(Iteration 17301 / 45930) loss: 1.847175\n",
      "(Iteration 17401 / 45930) loss: 1.855122\n",
      "(Iteration 17501 / 45930) loss: 2.007407\n",
      "(Iteration 17601 / 45930) loss: 2.166656\n",
      "(Iteration 17701 / 45930) loss: 2.155200\n",
      "(Iteration 17801 / 45930) loss: 2.163805\n",
      "(Iteration 17901 / 45930) loss: 1.916231\n",
      "(Iteration 18001 / 45930) loss: 2.076602\n",
      "(Iteration 18101 / 45930) loss: 1.931254\n",
      "(Iteration 18201 / 45930) loss: 1.793474\n",
      "(Iteration 18301 / 45930) loss: 2.082853\n",
      "(Epoch 12 / 30) train acc: 0.435000; val_acc: 0.439000\n",
      "(Iteration 18401 / 45930) loss: 2.056305\n",
      "(Iteration 18501 / 45930) loss: 1.914322\n",
      "(Iteration 18601 / 45930) loss: 2.033977\n",
      "(Iteration 18701 / 45930) loss: 2.088224\n",
      "(Iteration 18801 / 45930) loss: 2.012984\n",
      "(Iteration 18901 / 45930) loss: 1.964699\n",
      "(Iteration 19001 / 45930) loss: 1.849524\n",
      "(Iteration 19101 / 45930) loss: 1.986295\n",
      "(Iteration 19201 / 45930) loss: 1.735258\n",
      "(Iteration 19301 / 45930) loss: 1.961602\n",
      "(Iteration 19401 / 45930) loss: 2.035914\n",
      "(Iteration 19501 / 45930) loss: 1.893365\n",
      "(Iteration 19601 / 45930) loss: 2.001826\n",
      "(Iteration 19701 / 45930) loss: 2.070155\n",
      "(Iteration 19801 / 45930) loss: 1.942664\n",
      "(Iteration 19901 / 45930) loss: 1.996130\n",
      "(Epoch 13 / 30) train acc: 0.458000; val_acc: 0.435000\n",
      "(Iteration 20001 / 45930) loss: 1.941339\n",
      "(Iteration 20101 / 45930) loss: 1.933680\n",
      "(Iteration 20201 / 45930) loss: 1.903915\n",
      "(Iteration 20301 / 45930) loss: 2.016838\n",
      "(Iteration 20401 / 45930) loss: 1.872486\n",
      "(Iteration 20501 / 45930) loss: 1.911981\n",
      "(Iteration 20601 / 45930) loss: 2.058743\n",
      "(Iteration 20701 / 45930) loss: 2.118419\n",
      "(Iteration 20801 / 45930) loss: 2.055934\n",
      "(Iteration 20901 / 45930) loss: 1.798415\n",
      "(Iteration 21001 / 45930) loss: 1.893359\n",
      "(Iteration 21101 / 45930) loss: 1.911025\n",
      "(Iteration 21201 / 45930) loss: 1.955221\n",
      "(Iteration 21301 / 45930) loss: 1.859838\n",
      "(Iteration 21401 / 45930) loss: 1.962491\n",
      "(Epoch 14 / 30) train acc: 0.439000; val_acc: 0.432000\n",
      "(Iteration 21501 / 45930) loss: 2.049953\n",
      "(Iteration 21601 / 45930) loss: 1.713614\n",
      "(Iteration 21701 / 45930) loss: 2.028793\n",
      "(Iteration 21801 / 45930) loss: 1.846196\n",
      "(Iteration 21901 / 45930) loss: 2.085786\n",
      "(Iteration 22001 / 45930) loss: 1.993022\n",
      "(Iteration 22101 / 45930) loss: 2.052196\n",
      "(Iteration 22201 / 45930) loss: 1.943461\n",
      "(Iteration 22301 / 45930) loss: 1.844923\n",
      "(Iteration 22401 / 45930) loss: 1.824126\n",
      "(Iteration 22501 / 45930) loss: 2.187521\n",
      "(Iteration 22601 / 45930) loss: 2.000302\n",
      "(Iteration 22701 / 45930) loss: 2.145842\n",
      "(Iteration 22801 / 45930) loss: 1.818977\n",
      "(Iteration 22901 / 45930) loss: 1.775250\n",
      "(Epoch 15 / 30) train acc: 0.455000; val_acc: 0.432000\n",
      "(Iteration 23001 / 45930) loss: 2.155830\n",
      "(Iteration 23101 / 45930) loss: 1.991010\n",
      "(Iteration 23201 / 45930) loss: 2.016397\n",
      "(Iteration 23301 / 45930) loss: 1.826118\n",
      "(Iteration 23401 / 45930) loss: 1.974199\n",
      "(Iteration 23501 / 45930) loss: 1.870562\n",
      "(Iteration 23601 / 45930) loss: 1.882230\n",
      "(Iteration 23701 / 45930) loss: 2.120795\n",
      "(Iteration 23801 / 45930) loss: 2.017098\n",
      "(Iteration 23901 / 45930) loss: 1.958776\n",
      "(Iteration 24001 / 45930) loss: 1.973633\n",
      "(Iteration 24101 / 45930) loss: 1.967891\n",
      "(Iteration 24201 / 45930) loss: 1.984341\n",
      "(Iteration 24301 / 45930) loss: 1.977456\n",
      "(Iteration 24401 / 45930) loss: 1.962774\n",
      "(Epoch 16 / 30) train acc: 0.466000; val_acc: 0.433000\n",
      "(Iteration 24501 / 45930) loss: 1.995769\n",
      "(Iteration 24601 / 45930) loss: 1.926243\n",
      "(Iteration 24701 / 45930) loss: 2.051047\n",
      "(Iteration 24801 / 45930) loss: 1.862126\n",
      "(Iteration 24901 / 45930) loss: 1.970255\n",
      "(Iteration 25001 / 45930) loss: 2.043448\n",
      "(Iteration 25101 / 45930) loss: 1.968156\n",
      "(Iteration 25201 / 45930) loss: 1.861442\n",
      "(Iteration 25301 / 45930) loss: 1.971271\n",
      "(Iteration 25401 / 45930) loss: 1.977946\n",
      "(Iteration 25501 / 45930) loss: 1.820856\n",
      "(Iteration 25601 / 45930) loss: 2.054379\n",
      "(Iteration 25701 / 45930) loss: 1.838434\n",
      "(Iteration 25801 / 45930) loss: 1.948986\n",
      "(Iteration 25901 / 45930) loss: 1.961676\n",
      "(Iteration 26001 / 45930) loss: 1.903919\n",
      "(Epoch 17 / 30) train acc: 0.439000; val_acc: 0.440000\n",
      "(Iteration 26101 / 45930) loss: 1.933234\n",
      "(Iteration 26201 / 45930) loss: 1.926623\n",
      "(Iteration 26301 / 45930) loss: 1.951464\n",
      "(Iteration 26401 / 45930) loss: 2.122721\n",
      "(Iteration 26501 / 45930) loss: 2.120561\n",
      "(Iteration 26601 / 45930) loss: 2.021452\n",
      "(Iteration 26701 / 45930) loss: 1.947031\n",
      "(Iteration 26801 / 45930) loss: 2.009639\n",
      "(Iteration 26901 / 45930) loss: 1.844327\n",
      "(Iteration 27001 / 45930) loss: 2.072357\n",
      "(Iteration 27101 / 45930) loss: 1.974304\n",
      "(Iteration 27201 / 45930) loss: 2.018882\n",
      "(Iteration 27301 / 45930) loss: 1.939812\n",
      "(Iteration 27401 / 45930) loss: 1.866931\n",
      "(Iteration 27501 / 45930) loss: 1.980078\n",
      "(Epoch 18 / 30) train acc: 0.465000; val_acc: 0.434000\n",
      "(Iteration 27601 / 45930) loss: 2.006037\n",
      "(Iteration 27701 / 45930) loss: 1.770123\n",
      "(Iteration 27801 / 45930) loss: 1.924859\n",
      "(Iteration 27901 / 45930) loss: 1.978034\n",
      "(Iteration 28001 / 45930) loss: 1.913914\n",
      "(Iteration 28101 / 45930) loss: 2.000470\n",
      "(Iteration 28201 / 45930) loss: 1.842931\n",
      "(Iteration 28301 / 45930) loss: 2.024566\n",
      "(Iteration 28401 / 45930) loss: 1.892773\n",
      "(Iteration 28501 / 45930) loss: 1.934492\n",
      "(Iteration 28601 / 45930) loss: 1.915129\n",
      "(Iteration 28701 / 45930) loss: 1.716179\n",
      "(Iteration 28801 / 45930) loss: 1.904523\n",
      "(Iteration 28901 / 45930) loss: 2.079862\n",
      "(Iteration 29001 / 45930) loss: 1.938139\n",
      "(Epoch 19 / 30) train acc: 0.436000; val_acc: 0.435000\n",
      "(Iteration 29101 / 45930) loss: 1.960019\n",
      "(Iteration 29201 / 45930) loss: 1.913611\n",
      "(Iteration 29301 / 45930) loss: 2.062321\n",
      "(Iteration 29401 / 45930) loss: 2.061382\n",
      "(Iteration 29501 / 45930) loss: 2.028015\n",
      "(Iteration 29601 / 45930) loss: 2.031638\n",
      "(Iteration 29701 / 45930) loss: 1.806572\n",
      "(Iteration 29801 / 45930) loss: 1.862915\n",
      "(Iteration 29901 / 45930) loss: 1.935970\n",
      "(Iteration 30001 / 45930) loss: 1.851601\n",
      "(Iteration 30101 / 45930) loss: 1.975479\n",
      "(Iteration 30201 / 45930) loss: 1.909730\n",
      "(Iteration 30301 / 45930) loss: 1.898598\n",
      "(Iteration 30401 / 45930) loss: 1.946696\n",
      "(Iteration 30501 / 45930) loss: 1.810043\n",
      "(Iteration 30601 / 45930) loss: 2.016157\n",
      "(Epoch 20 / 30) train acc: 0.464000; val_acc: 0.435000\n",
      "(Iteration 30701 / 45930) loss: 2.011112\n",
      "(Iteration 30801 / 45930) loss: 1.865807\n",
      "(Iteration 30901 / 45930) loss: 1.886185\n",
      "(Iteration 31001 / 45930) loss: 2.035376\n",
      "(Iteration 31101 / 45930) loss: 1.835900\n",
      "(Iteration 31201 / 45930) loss: 1.970552\n",
      "(Iteration 31301 / 45930) loss: 1.867286\n",
      "(Iteration 31401 / 45930) loss: 1.879582\n",
      "(Iteration 31501 / 45930) loss: 1.826265\n",
      "(Iteration 31601 / 45930) loss: 2.052613\n",
      "(Iteration 31701 / 45930) loss: 1.891715\n",
      "(Iteration 31801 / 45930) loss: 1.938805\n",
      "(Iteration 31901 / 45930) loss: 1.761384\n",
      "(Iteration 32001 / 45930) loss: 1.991366\n",
      "(Iteration 32101 / 45930) loss: 2.012149\n",
      "(Epoch 21 / 30) train acc: 0.432000; val_acc: 0.434000\n",
      "(Iteration 32201 / 45930) loss: 2.060197\n",
      "(Iteration 32301 / 45930) loss: 1.832625\n",
      "(Iteration 32401 / 45930) loss: 1.953071\n",
      "(Iteration 32501 / 45930) loss: 1.942893\n",
      "(Iteration 32601 / 45930) loss: 1.798897\n",
      "(Iteration 32701 / 45930) loss: 1.975320\n",
      "(Iteration 32801 / 45930) loss: 2.039000\n",
      "(Iteration 32901 / 45930) loss: 2.151025\n",
      "(Iteration 33001 / 45930) loss: 1.831417\n",
      "(Iteration 33101 / 45930) loss: 2.092201\n",
      "(Iteration 33201 / 45930) loss: 1.892491\n",
      "(Iteration 33301 / 45930) loss: 1.985205\n",
      "(Iteration 33401 / 45930) loss: 2.116281\n",
      "(Iteration 33501 / 45930) loss: 2.132395\n",
      "(Iteration 33601 / 45930) loss: 1.883533\n",
      "(Epoch 22 / 30) train acc: 0.465000; val_acc: 0.436000\n",
      "(Iteration 33701 / 45930) loss: 2.188286\n",
      "(Iteration 33801 / 45930) loss: 1.713699\n",
      "(Iteration 33901 / 45930) loss: 1.951819\n",
      "(Iteration 34001 / 45930) loss: 1.966277\n",
      "(Iteration 34101 / 45930) loss: 1.841148\n",
      "(Iteration 34201 / 45930) loss: 1.808875\n",
      "(Iteration 34301 / 45930) loss: 2.012511\n",
      "(Iteration 34401 / 45930) loss: 2.166169\n",
      "(Iteration 34501 / 45930) loss: 1.803344\n",
      "(Iteration 34601 / 45930) loss: 1.926397\n",
      "(Iteration 34701 / 45930) loss: 1.948049\n",
      "(Iteration 34801 / 45930) loss: 1.859836\n",
      "(Iteration 34901 / 45930) loss: 2.009742\n",
      "(Iteration 35001 / 45930) loss: 1.809649\n",
      "(Iteration 35101 / 45930) loss: 1.872701\n",
      "(Iteration 35201 / 45930) loss: 2.055783\n",
      "(Epoch 23 / 30) train acc: 0.458000; val_acc: 0.430000\n",
      "(Iteration 35301 / 45930) loss: 1.969642\n",
      "(Iteration 35401 / 45930) loss: 1.952198\n",
      "(Iteration 35501 / 45930) loss: 1.871435\n",
      "(Iteration 35601 / 45930) loss: 1.964773\n",
      "(Iteration 35701 / 45930) loss: 2.025932\n",
      "(Iteration 35801 / 45930) loss: 2.245672\n",
      "(Iteration 35901 / 45930) loss: 2.088921\n",
      "(Iteration 36001 / 45930) loss: 2.099307\n",
      "(Iteration 36101 / 45930) loss: 1.885135\n",
      "(Iteration 36201 / 45930) loss: 1.910216\n",
      "(Iteration 36301 / 45930) loss: 1.921544\n",
      "(Iteration 36401 / 45930) loss: 2.014834\n",
      "(Iteration 36501 / 45930) loss: 1.923012\n",
      "(Iteration 36601 / 45930) loss: 1.916129\n",
      "(Iteration 36701 / 45930) loss: 1.999247\n",
      "(Epoch 24 / 30) train acc: 0.436000; val_acc: 0.435000\n",
      "(Iteration 36801 / 45930) loss: 1.854831\n",
      "(Iteration 36901 / 45930) loss: 2.082636\n",
      "(Iteration 37001 / 45930) loss: 1.952064\n",
      "(Iteration 37101 / 45930) loss: 2.001830\n",
      "(Iteration 37201 / 45930) loss: 1.961340\n",
      "(Iteration 37301 / 45930) loss: 1.819855\n",
      "(Iteration 37401 / 45930) loss: 1.854385\n",
      "(Iteration 37501 / 45930) loss: 2.070353\n",
      "(Iteration 37601 / 45930) loss: 2.120751\n",
      "(Iteration 37701 / 45930) loss: 1.952896\n",
      "(Iteration 37801 / 45930) loss: 1.916985\n",
      "(Iteration 37901 / 45930) loss: 1.987265\n",
      "(Iteration 38001 / 45930) loss: 1.947434\n",
      "(Iteration 38101 / 45930) loss: 1.883373\n",
      "(Iteration 38201 / 45930) loss: 2.003728\n",
      "(Epoch 25 / 30) train acc: 0.456000; val_acc: 0.438000\n",
      "(Iteration 38301 / 45930) loss: 1.700327\n",
      "(Iteration 38401 / 45930) loss: 2.010769\n",
      "(Iteration 38501 / 45930) loss: 1.952224\n",
      "(Iteration 38601 / 45930) loss: 1.947742\n",
      "(Iteration 38701 / 45930) loss: 1.996827\n",
      "(Iteration 38801 / 45930) loss: 1.842158\n",
      "(Iteration 38901 / 45930) loss: 1.889497\n",
      "(Iteration 39001 / 45930) loss: 1.991922\n",
      "(Iteration 39101 / 45930) loss: 1.901769\n",
      "(Iteration 39201 / 45930) loss: 2.001608\n",
      "(Iteration 39301 / 45930) loss: 1.863769\n",
      "(Iteration 39401 / 45930) loss: 1.904694\n",
      "(Iteration 39501 / 45930) loss: 2.040889\n",
      "(Iteration 39601 / 45930) loss: 2.027339\n",
      "(Iteration 39701 / 45930) loss: 1.883470\n",
      "(Iteration 39801 / 45930) loss: 2.070845\n",
      "(Epoch 26 / 30) train acc: 0.443000; val_acc: 0.429000\n",
      "(Iteration 39901 / 45930) loss: 2.105337\n",
      "(Iteration 40001 / 45930) loss: 2.002362\n",
      "(Iteration 40101 / 45930) loss: 2.010915\n",
      "(Iteration 40201 / 45930) loss: 1.993333\n",
      "(Iteration 40301 / 45930) loss: 1.909719\n",
      "(Iteration 40401 / 45930) loss: 2.039016\n",
      "(Iteration 40501 / 45930) loss: 1.969239\n",
      "(Iteration 40601 / 45930) loss: 2.138401\n",
      "(Iteration 40701 / 45930) loss: 1.994260\n",
      "(Iteration 40801 / 45930) loss: 2.170196\n",
      "(Iteration 40901 / 45930) loss: 1.925941\n",
      "(Iteration 41001 / 45930) loss: 2.066326\n",
      "(Iteration 41101 / 45930) loss: 1.961554\n",
      "(Iteration 41201 / 45930) loss: 1.929069\n",
      "(Iteration 41301 / 45930) loss: 2.123391\n",
      "(Epoch 27 / 30) train acc: 0.447000; val_acc: 0.438000\n",
      "(Iteration 41401 / 45930) loss: 1.884831\n",
      "(Iteration 41501 / 45930) loss: 1.945493\n",
      "(Iteration 41601 / 45930) loss: 1.933887\n",
      "(Iteration 41701 / 45930) loss: 1.855767\n",
      "(Iteration 41801 / 45930) loss: 1.984521\n",
      "(Iteration 41901 / 45930) loss: 1.967998\n",
      "(Iteration 42001 / 45930) loss: 1.995802\n",
      "(Iteration 42101 / 45930) loss: 1.827385\n",
      "(Iteration 42201 / 45930) loss: 2.021419\n",
      "(Iteration 42301 / 45930) loss: 1.976015\n",
      "(Iteration 42401 / 45930) loss: 1.893630\n",
      "(Iteration 42501 / 45930) loss: 1.967537\n",
      "(Iteration 42601 / 45930) loss: 1.993840\n",
      "(Iteration 42701 / 45930) loss: 1.788343\n",
      "(Iteration 42801 / 45930) loss: 1.846323\n",
      "(Epoch 28 / 30) train acc: 0.464000; val_acc: 0.431000\n",
      "(Iteration 42901 / 45930) loss: 1.911799\n",
      "(Iteration 43001 / 45930) loss: 1.947929\n",
      "(Iteration 43101 / 45930) loss: 1.920620\n",
      "(Iteration 43201 / 45930) loss: 1.998038\n",
      "(Iteration 43301 / 45930) loss: 1.923211\n",
      "(Iteration 43401 / 45930) loss: 1.797810\n",
      "(Iteration 43501 / 45930) loss: 2.032282\n",
      "(Iteration 43601 / 45930) loss: 1.885546\n",
      "(Iteration 43701 / 45930) loss: 1.926501\n",
      "(Iteration 43801 / 45930) loss: 1.961003\n",
      "(Iteration 43901 / 45930) loss: 1.878342\n",
      "(Iteration 44001 / 45930) loss: 1.958221\n",
      "(Iteration 44101 / 45930) loss: 1.991524\n",
      "(Iteration 44201 / 45930) loss: 1.905660\n",
      "(Iteration 44301 / 45930) loss: 1.925913\n",
      "(Epoch 29 / 30) train acc: 0.436000; val_acc: 0.425000\n",
      "(Iteration 44401 / 45930) loss: 2.141834\n",
      "(Iteration 44501 / 45930) loss: 1.970283\n",
      "(Iteration 44601 / 45930) loss: 1.932845\n",
      "(Iteration 44701 / 45930) loss: 2.068149\n",
      "(Iteration 44801 / 45930) loss: 1.948967\n",
      "(Iteration 44901 / 45930) loss: 1.809643\n",
      "(Iteration 45001 / 45930) loss: 1.851347\n",
      "(Iteration 45101 / 45930) loss: 1.697735\n",
      "(Iteration 45201 / 45930) loss: 2.097910\n",
      "(Iteration 45301 / 45930) loss: 2.040513\n",
      "(Iteration 45401 / 45930) loss: 1.968484\n",
      "(Iteration 45501 / 45930) loss: 1.897701\n",
      "(Iteration 45601 / 45930) loss: 1.944707\n",
      "(Iteration 45701 / 45930) loss: 2.038017\n",
      "(Iteration 45801 / 45930) loss: 1.882710\n",
      "(Iteration 45901 / 45930) loss: 1.975009\n",
      "(Epoch 30 / 30) train acc: 0.443000; val_acc: 0.432000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.01, 'num_epochs': 30, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 22950) loss: 2.303152\n",
      "(Epoch 0 / 30) train acc: 0.095000; val_acc: 0.116000\n",
      "(Iteration 101 / 22950) loss: 2.301848\n",
      "(Iteration 201 / 22950) loss: 2.301734\n",
      "(Iteration 301 / 22950) loss: 2.300565\n",
      "(Iteration 401 / 22950) loss: 2.299871\n",
      "(Iteration 501 / 22950) loss: 2.299618\n",
      "(Iteration 601 / 22950) loss: 2.293246\n",
      "(Iteration 701 / 22950) loss: 2.293756\n",
      "(Epoch 1 / 30) train acc: 0.180000; val_acc: 0.189000\n",
      "(Iteration 801 / 22950) loss: 2.287901\n",
      "(Iteration 901 / 22950) loss: 2.264002\n",
      "(Iteration 1001 / 22950) loss: 2.223335\n",
      "(Iteration 1101 / 22950) loss: 2.209244\n",
      "(Iteration 1201 / 22950) loss: 2.159078\n",
      "(Iteration 1301 / 22950) loss: 2.123729\n",
      "(Iteration 1401 / 22950) loss: 2.083301\n",
      "(Iteration 1501 / 22950) loss: 2.122948\n",
      "(Epoch 2 / 30) train acc: 0.285000; val_acc: 0.294000\n",
      "(Iteration 1601 / 22950) loss: 2.154236\n",
      "(Iteration 1701 / 22950) loss: 1.914770\n",
      "(Iteration 1801 / 22950) loss: 1.842972\n",
      "(Iteration 1901 / 22950) loss: 1.851732\n",
      "(Iteration 2001 / 22950) loss: 1.846266\n",
      "(Iteration 2101 / 22950) loss: 1.742294\n",
      "(Iteration 2201 / 22950) loss: 1.955326\n",
      "(Epoch 3 / 30) train acc: 0.367000; val_acc: 0.373000\n",
      "(Iteration 2301 / 22950) loss: 1.710916\n",
      "(Iteration 2401 / 22950) loss: 1.912755\n",
      "(Iteration 2501 / 22950) loss: 1.972191\n",
      "(Iteration 2601 / 22950) loss: 1.764505\n",
      "(Iteration 2701 / 22950) loss: 1.826856\n",
      "(Iteration 2801 / 22950) loss: 1.608147\n",
      "(Iteration 2901 / 22950) loss: 1.621329\n",
      "(Iteration 3001 / 22950) loss: 1.765654\n",
      "(Epoch 4 / 30) train acc: 0.404000; val_acc: 0.416000\n",
      "(Iteration 3101 / 22950) loss: 1.699672\n",
      "(Iteration 3201 / 22950) loss: 1.533690\n",
      "(Iteration 3301 / 22950) loss: 1.595671\n",
      "(Iteration 3401 / 22950) loss: 1.755199\n",
      "(Iteration 3501 / 22950) loss: 1.495069\n",
      "(Iteration 3601 / 22950) loss: 1.627974\n",
      "(Iteration 3701 / 22950) loss: 1.733600\n",
      "(Iteration 3801 / 22950) loss: 1.727790\n",
      "(Epoch 5 / 30) train acc: 0.431000; val_acc: 0.437000\n",
      "(Iteration 3901 / 22950) loss: 1.632241\n",
      "(Iteration 4001 / 22950) loss: 1.468787\n",
      "(Iteration 4101 / 22950) loss: 1.550818\n",
      "(Iteration 4201 / 22950) loss: 1.513798\n",
      "(Iteration 4301 / 22950) loss: 1.492180\n",
      "(Iteration 4401 / 22950) loss: 1.501430\n",
      "(Iteration 4501 / 22950) loss: 1.485444\n",
      "(Epoch 6 / 30) train acc: 0.452000; val_acc: 0.450000\n",
      "(Iteration 4601 / 22950) loss: 1.372662\n",
      "(Iteration 4701 / 22950) loss: 1.413261\n",
      "(Iteration 4801 / 22950) loss: 1.499108\n",
      "(Iteration 4901 / 22950) loss: 1.363886\n",
      "(Iteration 5001 / 22950) loss: 1.541688\n",
      "(Iteration 5101 / 22950) loss: 1.629323\n",
      "(Iteration 5201 / 22950) loss: 1.547520\n",
      "(Iteration 5301 / 22950) loss: 1.663932\n",
      "(Epoch 7 / 30) train acc: 0.492000; val_acc: 0.470000\n",
      "(Iteration 5401 / 22950) loss: 1.325606\n",
      "(Iteration 5501 / 22950) loss: 1.486657\n",
      "(Iteration 5601 / 22950) loss: 1.478955\n",
      "(Iteration 5701 / 22950) loss: 1.530539\n",
      "(Iteration 5801 / 22950) loss: 1.430301\n",
      "(Iteration 5901 / 22950) loss: 1.549801\n",
      "(Iteration 6001 / 22950) loss: 1.364028\n",
      "(Iteration 6101 / 22950) loss: 1.678399\n",
      "(Epoch 8 / 30) train acc: 0.483000; val_acc: 0.486000\n",
      "(Iteration 6201 / 22950) loss: 1.480490\n",
      "(Iteration 6301 / 22950) loss: 1.795184\n",
      "(Iteration 6401 / 22950) loss: 1.548424\n",
      "(Iteration 6501 / 22950) loss: 1.445164\n",
      "(Iteration 6601 / 22950) loss: 1.538584\n",
      "(Iteration 6701 / 22950) loss: 1.610149\n",
      "(Iteration 6801 / 22950) loss: 1.492289\n",
      "(Epoch 9 / 30) train acc: 0.513000; val_acc: 0.487000\n",
      "(Iteration 6901 / 22950) loss: 1.363561\n",
      "(Iteration 7001 / 22950) loss: 1.272801\n",
      "(Iteration 7101 / 22950) loss: 1.486963\n",
      "(Iteration 7201 / 22950) loss: 1.511066\n",
      "(Iteration 7301 / 22950) loss: 1.498897\n",
      "(Iteration 7401 / 22950) loss: 1.561119\n",
      "(Iteration 7501 / 22950) loss: 1.527616\n",
      "(Iteration 7601 / 22950) loss: 1.626569\n",
      "(Epoch 10 / 30) train acc: 0.501000; val_acc: 0.502000\n",
      "(Iteration 7701 / 22950) loss: 1.454410\n",
      "(Iteration 7801 / 22950) loss: 1.499818\n",
      "(Iteration 7901 / 22950) loss: 1.557116\n",
      "(Iteration 8001 / 22950) loss: 1.574604\n",
      "(Iteration 8101 / 22950) loss: 1.430408\n",
      "(Iteration 8201 / 22950) loss: 1.369075\n",
      "(Iteration 8301 / 22950) loss: 1.500517\n",
      "(Iteration 8401 / 22950) loss: 1.661631\n",
      "(Epoch 11 / 30) train acc: 0.534000; val_acc: 0.498000\n",
      "(Iteration 8501 / 22950) loss: 1.283720\n",
      "(Iteration 8601 / 22950) loss: 1.453002\n",
      "(Iteration 8701 / 22950) loss: 1.661666\n",
      "(Iteration 8801 / 22950) loss: 1.343856\n",
      "(Iteration 8901 / 22950) loss: 1.609829\n",
      "(Iteration 9001 / 22950) loss: 1.455128\n",
      "(Iteration 9101 / 22950) loss: 1.503454\n",
      "(Epoch 12 / 30) train acc: 0.514000; val_acc: 0.500000\n",
      "(Iteration 9201 / 22950) loss: 1.460949\n",
      "(Iteration 9301 / 22950) loss: 1.586703\n",
      "(Iteration 9401 / 22950) loss: 1.441080\n",
      "(Iteration 9501 / 22950) loss: 1.425881\n",
      "(Iteration 9601 / 22950) loss: 1.470773\n",
      "(Iteration 9701 / 22950) loss: 1.582395\n",
      "(Iteration 9801 / 22950) loss: 1.334955\n",
      "(Iteration 9901 / 22950) loss: 1.396347\n",
      "(Epoch 13 / 30) train acc: 0.499000; val_acc: 0.507000\n",
      "(Iteration 10001 / 22950) loss: 1.417203\n",
      "(Iteration 10101 / 22950) loss: 1.562671\n",
      "(Iteration 10201 / 22950) loss: 1.536334\n",
      "(Iteration 10301 / 22950) loss: 1.463893\n",
      "(Iteration 10401 / 22950) loss: 1.623337\n",
      "(Iteration 10501 / 22950) loss: 1.422006\n",
      "(Iteration 10601 / 22950) loss: 1.479912\n",
      "(Iteration 10701 / 22950) loss: 1.397236\n",
      "(Epoch 14 / 30) train acc: 0.523000; val_acc: 0.511000\n",
      "(Iteration 10801 / 22950) loss: 1.597566\n",
      "(Iteration 10901 / 22950) loss: 1.283998\n",
      "(Iteration 11001 / 22950) loss: 1.430182\n",
      "(Iteration 11101 / 22950) loss: 1.519799\n",
      "(Iteration 11201 / 22950) loss: 1.449569\n",
      "(Iteration 11301 / 22950) loss: 1.478857\n",
      "(Iteration 11401 / 22950) loss: 1.498845\n",
      "(Epoch 15 / 30) train acc: 0.509000; val_acc: 0.509000\n",
      "(Iteration 11501 / 22950) loss: 1.332103\n",
      "(Iteration 11601 / 22950) loss: 1.347704\n",
      "(Iteration 11701 / 22950) loss: 1.430245\n",
      "(Iteration 11801 / 22950) loss: 1.616717\n",
      "(Iteration 11901 / 22950) loss: 1.429958\n",
      "(Iteration 12001 / 22950) loss: 1.552659\n",
      "(Iteration 12101 / 22950) loss: 1.436888\n",
      "(Iteration 12201 / 22950) loss: 1.519319\n",
      "(Epoch 16 / 30) train acc: 0.499000; val_acc: 0.510000\n",
      "(Iteration 12301 / 22950) loss: 1.274673\n",
      "(Iteration 12401 / 22950) loss: 1.419627\n",
      "(Iteration 12501 / 22950) loss: 1.363254\n",
      "(Iteration 12601 / 22950) loss: 1.390780\n",
      "(Iteration 12701 / 22950) loss: 1.275666\n",
      "(Iteration 12801 / 22950) loss: 1.223660\n",
      "(Iteration 12901 / 22950) loss: 1.461828\n",
      "(Iteration 13001 / 22950) loss: 1.448360\n",
      "(Epoch 17 / 30) train acc: 0.512000; val_acc: 0.511000\n",
      "(Iteration 13101 / 22950) loss: 1.552348\n",
      "(Iteration 13201 / 22950) loss: 1.374805\n",
      "(Iteration 13301 / 22950) loss: 1.365052\n",
      "(Iteration 13401 / 22950) loss: 1.410086\n",
      "(Iteration 13501 / 22950) loss: 1.467153\n",
      "(Iteration 13601 / 22950) loss: 1.453380\n",
      "(Iteration 13701 / 22950) loss: 1.400336\n",
      "(Epoch 18 / 30) train acc: 0.531000; val_acc: 0.508000\n",
      "(Iteration 13801 / 22950) loss: 1.281754\n",
      "(Iteration 13901 / 22950) loss: 1.447199\n",
      "(Iteration 14001 / 22950) loss: 1.500493\n",
      "(Iteration 14101 / 22950) loss: 1.555384\n",
      "(Iteration 14201 / 22950) loss: 1.566289\n",
      "(Iteration 14301 / 22950) loss: 1.409273\n",
      "(Iteration 14401 / 22950) loss: 1.412350\n",
      "(Iteration 14501 / 22950) loss: 1.640431\n",
      "(Epoch 19 / 30) train acc: 0.497000; val_acc: 0.507000\n",
      "(Iteration 14601 / 22950) loss: 1.716727\n",
      "(Iteration 14701 / 22950) loss: 1.510629\n",
      "(Iteration 14801 / 22950) loss: 1.375278\n",
      "(Iteration 14901 / 22950) loss: 1.480832\n",
      "(Iteration 15001 / 22950) loss: 1.429454\n",
      "(Iteration 15101 / 22950) loss: 1.591950\n",
      "(Iteration 15201 / 22950) loss: 1.542405\n",
      "(Epoch 20 / 30) train acc: 0.486000; val_acc: 0.509000\n",
      "(Iteration 15301 / 22950) loss: 1.400963\n",
      "(Iteration 15401 / 22950) loss: 1.374782\n",
      "(Iteration 15501 / 22950) loss: 1.588213\n",
      "(Iteration 15601 / 22950) loss: 1.278137\n",
      "(Iteration 15701 / 22950) loss: 1.340613\n",
      "(Iteration 15801 / 22950) loss: 1.542391\n",
      "(Iteration 15901 / 22950) loss: 1.431606\n",
      "(Iteration 16001 / 22950) loss: 1.458129\n",
      "(Epoch 21 / 30) train acc: 0.514000; val_acc: 0.508000\n",
      "(Iteration 16101 / 22950) loss: 1.509547\n",
      "(Iteration 16201 / 22950) loss: 1.476693\n",
      "(Iteration 16301 / 22950) loss: 1.474282\n",
      "(Iteration 16401 / 22950) loss: 1.533747\n",
      "(Iteration 16501 / 22950) loss: 1.484097\n",
      "(Iteration 16601 / 22950) loss: 1.394318\n",
      "(Iteration 16701 / 22950) loss: 1.590634\n",
      "(Iteration 16801 / 22950) loss: 1.428423\n",
      "(Epoch 22 / 30) train acc: 0.494000; val_acc: 0.507000\n",
      "(Iteration 16901 / 22950) loss: 1.377207\n",
      "(Iteration 17001 / 22950) loss: 1.273294\n",
      "(Iteration 17101 / 22950) loss: 1.502145\n",
      "(Iteration 17201 / 22950) loss: 1.601026\n",
      "(Iteration 17301 / 22950) loss: 1.535465\n",
      "(Iteration 17401 / 22950) loss: 1.412639\n",
      "(Iteration 17501 / 22950) loss: 1.344042\n",
      "(Epoch 23 / 30) train acc: 0.518000; val_acc: 0.509000\n",
      "(Iteration 17601 / 22950) loss: 1.609462\n",
      "(Iteration 17701 / 22950) loss: 1.352259\n",
      "(Iteration 17801 / 22950) loss: 1.540519\n",
      "(Iteration 17901 / 22950) loss: 1.428643\n",
      "(Iteration 18001 / 22950) loss: 1.526016\n",
      "(Iteration 18101 / 22950) loss: 1.485756\n",
      "(Iteration 18201 / 22950) loss: 1.382624\n",
      "(Iteration 18301 / 22950) loss: 1.757790\n",
      "(Epoch 24 / 30) train acc: 0.516000; val_acc: 0.509000\n",
      "(Iteration 18401 / 22950) loss: 1.568734\n",
      "(Iteration 18501 / 22950) loss: 1.465618\n",
      "(Iteration 18601 / 22950) loss: 1.315268\n",
      "(Iteration 18701 / 22950) loss: 1.728197\n",
      "(Iteration 18801 / 22950) loss: 1.505568\n",
      "(Iteration 18901 / 22950) loss: 1.422776\n",
      "(Iteration 19001 / 22950) loss: 1.585539\n",
      "(Iteration 19101 / 22950) loss: 1.610404\n",
      "(Epoch 25 / 30) train acc: 0.499000; val_acc: 0.509000\n",
      "(Iteration 19201 / 22950) loss: 1.447894\n",
      "(Iteration 19301 / 22950) loss: 1.306744\n",
      "(Iteration 19401 / 22950) loss: 1.489314\n",
      "(Iteration 19501 / 22950) loss: 1.663553\n",
      "(Iteration 19601 / 22950) loss: 1.393751\n",
      "(Iteration 19701 / 22950) loss: 1.386060\n",
      "(Iteration 19801 / 22950) loss: 1.527553\n",
      "(Epoch 26 / 30) train acc: 0.501000; val_acc: 0.506000\n",
      "(Iteration 19901 / 22950) loss: 1.370553\n",
      "(Iteration 20001 / 22950) loss: 1.386020\n",
      "(Iteration 20101 / 22950) loss: 1.581990\n",
      "(Iteration 20201 / 22950) loss: 1.547240\n",
      "(Iteration 20301 / 22950) loss: 1.347597\n",
      "(Iteration 20401 / 22950) loss: 1.349651\n",
      "(Iteration 20501 / 22950) loss: 1.384565\n",
      "(Iteration 20601 / 22950) loss: 1.521387\n",
      "(Epoch 27 / 30) train acc: 0.523000; val_acc: 0.505000\n",
      "(Iteration 20701 / 22950) loss: 1.263465\n",
      "(Iteration 20801 / 22950) loss: 1.192529\n",
      "(Iteration 20901 / 22950) loss: 1.520933\n",
      "(Iteration 21001 / 22950) loss: 1.626581\n",
      "(Iteration 21101 / 22950) loss: 1.178228\n",
      "(Iteration 21201 / 22950) loss: 1.663956\n",
      "(Iteration 21301 / 22950) loss: 1.397097\n",
      "(Iteration 21401 / 22950) loss: 1.294466\n",
      "(Epoch 28 / 30) train acc: 0.541000; val_acc: 0.508000\n",
      "(Iteration 21501 / 22950) loss: 1.449762\n",
      "(Iteration 21601 / 22950) loss: 1.580357\n",
      "(Iteration 21701 / 22950) loss: 1.361524\n",
      "(Iteration 21801 / 22950) loss: 1.523844\n",
      "(Iteration 21901 / 22950) loss: 1.399197\n",
      "(Iteration 22001 / 22950) loss: 1.631349\n",
      "(Iteration 22101 / 22950) loss: 1.381176\n",
      "(Epoch 29 / 30) train acc: 0.517000; val_acc: 0.509000\n",
      "(Iteration 22201 / 22950) loss: 1.424395\n",
      "(Iteration 22301 / 22950) loss: 1.380003\n",
      "(Iteration 22401 / 22950) loss: 1.299913\n",
      "(Iteration 22501 / 22950) loss: 1.471352\n",
      "(Iteration 22601 / 22950) loss: 1.393080\n",
      "(Iteration 22701 / 22950) loss: 1.505050\n",
      "(Iteration 22801 / 22950) loss: 1.319168\n",
      "(Iteration 22901 / 22950) loss: 1.320302\n",
      "(Epoch 30 / 30) train acc: 0.538000; val_acc: 0.509000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.01, 'num_epochs': 30, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 45930) loss: 2.303151\n",
      "(Epoch 0 / 30) train acc: 0.090000; val_acc: 0.122000\n",
      "(Iteration 101 / 45930) loss: 2.301085\n",
      "(Iteration 201 / 45930) loss: 2.301626\n",
      "(Iteration 301 / 45930) loss: 2.303044\n",
      "(Iteration 401 / 45930) loss: 2.301133\n",
      "(Iteration 501 / 45930) loss: 2.301508\n",
      "(Iteration 601 / 45930) loss: 2.299734\n",
      "(Iteration 701 / 45930) loss: 2.293361\n",
      "(Iteration 801 / 45930) loss: 2.291313\n",
      "(Iteration 901 / 45930) loss: 2.260570\n",
      "(Iteration 1001 / 45930) loss: 2.205672\n",
      "(Iteration 1101 / 45930) loss: 2.205151\n",
      "(Iteration 1201 / 45930) loss: 2.100547\n",
      "(Iteration 1301 / 45930) loss: 2.147940\n",
      "(Iteration 1401 / 45930) loss: 2.003400\n",
      "(Iteration 1501 / 45930) loss: 2.105059\n",
      "(Epoch 1 / 30) train acc: 0.282000; val_acc: 0.287000\n",
      "(Iteration 1601 / 45930) loss: 1.895993\n",
      "(Iteration 1701 / 45930) loss: 1.883810\n",
      "(Iteration 1801 / 45930) loss: 1.786739\n",
      "(Iteration 1901 / 45930) loss: 1.778444\n",
      "(Iteration 2001 / 45930) loss: 1.934631\n",
      "(Iteration 2101 / 45930) loss: 1.720550\n",
      "(Iteration 2201 / 45930) loss: 1.779944\n",
      "(Iteration 2301 / 45930) loss: 1.757622\n",
      "(Iteration 2401 / 45930) loss: 1.533457\n",
      "(Iteration 2501 / 45930) loss: 1.671794\n",
      "(Iteration 2601 / 45930) loss: 1.755795\n",
      "(Iteration 2701 / 45930) loss: 1.894965\n",
      "(Iteration 2801 / 45930) loss: 1.711667\n",
      "(Iteration 2901 / 45930) loss: 1.541451\n",
      "(Iteration 3001 / 45930) loss: 1.644132\n",
      "(Epoch 2 / 30) train acc: 0.430000; val_acc: 0.415000\n",
      "(Iteration 3101 / 45930) loss: 1.642278\n",
      "(Iteration 3201 / 45930) loss: 1.789314\n",
      "(Iteration 3301 / 45930) loss: 2.002359\n",
      "(Iteration 3401 / 45930) loss: 1.362884\n",
      "(Iteration 3501 / 45930) loss: 1.629526\n",
      "(Iteration 3601 / 45930) loss: 1.404337\n",
      "(Iteration 3701 / 45930) loss: 1.580867\n",
      "(Iteration 3801 / 45930) loss: 1.504496\n",
      "(Iteration 3901 / 45930) loss: 1.377819\n",
      "(Iteration 4001 / 45930) loss: 1.227136\n",
      "(Iteration 4101 / 45930) loss: 1.634367\n",
      "(Iteration 4201 / 45930) loss: 1.509809\n",
      "(Iteration 4301 / 45930) loss: 1.320491\n",
      "(Iteration 4401 / 45930) loss: 1.609989\n",
      "(Iteration 4501 / 45930) loss: 1.156714\n",
      "(Epoch 3 / 30) train acc: 0.466000; val_acc: 0.479000\n",
      "(Iteration 4601 / 45930) loss: 1.724673\n",
      "(Iteration 4701 / 45930) loss: 1.548589\n",
      "(Iteration 4801 / 45930) loss: 1.483961\n",
      "(Iteration 4901 / 45930) loss: 1.640237\n",
      "(Iteration 5001 / 45930) loss: 1.454647\n",
      "(Iteration 5101 / 45930) loss: 1.445235\n",
      "(Iteration 5201 / 45930) loss: 1.444342\n",
      "(Iteration 5301 / 45930) loss: 1.560031\n",
      "(Iteration 5401 / 45930) loss: 1.401045\n",
      "(Iteration 5501 / 45930) loss: 1.367026\n",
      "(Iteration 5601 / 45930) loss: 1.573250\n",
      "(Iteration 5701 / 45930) loss: 1.294340\n",
      "(Iteration 5801 / 45930) loss: 1.766859\n",
      "(Iteration 5901 / 45930) loss: 1.481189\n",
      "(Iteration 6001 / 45930) loss: 1.481156\n",
      "(Iteration 6101 / 45930) loss: 1.397881\n",
      "(Epoch 4 / 30) train acc: 0.515000; val_acc: 0.493000\n",
      "(Iteration 6201 / 45930) loss: 1.386643\n",
      "(Iteration 6301 / 45930) loss: 1.799312\n",
      "(Iteration 6401 / 45930) loss: 1.595483\n",
      "(Iteration 6501 / 45930) loss: 1.602466\n",
      "(Iteration 6601 / 45930) loss: 1.602021\n",
      "(Iteration 6701 / 45930) loss: 1.536013\n",
      "(Iteration 6801 / 45930) loss: 1.575808\n",
      "(Iteration 6901 / 45930) loss: 1.456680\n",
      "(Iteration 7001 / 45930) loss: 1.538586\n",
      "(Iteration 7101 / 45930) loss: 1.355145\n",
      "(Iteration 7201 / 45930) loss: 1.862609\n",
      "(Iteration 7301 / 45930) loss: 1.382496\n",
      "(Iteration 7401 / 45930) loss: 1.651726\n",
      "(Iteration 7501 / 45930) loss: 1.488222\n",
      "(Iteration 7601 / 45930) loss: 1.415171\n",
      "(Epoch 5 / 30) train acc: 0.504000; val_acc: 0.500000\n",
      "(Iteration 7701 / 45930) loss: 1.330639\n",
      "(Iteration 7801 / 45930) loss: 1.769338\n",
      "(Iteration 7901 / 45930) loss: 1.437803\n",
      "(Iteration 8001 / 45930) loss: 1.670077\n",
      "(Iteration 8101 / 45930) loss: 1.536710\n",
      "(Iteration 8201 / 45930) loss: 1.572873\n",
      "(Iteration 8301 / 45930) loss: 1.741926\n",
      "(Iteration 8401 / 45930) loss: 1.473373\n",
      "(Iteration 8501 / 45930) loss: 1.654133\n",
      "(Iteration 8601 / 45930) loss: 1.357003\n",
      "(Iteration 8701 / 45930) loss: 1.650296\n",
      "(Iteration 8801 / 45930) loss: 1.496077\n",
      "(Iteration 8901 / 45930) loss: 1.361409\n",
      "(Iteration 9001 / 45930) loss: 1.369496\n",
      "(Iteration 9101 / 45930) loss: 1.580849\n",
      "(Epoch 6 / 30) train acc: 0.517000; val_acc: 0.508000\n",
      "(Iteration 9201 / 45930) loss: 1.407717\n",
      "(Iteration 9301 / 45930) loss: 1.457770\n",
      "(Iteration 9401 / 45930) loss: 1.519707\n",
      "(Iteration 9501 / 45930) loss: 1.595475\n",
      "(Iteration 9601 / 45930) loss: 1.443671\n",
      "(Iteration 9701 / 45930) loss: 1.484206\n",
      "(Iteration 9801 / 45930) loss: 1.420291\n",
      "(Iteration 9901 / 45930) loss: 1.465354\n",
      "(Iteration 10001 / 45930) loss: 1.567325\n",
      "(Iteration 10101 / 45930) loss: 1.595248\n",
      "(Iteration 10201 / 45930) loss: 1.659143\n",
      "(Iteration 10301 / 45930) loss: 1.463023\n",
      "(Iteration 10401 / 45930) loss: 1.441922\n",
      "(Iteration 10501 / 45930) loss: 1.338927\n",
      "(Iteration 10601 / 45930) loss: 1.494630\n",
      "(Iteration 10701 / 45930) loss: 1.518372\n",
      "(Epoch 7 / 30) train acc: 0.517000; val_acc: 0.507000\n",
      "(Iteration 10801 / 45930) loss: 1.443316\n",
      "(Iteration 10901 / 45930) loss: 1.493840\n",
      "(Iteration 11001 / 45930) loss: 1.693932\n",
      "(Iteration 11101 / 45930) loss: 1.390260\n",
      "(Iteration 11201 / 45930) loss: 1.229253\n",
      "(Iteration 11301 / 45930) loss: 1.190454\n",
      "(Iteration 11401 / 45930) loss: 1.444086\n",
      "(Iteration 11501 / 45930) loss: 1.673152\n",
      "(Iteration 11601 / 45930) loss: 1.350309\n",
      "(Iteration 11701 / 45930) loss: 1.162610\n",
      "(Iteration 11801 / 45930) loss: 1.225640\n",
      "(Iteration 11901 / 45930) loss: 1.511695\n",
      "(Iteration 12001 / 45930) loss: 1.290103\n",
      "(Iteration 12101 / 45930) loss: 1.417694\n",
      "(Iteration 12201 / 45930) loss: 1.625873\n",
      "(Epoch 8 / 30) train acc: 0.543000; val_acc: 0.513000\n",
      "(Iteration 12301 / 45930) loss: 1.621979\n",
      "(Iteration 12401 / 45930) loss: 1.398585\n",
      "(Iteration 12501 / 45930) loss: 1.477026\n",
      "(Iteration 12601 / 45930) loss: 1.176214\n",
      "(Iteration 12701 / 45930) loss: 1.801889\n",
      "(Iteration 12801 / 45930) loss: 1.664333\n",
      "(Iteration 12901 / 45930) loss: 1.488018\n",
      "(Iteration 13001 / 45930) loss: 1.708546\n",
      "(Iteration 13101 / 45930) loss: 1.638697\n",
      "(Iteration 13201 / 45930) loss: 1.318438\n",
      "(Iteration 13301 / 45930) loss: 1.161151\n",
      "(Iteration 13401 / 45930) loss: 1.394343\n",
      "(Iteration 13501 / 45930) loss: 1.416106\n",
      "(Iteration 13601 / 45930) loss: 1.720807\n",
      "(Iteration 13701 / 45930) loss: 1.637209\n",
      "(Epoch 9 / 30) train acc: 0.520000; val_acc: 0.518000\n",
      "(Iteration 13801 / 45930) loss: 1.183123\n",
      "(Iteration 13901 / 45930) loss: 1.703584\n",
      "(Iteration 14001 / 45930) loss: 1.657738\n",
      "(Iteration 14101 / 45930) loss: 1.580443\n",
      "(Iteration 14201 / 45930) loss: 1.552088\n",
      "(Iteration 14301 / 45930) loss: 1.298234\n",
      "(Iteration 14401 / 45930) loss: 1.421063\n",
      "(Iteration 14501 / 45930) loss: 1.527026\n",
      "(Iteration 14601 / 45930) loss: 1.683688\n",
      "(Iteration 14701 / 45930) loss: 1.254428\n",
      "(Iteration 14801 / 45930) loss: 1.316219\n",
      "(Iteration 14901 / 45930) loss: 1.481995\n",
      "(Iteration 15001 / 45930) loss: 1.168923\n",
      "(Iteration 15101 / 45930) loss: 1.275123\n",
      "(Iteration 15201 / 45930) loss: 1.326604\n",
      "(Iteration 15301 / 45930) loss: 1.211559\n",
      "(Epoch 10 / 30) train acc: 0.530000; val_acc: 0.516000\n",
      "(Iteration 15401 / 45930) loss: 1.405642\n",
      "(Iteration 15501 / 45930) loss: 1.411488\n",
      "(Iteration 15601 / 45930) loss: 1.537306\n",
      "(Iteration 15701 / 45930) loss: 1.901068\n",
      "(Iteration 15801 / 45930) loss: 1.398717\n",
      "(Iteration 15901 / 45930) loss: 1.527307\n",
      "(Iteration 16001 / 45930) loss: 1.798943\n",
      "(Iteration 16101 / 45930) loss: 1.465228\n",
      "(Iteration 16201 / 45930) loss: 1.646187\n",
      "(Iteration 16301 / 45930) loss: 1.728642\n",
      "(Iteration 16401 / 45930) loss: 1.571613\n",
      "(Iteration 16501 / 45930) loss: 1.527847\n",
      "(Iteration 16601 / 45930) loss: 1.278869\n",
      "(Iteration 16701 / 45930) loss: 1.367978\n",
      "(Iteration 16801 / 45930) loss: 1.425879\n",
      "(Epoch 11 / 30) train acc: 0.531000; val_acc: 0.517000\n",
      "(Iteration 16901 / 45930) loss: 1.455380\n",
      "(Iteration 17001 / 45930) loss: 1.300997\n",
      "(Iteration 17101 / 45930) loss: 1.368937\n",
      "(Iteration 17201 / 45930) loss: 1.697850\n",
      "(Iteration 17301 / 45930) loss: 1.222130\n",
      "(Iteration 17401 / 45930) loss: 1.374130\n",
      "(Iteration 17501 / 45930) loss: 1.158373\n",
      "(Iteration 17601 / 45930) loss: 1.202169\n",
      "(Iteration 17701 / 45930) loss: 1.373841\n",
      "(Iteration 17801 / 45930) loss: 1.658839\n",
      "(Iteration 17901 / 45930) loss: 1.501102\n",
      "(Iteration 18001 / 45930) loss: 1.365558\n",
      "(Iteration 18101 / 45930) loss: 1.412290\n",
      "(Iteration 18201 / 45930) loss: 1.262581\n",
      "(Iteration 18301 / 45930) loss: 1.482689\n",
      "(Epoch 12 / 30) train acc: 0.546000; val_acc: 0.515000\n",
      "(Iteration 18401 / 45930) loss: 1.296264\n",
      "(Iteration 18501 / 45930) loss: 1.683033\n",
      "(Iteration 18601 / 45930) loss: 1.398020\n",
      "(Iteration 18701 / 45930) loss: 1.682428\n",
      "(Iteration 18801 / 45930) loss: 1.062449\n",
      "(Iteration 18901 / 45930) loss: 1.462813\n",
      "(Iteration 19001 / 45930) loss: 1.497188\n",
      "(Iteration 19101 / 45930) loss: 1.498225\n",
      "(Iteration 19201 / 45930) loss: 1.117877\n",
      "(Iteration 19301 / 45930) loss: 1.180382\n",
      "(Iteration 19401 / 45930) loss: 1.444947\n",
      "(Iteration 19501 / 45930) loss: 1.278477\n",
      "(Iteration 19601 / 45930) loss: 1.661171\n",
      "(Iteration 19701 / 45930) loss: 1.640680\n",
      "(Iteration 19801 / 45930) loss: 1.604763\n",
      "(Iteration 19901 / 45930) loss: 1.554984\n",
      "(Epoch 13 / 30) train acc: 0.562000; val_acc: 0.517000\n",
      "(Iteration 20001 / 45930) loss: 1.314783\n",
      "(Iteration 20101 / 45930) loss: 1.510785\n",
      "(Iteration 20201 / 45930) loss: 1.525309\n",
      "(Iteration 20301 / 45930) loss: 1.306582\n",
      "(Iteration 20401 / 45930) loss: 1.423283\n",
      "(Iteration 20501 / 45930) loss: 1.266555\n",
      "(Iteration 20601 / 45930) loss: 1.310041\n",
      "(Iteration 20701 / 45930) loss: 1.282081\n",
      "(Iteration 20801 / 45930) loss: 1.534431\n",
      "(Iteration 20901 / 45930) loss: 1.663004\n",
      "(Iteration 21001 / 45930) loss: 1.149807\n",
      "(Iteration 21101 / 45930) loss: 1.493527\n",
      "(Iteration 21201 / 45930) loss: 1.275385\n",
      "(Iteration 21301 / 45930) loss: 1.264253\n",
      "(Iteration 21401 / 45930) loss: 1.429033\n",
      "(Epoch 14 / 30) train acc: 0.528000; val_acc: 0.509000\n",
      "(Iteration 21501 / 45930) loss: 1.241611\n",
      "(Iteration 21601 / 45930) loss: 1.344770\n",
      "(Iteration 21701 / 45930) loss: 1.310787\n",
      "(Iteration 21801 / 45930) loss: 1.551266\n",
      "(Iteration 21901 / 45930) loss: 1.456280\n",
      "(Iteration 22001 / 45930) loss: 1.722934\n",
      "(Iteration 22101 / 45930) loss: 1.529951\n",
      "(Iteration 22201 / 45930) loss: 1.371551\n",
      "(Iteration 22301 / 45930) loss: 1.317416\n",
      "(Iteration 22401 / 45930) loss: 1.495338\n",
      "(Iteration 22501 / 45930) loss: 1.388500\n",
      "(Iteration 22601 / 45930) loss: 1.254852\n",
      "(Iteration 22701 / 45930) loss: 1.491158\n",
      "(Iteration 22801 / 45930) loss: 1.540159\n",
      "(Iteration 22901 / 45930) loss: 1.612872\n",
      "(Epoch 15 / 30) train acc: 0.547000; val_acc: 0.520000\n",
      "(Iteration 23001 / 45930) loss: 1.601773\n",
      "(Iteration 23101 / 45930) loss: 1.454027\n",
      "(Iteration 23201 / 45930) loss: 1.276017\n",
      "(Iteration 23301 / 45930) loss: 1.399260\n",
      "(Iteration 23401 / 45930) loss: 1.365949\n",
      "(Iteration 23501 / 45930) loss: 1.509945\n",
      "(Iteration 23601 / 45930) loss: 1.258291\n",
      "(Iteration 23701 / 45930) loss: 1.386517\n",
      "(Iteration 23801 / 45930) loss: 1.482269\n",
      "(Iteration 23901 / 45930) loss: 1.440102\n",
      "(Iteration 24001 / 45930) loss: 1.163614\n",
      "(Iteration 24101 / 45930) loss: 1.419295\n",
      "(Iteration 24201 / 45930) loss: 1.756551\n",
      "(Iteration 24301 / 45930) loss: 1.657442\n",
      "(Iteration 24401 / 45930) loss: 1.506569\n",
      "(Epoch 16 / 30) train acc: 0.553000; val_acc: 0.518000\n",
      "(Iteration 24501 / 45930) loss: 1.270343\n",
      "(Iteration 24601 / 45930) loss: 1.183639\n",
      "(Iteration 24701 / 45930) loss: 1.428851\n",
      "(Iteration 24801 / 45930) loss: 1.192492\n",
      "(Iteration 24901 / 45930) loss: 1.783050\n",
      "(Iteration 25001 / 45930) loss: 1.237079\n",
      "(Iteration 25101 / 45930) loss: 1.333806\n",
      "(Iteration 25201 / 45930) loss: 1.504598\n",
      "(Iteration 25301 / 45930) loss: 1.473374\n",
      "(Iteration 25401 / 45930) loss: 1.715338\n",
      "(Iteration 25501 / 45930) loss: 1.855902\n",
      "(Iteration 25601 / 45930) loss: 1.581843\n",
      "(Iteration 25701 / 45930) loss: 1.405946\n",
      "(Iteration 25801 / 45930) loss: 1.211972\n",
      "(Iteration 25901 / 45930) loss: 1.309715\n",
      "(Iteration 26001 / 45930) loss: 1.415809\n",
      "(Epoch 17 / 30) train acc: 0.538000; val_acc: 0.520000\n",
      "(Iteration 26101 / 45930) loss: 1.621005\n",
      "(Iteration 26201 / 45930) loss: 1.638462\n",
      "(Iteration 26301 / 45930) loss: 1.335645\n",
      "(Iteration 26401 / 45930) loss: 1.448831\n",
      "(Iteration 26501 / 45930) loss: 1.551950\n",
      "(Iteration 26601 / 45930) loss: 1.381105\n",
      "(Iteration 26701 / 45930) loss: 1.739226\n",
      "(Iteration 26801 / 45930) loss: 1.459765\n",
      "(Iteration 26901 / 45930) loss: 1.754581\n",
      "(Iteration 27001 / 45930) loss: 1.748755\n",
      "(Iteration 27101 / 45930) loss: 1.198052\n",
      "(Iteration 27201 / 45930) loss: 1.573927\n",
      "(Iteration 27301 / 45930) loss: 1.232741\n",
      "(Iteration 27401 / 45930) loss: 1.187302\n",
      "(Iteration 27501 / 45930) loss: 1.424976\n",
      "(Epoch 18 / 30) train acc: 0.524000; val_acc: 0.518000\n",
      "(Iteration 27601 / 45930) loss: 1.753376\n",
      "(Iteration 27701 / 45930) loss: 1.477340\n",
      "(Iteration 27801 / 45930) loss: 1.314326\n",
      "(Iteration 27901 / 45930) loss: 1.449419\n",
      "(Iteration 28001 / 45930) loss: 1.344739\n",
      "(Iteration 28101 / 45930) loss: 1.463641\n",
      "(Iteration 28201 / 45930) loss: 1.550072\n",
      "(Iteration 28301 / 45930) loss: 1.565412\n",
      "(Iteration 28401 / 45930) loss: 1.544422\n",
      "(Iteration 28501 / 45930) loss: 1.314722\n",
      "(Iteration 28601 / 45930) loss: 1.549576\n",
      "(Iteration 28701 / 45930) loss: 1.509121\n",
      "(Iteration 28801 / 45930) loss: 1.601849\n",
      "(Iteration 28901 / 45930) loss: 1.564932\n",
      "(Iteration 29001 / 45930) loss: 1.636474\n",
      "(Epoch 19 / 30) train acc: 0.551000; val_acc: 0.515000\n",
      "(Iteration 29101 / 45930) loss: 1.733193\n",
      "(Iteration 29201 / 45930) loss: 1.247037\n",
      "(Iteration 29301 / 45930) loss: 1.388927\n",
      "(Iteration 29401 / 45930) loss: 1.648096\n",
      "(Iteration 29501 / 45930) loss: 1.505528\n",
      "(Iteration 29601 / 45930) loss: 1.293700\n",
      "(Iteration 29701 / 45930) loss: 1.260462\n",
      "(Iteration 29801 / 45930) loss: 1.304773\n",
      "(Iteration 29901 / 45930) loss: 1.716777\n",
      "(Iteration 30001 / 45930) loss: 1.105765\n",
      "(Iteration 30101 / 45930) loss: 1.320395\n",
      "(Iteration 30201 / 45930) loss: 1.113882\n",
      "(Iteration 30301 / 45930) loss: 1.311944\n",
      "(Iteration 30401 / 45930) loss: 1.480692\n",
      "(Iteration 30501 / 45930) loss: 1.423676\n",
      "(Iteration 30601 / 45930) loss: 1.608085\n",
      "(Epoch 20 / 30) train acc: 0.533000; val_acc: 0.523000\n",
      "(Iteration 30701 / 45930) loss: 1.060174\n",
      "(Iteration 30801 / 45930) loss: 1.178192\n",
      "(Iteration 30901 / 45930) loss: 1.507945\n",
      "(Iteration 31001 / 45930) loss: 1.269483\n",
      "(Iteration 31101 / 45930) loss: 1.471706\n",
      "(Iteration 31201 / 45930) loss: 1.300024\n",
      "(Iteration 31301 / 45930) loss: 1.591745\n",
      "(Iteration 31401 / 45930) loss: 1.402210\n",
      "(Iteration 31501 / 45930) loss: 1.346106\n",
      "(Iteration 31601 / 45930) loss: 1.369911\n",
      "(Iteration 31701 / 45930) loss: 1.283295\n",
      "(Iteration 31801 / 45930) loss: 1.610936\n",
      "(Iteration 31901 / 45930) loss: 1.443637\n",
      "(Iteration 32001 / 45930) loss: 1.302487\n",
      "(Iteration 32101 / 45930) loss: 1.516967\n",
      "(Epoch 21 / 30) train acc: 0.535000; val_acc: 0.516000\n",
      "(Iteration 32201 / 45930) loss: 1.451813\n",
      "(Iteration 32301 / 45930) loss: 1.392980\n",
      "(Iteration 32401 / 45930) loss: 1.326883\n",
      "(Iteration 32501 / 45930) loss: 1.343691\n",
      "(Iteration 32601 / 45930) loss: 1.707848\n",
      "(Iteration 32701 / 45930) loss: 1.408025\n",
      "(Iteration 32801 / 45930) loss: 1.752211\n",
      "(Iteration 32901 / 45930) loss: 1.523377\n",
      "(Iteration 33001 / 45930) loss: 1.308003\n",
      "(Iteration 33101 / 45930) loss: 1.632003\n",
      "(Iteration 33201 / 45930) loss: 1.669631\n",
      "(Iteration 33301 / 45930) loss: 1.345421\n",
      "(Iteration 33401 / 45930) loss: 1.489592\n",
      "(Iteration 33501 / 45930) loss: 1.574746\n",
      "(Iteration 33601 / 45930) loss: 1.412482\n",
      "(Epoch 22 / 30) train acc: 0.548000; val_acc: 0.520000\n",
      "(Iteration 33701 / 45930) loss: 1.479714\n",
      "(Iteration 33801 / 45930) loss: 1.382686\n",
      "(Iteration 33901 / 45930) loss: 1.417671\n",
      "(Iteration 34001 / 45930) loss: 1.576109\n",
      "(Iteration 34101 / 45930) loss: 1.575532\n",
      "(Iteration 34201 / 45930) loss: 1.540145\n",
      "(Iteration 34301 / 45930) loss: 1.395600\n",
      "(Iteration 34401 / 45930) loss: 1.522282\n",
      "(Iteration 34501 / 45930) loss: 1.749931\n",
      "(Iteration 34601 / 45930) loss: 1.386404\n",
      "(Iteration 34701 / 45930) loss: 1.208645\n",
      "(Iteration 34801 / 45930) loss: 1.587518\n",
      "(Iteration 34901 / 45930) loss: 1.601975\n",
      "(Iteration 35001 / 45930) loss: 1.226179\n",
      "(Iteration 35101 / 45930) loss: 1.407059\n",
      "(Iteration 35201 / 45930) loss: 1.398658\n",
      "(Epoch 23 / 30) train acc: 0.559000; val_acc: 0.525000\n",
      "(Iteration 35301 / 45930) loss: 1.395400\n",
      "(Iteration 35401 / 45930) loss: 1.255997\n",
      "(Iteration 35501 / 45930) loss: 1.353790\n",
      "(Iteration 35601 / 45930) loss: 1.369641\n",
      "(Iteration 35701 / 45930) loss: 1.326410\n",
      "(Iteration 35801 / 45930) loss: 1.410467\n",
      "(Iteration 35901 / 45930) loss: 1.587283\n",
      "(Iteration 36001 / 45930) loss: 1.421010\n",
      "(Iteration 36101 / 45930) loss: 1.272712\n",
      "(Iteration 36201 / 45930) loss: 1.436640\n",
      "(Iteration 36301 / 45930) loss: 1.554811\n",
      "(Iteration 36401 / 45930) loss: 1.547535\n",
      "(Iteration 36501 / 45930) loss: 1.492988\n",
      "(Iteration 36601 / 45930) loss: 1.913455\n",
      "(Iteration 36701 / 45930) loss: 1.590921\n",
      "(Epoch 24 / 30) train acc: 0.566000; val_acc: 0.524000\n",
      "(Iteration 36801 / 45930) loss: 1.750260\n",
      "(Iteration 36901 / 45930) loss: 1.400893\n",
      "(Iteration 37001 / 45930) loss: 1.299822\n",
      "(Iteration 37101 / 45930) loss: 1.181041\n",
      "(Iteration 37201 / 45930) loss: 1.158535\n",
      "(Iteration 37301 / 45930) loss: 1.173916\n",
      "(Iteration 37401 / 45930) loss: 1.372505\n",
      "(Iteration 37501 / 45930) loss: 1.421135\n",
      "(Iteration 37601 / 45930) loss: 1.676700\n",
      "(Iteration 37701 / 45930) loss: 1.636828\n",
      "(Iteration 37801 / 45930) loss: 1.320583\n",
      "(Iteration 37901 / 45930) loss: 1.243883\n",
      "(Iteration 38001 / 45930) loss: 1.275618\n",
      "(Iteration 38101 / 45930) loss: 1.676266\n",
      "(Iteration 38201 / 45930) loss: 1.329023\n",
      "(Epoch 25 / 30) train acc: 0.540000; val_acc: 0.523000\n",
      "(Iteration 38301 / 45930) loss: 1.482642\n",
      "(Iteration 38401 / 45930) loss: 1.371859\n",
      "(Iteration 38501 / 45930) loss: 1.765929\n",
      "(Iteration 38601 / 45930) loss: 1.581244\n",
      "(Iteration 38701 / 45930) loss: 1.426838\n",
      "(Iteration 38801 / 45930) loss: 1.597277\n",
      "(Iteration 38901 / 45930) loss: 1.538565\n",
      "(Iteration 39001 / 45930) loss: 1.783781\n",
      "(Iteration 39101 / 45930) loss: 1.365258\n",
      "(Iteration 39201 / 45930) loss: 1.629540\n",
      "(Iteration 39301 / 45930) loss: 1.524630\n",
      "(Iteration 39401 / 45930) loss: 1.058783\n",
      "(Iteration 39501 / 45930) loss: 1.489387\n",
      "(Iteration 39601 / 45930) loss: 1.657437\n",
      "(Iteration 39701 / 45930) loss: 1.546460\n",
      "(Iteration 39801 / 45930) loss: 1.479853\n",
      "(Epoch 26 / 30) train acc: 0.555000; val_acc: 0.520000\n",
      "(Iteration 39901 / 45930) loss: 1.416665\n",
      "(Iteration 40001 / 45930) loss: 1.458055\n",
      "(Iteration 40101 / 45930) loss: 1.477590\n",
      "(Iteration 40201 / 45930) loss: 1.511986\n",
      "(Iteration 40301 / 45930) loss: 1.627698\n",
      "(Iteration 40401 / 45930) loss: 1.436668\n",
      "(Iteration 40501 / 45930) loss: 1.782730\n",
      "(Iteration 40601 / 45930) loss: 1.370218\n",
      "(Iteration 40701 / 45930) loss: 1.357904\n",
      "(Iteration 40801 / 45930) loss: 1.377243\n",
      "(Iteration 40901 / 45930) loss: 1.422501\n",
      "(Iteration 41001 / 45930) loss: 1.394298\n",
      "(Iteration 41101 / 45930) loss: 1.361890\n",
      "(Iteration 41201 / 45930) loss: 1.697440\n",
      "(Iteration 41301 / 45930) loss: 1.225537\n",
      "(Epoch 27 / 30) train acc: 0.552000; val_acc: 0.524000\n",
      "(Iteration 41401 / 45930) loss: 1.237577\n",
      "(Iteration 41501 / 45930) loss: 1.589624\n",
      "(Iteration 41601 / 45930) loss: 1.367238\n",
      "(Iteration 41701 / 45930) loss: 1.487715\n",
      "(Iteration 41801 / 45930) loss: 1.165793\n",
      "(Iteration 41901 / 45930) loss: 1.673221\n",
      "(Iteration 42001 / 45930) loss: 1.503041\n",
      "(Iteration 42101 / 45930) loss: 1.536813\n",
      "(Iteration 42201 / 45930) loss: 1.213922\n",
      "(Iteration 42301 / 45930) loss: 1.450769\n",
      "(Iteration 42401 / 45930) loss: 1.469430\n",
      "(Iteration 42501 / 45930) loss: 1.523818\n",
      "(Iteration 42601 / 45930) loss: 1.639717\n",
      "(Iteration 42701 / 45930) loss: 1.602908\n",
      "(Iteration 42801 / 45930) loss: 1.275214\n",
      "(Epoch 28 / 30) train acc: 0.536000; val_acc: 0.524000\n",
      "(Iteration 42901 / 45930) loss: 1.287280\n",
      "(Iteration 43001 / 45930) loss: 1.156984\n",
      "(Iteration 43101 / 45930) loss: 1.089544\n",
      "(Iteration 43201 / 45930) loss: 1.670136\n",
      "(Iteration 43301 / 45930) loss: 1.146855\n",
      "(Iteration 43401 / 45930) loss: 1.402883\n",
      "(Iteration 43501 / 45930) loss: 1.559574\n",
      "(Iteration 43601 / 45930) loss: 1.511691\n",
      "(Iteration 43701 / 45930) loss: 1.483450\n",
      "(Iteration 43801 / 45930) loss: 1.339834\n",
      "(Iteration 43901 / 45930) loss: 1.343883\n",
      "(Iteration 44001 / 45930) loss: 1.212135\n",
      "(Iteration 44101 / 45930) loss: 1.487582\n",
      "(Iteration 44201 / 45930) loss: 1.563775\n",
      "(Iteration 44301 / 45930) loss: 1.330470\n",
      "(Epoch 29 / 30) train acc: 0.528000; val_acc: 0.523000\n",
      "(Iteration 44401 / 45930) loss: 1.128206\n",
      "(Iteration 44501 / 45930) loss: 1.344762\n",
      "(Iteration 44601 / 45930) loss: 1.521909\n",
      "(Iteration 44701 / 45930) loss: 1.454800\n",
      "(Iteration 44801 / 45930) loss: 1.521347\n",
      "(Iteration 44901 / 45930) loss: 1.567256\n",
      "(Iteration 45001 / 45930) loss: 1.155758\n",
      "(Iteration 45101 / 45930) loss: 1.402842\n",
      "(Iteration 45201 / 45930) loss: 1.228636\n",
      "(Iteration 45301 / 45930) loss: 1.490278\n",
      "(Iteration 45401 / 45930) loss: 1.293415\n",
      "(Iteration 45501 / 45930) loss: 1.421475\n",
      "(Iteration 45601 / 45930) loss: 1.656715\n",
      "(Iteration 45701 / 45930) loss: 1.465235\n",
      "(Iteration 45801 / 45930) loss: 1.316880\n",
      "(Iteration 45901 / 45930) loss: 1.287152\n",
      "(Epoch 30 / 30) train acc: 0.531000; val_acc: 0.524000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.01, 'num_epochs': 40, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 30600) loss: 2.308242\n",
      "(Epoch 0 / 40) train acc: 0.101000; val_acc: 0.096000\n",
      "(Iteration 101 / 30600) loss: 2.307196\n",
      "(Iteration 201 / 30600) loss: 2.304437\n",
      "(Iteration 301 / 30600) loss: 2.305590\n",
      "(Iteration 401 / 30600) loss: 2.303481\n",
      "(Iteration 501 / 30600) loss: 2.304059\n",
      "(Iteration 601 / 30600) loss: 2.302731\n",
      "(Iteration 701 / 30600) loss: 2.301294\n",
      "(Epoch 1 / 40) train acc: 0.214000; val_acc: 0.202000\n",
      "(Iteration 801 / 30600) loss: 2.300814\n",
      "(Iteration 901 / 30600) loss: 2.294832\n",
      "(Iteration 1001 / 30600) loss: 2.296654\n",
      "(Iteration 1101 / 30600) loss: 2.286736\n",
      "(Iteration 1201 / 30600) loss: 2.276756\n",
      "(Iteration 1301 / 30600) loss: 2.278351\n",
      "(Iteration 1401 / 30600) loss: 2.240697\n",
      "(Iteration 1501 / 30600) loss: 2.262069\n",
      "(Epoch 2 / 40) train acc: 0.244000; val_acc: 0.255000\n",
      "(Iteration 1601 / 30600) loss: 2.268334\n",
      "(Iteration 1701 / 30600) loss: 2.247353\n",
      "(Iteration 1801 / 30600) loss: 2.228962\n",
      "(Iteration 1901 / 30600) loss: 2.220911\n",
      "(Iteration 2001 / 30600) loss: 2.118204\n",
      "(Iteration 2101 / 30600) loss: 2.041515\n",
      "(Iteration 2201 / 30600) loss: 2.132795\n",
      "(Epoch 3 / 40) train acc: 0.279000; val_acc: 0.279000\n",
      "(Iteration 2301 / 30600) loss: 2.098550\n",
      "(Iteration 2401 / 30600) loss: 2.155811\n",
      "(Iteration 2501 / 30600) loss: 2.081949\n",
      "(Iteration 2601 / 30600) loss: 2.144895\n",
      "(Iteration 2701 / 30600) loss: 2.029082\n",
      "(Iteration 2801 / 30600) loss: 2.008303\n",
      "(Iteration 2901 / 30600) loss: 2.006415\n",
      "(Iteration 3001 / 30600) loss: 2.025720\n",
      "(Epoch 4 / 40) train acc: 0.301000; val_acc: 0.299000\n",
      "(Iteration 3101 / 30600) loss: 2.014471\n",
      "(Iteration 3201 / 30600) loss: 2.042501\n",
      "(Iteration 3301 / 30600) loss: 2.013252\n",
      "(Iteration 3401 / 30600) loss: 1.981435\n",
      "(Iteration 3501 / 30600) loss: 2.021249\n",
      "(Iteration 3601 / 30600) loss: 2.023561\n",
      "(Iteration 3701 / 30600) loss: 2.135972\n",
      "(Iteration 3801 / 30600) loss: 1.981422\n",
      "(Epoch 5 / 40) train acc: 0.299000; val_acc: 0.316000\n",
      "(Iteration 3901 / 30600) loss: 2.102581\n",
      "(Iteration 4001 / 30600) loss: 2.050100\n",
      "(Iteration 4101 / 30600) loss: 2.090588\n",
      "(Iteration 4201 / 30600) loss: 2.061960\n",
      "(Iteration 4301 / 30600) loss: 1.941174\n",
      "(Iteration 4401 / 30600) loss: 2.036467\n",
      "(Iteration 4501 / 30600) loss: 2.151143\n",
      "(Epoch 6 / 40) train acc: 0.361000; val_acc: 0.350000\n",
      "(Iteration 4601 / 30600) loss: 1.987026\n",
      "(Iteration 4701 / 30600) loss: 2.112187\n",
      "(Iteration 4801 / 30600) loss: 2.086348\n",
      "(Iteration 4901 / 30600) loss: 2.025087\n",
      "(Iteration 5001 / 30600) loss: 1.957317\n",
      "(Iteration 5101 / 30600) loss: 2.106213\n",
      "(Iteration 5201 / 30600) loss: 1.948211\n",
      "(Iteration 5301 / 30600) loss: 2.051709\n",
      "(Epoch 7 / 40) train acc: 0.391000; val_acc: 0.362000\n",
      "(Iteration 5401 / 30600) loss: 2.041482\n",
      "(Iteration 5501 / 30600) loss: 2.056552\n",
      "(Iteration 5601 / 30600) loss: 2.043171\n",
      "(Iteration 5701 / 30600) loss: 1.990255\n",
      "(Iteration 5801 / 30600) loss: 2.018087\n",
      "(Iteration 5901 / 30600) loss: 1.831029\n",
      "(Iteration 6001 / 30600) loss: 1.965803\n",
      "(Iteration 6101 / 30600) loss: 2.085768\n",
      "(Epoch 8 / 40) train acc: 0.366000; val_acc: 0.383000\n",
      "(Iteration 6201 / 30600) loss: 2.075598\n",
      "(Iteration 6301 / 30600) loss: 1.988490\n",
      "(Iteration 6401 / 30600) loss: 1.935147\n",
      "(Iteration 6501 / 30600) loss: 1.871297\n",
      "(Iteration 6601 / 30600) loss: 1.940068\n",
      "(Iteration 6701 / 30600) loss: 2.005972\n",
      "(Iteration 6801 / 30600) loss: 1.951914\n",
      "(Epoch 9 / 40) train acc: 0.393000; val_acc: 0.392000\n",
      "(Iteration 6901 / 30600) loss: 2.001041\n",
      "(Iteration 7001 / 30600) loss: 2.054203\n",
      "(Iteration 7101 / 30600) loss: 1.964643\n",
      "(Iteration 7201 / 30600) loss: 2.091299\n",
      "(Iteration 7301 / 30600) loss: 2.011313\n",
      "(Iteration 7401 / 30600) loss: 2.103670\n",
      "(Iteration 7501 / 30600) loss: 1.800104\n",
      "(Iteration 7601 / 30600) loss: 2.040098\n",
      "(Epoch 10 / 40) train acc: 0.411000; val_acc: 0.407000\n",
      "(Iteration 7701 / 30600) loss: 2.043620\n",
      "(Iteration 7801 / 30600) loss: 2.078253\n",
      "(Iteration 7901 / 30600) loss: 2.003773\n",
      "(Iteration 8001 / 30600) loss: 2.164750\n",
      "(Iteration 8101 / 30600) loss: 2.078856\n",
      "(Iteration 8201 / 30600) loss: 2.058007\n",
      "(Iteration 8301 / 30600) loss: 2.004329\n",
      "(Iteration 8401 / 30600) loss: 1.857017\n",
      "(Epoch 11 / 40) train acc: 0.402000; val_acc: 0.406000\n",
      "(Iteration 8501 / 30600) loss: 2.018534\n",
      "(Iteration 8601 / 30600) loss: 2.017584\n",
      "(Iteration 8701 / 30600) loss: 1.936949\n",
      "(Iteration 8801 / 30600) loss: 1.888518\n",
      "(Iteration 8901 / 30600) loss: 1.926734\n",
      "(Iteration 9001 / 30600) loss: 2.125633\n",
      "(Iteration 9101 / 30600) loss: 1.885993\n",
      "(Epoch 12 / 40) train acc: 0.434000; val_acc: 0.406000\n",
      "(Iteration 9201 / 30600) loss: 2.180566\n",
      "(Iteration 9301 / 30600) loss: 1.923536\n",
      "(Iteration 9401 / 30600) loss: 1.979289\n",
      "(Iteration 9501 / 30600) loss: 2.009155\n",
      "(Iteration 9601 / 30600) loss: 2.031134\n",
      "(Iteration 9701 / 30600) loss: 1.938286\n",
      "(Iteration 9801 / 30600) loss: 1.911404\n",
      "(Iteration 9901 / 30600) loss: 2.033447\n",
      "(Epoch 13 / 40) train acc: 0.433000; val_acc: 0.420000\n",
      "(Iteration 10001 / 30600) loss: 2.022490\n",
      "(Iteration 10101 / 30600) loss: 1.957009\n",
      "(Iteration 10201 / 30600) loss: 2.085749\n",
      "(Iteration 10301 / 30600) loss: 2.059696\n",
      "(Iteration 10401 / 30600) loss: 2.051393\n",
      "(Iteration 10501 / 30600) loss: 2.012866\n",
      "(Iteration 10601 / 30600) loss: 1.938515\n",
      "(Iteration 10701 / 30600) loss: 2.029607\n",
      "(Epoch 14 / 40) train acc: 0.420000; val_acc: 0.412000\n",
      "(Iteration 10801 / 30600) loss: 2.006433\n",
      "(Iteration 10901 / 30600) loss: 1.972903\n",
      "(Iteration 11001 / 30600) loss: 2.019744\n",
      "(Iteration 11101 / 30600) loss: 2.052610\n",
      "(Iteration 11201 / 30600) loss: 2.016623\n",
      "(Iteration 11301 / 30600) loss: 1.979409\n",
      "(Iteration 11401 / 30600) loss: 2.038045\n",
      "(Epoch 15 / 40) train acc: 0.439000; val_acc: 0.415000\n",
      "(Iteration 11501 / 30600) loss: 1.993125\n",
      "(Iteration 11601 / 30600) loss: 1.895782\n",
      "(Iteration 11701 / 30600) loss: 1.971236\n",
      "(Iteration 11801 / 30600) loss: 2.019014\n",
      "(Iteration 11901 / 30600) loss: 2.064859\n",
      "(Iteration 12001 / 30600) loss: 2.055140\n",
      "(Iteration 12101 / 30600) loss: 1.984181\n",
      "(Iteration 12201 / 30600) loss: 1.969387\n",
      "(Epoch 16 / 40) train acc: 0.422000; val_acc: 0.405000\n",
      "(Iteration 12301 / 30600) loss: 1.975567\n",
      "(Iteration 12401 / 30600) loss: 2.019953\n",
      "(Iteration 12501 / 30600) loss: 2.030287\n",
      "(Iteration 12601 / 30600) loss: 1.997923\n",
      "(Iteration 12701 / 30600) loss: 1.992261\n",
      "(Iteration 12801 / 30600) loss: 2.114513\n",
      "(Iteration 12901 / 30600) loss: 1.929458\n",
      "(Iteration 13001 / 30600) loss: 2.000290\n",
      "(Epoch 17 / 40) train acc: 0.435000; val_acc: 0.409000\n",
      "(Iteration 13101 / 30600) loss: 2.024620\n",
      "(Iteration 13201 / 30600) loss: 2.064932\n",
      "(Iteration 13301 / 30600) loss: 1.841625\n",
      "(Iteration 13401 / 30600) loss: 1.963079\n",
      "(Iteration 13501 / 30600) loss: 2.028592\n",
      "(Iteration 13601 / 30600) loss: 1.933409\n",
      "(Iteration 13701 / 30600) loss: 2.157257\n",
      "(Epoch 18 / 40) train acc: 0.413000; val_acc: 0.418000\n",
      "(Iteration 13801 / 30600) loss: 1.942853\n",
      "(Iteration 13901 / 30600) loss: 1.988018\n",
      "(Iteration 14001 / 30600) loss: 1.862449\n",
      "(Iteration 14101 / 30600) loss: 1.978781\n",
      "(Iteration 14201 / 30600) loss: 1.947135\n",
      "(Iteration 14301 / 30600) loss: 1.959251\n",
      "(Iteration 14401 / 30600) loss: 1.825337\n",
      "(Iteration 14501 / 30600) loss: 1.955251\n",
      "(Epoch 19 / 40) train acc: 0.397000; val_acc: 0.418000\n",
      "(Iteration 14601 / 30600) loss: 2.044896\n",
      "(Iteration 14701 / 30600) loss: 1.920447\n",
      "(Iteration 14801 / 30600) loss: 1.925605\n",
      "(Iteration 14901 / 30600) loss: 2.031367\n",
      "(Iteration 15001 / 30600) loss: 1.978323\n",
      "(Iteration 15101 / 30600) loss: 2.110103\n",
      "(Iteration 15201 / 30600) loss: 1.933172\n",
      "(Epoch 20 / 40) train acc: 0.420000; val_acc: 0.421000\n",
      "(Iteration 15301 / 30600) loss: 2.087921\n",
      "(Iteration 15401 / 30600) loss: 1.884936\n",
      "(Iteration 15501 / 30600) loss: 2.001070\n",
      "(Iteration 15601 / 30600) loss: 1.892182\n",
      "(Iteration 15701 / 30600) loss: 2.010586\n",
      "(Iteration 15801 / 30600) loss: 2.002837\n",
      "(Iteration 15901 / 30600) loss: 2.031144\n",
      "(Iteration 16001 / 30600) loss: 1.972954\n",
      "(Epoch 21 / 40) train acc: 0.438000; val_acc: 0.410000\n",
      "(Iteration 16101 / 30600) loss: 2.049014\n",
      "(Iteration 16201 / 30600) loss: 1.954260\n",
      "(Iteration 16301 / 30600) loss: 1.919771\n",
      "(Iteration 16401 / 30600) loss: 1.905513\n",
      "(Iteration 16501 / 30600) loss: 1.946167\n",
      "(Iteration 16601 / 30600) loss: 1.881090\n",
      "(Iteration 16701 / 30600) loss: 1.964169\n",
      "(Iteration 16801 / 30600) loss: 1.991382\n",
      "(Epoch 22 / 40) train acc: 0.465000; val_acc: 0.413000\n",
      "(Iteration 16901 / 30600) loss: 1.954455\n",
      "(Iteration 17001 / 30600) loss: 1.953083\n",
      "(Iteration 17101 / 30600) loss: 2.024194\n",
      "(Iteration 17201 / 30600) loss: 1.900512\n",
      "(Iteration 17301 / 30600) loss: 2.046464\n",
      "(Iteration 17401 / 30600) loss: 1.916669\n",
      "(Iteration 17501 / 30600) loss: 1.898179\n",
      "(Epoch 23 / 40) train acc: 0.430000; val_acc: 0.418000\n",
      "(Iteration 17601 / 30600) loss: 1.960617\n",
      "(Iteration 17701 / 30600) loss: 1.838637\n",
      "(Iteration 17801 / 30600) loss: 1.937236\n",
      "(Iteration 17901 / 30600) loss: 2.034413\n",
      "(Iteration 18001 / 30600) loss: 1.962879\n",
      "(Iteration 18101 / 30600) loss: 2.070557\n",
      "(Iteration 18201 / 30600) loss: 1.937257\n",
      "(Iteration 18301 / 30600) loss: 2.021222\n",
      "(Epoch 24 / 40) train acc: 0.401000; val_acc: 0.411000\n",
      "(Iteration 18401 / 30600) loss: 1.995065\n",
      "(Iteration 18501 / 30600) loss: 2.024626\n",
      "(Iteration 18601 / 30600) loss: 2.047418\n",
      "(Iteration 18701 / 30600) loss: 1.914160\n",
      "(Iteration 18801 / 30600) loss: 1.871575\n",
      "(Iteration 18901 / 30600) loss: 1.898066\n",
      "(Iteration 19001 / 30600) loss: 1.949372\n",
      "(Iteration 19101 / 30600) loss: 1.969897\n",
      "(Epoch 25 / 40) train acc: 0.440000; val_acc: 0.415000\n",
      "(Iteration 19201 / 30600) loss: 1.929281\n",
      "(Iteration 19301 / 30600) loss: 2.096554\n",
      "(Iteration 19401 / 30600) loss: 1.936836\n",
      "(Iteration 19501 / 30600) loss: 2.038212\n",
      "(Iteration 19601 / 30600) loss: 1.869296\n",
      "(Iteration 19701 / 30600) loss: 1.909571\n",
      "(Iteration 19801 / 30600) loss: 1.923821\n",
      "(Epoch 26 / 40) train acc: 0.443000; val_acc: 0.414000\n",
      "(Iteration 19901 / 30600) loss: 2.011130\n",
      "(Iteration 20001 / 30600) loss: 1.917516\n",
      "(Iteration 20101 / 30600) loss: 1.893448\n",
      "(Iteration 20201 / 30600) loss: 1.984420\n",
      "(Iteration 20301 / 30600) loss: 2.065018\n",
      "(Iteration 20401 / 30600) loss: 2.049442\n",
      "(Iteration 20501 / 30600) loss: 1.911287\n",
      "(Iteration 20601 / 30600) loss: 1.880506\n",
      "(Epoch 27 / 40) train acc: 0.427000; val_acc: 0.420000\n",
      "(Iteration 20701 / 30600) loss: 2.029890\n",
      "(Iteration 20801 / 30600) loss: 1.892372\n",
      "(Iteration 20901 / 30600) loss: 2.030571\n",
      "(Iteration 21001 / 30600) loss: 1.922269\n",
      "(Iteration 21101 / 30600) loss: 1.954285\n",
      "(Iteration 21201 / 30600) loss: 1.978949\n",
      "(Iteration 21301 / 30600) loss: 2.044553\n",
      "(Iteration 21401 / 30600) loss: 2.028878\n",
      "(Epoch 28 / 40) train acc: 0.425000; val_acc: 0.419000\n",
      "(Iteration 21501 / 30600) loss: 1.953037\n",
      "(Iteration 21601 / 30600) loss: 2.070538\n",
      "(Iteration 21701 / 30600) loss: 1.983000\n",
      "(Iteration 21801 / 30600) loss: 2.002148\n",
      "(Iteration 21901 / 30600) loss: 1.930988\n",
      "(Iteration 22001 / 30600) loss: 1.857136\n",
      "(Iteration 22101 / 30600) loss: 1.844459\n",
      "(Epoch 29 / 40) train acc: 0.426000; val_acc: 0.424000\n",
      "(Iteration 22201 / 30600) loss: 1.981644\n",
      "(Iteration 22301 / 30600) loss: 2.034570\n",
      "(Iteration 22401 / 30600) loss: 2.104074\n",
      "(Iteration 22501 / 30600) loss: 1.922928\n",
      "(Iteration 22601 / 30600) loss: 2.009511\n",
      "(Iteration 22701 / 30600) loss: 2.052251\n",
      "(Iteration 22801 / 30600) loss: 1.968844\n",
      "(Iteration 22901 / 30600) loss: 2.024460\n",
      "(Epoch 30 / 40) train acc: 0.455000; val_acc: 0.417000\n",
      "(Iteration 23001 / 30600) loss: 2.013991\n",
      "(Iteration 23101 / 30600) loss: 2.025663\n",
      "(Iteration 23201 / 30600) loss: 1.995491\n",
      "(Iteration 23301 / 30600) loss: 1.926031\n",
      "(Iteration 23401 / 30600) loss: 1.864022\n",
      "(Iteration 23501 / 30600) loss: 1.935808\n",
      "(Iteration 23601 / 30600) loss: 1.966911\n",
      "(Iteration 23701 / 30600) loss: 2.049370\n",
      "(Epoch 31 / 40) train acc: 0.397000; val_acc: 0.416000\n",
      "(Iteration 23801 / 30600) loss: 2.001176\n",
      "(Iteration 23901 / 30600) loss: 1.878931\n",
      "(Iteration 24001 / 30600) loss: 1.979727\n",
      "(Iteration 24101 / 30600) loss: 2.115388\n",
      "(Iteration 24201 / 30600) loss: 1.967361\n",
      "(Iteration 24301 / 30600) loss: 2.117050\n",
      "(Iteration 24401 / 30600) loss: 2.002851\n",
      "(Epoch 32 / 40) train acc: 0.401000; val_acc: 0.420000\n",
      "(Iteration 24501 / 30600) loss: 1.881654\n",
      "(Iteration 24601 / 30600) loss: 1.904627\n",
      "(Iteration 24701 / 30600) loss: 1.915558\n",
      "(Iteration 24801 / 30600) loss: 1.982278\n",
      "(Iteration 24901 / 30600) loss: 2.084117\n",
      "(Iteration 25001 / 30600) loss: 2.129921\n",
      "(Iteration 25101 / 30600) loss: 1.945687\n",
      "(Iteration 25201 / 30600) loss: 1.917566\n",
      "(Epoch 33 / 40) train acc: 0.419000; val_acc: 0.423000\n",
      "(Iteration 25301 / 30600) loss: 1.976139\n",
      "(Iteration 25401 / 30600) loss: 2.021686\n",
      "(Iteration 25501 / 30600) loss: 1.986814\n",
      "(Iteration 25601 / 30600) loss: 2.049946\n",
      "(Iteration 25701 / 30600) loss: 2.023752\n",
      "(Iteration 25801 / 30600) loss: 1.959032\n",
      "(Iteration 25901 / 30600) loss: 1.910277\n",
      "(Iteration 26001 / 30600) loss: 2.015659\n",
      "(Epoch 34 / 40) train acc: 0.424000; val_acc: 0.421000\n",
      "(Iteration 26101 / 30600) loss: 1.957558\n",
      "(Iteration 26201 / 30600) loss: 1.948241\n",
      "(Iteration 26301 / 30600) loss: 1.964854\n",
      "(Iteration 26401 / 30600) loss: 1.998681\n",
      "(Iteration 26501 / 30600) loss: 1.909321\n",
      "(Iteration 26601 / 30600) loss: 1.928834\n",
      "(Iteration 26701 / 30600) loss: 1.973190\n",
      "(Epoch 35 / 40) train acc: 0.446000; val_acc: 0.418000\n",
      "(Iteration 26801 / 30600) loss: 2.048063\n",
      "(Iteration 26901 / 30600) loss: 1.957754\n",
      "(Iteration 27001 / 30600) loss: 1.872429\n",
      "(Iteration 27101 / 30600) loss: 1.966687\n",
      "(Iteration 27201 / 30600) loss: 2.085376\n",
      "(Iteration 27301 / 30600) loss: 1.923669\n",
      "(Iteration 27401 / 30600) loss: 1.946008\n",
      "(Iteration 27501 / 30600) loss: 1.980596\n",
      "(Epoch 36 / 40) train acc: 0.434000; val_acc: 0.418000\n",
      "(Iteration 27601 / 30600) loss: 1.933861\n",
      "(Iteration 27701 / 30600) loss: 1.978268\n",
      "(Iteration 27801 / 30600) loss: 1.859052\n",
      "(Iteration 27901 / 30600) loss: 1.953434\n",
      "(Iteration 28001 / 30600) loss: 1.945659\n",
      "(Iteration 28101 / 30600) loss: 1.881420\n",
      "(Iteration 28201 / 30600) loss: 1.983414\n",
      "(Iteration 28301 / 30600) loss: 2.014741\n",
      "(Epoch 37 / 40) train acc: 0.413000; val_acc: 0.419000\n",
      "(Iteration 28401 / 30600) loss: 2.028921\n",
      "(Iteration 28501 / 30600) loss: 1.900126\n",
      "(Iteration 28601 / 30600) loss: 1.941550\n",
      "(Iteration 28701 / 30600) loss: 2.111855\n",
      "(Iteration 28801 / 30600) loss: 2.023229\n",
      "(Iteration 28901 / 30600) loss: 1.955276\n",
      "(Iteration 29001 / 30600) loss: 1.925340\n",
      "(Epoch 38 / 40) train acc: 0.439000; val_acc: 0.418000\n",
      "(Iteration 29101 / 30600) loss: 1.935805\n",
      "(Iteration 29201 / 30600) loss: 1.970033\n",
      "(Iteration 29301 / 30600) loss: 1.945584\n",
      "(Iteration 29401 / 30600) loss: 2.027801\n",
      "(Iteration 29501 / 30600) loss: 2.018437\n",
      "(Iteration 29601 / 30600) loss: 1.907695\n",
      "(Iteration 29701 / 30600) loss: 2.032513\n",
      "(Iteration 29801 / 30600) loss: 1.994600\n",
      "(Epoch 39 / 40) train acc: 0.422000; val_acc: 0.417000\n",
      "(Iteration 29901 / 30600) loss: 1.957844\n",
      "(Iteration 30001 / 30600) loss: 1.931049\n",
      "(Iteration 30101 / 30600) loss: 1.953065\n",
      "(Iteration 30201 / 30600) loss: 1.946826\n",
      "(Iteration 30301 / 30600) loss: 1.938394\n",
      "(Iteration 30401 / 30600) loss: 2.036873\n",
      "(Iteration 30501 / 30600) loss: 1.863401\n",
      "(Epoch 40 / 40) train acc: 0.411000; val_acc: 0.419000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.01, 'num_epochs': 40, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 61240) loss: 2.308341\n",
      "(Epoch 0 / 40) train acc: 0.102000; val_acc: 0.112000\n",
      "(Iteration 101 / 61240) loss: 2.305972\n",
      "(Iteration 201 / 61240) loss: 2.305551\n",
      "(Iteration 301 / 61240) loss: 2.305003\n",
      "(Iteration 401 / 61240) loss: 2.303336\n",
      "(Iteration 501 / 61240) loss: 2.306992\n",
      "(Iteration 601 / 61240) loss: 2.299761\n",
      "(Iteration 701 / 61240) loss: 2.298619\n",
      "(Iteration 801 / 61240) loss: 2.294699\n",
      "(Iteration 901 / 61240) loss: 2.301706\n",
      "(Iteration 1001 / 61240) loss: 2.283690\n",
      "(Iteration 1101 / 61240) loss: 2.252092\n",
      "(Iteration 1201 / 61240) loss: 2.256548\n",
      "(Iteration 1301 / 61240) loss: 2.231239\n",
      "(Iteration 1401 / 61240) loss: 2.212161\n",
      "(Iteration 1501 / 61240) loss: 2.239550\n",
      "(Epoch 1 / 40) train acc: 0.220000; val_acc: 0.251000\n",
      "(Iteration 1601 / 61240) loss: 2.206635\n",
      "(Iteration 1701 / 61240) loss: 2.152690\n",
      "(Iteration 1801 / 61240) loss: 2.175606\n",
      "(Iteration 1901 / 61240) loss: 2.147354\n",
      "(Iteration 2001 / 61240) loss: 2.123071\n",
      "(Iteration 2101 / 61240) loss: 2.179603\n",
      "(Iteration 2201 / 61240) loss: 2.106860\n",
      "(Iteration 2301 / 61240) loss: 2.057458\n",
      "(Iteration 2401 / 61240) loss: 2.154247\n",
      "(Iteration 2501 / 61240) loss: 2.071831\n",
      "(Iteration 2601 / 61240) loss: 2.141754\n",
      "(Iteration 2701 / 61240) loss: 2.091646\n",
      "(Iteration 2801 / 61240) loss: 1.982881\n",
      "(Iteration 2901 / 61240) loss: 2.070774\n",
      "(Iteration 3001 / 61240) loss: 2.091112\n",
      "(Epoch 2 / 40) train acc: 0.278000; val_acc: 0.312000\n",
      "(Iteration 3101 / 61240) loss: 1.889967\n",
      "(Iteration 3201 / 61240) loss: 1.873042\n",
      "(Iteration 3301 / 61240) loss: 2.059503\n",
      "(Iteration 3401 / 61240) loss: 1.973906\n",
      "(Iteration 3501 / 61240) loss: 2.028628\n",
      "(Iteration 3601 / 61240) loss: 2.051835\n",
      "(Iteration 3701 / 61240) loss: 2.250195\n",
      "(Iteration 3801 / 61240) loss: 2.020628\n",
      "(Iteration 3901 / 61240) loss: 1.969063\n",
      "(Iteration 4001 / 61240) loss: 1.912010\n",
      "(Iteration 4101 / 61240) loss: 2.167821\n",
      "(Iteration 4201 / 61240) loss: 2.026739\n",
      "(Iteration 4301 / 61240) loss: 2.069058\n",
      "(Iteration 4401 / 61240) loss: 2.004704\n",
      "(Iteration 4501 / 61240) loss: 1.939955\n",
      "(Epoch 3 / 40) train acc: 0.392000; val_acc: 0.382000\n",
      "(Iteration 4601 / 61240) loss: 1.802158\n",
      "(Iteration 4701 / 61240) loss: 1.999700\n",
      "(Iteration 4801 / 61240) loss: 1.887697\n",
      "(Iteration 4901 / 61240) loss: 1.912679\n",
      "(Iteration 5001 / 61240) loss: 1.917756\n",
      "(Iteration 5101 / 61240) loss: 2.023462\n",
      "(Iteration 5201 / 61240) loss: 2.053368\n",
      "(Iteration 5301 / 61240) loss: 2.078373\n",
      "(Iteration 5401 / 61240) loss: 2.030280\n",
      "(Iteration 5501 / 61240) loss: 1.988204\n",
      "(Iteration 5601 / 61240) loss: 2.020757\n",
      "(Iteration 5701 / 61240) loss: 2.011769\n",
      "(Iteration 5801 / 61240) loss: 1.952723\n",
      "(Iteration 5901 / 61240) loss: 1.982242\n",
      "(Iteration 6001 / 61240) loss: 1.976802\n",
      "(Iteration 6101 / 61240) loss: 2.149420\n",
      "(Epoch 4 / 40) train acc: 0.414000; val_acc: 0.400000\n",
      "(Iteration 6201 / 61240) loss: 1.978268\n",
      "(Iteration 6301 / 61240) loss: 2.155999\n",
      "(Iteration 6401 / 61240) loss: 1.849622\n",
      "(Iteration 6501 / 61240) loss: 2.044636\n",
      "(Iteration 6601 / 61240) loss: 2.026349\n",
      "(Iteration 6701 / 61240) loss: 1.967819\n",
      "(Iteration 6801 / 61240) loss: 2.127926\n",
      "(Iteration 6901 / 61240) loss: 2.079440\n",
      "(Iteration 7001 / 61240) loss: 2.063963\n",
      "(Iteration 7101 / 61240) loss: 1.954102\n",
      "(Iteration 7201 / 61240) loss: 1.979967\n",
      "(Iteration 7301 / 61240) loss: 1.953138\n",
      "(Iteration 7401 / 61240) loss: 1.991260\n",
      "(Iteration 7501 / 61240) loss: 1.902362\n",
      "(Iteration 7601 / 61240) loss: 2.079622\n",
      "(Epoch 5 / 40) train acc: 0.422000; val_acc: 0.406000\n",
      "(Iteration 7701 / 61240) loss: 1.917121\n",
      "(Iteration 7801 / 61240) loss: 2.145957\n",
      "(Iteration 7901 / 61240) loss: 1.982237\n",
      "(Iteration 8001 / 61240) loss: 1.879981\n",
      "(Iteration 8101 / 61240) loss: 2.024438\n",
      "(Iteration 8201 / 61240) loss: 1.897053\n",
      "(Iteration 8301 / 61240) loss: 2.203989\n",
      "(Iteration 8401 / 61240) loss: 2.021514\n",
      "(Iteration 8501 / 61240) loss: 1.840793\n",
      "(Iteration 8601 / 61240) loss: 1.898851\n",
      "(Iteration 8701 / 61240) loss: 2.137055\n",
      "(Iteration 8801 / 61240) loss: 1.965903\n",
      "(Iteration 8901 / 61240) loss: 1.926952\n",
      "(Iteration 9001 / 61240) loss: 1.907205\n",
      "(Iteration 9101 / 61240) loss: 1.881459\n",
      "(Epoch 6 / 40) train acc: 0.436000; val_acc: 0.426000\n",
      "(Iteration 9201 / 61240) loss: 2.314919\n",
      "(Iteration 9301 / 61240) loss: 1.741256\n",
      "(Iteration 9401 / 61240) loss: 2.007571\n",
      "(Iteration 9501 / 61240) loss: 1.910840\n",
      "(Iteration 9601 / 61240) loss: 1.788545\n",
      "(Iteration 9701 / 61240) loss: 1.891534\n",
      "(Iteration 9801 / 61240) loss: 2.020474\n",
      "(Iteration 9901 / 61240) loss: 1.946495\n",
      "(Iteration 10001 / 61240) loss: 1.993770\n",
      "(Iteration 10101 / 61240) loss: 1.839594\n",
      "(Iteration 10201 / 61240) loss: 2.039344\n",
      "(Iteration 10301 / 61240) loss: 1.972033\n",
      "(Iteration 10401 / 61240) loss: 1.965747\n",
      "(Iteration 10501 / 61240) loss: 1.888963\n",
      "(Iteration 10601 / 61240) loss: 1.952005\n",
      "(Iteration 10701 / 61240) loss: 1.768329\n",
      "(Epoch 7 / 40) train acc: 0.413000; val_acc: 0.414000\n",
      "(Iteration 10801 / 61240) loss: 1.949740\n",
      "(Iteration 10901 / 61240) loss: 1.995289\n",
      "(Iteration 11001 / 61240) loss: 2.101817\n",
      "(Iteration 11101 / 61240) loss: 2.043203\n",
      "(Iteration 11201 / 61240) loss: 2.081034\n",
      "(Iteration 11301 / 61240) loss: 1.859529\n",
      "(Iteration 11401 / 61240) loss: 2.208859\n",
      "(Iteration 11501 / 61240) loss: 1.827915\n",
      "(Iteration 11601 / 61240) loss: 2.104453\n",
      "(Iteration 11701 / 61240) loss: 1.895478\n",
      "(Iteration 11801 / 61240) loss: 1.937550\n",
      "(Iteration 11901 / 61240) loss: 1.968554\n",
      "(Iteration 12001 / 61240) loss: 2.070589\n",
      "(Iteration 12101 / 61240) loss: 1.823950\n",
      "(Iteration 12201 / 61240) loss: 1.915288\n",
      "(Epoch 8 / 40) train acc: 0.453000; val_acc: 0.426000\n",
      "(Iteration 12301 / 61240) loss: 1.922213\n",
      "(Iteration 12401 / 61240) loss: 2.036047\n",
      "(Iteration 12501 / 61240) loss: 1.900657\n",
      "(Iteration 12601 / 61240) loss: 1.763885\n",
      "(Iteration 12701 / 61240) loss: 1.947455\n",
      "(Iteration 12801 / 61240) loss: 2.167779\n",
      "(Iteration 12901 / 61240) loss: 1.937284\n",
      "(Iteration 13001 / 61240) loss: 1.992201\n",
      "(Iteration 13101 / 61240) loss: 1.849517\n",
      "(Iteration 13201 / 61240) loss: 1.848513\n",
      "(Iteration 13301 / 61240) loss: 1.767070\n",
      "(Iteration 13401 / 61240) loss: 2.046718\n",
      "(Iteration 13501 / 61240) loss: 1.802381\n",
      "(Iteration 13601 / 61240) loss: 2.069555\n",
      "(Iteration 13701 / 61240) loss: 1.913747\n",
      "(Epoch 9 / 40) train acc: 0.403000; val_acc: 0.425000\n",
      "(Iteration 13801 / 61240) loss: 1.740333\n",
      "(Iteration 13901 / 61240) loss: 1.985249\n",
      "(Iteration 14001 / 61240) loss: 1.874027\n",
      "(Iteration 14101 / 61240) loss: 1.929901\n",
      "(Iteration 14201 / 61240) loss: 1.983639\n",
      "(Iteration 14301 / 61240) loss: 1.932625\n",
      "(Iteration 14401 / 61240) loss: 2.134319\n",
      "(Iteration 14501 / 61240) loss: 2.071513\n",
      "(Iteration 14601 / 61240) loss: 1.853148\n",
      "(Iteration 14701 / 61240) loss: 1.851941\n",
      "(Iteration 14801 / 61240) loss: 2.001329\n",
      "(Iteration 14901 / 61240) loss: 1.832184\n",
      "(Iteration 15001 / 61240) loss: 2.030711\n",
      "(Iteration 15101 / 61240) loss: 2.073956\n",
      "(Iteration 15201 / 61240) loss: 1.907395\n",
      "(Iteration 15301 / 61240) loss: 1.976128\n",
      "(Epoch 10 / 40) train acc: 0.427000; val_acc: 0.425000\n",
      "(Iteration 15401 / 61240) loss: 1.992598\n",
      "(Iteration 15501 / 61240) loss: 2.176557\n",
      "(Iteration 15601 / 61240) loss: 1.885858\n",
      "(Iteration 15701 / 61240) loss: 1.867799\n",
      "(Iteration 15801 / 61240) loss: 1.773044\n",
      "(Iteration 15901 / 61240) loss: 2.003399\n",
      "(Iteration 16001 / 61240) loss: 1.888249\n",
      "(Iteration 16101 / 61240) loss: 1.962862\n",
      "(Iteration 16201 / 61240) loss: 1.896753\n",
      "(Iteration 16301 / 61240) loss: 2.068445\n",
      "(Iteration 16401 / 61240) loss: 2.069362\n",
      "(Iteration 16501 / 61240) loss: 2.013037\n",
      "(Iteration 16601 / 61240) loss: 1.816106\n",
      "(Iteration 16701 / 61240) loss: 1.848620\n",
      "(Iteration 16801 / 61240) loss: 1.879367\n",
      "(Epoch 11 / 40) train acc: 0.422000; val_acc: 0.433000\n",
      "(Iteration 16901 / 61240) loss: 1.868094\n",
      "(Iteration 17001 / 61240) loss: 1.979220\n",
      "(Iteration 17101 / 61240) loss: 2.042918\n",
      "(Iteration 17201 / 61240) loss: 2.098097\n",
      "(Iteration 17301 / 61240) loss: 2.033139\n",
      "(Iteration 17401 / 61240) loss: 1.755468\n",
      "(Iteration 17501 / 61240) loss: 1.929239\n",
      "(Iteration 17601 / 61240) loss: 1.954935\n",
      "(Iteration 17701 / 61240) loss: 2.048055\n",
      "(Iteration 17801 / 61240) loss: 2.066180\n",
      "(Iteration 17901 / 61240) loss: 1.867623\n",
      "(Iteration 18001 / 61240) loss: 1.887885\n",
      "(Iteration 18101 / 61240) loss: 1.870940\n",
      "(Iteration 18201 / 61240) loss: 2.022562\n",
      "(Iteration 18301 / 61240) loss: 1.992101\n",
      "(Epoch 12 / 40) train acc: 0.464000; val_acc: 0.427000\n",
      "(Iteration 18401 / 61240) loss: 1.866207\n",
      "(Iteration 18501 / 61240) loss: 1.934552\n",
      "(Iteration 18601 / 61240) loss: 1.888172\n",
      "(Iteration 18701 / 61240) loss: 1.994568\n",
      "(Iteration 18801 / 61240) loss: 2.074980\n",
      "(Iteration 18901 / 61240) loss: 2.032154\n",
      "(Iteration 19001 / 61240) loss: 2.146155\n",
      "(Iteration 19101 / 61240) loss: 1.951109\n",
      "(Iteration 19201 / 61240) loss: 2.109872\n",
      "(Iteration 19301 / 61240) loss: 1.889529\n",
      "(Iteration 19401 / 61240) loss: 1.860138\n",
      "(Iteration 19501 / 61240) loss: 1.967278\n",
      "(Iteration 19601 / 61240) loss: 2.092175\n",
      "(Iteration 19701 / 61240) loss: 1.939990\n",
      "(Iteration 19801 / 61240) loss: 1.862822\n",
      "(Iteration 19901 / 61240) loss: 1.866414\n",
      "(Epoch 13 / 40) train acc: 0.480000; val_acc: 0.429000\n",
      "(Iteration 20001 / 61240) loss: 1.934302\n",
      "(Iteration 20101 / 61240) loss: 1.899672\n",
      "(Iteration 20201 / 61240) loss: 2.040581\n",
      "(Iteration 20301 / 61240) loss: 2.089755\n",
      "(Iteration 20401 / 61240) loss: 1.997774\n",
      "(Iteration 20501 / 61240) loss: 2.020208\n",
      "(Iteration 20601 / 61240) loss: 2.001988\n",
      "(Iteration 20701 / 61240) loss: 2.080236\n",
      "(Iteration 20801 / 61240) loss: 1.849040\n",
      "(Iteration 20901 / 61240) loss: 1.982557\n",
      "(Iteration 21001 / 61240) loss: 1.841217\n",
      "(Iteration 21101 / 61240) loss: 1.995866\n",
      "(Iteration 21201 / 61240) loss: 1.887778\n",
      "(Iteration 21301 / 61240) loss: 1.813967\n",
      "(Iteration 21401 / 61240) loss: 1.882127\n",
      "(Epoch 14 / 40) train acc: 0.440000; val_acc: 0.429000\n",
      "(Iteration 21501 / 61240) loss: 1.958154\n",
      "(Iteration 21601 / 61240) loss: 1.886603\n",
      "(Iteration 21701 / 61240) loss: 2.098951\n",
      "(Iteration 21801 / 61240) loss: 2.087287\n",
      "(Iteration 21901 / 61240) loss: 1.670744\n",
      "(Iteration 22001 / 61240) loss: 1.939670\n",
      "(Iteration 22101 / 61240) loss: 1.943915\n",
      "(Iteration 22201 / 61240) loss: 1.800627\n",
      "(Iteration 22301 / 61240) loss: 1.930593\n",
      "(Iteration 22401 / 61240) loss: 1.859002\n",
      "(Iteration 22501 / 61240) loss: 1.726391\n",
      "(Iteration 22601 / 61240) loss: 1.816286\n",
      "(Iteration 22701 / 61240) loss: 1.988224\n",
      "(Iteration 22801 / 61240) loss: 1.967483\n",
      "(Iteration 22901 / 61240) loss: 1.914207\n",
      "(Epoch 15 / 40) train acc: 0.406000; val_acc: 0.428000\n",
      "(Iteration 23001 / 61240) loss: 1.996958\n",
      "(Iteration 23101 / 61240) loss: 1.940885\n",
      "(Iteration 23201 / 61240) loss: 2.023476\n",
      "(Iteration 23301 / 61240) loss: 1.770940\n",
      "(Iteration 23401 / 61240) loss: 1.967829\n",
      "(Iteration 23501 / 61240) loss: 2.070712\n",
      "(Iteration 23601 / 61240) loss: 2.134227\n",
      "(Iteration 23701 / 61240) loss: 1.814762\n",
      "(Iteration 23801 / 61240) loss: 1.928514\n",
      "(Iteration 23901 / 61240) loss: 1.886493\n",
      "(Iteration 24001 / 61240) loss: 1.835671\n",
      "(Iteration 24101 / 61240) loss: 2.087036\n",
      "(Iteration 24201 / 61240) loss: 2.003802\n",
      "(Iteration 24301 / 61240) loss: 1.988820\n",
      "(Iteration 24401 / 61240) loss: 2.029113\n",
      "(Epoch 16 / 40) train acc: 0.461000; val_acc: 0.441000\n",
      "(Iteration 24501 / 61240) loss: 1.803074\n",
      "(Iteration 24601 / 61240) loss: 1.827305\n",
      "(Iteration 24701 / 61240) loss: 1.899310\n",
      "(Iteration 24801 / 61240) loss: 1.909061\n",
      "(Iteration 24901 / 61240) loss: 2.143337\n",
      "(Iteration 25001 / 61240) loss: 2.061156\n",
      "(Iteration 25101 / 61240) loss: 1.932273\n",
      "(Iteration 25201 / 61240) loss: 1.846464\n",
      "(Iteration 25301 / 61240) loss: 1.949529\n",
      "(Iteration 25401 / 61240) loss: 1.881864\n",
      "(Iteration 25501 / 61240) loss: 1.941813\n",
      "(Iteration 25601 / 61240) loss: 1.923616\n",
      "(Iteration 25701 / 61240) loss: 1.843311\n",
      "(Iteration 25801 / 61240) loss: 1.789969\n",
      "(Iteration 25901 / 61240) loss: 1.923369\n",
      "(Iteration 26001 / 61240) loss: 1.949877\n",
      "(Epoch 17 / 40) train acc: 0.450000; val_acc: 0.440000\n",
      "(Iteration 26101 / 61240) loss: 2.022378\n",
      "(Iteration 26201 / 61240) loss: 1.964680\n",
      "(Iteration 26301 / 61240) loss: 2.042035\n",
      "(Iteration 26401 / 61240) loss: 1.961225\n",
      "(Iteration 26501 / 61240) loss: 2.128618\n",
      "(Iteration 26601 / 61240) loss: 1.897581\n",
      "(Iteration 26701 / 61240) loss: 1.999013\n",
      "(Iteration 26801 / 61240) loss: 2.147861\n",
      "(Iteration 26901 / 61240) loss: 2.022281\n",
      "(Iteration 27001 / 61240) loss: 1.897386\n",
      "(Iteration 27101 / 61240) loss: 1.726414\n",
      "(Iteration 27201 / 61240) loss: 1.890343\n",
      "(Iteration 27301 / 61240) loss: 1.940182\n",
      "(Iteration 27401 / 61240) loss: 2.109719\n",
      "(Iteration 27501 / 61240) loss: 1.918481\n",
      "(Epoch 18 / 40) train acc: 0.466000; val_acc: 0.433000\n",
      "(Iteration 27601 / 61240) loss: 1.929800\n",
      "(Iteration 27701 / 61240) loss: 2.074242\n",
      "(Iteration 27801 / 61240) loss: 1.985251\n",
      "(Iteration 27901 / 61240) loss: 1.962702\n",
      "(Iteration 28001 / 61240) loss: 1.797499\n",
      "(Iteration 28101 / 61240) loss: 1.888632\n",
      "(Iteration 28201 / 61240) loss: 1.919009\n",
      "(Iteration 28301 / 61240) loss: 1.980362\n",
      "(Iteration 28401 / 61240) loss: 1.928006\n",
      "(Iteration 28501 / 61240) loss: 1.965614\n",
      "(Iteration 28601 / 61240) loss: 1.771701\n",
      "(Iteration 28701 / 61240) loss: 1.823547\n",
      "(Iteration 28801 / 61240) loss: 2.056133\n",
      "(Iteration 28901 / 61240) loss: 1.904143\n",
      "(Iteration 29001 / 61240) loss: 2.186075\n",
      "(Epoch 19 / 40) train acc: 0.407000; val_acc: 0.438000\n",
      "(Iteration 29101 / 61240) loss: 2.052487\n",
      "(Iteration 29201 / 61240) loss: 1.972096\n",
      "(Iteration 29301 / 61240) loss: 1.911187\n",
      "(Iteration 29401 / 61240) loss: 2.092233\n",
      "(Iteration 29501 / 61240) loss: 1.819107\n",
      "(Iteration 29601 / 61240) loss: 2.028880\n",
      "(Iteration 29701 / 61240) loss: 1.934375\n",
      "(Iteration 29801 / 61240) loss: 1.911490\n",
      "(Iteration 29901 / 61240) loss: 1.910396\n",
      "(Iteration 30001 / 61240) loss: 2.038986\n",
      "(Iteration 30101 / 61240) loss: 2.047948\n",
      "(Iteration 30201 / 61240) loss: 1.983158\n",
      "(Iteration 30301 / 61240) loss: 1.973557\n",
      "(Iteration 30401 / 61240) loss: 1.940296\n",
      "(Iteration 30501 / 61240) loss: 1.991265\n",
      "(Iteration 30601 / 61240) loss: 1.948882\n",
      "(Epoch 20 / 40) train acc: 0.447000; val_acc: 0.432000\n",
      "(Iteration 30701 / 61240) loss: 2.037284\n",
      "(Iteration 30801 / 61240) loss: 1.746452\n",
      "(Iteration 30901 / 61240) loss: 1.984223\n",
      "(Iteration 31001 / 61240) loss: 1.964922\n",
      "(Iteration 31101 / 61240) loss: 1.861022\n",
      "(Iteration 31201 / 61240) loss: 1.988413\n",
      "(Iteration 31301 / 61240) loss: 1.886519\n",
      "(Iteration 31401 / 61240) loss: 1.950469\n",
      "(Iteration 31501 / 61240) loss: 1.822159\n",
      "(Iteration 31601 / 61240) loss: 1.826467\n",
      "(Iteration 31701 / 61240) loss: 1.740492\n",
      "(Iteration 31801 / 61240) loss: 2.085341\n",
      "(Iteration 31901 / 61240) loss: 2.048217\n",
      "(Iteration 32001 / 61240) loss: 1.792974\n",
      "(Iteration 32101 / 61240) loss: 1.984365\n",
      "(Epoch 21 / 40) train acc: 0.446000; val_acc: 0.431000\n",
      "(Iteration 32201 / 61240) loss: 1.791071\n",
      "(Iteration 32301 / 61240) loss: 1.968903\n",
      "(Iteration 32401 / 61240) loss: 1.847129\n",
      "(Iteration 32501 / 61240) loss: 2.076644\n",
      "(Iteration 32601 / 61240) loss: 1.854408\n",
      "(Iteration 32701 / 61240) loss: 1.904545\n",
      "(Iteration 32801 / 61240) loss: 2.053728\n",
      "(Iteration 32901 / 61240) loss: 1.957469\n",
      "(Iteration 33001 / 61240) loss: 1.851235\n",
      "(Iteration 33101 / 61240) loss: 1.926721\n",
      "(Iteration 33201 / 61240) loss: 2.005581\n",
      "(Iteration 33301 / 61240) loss: 1.902443\n",
      "(Iteration 33401 / 61240) loss: 1.828943\n",
      "(Iteration 33501 / 61240) loss: 1.847206\n",
      "(Iteration 33601 / 61240) loss: 1.954925\n",
      "(Epoch 22 / 40) train acc: 0.436000; val_acc: 0.434000\n",
      "(Iteration 33701 / 61240) loss: 1.976669\n",
      "(Iteration 33801 / 61240) loss: 2.027611\n",
      "(Iteration 33901 / 61240) loss: 2.037505\n",
      "(Iteration 34001 / 61240) loss: 1.828846\n",
      "(Iteration 34101 / 61240) loss: 1.899325\n",
      "(Iteration 34201 / 61240) loss: 1.806855\n",
      "(Iteration 34301 / 61240) loss: 1.863563\n",
      "(Iteration 34401 / 61240) loss: 1.887436\n",
      "(Iteration 34501 / 61240) loss: 1.963637\n",
      "(Iteration 34601 / 61240) loss: 2.129690\n",
      "(Iteration 34701 / 61240) loss: 1.997787\n",
      "(Iteration 34801 / 61240) loss: 2.035550\n",
      "(Iteration 34901 / 61240) loss: 1.840785\n",
      "(Iteration 35001 / 61240) loss: 1.995799\n",
      "(Iteration 35101 / 61240) loss: 1.747376\n",
      "(Iteration 35201 / 61240) loss: 2.013779\n",
      "(Epoch 23 / 40) train acc: 0.448000; val_acc: 0.436000\n",
      "(Iteration 35301 / 61240) loss: 1.923669\n",
      "(Iteration 35401 / 61240) loss: 2.258640\n",
      "(Iteration 35501 / 61240) loss: 2.027271\n",
      "(Iteration 35601 / 61240) loss: 2.100240\n",
      "(Iteration 35701 / 61240) loss: 2.084824\n",
      "(Iteration 35801 / 61240) loss: 1.931837\n",
      "(Iteration 35901 / 61240) loss: 2.014510\n",
      "(Iteration 36001 / 61240) loss: 2.148649\n",
      "(Iteration 36101 / 61240) loss: 2.012100\n",
      "(Iteration 36201 / 61240) loss: 1.982058\n",
      "(Iteration 36301 / 61240) loss: 1.825436\n",
      "(Iteration 36401 / 61240) loss: 2.046158\n",
      "(Iteration 36501 / 61240) loss: 1.853165\n",
      "(Iteration 36601 / 61240) loss: 1.822735\n",
      "(Iteration 36701 / 61240) loss: 1.989064\n",
      "(Epoch 24 / 40) train acc: 0.466000; val_acc: 0.431000\n",
      "(Iteration 36801 / 61240) loss: 1.793033\n",
      "(Iteration 36901 / 61240) loss: 2.025318\n",
      "(Iteration 37001 / 61240) loss: 1.821588\n",
      "(Iteration 37101 / 61240) loss: 1.948845\n",
      "(Iteration 37201 / 61240) loss: 2.053695\n",
      "(Iteration 37301 / 61240) loss: 1.846557\n",
      "(Iteration 37401 / 61240) loss: 1.976548\n",
      "(Iteration 37501 / 61240) loss: 2.028584\n",
      "(Iteration 37601 / 61240) loss: 1.843619\n",
      "(Iteration 37701 / 61240) loss: 1.895874\n",
      "(Iteration 37801 / 61240) loss: 2.029466\n",
      "(Iteration 37901 / 61240) loss: 1.936640\n",
      "(Iteration 38001 / 61240) loss: 1.797514\n",
      "(Iteration 38101 / 61240) loss: 2.037726\n",
      "(Iteration 38201 / 61240) loss: 2.297897\n",
      "(Epoch 25 / 40) train acc: 0.470000; val_acc: 0.436000\n",
      "(Iteration 38301 / 61240) loss: 2.002473\n",
      "(Iteration 38401 / 61240) loss: 1.879865\n",
      "(Iteration 38501 / 61240) loss: 1.975972\n",
      "(Iteration 38601 / 61240) loss: 1.950976\n",
      "(Iteration 38701 / 61240) loss: 1.970070\n",
      "(Iteration 38801 / 61240) loss: 2.034366\n",
      "(Iteration 38901 / 61240) loss: 1.771530\n",
      "(Iteration 39001 / 61240) loss: 1.942873\n",
      "(Iteration 39101 / 61240) loss: 1.974281\n",
      "(Iteration 39201 / 61240) loss: 1.959782\n",
      "(Iteration 39301 / 61240) loss: 1.935093\n",
      "(Iteration 39401 / 61240) loss: 1.804310\n",
      "(Iteration 39501 / 61240) loss: 1.880742\n",
      "(Iteration 39601 / 61240) loss: 1.957794\n",
      "(Iteration 39701 / 61240) loss: 1.885464\n",
      "(Iteration 39801 / 61240) loss: 1.952454\n",
      "(Epoch 26 / 40) train acc: 0.444000; val_acc: 0.434000\n",
      "(Iteration 39901 / 61240) loss: 2.097523\n",
      "(Iteration 40001 / 61240) loss: 2.115266\n",
      "(Iteration 40101 / 61240) loss: 1.878486\n",
      "(Iteration 40201 / 61240) loss: 2.022769\n",
      "(Iteration 40301 / 61240) loss: 1.909521\n",
      "(Iteration 40401 / 61240) loss: 1.893670\n",
      "(Iteration 40501 / 61240) loss: 1.873170\n",
      "(Iteration 40601 / 61240) loss: 1.794922\n",
      "(Iteration 40701 / 61240) loss: 2.071099\n",
      "(Iteration 40801 / 61240) loss: 1.888147\n",
      "(Iteration 40901 / 61240) loss: 2.141598\n",
      "(Iteration 41001 / 61240) loss: 1.905773\n",
      "(Iteration 41101 / 61240) loss: 1.936171\n",
      "(Iteration 41201 / 61240) loss: 2.015699\n",
      "(Iteration 41301 / 61240) loss: 2.011925\n",
      "(Epoch 27 / 40) train acc: 0.448000; val_acc: 0.439000\n",
      "(Iteration 41401 / 61240) loss: 1.832722\n",
      "(Iteration 41501 / 61240) loss: 2.014339\n",
      "(Iteration 41601 / 61240) loss: 1.898689\n",
      "(Iteration 41701 / 61240) loss: 2.015660\n",
      "(Iteration 41801 / 61240) loss: 2.008476\n",
      "(Iteration 41901 / 61240) loss: 1.843254\n",
      "(Iteration 42001 / 61240) loss: 2.071570\n",
      "(Iteration 42101 / 61240) loss: 1.996769\n",
      "(Iteration 42201 / 61240) loss: 2.173551\n",
      "(Iteration 42301 / 61240) loss: 1.809878\n",
      "(Iteration 42401 / 61240) loss: 2.033262\n",
      "(Iteration 42501 / 61240) loss: 1.853767\n",
      "(Iteration 42601 / 61240) loss: 2.058293\n",
      "(Iteration 42701 / 61240) loss: 2.120562\n",
      "(Iteration 42801 / 61240) loss: 2.034556\n",
      "(Epoch 28 / 40) train acc: 0.427000; val_acc: 0.435000\n",
      "(Iteration 42901 / 61240) loss: 2.018449\n",
      "(Iteration 43001 / 61240) loss: 1.993952\n",
      "(Iteration 43101 / 61240) loss: 2.025701\n",
      "(Iteration 43201 / 61240) loss: 1.830182\n",
      "(Iteration 43301 / 61240) loss: 1.909813\n",
      "(Iteration 43401 / 61240) loss: 1.969594\n",
      "(Iteration 43501 / 61240) loss: 1.924263\n",
      "(Iteration 43601 / 61240) loss: 1.950313\n",
      "(Iteration 43701 / 61240) loss: 1.878222\n",
      "(Iteration 43801 / 61240) loss: 1.979659\n",
      "(Iteration 43901 / 61240) loss: 1.726159\n",
      "(Iteration 44001 / 61240) loss: 1.932659\n",
      "(Iteration 44101 / 61240) loss: 1.943860\n",
      "(Iteration 44201 / 61240) loss: 1.904919\n",
      "(Iteration 44301 / 61240) loss: 2.040463\n",
      "(Epoch 29 / 40) train acc: 0.456000; val_acc: 0.437000\n",
      "(Iteration 44401 / 61240) loss: 1.986242\n",
      "(Iteration 44501 / 61240) loss: 1.972115\n",
      "(Iteration 44601 / 61240) loss: 1.847136\n",
      "(Iteration 44701 / 61240) loss: 1.893331\n",
      "(Iteration 44801 / 61240) loss: 2.026838\n",
      "(Iteration 44901 / 61240) loss: 1.995695\n",
      "(Iteration 45001 / 61240) loss: 1.911404\n",
      "(Iteration 45101 / 61240) loss: 2.018001\n",
      "(Iteration 45201 / 61240) loss: 1.885786\n",
      "(Iteration 45301 / 61240) loss: 2.017821\n",
      "(Iteration 45401 / 61240) loss: 1.891138\n",
      "(Iteration 45501 / 61240) loss: 1.962196\n",
      "(Iteration 45601 / 61240) loss: 1.940539\n",
      "(Iteration 45701 / 61240) loss: 1.955586\n",
      "(Iteration 45801 / 61240) loss: 1.922200\n",
      "(Iteration 45901 / 61240) loss: 1.962281\n",
      "(Epoch 30 / 40) train acc: 0.452000; val_acc: 0.436000\n",
      "(Iteration 46001 / 61240) loss: 2.011226\n",
      "(Iteration 46101 / 61240) loss: 2.047749\n",
      "(Iteration 46201 / 61240) loss: 1.859900\n",
      "(Iteration 46301 / 61240) loss: 2.038197\n",
      "(Iteration 46401 / 61240) loss: 1.922143\n",
      "(Iteration 46501 / 61240) loss: 1.954426\n",
      "(Iteration 46601 / 61240) loss: 2.004843\n",
      "(Iteration 46701 / 61240) loss: 1.690624\n",
      "(Iteration 46801 / 61240) loss: 1.910311\n",
      "(Iteration 46901 / 61240) loss: 2.057110\n",
      "(Iteration 47001 / 61240) loss: 1.873529\n",
      "(Iteration 47101 / 61240) loss: 1.973427\n",
      "(Iteration 47201 / 61240) loss: 1.895665\n",
      "(Iteration 47301 / 61240) loss: 1.905384\n",
      "(Iteration 47401 / 61240) loss: 1.952786\n",
      "(Epoch 31 / 40) train acc: 0.422000; val_acc: 0.437000\n",
      "(Iteration 47501 / 61240) loss: 1.782140\n",
      "(Iteration 47601 / 61240) loss: 2.018372\n",
      "(Iteration 47701 / 61240) loss: 1.984567\n",
      "(Iteration 47801 / 61240) loss: 1.897695\n",
      "(Iteration 47901 / 61240) loss: 1.774035\n",
      "(Iteration 48001 / 61240) loss: 1.943780\n",
      "(Iteration 48101 / 61240) loss: 2.252318\n",
      "(Iteration 48201 / 61240) loss: 2.009805\n",
      "(Iteration 48301 / 61240) loss: 1.837738\n",
      "(Iteration 48401 / 61240) loss: 1.858916\n",
      "(Iteration 48501 / 61240) loss: 1.955067\n",
      "(Iteration 48601 / 61240) loss: 2.027685\n",
      "(Iteration 48701 / 61240) loss: 2.034240\n",
      "(Iteration 48801 / 61240) loss: 2.031260\n",
      "(Iteration 48901 / 61240) loss: 1.849853\n",
      "(Epoch 32 / 40) train acc: 0.478000; val_acc: 0.436000\n",
      "(Iteration 49001 / 61240) loss: 2.144698\n",
      "(Iteration 49101 / 61240) loss: 1.912309\n",
      "(Iteration 49201 / 61240) loss: 1.871907\n",
      "(Iteration 49301 / 61240) loss: 1.963626\n",
      "(Iteration 49401 / 61240) loss: 1.877641\n",
      "(Iteration 49501 / 61240) loss: 2.047480\n",
      "(Iteration 49601 / 61240) loss: 1.921900\n",
      "(Iteration 49701 / 61240) loss: 1.903403\n",
      "(Iteration 49801 / 61240) loss: 2.013305\n",
      "(Iteration 49901 / 61240) loss: 2.128353\n",
      "(Iteration 50001 / 61240) loss: 1.977376\n",
      "(Iteration 50101 / 61240) loss: 2.034819\n",
      "(Iteration 50201 / 61240) loss: 2.040802\n",
      "(Iteration 50301 / 61240) loss: 2.040746\n",
      "(Iteration 50401 / 61240) loss: 2.155871\n",
      "(Iteration 50501 / 61240) loss: 1.873910\n",
      "(Epoch 33 / 40) train acc: 0.431000; val_acc: 0.435000\n",
      "(Iteration 50601 / 61240) loss: 2.124883\n",
      "(Iteration 50701 / 61240) loss: 1.924626\n",
      "(Iteration 50801 / 61240) loss: 2.036485\n",
      "(Iteration 50901 / 61240) loss: 1.929240\n",
      "(Iteration 51001 / 61240) loss: 1.988229\n",
      "(Iteration 51101 / 61240) loss: 1.945159\n",
      "(Iteration 51201 / 61240) loss: 2.058725\n",
      "(Iteration 51301 / 61240) loss: 1.977448\n",
      "(Iteration 51401 / 61240) loss: 1.926345\n",
      "(Iteration 51501 / 61240) loss: 1.838357\n",
      "(Iteration 51601 / 61240) loss: 1.940679\n",
      "(Iteration 51701 / 61240) loss: 2.169977\n",
      "(Iteration 51801 / 61240) loss: 1.870979\n",
      "(Iteration 51901 / 61240) loss: 1.977392\n",
      "(Iteration 52001 / 61240) loss: 2.098767\n",
      "(Epoch 34 / 40) train acc: 0.443000; val_acc: 0.439000\n",
      "(Iteration 52101 / 61240) loss: 1.842117\n",
      "(Iteration 52201 / 61240) loss: 1.929778\n",
      "(Iteration 52301 / 61240) loss: 2.133939\n",
      "(Iteration 52401 / 61240) loss: 1.862453\n",
      "(Iteration 52501 / 61240) loss: 1.959050\n",
      "(Iteration 52601 / 61240) loss: 2.010535\n",
      "(Iteration 52701 / 61240) loss: 1.961527\n",
      "(Iteration 52801 / 61240) loss: 1.939323\n",
      "(Iteration 52901 / 61240) loss: 2.026110\n",
      "(Iteration 53001 / 61240) loss: 1.821771\n",
      "(Iteration 53101 / 61240) loss: 1.910817\n",
      "(Iteration 53201 / 61240) loss: 1.913686\n",
      "(Iteration 53301 / 61240) loss: 1.874108\n",
      "(Iteration 53401 / 61240) loss: 1.826534\n",
      "(Iteration 53501 / 61240) loss: 1.713038\n",
      "(Epoch 35 / 40) train acc: 0.451000; val_acc: 0.437000\n",
      "(Iteration 53601 / 61240) loss: 1.714222\n",
      "(Iteration 53701 / 61240) loss: 2.140047\n",
      "(Iteration 53801 / 61240) loss: 1.951540\n",
      "(Iteration 53901 / 61240) loss: 1.888566\n",
      "(Iteration 54001 / 61240) loss: 1.826633\n",
      "(Iteration 54101 / 61240) loss: 1.796789\n",
      "(Iteration 54201 / 61240) loss: 1.939896\n",
      "(Iteration 54301 / 61240) loss: 1.963564\n",
      "(Iteration 54401 / 61240) loss: 1.989281\n",
      "(Iteration 54501 / 61240) loss: 1.804883\n",
      "(Iteration 54601 / 61240) loss: 1.867375\n",
      "(Iteration 54701 / 61240) loss: 1.928759\n",
      "(Iteration 54801 / 61240) loss: 1.752141\n",
      "(Iteration 54901 / 61240) loss: 2.039633\n",
      "(Iteration 55001 / 61240) loss: 2.045031\n",
      "(Iteration 55101 / 61240) loss: 2.010116\n",
      "(Epoch 36 / 40) train acc: 0.447000; val_acc: 0.438000\n",
      "(Iteration 55201 / 61240) loss: 1.934555\n",
      "(Iteration 55301 / 61240) loss: 2.027970\n",
      "(Iteration 55401 / 61240) loss: 1.782272\n",
      "(Iteration 55501 / 61240) loss: 1.883122\n",
      "(Iteration 55601 / 61240) loss: 1.975203\n",
      "(Iteration 55701 / 61240) loss: 1.857156\n",
      "(Iteration 55801 / 61240) loss: 2.000450\n",
      "(Iteration 55901 / 61240) loss: 1.993441\n",
      "(Iteration 56001 / 61240) loss: 1.990217\n",
      "(Iteration 56101 / 61240) loss: 1.847731\n",
      "(Iteration 56201 / 61240) loss: 2.077260\n",
      "(Iteration 56301 / 61240) loss: 1.728342\n",
      "(Iteration 56401 / 61240) loss: 1.716782\n",
      "(Iteration 56501 / 61240) loss: 1.863557\n",
      "(Iteration 56601 / 61240) loss: 2.052256\n",
      "(Epoch 37 / 40) train acc: 0.441000; val_acc: 0.429000\n",
      "(Iteration 56701 / 61240) loss: 1.886404\n",
      "(Iteration 56801 / 61240) loss: 1.953751\n",
      "(Iteration 56901 / 61240) loss: 2.068672\n",
      "(Iteration 57001 / 61240) loss: 1.945243\n",
      "(Iteration 57101 / 61240) loss: 1.965265\n",
      "(Iteration 57201 / 61240) loss: 1.829609\n",
      "(Iteration 57301 / 61240) loss: 1.736202\n",
      "(Iteration 57401 / 61240) loss: 1.827461\n",
      "(Iteration 57501 / 61240) loss: 1.905011\n",
      "(Iteration 57601 / 61240) loss: 2.002862\n",
      "(Iteration 57701 / 61240) loss: 1.830860\n",
      "(Iteration 57801 / 61240) loss: 2.059657\n",
      "(Iteration 57901 / 61240) loss: 1.864145\n",
      "(Iteration 58001 / 61240) loss: 2.000640\n",
      "(Iteration 58101 / 61240) loss: 2.085373\n",
      "(Epoch 38 / 40) train acc: 0.451000; val_acc: 0.433000\n",
      "(Iteration 58201 / 61240) loss: 1.662494\n",
      "(Iteration 58301 / 61240) loss: 2.247158\n",
      "(Iteration 58401 / 61240) loss: 2.022589\n",
      "(Iteration 58501 / 61240) loss: 1.787298\n",
      "(Iteration 58601 / 61240) loss: 1.762006\n",
      "(Iteration 58701 / 61240) loss: 1.971495\n",
      "(Iteration 58801 / 61240) loss: 1.868735\n",
      "(Iteration 58901 / 61240) loss: 1.995386\n",
      "(Iteration 59001 / 61240) loss: 1.782655\n",
      "(Iteration 59101 / 61240) loss: 2.109097\n",
      "(Iteration 59201 / 61240) loss: 1.945683\n",
      "(Iteration 59301 / 61240) loss: 2.001602\n",
      "(Iteration 59401 / 61240) loss: 1.947050\n",
      "(Iteration 59501 / 61240) loss: 1.927195\n",
      "(Iteration 59601 / 61240) loss: 1.852268\n",
      "(Iteration 59701 / 61240) loss: 1.994851\n",
      "(Epoch 39 / 40) train acc: 0.438000; val_acc: 0.435000\n",
      "(Iteration 59801 / 61240) loss: 1.994706\n",
      "(Iteration 59901 / 61240) loss: 1.926269\n",
      "(Iteration 60001 / 61240) loss: 1.844789\n",
      "(Iteration 60101 / 61240) loss: 1.925979\n",
      "(Iteration 60201 / 61240) loss: 2.011629\n",
      "(Iteration 60301 / 61240) loss: 2.093292\n",
      "(Iteration 60401 / 61240) loss: 1.976267\n",
      "(Iteration 60501 / 61240) loss: 1.940846\n",
      "(Iteration 60601 / 61240) loss: 1.874205\n",
      "(Iteration 60701 / 61240) loss: 1.871715\n",
      "(Iteration 60801 / 61240) loss: 1.863420\n",
      "(Iteration 60901 / 61240) loss: 1.834889\n",
      "(Iteration 61001 / 61240) loss: 2.190825\n",
      "(Iteration 61101 / 61240) loss: 2.100789\n",
      "(Iteration 61201 / 61240) loss: 1.806542\n",
      "(Epoch 40 / 40) train acc: 0.459000; val_acc: 0.438000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.01, 'num_epochs': 40, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 30600) loss: 2.303227\n",
      "(Epoch 0 / 40) train acc: 0.089000; val_acc: 0.108000\n",
      "(Iteration 101 / 30600) loss: 2.303564\n",
      "(Iteration 201 / 30600) loss: 2.302314\n",
      "(Iteration 301 / 30600) loss: 2.302445\n",
      "(Iteration 401 / 30600) loss: 2.301303\n",
      "(Iteration 501 / 30600) loss: 2.300078\n",
      "(Iteration 601 / 30600) loss: 2.297945\n",
      "(Iteration 701 / 30600) loss: 2.292987\n",
      "(Epoch 1 / 40) train acc: 0.283000; val_acc: 0.269000\n",
      "(Iteration 801 / 30600) loss: 2.280418\n",
      "(Iteration 901 / 30600) loss: 2.267662\n",
      "(Iteration 1001 / 30600) loss: 2.236141\n",
      "(Iteration 1101 / 30600) loss: 2.168676\n",
      "(Iteration 1201 / 30600) loss: 2.199438\n",
      "(Iteration 1301 / 30600) loss: 2.068876\n",
      "(Iteration 1401 / 30600) loss: 2.082797\n",
      "(Iteration 1501 / 30600) loss: 2.043433\n",
      "(Epoch 2 / 40) train acc: 0.285000; val_acc: 0.309000\n",
      "(Iteration 1601 / 30600) loss: 1.887914\n",
      "(Iteration 1701 / 30600) loss: 1.941278\n",
      "(Iteration 1801 / 30600) loss: 1.960074\n",
      "(Iteration 1901 / 30600) loss: 1.816097\n",
      "(Iteration 2001 / 30600) loss: 1.821833\n",
      "(Iteration 2101 / 30600) loss: 1.879575\n",
      "(Iteration 2201 / 30600) loss: 1.863054\n",
      "(Epoch 3 / 40) train acc: 0.379000; val_acc: 0.364000\n",
      "(Iteration 2301 / 30600) loss: 1.739370\n",
      "(Iteration 2401 / 30600) loss: 1.727163\n",
      "(Iteration 2501 / 30600) loss: 1.765282\n",
      "(Iteration 2601 / 30600) loss: 1.746602\n",
      "(Iteration 2701 / 30600) loss: 1.738407\n",
      "(Iteration 2801 / 30600) loss: 1.624470\n",
      "(Iteration 2901 / 30600) loss: 1.847561\n",
      "(Iteration 3001 / 30600) loss: 1.734896\n",
      "(Epoch 4 / 40) train acc: 0.427000; val_acc: 0.417000\n",
      "(Iteration 3101 / 30600) loss: 1.640275\n",
      "(Iteration 3201 / 30600) loss: 1.655785\n",
      "(Iteration 3301 / 30600) loss: 1.483254\n",
      "(Iteration 3401 / 30600) loss: 1.526979\n",
      "(Iteration 3501 / 30600) loss: 1.699628\n",
      "(Iteration 3601 / 30600) loss: 1.755462\n",
      "(Iteration 3701 / 30600) loss: 1.551313\n",
      "(Iteration 3801 / 30600) loss: 1.570283\n",
      "(Epoch 5 / 40) train acc: 0.459000; val_acc: 0.441000\n",
      "(Iteration 3901 / 30600) loss: 1.425039\n",
      "(Iteration 4001 / 30600) loss: 1.569320\n",
      "(Iteration 4101 / 30600) loss: 1.536430\n",
      "(Iteration 4201 / 30600) loss: 1.459756\n",
      "(Iteration 4301 / 30600) loss: 1.540689\n",
      "(Iteration 4401 / 30600) loss: 1.547898\n",
      "(Iteration 4501 / 30600) loss: 1.432552\n",
      "(Epoch 6 / 40) train acc: 0.463000; val_acc: 0.455000\n",
      "(Iteration 4601 / 30600) loss: 1.529902\n",
      "(Iteration 4701 / 30600) loss: 1.721633\n",
      "(Iteration 4801 / 30600) loss: 1.504013\n",
      "(Iteration 4901 / 30600) loss: 1.648168\n",
      "(Iteration 5001 / 30600) loss: 1.565195\n",
      "(Iteration 5101 / 30600) loss: 1.426288\n",
      "(Iteration 5201 / 30600) loss: 1.595124\n",
      "(Iteration 5301 / 30600) loss: 1.288933\n",
      "(Epoch 7 / 40) train acc: 0.483000; val_acc: 0.471000\n",
      "(Iteration 5401 / 30600) loss: 1.674759\n",
      "(Iteration 5501 / 30600) loss: 1.460501\n",
      "(Iteration 5601 / 30600) loss: 1.595209\n",
      "(Iteration 5701 / 30600) loss: 1.730058\n",
      "(Iteration 5801 / 30600) loss: 1.733772\n",
      "(Iteration 5901 / 30600) loss: 1.655378\n",
      "(Iteration 6001 / 30600) loss: 1.605549\n",
      "(Iteration 6101 / 30600) loss: 1.586561\n",
      "(Epoch 8 / 40) train acc: 0.499000; val_acc: 0.490000\n",
      "(Iteration 6201 / 30600) loss: 1.523447\n",
      "(Iteration 6301 / 30600) loss: 1.686218\n",
      "(Iteration 6401 / 30600) loss: 1.457858\n",
      "(Iteration 6501 / 30600) loss: 1.341939\n",
      "(Iteration 6601 / 30600) loss: 1.596574\n",
      "(Iteration 6701 / 30600) loss: 1.388928\n",
      "(Iteration 6801 / 30600) loss: 1.375000\n",
      "(Epoch 9 / 40) train acc: 0.503000; val_acc: 0.494000\n",
      "(Iteration 6901 / 30600) loss: 1.425028\n",
      "(Iteration 7001 / 30600) loss: 1.598901\n",
      "(Iteration 7101 / 30600) loss: 1.443771\n",
      "(Iteration 7201 / 30600) loss: 1.639150\n",
      "(Iteration 7301 / 30600) loss: 1.539305\n",
      "(Iteration 7401 / 30600) loss: 1.438327\n",
      "(Iteration 7501 / 30600) loss: 1.635554\n",
      "(Iteration 7601 / 30600) loss: 1.565063\n",
      "(Epoch 10 / 40) train acc: 0.513000; val_acc: 0.495000\n",
      "(Iteration 7701 / 30600) loss: 1.700739\n",
      "(Iteration 7801 / 30600) loss: 1.613552\n",
      "(Iteration 7901 / 30600) loss: 1.412718\n",
      "(Iteration 8001 / 30600) loss: 1.445598\n",
      "(Iteration 8101 / 30600) loss: 1.577734\n",
      "(Iteration 8201 / 30600) loss: 1.451337\n",
      "(Iteration 8301 / 30600) loss: 1.607248\n",
      "(Iteration 8401 / 30600) loss: 1.500755\n",
      "(Epoch 11 / 40) train acc: 0.510000; val_acc: 0.493000\n",
      "(Iteration 8501 / 30600) loss: 1.549291\n",
      "(Iteration 8601 / 30600) loss: 1.428246\n",
      "(Iteration 8701 / 30600) loss: 1.500957\n",
      "(Iteration 8801 / 30600) loss: 1.626935\n",
      "(Iteration 8901 / 30600) loss: 1.386753\n",
      "(Iteration 9001 / 30600) loss: 1.667688\n",
      "(Iteration 9101 / 30600) loss: 1.482978\n",
      "(Epoch 12 / 40) train acc: 0.472000; val_acc: 0.499000\n",
      "(Iteration 9201 / 30600) loss: 1.424219\n",
      "(Iteration 9301 / 30600) loss: 1.554242\n",
      "(Iteration 9401 / 30600) loss: 1.407133\n",
      "(Iteration 9501 / 30600) loss: 1.272679\n",
      "(Iteration 9601 / 30600) loss: 1.526891\n",
      "(Iteration 9701 / 30600) loss: 1.449942\n",
      "(Iteration 9801 / 30600) loss: 1.562357\n",
      "(Iteration 9901 / 30600) loss: 1.739988\n",
      "(Epoch 13 / 40) train acc: 0.523000; val_acc: 0.505000\n",
      "(Iteration 10001 / 30600) loss: 1.331230\n",
      "(Iteration 10101 / 30600) loss: 1.358632\n",
      "(Iteration 10201 / 30600) loss: 1.411452\n",
      "(Iteration 10301 / 30600) loss: 1.515723\n",
      "(Iteration 10401 / 30600) loss: 1.341965\n",
      "(Iteration 10501 / 30600) loss: 1.485905\n",
      "(Iteration 10601 / 30600) loss: 1.478622\n",
      "(Iteration 10701 / 30600) loss: 1.523854\n",
      "(Epoch 14 / 40) train acc: 0.501000; val_acc: 0.509000\n",
      "(Iteration 10801 / 30600) loss: 1.710924\n",
      "(Iteration 10901 / 30600) loss: 1.595311\n",
      "(Iteration 11001 / 30600) loss: 1.429559\n",
      "(Iteration 11101 / 30600) loss: 1.615573\n",
      "(Iteration 11201 / 30600) loss: 1.603322\n",
      "(Iteration 11301 / 30600) loss: 1.632105\n",
      "(Iteration 11401 / 30600) loss: 1.732110\n",
      "(Epoch 15 / 40) train acc: 0.535000; val_acc: 0.504000\n",
      "(Iteration 11501 / 30600) loss: 1.369970\n",
      "(Iteration 11601 / 30600) loss: 1.546048\n",
      "(Iteration 11701 / 30600) loss: 1.340978\n",
      "(Iteration 11801 / 30600) loss: 1.562189\n",
      "(Iteration 11901 / 30600) loss: 1.320167\n",
      "(Iteration 12001 / 30600) loss: 1.491707\n",
      "(Iteration 12101 / 30600) loss: 1.467259\n",
      "(Iteration 12201 / 30600) loss: 1.652711\n",
      "(Epoch 16 / 40) train acc: 0.506000; val_acc: 0.512000\n",
      "(Iteration 12301 / 30600) loss: 1.427623\n",
      "(Iteration 12401 / 30600) loss: 1.395760\n",
      "(Iteration 12501 / 30600) loss: 1.588922\n",
      "(Iteration 12601 / 30600) loss: 1.454155\n",
      "(Iteration 12701 / 30600) loss: 1.522229\n",
      "(Iteration 12801 / 30600) loss: 1.418909\n",
      "(Iteration 12901 / 30600) loss: 1.499598\n",
      "(Iteration 13001 / 30600) loss: 1.513705\n",
      "(Epoch 17 / 40) train acc: 0.527000; val_acc: 0.512000\n",
      "(Iteration 13101 / 30600) loss: 1.510223\n",
      "(Iteration 13201 / 30600) loss: 1.399380\n",
      "(Iteration 13301 / 30600) loss: 1.577602\n",
      "(Iteration 13401 / 30600) loss: 1.584569\n",
      "(Iteration 13501 / 30600) loss: 1.374970\n",
      "(Iteration 13601 / 30600) loss: 1.607841\n",
      "(Iteration 13701 / 30600) loss: 1.509009\n",
      "(Epoch 18 / 40) train acc: 0.508000; val_acc: 0.512000\n",
      "(Iteration 13801 / 30600) loss: 1.356355\n",
      "(Iteration 13901 / 30600) loss: 1.571271\n",
      "(Iteration 14001 / 30600) loss: 1.632208\n",
      "(Iteration 14101 / 30600) loss: 1.383427\n",
      "(Iteration 14201 / 30600) loss: 1.403263\n",
      "(Iteration 14301 / 30600) loss: 1.500712\n",
      "(Iteration 14401 / 30600) loss: 1.375947\n",
      "(Iteration 14501 / 30600) loss: 1.648265\n",
      "(Epoch 19 / 40) train acc: 0.497000; val_acc: 0.509000\n",
      "(Iteration 14601 / 30600) loss: 1.684105\n",
      "(Iteration 14701 / 30600) loss: 1.563243\n",
      "(Iteration 14801 / 30600) loss: 1.621965\n",
      "(Iteration 14901 / 30600) loss: 1.256766\n",
      "(Iteration 15001 / 30600) loss: 1.323834\n",
      "(Iteration 15101 / 30600) loss: 1.710017\n",
      "(Iteration 15201 / 30600) loss: 1.401650\n",
      "(Epoch 20 / 40) train acc: 0.513000; val_acc: 0.510000\n",
      "(Iteration 15301 / 30600) loss: 1.574965\n",
      "(Iteration 15401 / 30600) loss: 1.409152\n",
      "(Iteration 15501 / 30600) loss: 1.651933\n",
      "(Iteration 15601 / 30600) loss: 1.385975\n",
      "(Iteration 15701 / 30600) loss: 1.450343\n",
      "(Iteration 15801 / 30600) loss: 1.345408\n",
      "(Iteration 15901 / 30600) loss: 1.465754\n",
      "(Iteration 16001 / 30600) loss: 1.480252\n",
      "(Epoch 21 / 40) train acc: 0.519000; val_acc: 0.508000\n",
      "(Iteration 16101 / 30600) loss: 1.199920\n",
      "(Iteration 16201 / 30600) loss: 1.369771\n",
      "(Iteration 16301 / 30600) loss: 1.446402\n",
      "(Iteration 16401 / 30600) loss: 1.587559\n",
      "(Iteration 16501 / 30600) loss: 1.579610\n",
      "(Iteration 16601 / 30600) loss: 1.479699\n",
      "(Iteration 16701 / 30600) loss: 1.342700\n",
      "(Iteration 16801 / 30600) loss: 1.606515\n",
      "(Epoch 22 / 40) train acc: 0.510000; val_acc: 0.507000\n",
      "(Iteration 16901 / 30600) loss: 1.431927\n",
      "(Iteration 17001 / 30600) loss: 1.209023\n",
      "(Iteration 17101 / 30600) loss: 1.242547\n",
      "(Iteration 17201 / 30600) loss: 1.396578\n",
      "(Iteration 17301 / 30600) loss: 1.543042\n",
      "(Iteration 17401 / 30600) loss: 1.666388\n",
      "(Iteration 17501 / 30600) loss: 1.527483\n",
      "(Epoch 23 / 40) train acc: 0.523000; val_acc: 0.506000\n",
      "(Iteration 17601 / 30600) loss: 1.525440\n",
      "(Iteration 17701 / 30600) loss: 1.326391\n",
      "(Iteration 17801 / 30600) loss: 1.513679\n",
      "(Iteration 17901 / 30600) loss: 1.344001\n",
      "(Iteration 18001 / 30600) loss: 1.340812\n",
      "(Iteration 18101 / 30600) loss: 1.500469\n",
      "(Iteration 18201 / 30600) loss: 1.613716\n",
      "(Iteration 18301 / 30600) loss: 1.610103\n",
      "(Epoch 24 / 40) train acc: 0.517000; val_acc: 0.505000\n",
      "(Iteration 18401 / 30600) loss: 1.442021\n",
      "(Iteration 18501 / 30600) loss: 1.371472\n",
      "(Iteration 18601 / 30600) loss: 1.396922\n",
      "(Iteration 18701 / 30600) loss: 1.526754\n",
      "(Iteration 18801 / 30600) loss: 1.343277\n",
      "(Iteration 18901 / 30600) loss: 1.341247\n",
      "(Iteration 19001 / 30600) loss: 1.590603\n",
      "(Iteration 19101 / 30600) loss: 1.302063\n",
      "(Epoch 25 / 40) train acc: 0.513000; val_acc: 0.504000\n",
      "(Iteration 19201 / 30600) loss: 1.375183\n",
      "(Iteration 19301 / 30600) loss: 1.288204\n",
      "(Iteration 19401 / 30600) loss: 1.560659\n",
      "(Iteration 19501 / 30600) loss: 1.573480\n",
      "(Iteration 19601 / 30600) loss: 1.307204\n",
      "(Iteration 19701 / 30600) loss: 1.454476\n",
      "(Iteration 19801 / 30600) loss: 1.405014\n",
      "(Epoch 26 / 40) train acc: 0.546000; val_acc: 0.507000\n",
      "(Iteration 19901 / 30600) loss: 1.314580\n",
      "(Iteration 20001 / 30600) loss: 1.513463\n",
      "(Iteration 20101 / 30600) loss: 1.621746\n",
      "(Iteration 20201 / 30600) loss: 1.503804\n",
      "(Iteration 20301 / 30600) loss: 1.487357\n",
      "(Iteration 20401 / 30600) loss: 1.526654\n",
      "(Iteration 20501 / 30600) loss: 1.471097\n",
      "(Iteration 20601 / 30600) loss: 1.545808\n",
      "(Epoch 27 / 40) train acc: 0.512000; val_acc: 0.504000\n",
      "(Iteration 20701 / 30600) loss: 1.680838\n",
      "(Iteration 20801 / 30600) loss: 1.547833\n",
      "(Iteration 20901 / 30600) loss: 1.352889\n",
      "(Iteration 21001 / 30600) loss: 1.597242\n",
      "(Iteration 21101 / 30600) loss: 1.571737\n",
      "(Iteration 21201 / 30600) loss: 1.726650\n",
      "(Iteration 21301 / 30600) loss: 1.322579\n",
      "(Iteration 21401 / 30600) loss: 1.312372\n",
      "(Epoch 28 / 40) train acc: 0.527000; val_acc: 0.504000\n",
      "(Iteration 21501 / 30600) loss: 1.567725\n",
      "(Iteration 21601 / 30600) loss: 1.395405\n",
      "(Iteration 21701 / 30600) loss: 1.489732\n",
      "(Iteration 21801 / 30600) loss: 1.608060\n",
      "(Iteration 21901 / 30600) loss: 1.278182\n",
      "(Iteration 22001 / 30600) loss: 1.417081\n",
      "(Iteration 22101 / 30600) loss: 1.369050\n",
      "(Epoch 29 / 40) train acc: 0.490000; val_acc: 0.506000\n",
      "(Iteration 22201 / 30600) loss: 1.440018\n",
      "(Iteration 22301 / 30600) loss: 1.375468\n",
      "(Iteration 22401 / 30600) loss: 1.262607\n",
      "(Iteration 22501 / 30600) loss: 1.415120\n",
      "(Iteration 22601 / 30600) loss: 1.371773\n",
      "(Iteration 22701 / 30600) loss: 1.537987\n",
      "(Iteration 22801 / 30600) loss: 1.610159\n",
      "(Iteration 22901 / 30600) loss: 1.442109\n",
      "(Epoch 30 / 40) train acc: 0.539000; val_acc: 0.506000\n",
      "(Iteration 23001 / 30600) loss: 1.384340\n",
      "(Iteration 23101 / 30600) loss: 1.377272\n",
      "(Iteration 23201 / 30600) loss: 1.546074\n",
      "(Iteration 23301 / 30600) loss: 1.658825\n",
      "(Iteration 23401 / 30600) loss: 1.426795\n",
      "(Iteration 23501 / 30600) loss: 1.474086\n",
      "(Iteration 23601 / 30600) loss: 1.228247\n",
      "(Iteration 23701 / 30600) loss: 1.344832\n",
      "(Epoch 31 / 40) train acc: 0.541000; val_acc: 0.505000\n",
      "(Iteration 23801 / 30600) loss: 1.611927\n",
      "(Iteration 23901 / 30600) loss: 1.417284\n",
      "(Iteration 24001 / 30600) loss: 1.436618\n",
      "(Iteration 24101 / 30600) loss: 1.453370\n",
      "(Iteration 24201 / 30600) loss: 1.480011\n",
      "(Iteration 24301 / 30600) loss: 1.322723\n",
      "(Iteration 24401 / 30600) loss: 1.723277\n",
      "(Epoch 32 / 40) train acc: 0.513000; val_acc: 0.506000\n",
      "(Iteration 24501 / 30600) loss: 1.541769\n",
      "(Iteration 24601 / 30600) loss: 1.413713\n",
      "(Iteration 24701 / 30600) loss: 1.499031\n",
      "(Iteration 24801 / 30600) loss: 1.487517\n",
      "(Iteration 24901 / 30600) loss: 1.361215\n",
      "(Iteration 25001 / 30600) loss: 1.614684\n",
      "(Iteration 25101 / 30600) loss: 1.410062\n",
      "(Iteration 25201 / 30600) loss: 1.486480\n",
      "(Epoch 33 / 40) train acc: 0.520000; val_acc: 0.507000\n",
      "(Iteration 25301 / 30600) loss: 1.428276\n",
      "(Iteration 25401 / 30600) loss: 1.388493\n",
      "(Iteration 25501 / 30600) loss: 1.309684\n",
      "(Iteration 25601 / 30600) loss: 1.506368\n",
      "(Iteration 25701 / 30600) loss: 1.673266\n",
      "(Iteration 25801 / 30600) loss: 1.436047\n",
      "(Iteration 25901 / 30600) loss: 1.547508\n",
      "(Iteration 26001 / 30600) loss: 1.567288\n",
      "(Epoch 34 / 40) train acc: 0.527000; val_acc: 0.507000\n",
      "(Iteration 26101 / 30600) loss: 1.389750\n",
      "(Iteration 26201 / 30600) loss: 1.609912\n",
      "(Iteration 26301 / 30600) loss: 1.424461\n",
      "(Iteration 26401 / 30600) loss: 1.435280\n",
      "(Iteration 26501 / 30600) loss: 1.349467\n",
      "(Iteration 26601 / 30600) loss: 1.350182\n",
      "(Iteration 26701 / 30600) loss: 1.364881\n",
      "(Epoch 35 / 40) train acc: 0.523000; val_acc: 0.506000\n",
      "(Iteration 26801 / 30600) loss: 1.660589\n",
      "(Iteration 26901 / 30600) loss: 1.522321\n",
      "(Iteration 27001 / 30600) loss: 1.442023\n",
      "(Iteration 27101 / 30600) loss: 1.464052\n",
      "(Iteration 27201 / 30600) loss: 1.523098\n",
      "(Iteration 27301 / 30600) loss: 1.590253\n",
      "(Iteration 27401 / 30600) loss: 1.386904\n",
      "(Iteration 27501 / 30600) loss: 1.399774\n",
      "(Epoch 36 / 40) train acc: 0.567000; val_acc: 0.505000\n",
      "(Iteration 27601 / 30600) loss: 1.482740\n",
      "(Iteration 27701 / 30600) loss: 1.547078\n",
      "(Iteration 27801 / 30600) loss: 1.569231\n",
      "(Iteration 27901 / 30600) loss: 1.380239\n",
      "(Iteration 28001 / 30600) loss: 1.249534\n",
      "(Iteration 28101 / 30600) loss: 1.485250\n",
      "(Iteration 28201 / 30600) loss: 1.430415\n",
      "(Iteration 28301 / 30600) loss: 1.353206\n",
      "(Epoch 37 / 40) train acc: 0.539000; val_acc: 0.506000\n",
      "(Iteration 28401 / 30600) loss: 1.439991\n",
      "(Iteration 28501 / 30600) loss: 1.568745\n",
      "(Iteration 28601 / 30600) loss: 1.519107\n",
      "(Iteration 28701 / 30600) loss: 1.545054\n",
      "(Iteration 28801 / 30600) loss: 1.478310\n",
      "(Iteration 28901 / 30600) loss: 1.520794\n",
      "(Iteration 29001 / 30600) loss: 1.437763\n",
      "(Epoch 38 / 40) train acc: 0.499000; val_acc: 0.506000\n",
      "(Iteration 29101 / 30600) loss: 1.585605\n",
      "(Iteration 29201 / 30600) loss: 1.412640\n",
      "(Iteration 29301 / 30600) loss: 1.486059\n",
      "(Iteration 29401 / 30600) loss: 1.524939\n",
      "(Iteration 29501 / 30600) loss: 1.400881\n",
      "(Iteration 29601 / 30600) loss: 1.623201\n",
      "(Iteration 29701 / 30600) loss: 1.626190\n",
      "(Iteration 29801 / 30600) loss: 1.496969\n",
      "(Epoch 39 / 40) train acc: 0.529000; val_acc: 0.508000\n",
      "(Iteration 29901 / 30600) loss: 1.439058\n",
      "(Iteration 30001 / 30600) loss: 1.469467\n",
      "(Iteration 30101 / 30600) loss: 1.339094\n",
      "(Iteration 30201 / 30600) loss: 1.300202\n",
      "(Iteration 30301 / 30600) loss: 1.659965\n",
      "(Iteration 30401 / 30600) loss: 1.553825\n",
      "(Iteration 30501 / 30600) loss: 1.475227\n",
      "(Epoch 40 / 40) train acc: 0.529000; val_acc: 0.508000\n",
      "Training with parameters: {'hidden_size': 700, 'learning_rate': 0.01, 'num_epochs': 40, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 61240) loss: 2.303191\n",
      "(Epoch 0 / 40) train acc: 0.096000; val_acc: 0.077000\n",
      "(Iteration 101 / 61240) loss: 2.302486\n",
      "(Iteration 201 / 61240) loss: 2.301627\n",
      "(Iteration 301 / 61240) loss: 2.301057\n",
      "(Iteration 401 / 61240) loss: 2.301913\n",
      "(Iteration 501 / 61240) loss: 2.297320\n",
      "(Iteration 601 / 61240) loss: 2.293742\n",
      "(Iteration 701 / 61240) loss: 2.285605\n",
      "(Iteration 801 / 61240) loss: 2.272245\n",
      "(Iteration 901 / 61240) loss: 2.261552\n",
      "(Iteration 1001 / 61240) loss: 2.249557\n",
      "(Iteration 1101 / 61240) loss: 2.117690\n",
      "(Iteration 1201 / 61240) loss: 2.130822\n",
      "(Iteration 1301 / 61240) loss: 1.997742\n",
      "(Iteration 1401 / 61240) loss: 2.057778\n",
      "(Iteration 1501 / 61240) loss: 1.849095\n",
      "(Epoch 1 / 40) train acc: 0.264000; val_acc: 0.286000\n",
      "(Iteration 1601 / 61240) loss: 1.901669\n",
      "(Iteration 1701 / 61240) loss: 2.045153\n",
      "(Iteration 1801 / 61240) loss: 1.997171\n",
      "(Iteration 1901 / 61240) loss: 1.874181\n",
      "(Iteration 2001 / 61240) loss: 1.936851\n",
      "(Iteration 2101 / 61240) loss: 1.624953\n",
      "(Iteration 2201 / 61240) loss: 1.854543\n",
      "(Iteration 2301 / 61240) loss: 1.773752\n",
      "(Iteration 2401 / 61240) loss: 1.661774\n",
      "(Iteration 2501 / 61240) loss: 1.607552\n",
      "(Iteration 2601 / 61240) loss: 1.922457\n",
      "(Iteration 2701 / 61240) loss: 1.551854\n",
      "(Iteration 2801 / 61240) loss: 1.727059\n",
      "(Iteration 2901 / 61240) loss: 1.473512\n",
      "(Iteration 3001 / 61240) loss: 1.604996\n",
      "(Epoch 2 / 40) train acc: 0.441000; val_acc: 0.422000\n",
      "(Iteration 3101 / 61240) loss: 1.796804\n",
      "(Iteration 3201 / 61240) loss: 1.730744\n",
      "(Iteration 3301 / 61240) loss: 1.373266\n",
      "(Iteration 3401 / 61240) loss: 1.413487\n",
      "(Iteration 3501 / 61240) loss: 1.486663\n",
      "(Iteration 3601 / 61240) loss: 1.789928\n",
      "(Iteration 3701 / 61240) loss: 1.565168\n",
      "(Iteration 3801 / 61240) loss: 1.229850\n",
      "(Iteration 3901 / 61240) loss: 1.371955\n",
      "(Iteration 4001 / 61240) loss: 1.443591\n",
      "(Iteration 4101 / 61240) loss: 1.299888\n",
      "(Iteration 4201 / 61240) loss: 1.336514\n",
      "(Iteration 4301 / 61240) loss: 1.774112\n",
      "(Iteration 4401 / 61240) loss: 1.627413\n",
      "(Iteration 4501 / 61240) loss: 1.506404\n",
      "(Epoch 3 / 40) train acc: 0.512000; val_acc: 0.477000\n",
      "(Iteration 4601 / 61240) loss: 1.685465\n",
      "(Iteration 4701 / 61240) loss: 1.391035\n",
      "(Iteration 4801 / 61240) loss: 1.407863\n",
      "(Iteration 4901 / 61240) loss: 1.305628\n",
      "(Iteration 5001 / 61240) loss: 1.787000\n",
      "(Iteration 5101 / 61240) loss: 1.438555\n",
      "(Iteration 5201 / 61240) loss: 1.621848\n",
      "(Iteration 5301 / 61240) loss: 1.593836\n",
      "(Iteration 5401 / 61240) loss: 1.294393\n",
      "(Iteration 5501 / 61240) loss: 1.728889\n",
      "(Iteration 5601 / 61240) loss: 1.480803\n",
      "(Iteration 5701 / 61240) loss: 1.374744\n",
      "(Iteration 5801 / 61240) loss: 1.418065\n",
      "(Iteration 5901 / 61240) loss: 1.363200\n",
      "(Iteration 6001 / 61240) loss: 1.547591\n",
      "(Iteration 6101 / 61240) loss: 1.565300\n",
      "(Epoch 4 / 40) train acc: 0.507000; val_acc: 0.504000\n",
      "(Iteration 6201 / 61240) loss: 1.331673\n",
      "(Iteration 6301 / 61240) loss: 1.749513\n",
      "(Iteration 6401 / 61240) loss: 1.662332\n",
      "(Iteration 6501 / 61240) loss: 1.807172\n",
      "(Iteration 6601 / 61240) loss: 1.568732\n",
      "(Iteration 6701 / 61240) loss: 1.506608\n",
      "(Iteration 6801 / 61240) loss: 1.419086\n",
      "(Iteration 6901 / 61240) loss: 1.398985\n",
      "(Iteration 7001 / 61240) loss: 1.328672\n",
      "(Iteration 7101 / 61240) loss: 1.704360\n",
      "(Iteration 7201 / 61240) loss: 1.311906\n",
      "(Iteration 7301 / 61240) loss: 1.624883\n",
      "(Iteration 7401 / 61240) loss: 1.501260\n",
      "(Iteration 7501 / 61240) loss: 1.280971\n",
      "(Iteration 7601 / 61240) loss: 1.897141\n",
      "(Epoch 5 / 40) train acc: 0.520000; val_acc: 0.511000\n",
      "(Iteration 7701 / 61240) loss: 1.674429\n",
      "(Iteration 7801 / 61240) loss: 1.805856\n",
      "(Iteration 7901 / 61240) loss: 1.442532\n",
      "(Iteration 8001 / 61240) loss: 1.534429\n",
      "(Iteration 8101 / 61240) loss: 1.437695\n",
      "(Iteration 8201 / 61240) loss: 1.365775\n",
      "(Iteration 8301 / 61240) loss: 1.739670\n",
      "(Iteration 8401 / 61240) loss: 1.481963\n",
      "(Iteration 8501 / 61240) loss: 1.551046\n",
      "(Iteration 8601 / 61240) loss: 1.434178\n",
      "(Iteration 8701 / 61240) loss: 1.405536\n",
      "(Iteration 8801 / 61240) loss: 1.013861\n",
      "(Iteration 8901 / 61240) loss: 1.604631\n",
      "(Iteration 9001 / 61240) loss: 1.433756\n",
      "(Iteration 9101 / 61240) loss: 1.555813\n",
      "(Epoch 6 / 40) train acc: 0.510000; val_acc: 0.511000\n",
      "(Iteration 9201 / 61240) loss: 1.301983\n",
      "(Iteration 9301 / 61240) loss: 1.319458\n",
      "(Iteration 9401 / 61240) loss: 1.455994\n",
      "(Iteration 9501 / 61240) loss: 1.371602\n",
      "(Iteration 9601 / 61240) loss: 1.462468\n",
      "(Iteration 9701 / 61240) loss: 1.426356\n",
      "(Iteration 9801 / 61240) loss: 1.090060\n",
      "(Iteration 9901 / 61240) loss: 1.856747\n",
      "(Iteration 10001 / 61240) loss: 1.456277\n",
      "(Iteration 10101 / 61240) loss: 1.656441\n",
      "(Iteration 10201 / 61240) loss: 1.365831\n",
      "(Iteration 10301 / 61240) loss: 1.310560\n",
      "(Iteration 10401 / 61240) loss: 1.592451\n",
      "(Iteration 10501 / 61240) loss: 1.382428\n",
      "(Iteration 10601 / 61240) loss: 1.413624\n",
      "(Iteration 10701 / 61240) loss: 1.220399\n",
      "(Epoch 7 / 40) train acc: 0.514000; val_acc: 0.513000\n",
      "(Iteration 10801 / 61240) loss: 1.557502\n",
      "(Iteration 10901 / 61240) loss: 1.396274\n",
      "(Iteration 11001 / 61240) loss: 1.411108\n",
      "(Iteration 11101 / 61240) loss: 1.570551\n",
      "(Iteration 11201 / 61240) loss: 1.404982\n",
      "(Iteration 11301 / 61240) loss: 1.554352\n",
      "(Iteration 11401 / 61240) loss: 1.620754\n",
      "(Iteration 11501 / 61240) loss: 1.425006\n",
      "(Iteration 11601 / 61240) loss: 1.479683\n",
      "(Iteration 11701 / 61240) loss: 1.411519\n",
      "(Iteration 11801 / 61240) loss: 1.322312\n",
      "(Iteration 11901 / 61240) loss: 1.575793\n",
      "(Iteration 12001 / 61240) loss: 1.440230\n",
      "(Iteration 12101 / 61240) loss: 1.469269\n",
      "(Iteration 12201 / 61240) loss: 1.775848\n",
      "(Epoch 8 / 40) train acc: 0.548000; val_acc: 0.516000\n",
      "(Iteration 12301 / 61240) loss: 1.999007\n",
      "(Iteration 12401 / 61240) loss: 1.789781\n",
      "(Iteration 12501 / 61240) loss: 1.407850\n",
      "(Iteration 12601 / 61240) loss: 1.534291\n",
      "(Iteration 12701 / 61240) loss: 1.459125\n",
      "(Iteration 12801 / 61240) loss: 1.510297\n",
      "(Iteration 12901 / 61240) loss: 1.372745\n",
      "(Iteration 13001 / 61240) loss: 1.312438\n",
      "(Iteration 13101 / 61240) loss: 1.737155\n",
      "(Iteration 13201 / 61240) loss: 1.739259\n",
      "(Iteration 13301 / 61240) loss: 1.486217\n",
      "(Iteration 13401 / 61240) loss: 1.494580\n",
      "(Iteration 13501 / 61240) loss: 1.359895\n",
      "(Iteration 13601 / 61240) loss: 1.423448\n",
      "(Iteration 13701 / 61240) loss: 1.418583\n",
      "(Epoch 9 / 40) train acc: 0.548000; val_acc: 0.511000\n",
      "(Iteration 13801 / 61240) loss: 1.767112\n",
      "(Iteration 13901 / 61240) loss: 1.626856\n",
      "(Iteration 14001 / 61240) loss: 1.672587\n",
      "(Iteration 14101 / 61240) loss: 1.560120\n",
      "(Iteration 14201 / 61240) loss: 1.466458\n",
      "(Iteration 14301 / 61240) loss: 1.672630\n",
      "(Iteration 14401 / 61240) loss: 1.413610\n",
      "(Iteration 14501 / 61240) loss: 1.537198\n",
      "(Iteration 14601 / 61240) loss: 1.549869\n",
      "(Iteration 14701 / 61240) loss: 1.265408\n",
      "(Iteration 14801 / 61240) loss: 1.398650\n",
      "(Iteration 14901 / 61240) loss: 1.480426\n",
      "(Iteration 15001 / 61240) loss: 1.178223\n",
      "(Iteration 15101 / 61240) loss: 1.614281\n",
      "(Iteration 15201 / 61240) loss: 1.440656\n",
      "(Iteration 15301 / 61240) loss: 1.443676\n",
      "(Epoch 10 / 40) train acc: 0.564000; val_acc: 0.517000\n",
      "(Iteration 15401 / 61240) loss: 1.322354\n",
      "(Iteration 15501 / 61240) loss: 1.707846\n",
      "(Iteration 15601 / 61240) loss: 1.614555\n",
      "(Iteration 15701 / 61240) loss: 1.369443\n",
      "(Iteration 15801 / 61240) loss: 1.692461\n",
      "(Iteration 15901 / 61240) loss: 1.055725\n",
      "(Iteration 16001 / 61240) loss: 1.253169\n",
      "(Iteration 16101 / 61240) loss: 1.322092\n",
      "(Iteration 16201 / 61240) loss: 1.431005\n",
      "(Iteration 16301 / 61240) loss: 1.520878\n",
      "(Iteration 16401 / 61240) loss: 1.304544\n",
      "(Iteration 16501 / 61240) loss: 1.627048\n",
      "(Iteration 16601 / 61240) loss: 1.389032\n",
      "(Iteration 16701 / 61240) loss: 1.271398\n",
      "(Iteration 16801 / 61240) loss: 1.332615\n",
      "(Epoch 11 / 40) train acc: 0.550000; val_acc: 0.518000\n",
      "(Iteration 16901 / 61240) loss: 1.399195\n",
      "(Iteration 17001 / 61240) loss: 1.240835\n",
      "(Iteration 17101 / 61240) loss: 1.851773\n",
      "(Iteration 17201 / 61240) loss: 1.272941\n",
      "(Iteration 17301 / 61240) loss: 1.220190\n",
      "(Iteration 17401 / 61240) loss: 1.478935\n",
      "(Iteration 17501 / 61240) loss: 1.430797\n",
      "(Iteration 17601 / 61240) loss: 1.656996\n",
      "(Iteration 17701 / 61240) loss: 1.507627\n",
      "(Iteration 17801 / 61240) loss: 1.666157\n",
      "(Iteration 17901 / 61240) loss: 1.566262\n",
      "(Iteration 18001 / 61240) loss: 1.493801\n",
      "(Iteration 18101 / 61240) loss: 1.622292\n",
      "(Iteration 18201 / 61240) loss: 1.390206\n",
      "(Iteration 18301 / 61240) loss: 1.294319\n",
      "(Epoch 12 / 40) train acc: 0.551000; val_acc: 0.517000\n",
      "(Iteration 18401 / 61240) loss: 1.401815\n",
      "(Iteration 18501 / 61240) loss: 1.356146\n",
      "(Iteration 18601 / 61240) loss: 1.040194\n",
      "(Iteration 18701 / 61240) loss: 1.272388\n",
      "(Iteration 18801 / 61240) loss: 1.616869\n",
      "(Iteration 18901 / 61240) loss: 1.351062\n",
      "(Iteration 19001 / 61240) loss: 1.560099\n",
      "(Iteration 19101 / 61240) loss: 1.542593\n",
      "(Iteration 19201 / 61240) loss: 1.189660\n",
      "(Iteration 19301 / 61240) loss: 1.585668\n",
      "(Iteration 19401 / 61240) loss: 1.623809\n",
      "(Iteration 19501 / 61240) loss: 1.613341\n",
      "(Iteration 19601 / 61240) loss: 1.226149\n",
      "(Iteration 19701 / 61240) loss: 1.504550\n",
      "(Iteration 19801 / 61240) loss: 1.363733\n",
      "(Iteration 19901 / 61240) loss: 1.369119\n",
      "(Epoch 13 / 40) train acc: 0.516000; val_acc: 0.518000\n",
      "(Iteration 20001 / 61240) loss: 1.660686\n",
      "(Iteration 20101 / 61240) loss: 1.537504\n",
      "(Iteration 20201 / 61240) loss: 1.487729\n",
      "(Iteration 20301 / 61240) loss: 1.689138\n",
      "(Iteration 20401 / 61240) loss: 1.298748\n",
      "(Iteration 20501 / 61240) loss: 1.563083\n",
      "(Iteration 20601 / 61240) loss: 1.266131\n",
      "(Iteration 20701 / 61240) loss: 1.363702\n",
      "(Iteration 20801 / 61240) loss: 1.686574\n",
      "(Iteration 20901 / 61240) loss: 1.544948\n",
      "(Iteration 21001 / 61240) loss: 1.200304\n",
      "(Iteration 21101 / 61240) loss: 1.425086\n",
      "(Iteration 21201 / 61240) loss: 1.527503\n",
      "(Iteration 21301 / 61240) loss: 1.608465\n",
      "(Iteration 21401 / 61240) loss: 1.626255\n",
      "(Epoch 14 / 40) train acc: 0.537000; val_acc: 0.518000\n",
      "(Iteration 21501 / 61240) loss: 1.691083\n",
      "(Iteration 21601 / 61240) loss: 1.653543\n",
      "(Iteration 21701 / 61240) loss: 1.355799\n",
      "(Iteration 21801 / 61240) loss: 1.421662\n",
      "(Iteration 21901 / 61240) loss: 1.543561\n",
      "(Iteration 22001 / 61240) loss: 1.956186\n",
      "(Iteration 22101 / 61240) loss: 1.427911\n",
      "(Iteration 22201 / 61240) loss: 1.220541\n",
      "(Iteration 22301 / 61240) loss: 1.655946\n",
      "(Iteration 22401 / 61240) loss: 1.628964\n",
      "(Iteration 22501 / 61240) loss: 1.118893\n",
      "(Iteration 22601 / 61240) loss: 1.079391\n",
      "(Iteration 22701 / 61240) loss: 1.400427\n",
      "(Iteration 22801 / 61240) loss: 1.322547\n",
      "(Iteration 22901 / 61240) loss: 1.514604\n",
      "(Epoch 15 / 40) train acc: 0.540000; val_acc: 0.521000\n",
      "(Iteration 23001 / 61240) loss: 1.904350\n",
      "(Iteration 23101 / 61240) loss: 1.515290\n",
      "(Iteration 23201 / 61240) loss: 1.350762\n",
      "(Iteration 23301 / 61240) loss: 1.337981\n",
      "(Iteration 23401 / 61240) loss: 1.301399\n",
      "(Iteration 23501 / 61240) loss: 1.573630\n",
      "(Iteration 23601 / 61240) loss: 1.260259\n",
      "(Iteration 23701 / 61240) loss: 1.387468\n",
      "(Iteration 23801 / 61240) loss: 1.361426\n",
      "(Iteration 23901 / 61240) loss: 1.406799\n",
      "(Iteration 24001 / 61240) loss: 1.637712\n",
      "(Iteration 24101 / 61240) loss: 1.585924\n",
      "(Iteration 24201 / 61240) loss: 1.069644\n",
      "(Iteration 24301 / 61240) loss: 1.251171\n",
      "(Iteration 24401 / 61240) loss: 1.697342\n",
      "(Epoch 16 / 40) train acc: 0.574000; val_acc: 0.519000\n",
      "(Iteration 24501 / 61240) loss: 1.294875\n",
      "(Iteration 24601 / 61240) loss: 1.540620\n",
      "(Iteration 24701 / 61240) loss: 1.635183\n",
      "(Iteration 24801 / 61240) loss: 1.363342\n",
      "(Iteration 24901 / 61240) loss: 1.520573\n",
      "(Iteration 25001 / 61240) loss: 1.543464\n",
      "(Iteration 25101 / 61240) loss: 1.417865\n",
      "(Iteration 25201 / 61240) loss: 1.343684\n",
      "(Iteration 25301 / 61240) loss: 1.451363\n",
      "(Iteration 25401 / 61240) loss: 1.435530\n",
      "(Iteration 25501 / 61240) loss: 1.580948\n",
      "(Iteration 25601 / 61240) loss: 1.539692\n",
      "(Iteration 25701 / 61240) loss: 1.483356\n",
      "(Iteration 25801 / 61240) loss: 1.167186\n",
      "(Iteration 25901 / 61240) loss: 1.348303\n",
      "(Iteration 26001 / 61240) loss: 1.604005\n",
      "(Epoch 17 / 40) train acc: 0.537000; val_acc: 0.514000\n",
      "(Iteration 26101 / 61240) loss: 1.498462\n",
      "(Iteration 26201 / 61240) loss: 1.091733\n",
      "(Iteration 26301 / 61240) loss: 1.287404\n",
      "(Iteration 26401 / 61240) loss: 1.346734\n",
      "(Iteration 26501 / 61240) loss: 1.480300\n",
      "(Iteration 26601 / 61240) loss: 1.521652\n",
      "(Iteration 26701 / 61240) loss: 1.284137\n",
      "(Iteration 26801 / 61240) loss: 1.289905\n",
      "(Iteration 26901 / 61240) loss: 1.681395\n",
      "(Iteration 27001 / 61240) loss: 1.566205\n",
      "(Iteration 27101 / 61240) loss: 1.550263\n",
      "(Iteration 27201 / 61240) loss: 1.281253\n",
      "(Iteration 27301 / 61240) loss: 1.066155\n",
      "(Iteration 27401 / 61240) loss: 1.652053\n",
      "(Iteration 27501 / 61240) loss: 1.403924\n",
      "(Epoch 18 / 40) train acc: 0.535000; val_acc: 0.513000\n",
      "(Iteration 27601 / 61240) loss: 1.359460\n",
      "(Iteration 27701 / 61240) loss: 1.334812\n",
      "(Iteration 27801 / 61240) loss: 1.629961\n",
      "(Iteration 27901 / 61240) loss: 1.393663\n",
      "(Iteration 28001 / 61240) loss: 1.524957\n",
      "(Iteration 28101 / 61240) loss: 1.598618\n",
      "(Iteration 28201 / 61240) loss: 1.751289\n",
      "(Iteration 28301 / 61240) loss: 1.671190\n",
      "(Iteration 28401 / 61240) loss: 1.668722\n",
      "(Iteration 28501 / 61240) loss: 1.387768\n",
      "(Iteration 28601 / 61240) loss: 1.402629\n",
      "(Iteration 28701 / 61240) loss: 1.662339\n",
      "(Iteration 28801 / 61240) loss: 1.583341\n",
      "(Iteration 28901 / 61240) loss: 1.403450\n",
      "(Iteration 29001 / 61240) loss: 1.300753\n",
      "(Epoch 19 / 40) train acc: 0.527000; val_acc: 0.515000\n",
      "(Iteration 29101 / 61240) loss: 1.398443\n",
      "(Iteration 29201 / 61240) loss: 1.621040\n",
      "(Iteration 29301 / 61240) loss: 1.252736\n",
      "(Iteration 29401 / 61240) loss: 1.480181\n",
      "(Iteration 29501 / 61240) loss: 1.310012\n",
      "(Iteration 29601 / 61240) loss: 1.687831\n",
      "(Iteration 29701 / 61240) loss: 1.461397\n",
      "(Iteration 29801 / 61240) loss: 1.388446\n",
      "(Iteration 29901 / 61240) loss: 1.317467\n",
      "(Iteration 30001 / 61240) loss: 1.594649\n",
      "(Iteration 30101 / 61240) loss: 1.634536\n",
      "(Iteration 30201 / 61240) loss: 1.315163\n",
      "(Iteration 30301 / 61240) loss: 1.939932\n",
      "(Iteration 30401 / 61240) loss: 1.455305\n",
      "(Iteration 30501 / 61240) loss: 1.456520\n",
      "(Iteration 30601 / 61240) loss: 1.271164\n",
      "(Epoch 20 / 40) train acc: 0.566000; val_acc: 0.518000\n",
      "(Iteration 30701 / 61240) loss: 1.547330\n",
      "(Iteration 30801 / 61240) loss: 1.405212\n",
      "(Iteration 30901 / 61240) loss: 1.398642\n",
      "(Iteration 31001 / 61240) loss: 1.416067\n",
      "(Iteration 31101 / 61240) loss: 1.469572\n",
      "(Iteration 31201 / 61240) loss: 1.395536\n",
      "(Iteration 31301 / 61240) loss: 1.581749\n",
      "(Iteration 31401 / 61240) loss: 1.470804\n",
      "(Iteration 31501 / 61240) loss: 1.368749\n",
      "(Iteration 31601 / 61240) loss: 1.517133\n",
      "(Iteration 31701 / 61240) loss: 1.569006\n",
      "(Iteration 31801 / 61240) loss: 1.561102\n",
      "(Iteration 31901 / 61240) loss: 1.389254\n",
      "(Iteration 32001 / 61240) loss: 1.295057\n",
      "(Iteration 32101 / 61240) loss: 1.517082\n",
      "(Epoch 21 / 40) train acc: 0.529000; val_acc: 0.517000\n",
      "(Iteration 32201 / 61240) loss: 1.274826\n",
      "(Iteration 32301 / 61240) loss: 1.422398\n",
      "(Iteration 32401 / 61240) loss: 1.260161\n",
      "(Iteration 32501 / 61240) loss: 1.413410\n",
      "(Iteration 32601 / 61240) loss: 1.530433\n",
      "(Iteration 32701 / 61240) loss: 1.518087\n",
      "(Iteration 32801 / 61240) loss: 1.393779\n",
      "(Iteration 32901 / 61240) loss: 1.364428\n",
      "(Iteration 33001 / 61240) loss: 1.288213\n",
      "(Iteration 33101 / 61240) loss: 1.218749\n",
      "(Iteration 33201 / 61240) loss: 1.492686\n",
      "(Iteration 33301 / 61240) loss: 1.394303\n",
      "(Iteration 33401 / 61240) loss: 1.409753\n",
      "(Iteration 33501 / 61240) loss: 1.267174\n",
      "(Iteration 33601 / 61240) loss: 1.364478\n",
      "(Epoch 22 / 40) train acc: 0.547000; val_acc: 0.513000\n",
      "(Iteration 33701 / 61240) loss: 1.610186\n",
      "(Iteration 33801 / 61240) loss: 1.569688\n",
      "(Iteration 33901 / 61240) loss: 1.379963\n",
      "(Iteration 34001 / 61240) loss: 1.311939\n",
      "(Iteration 34101 / 61240) loss: 1.607586\n",
      "(Iteration 34201 / 61240) loss: 1.530658\n",
      "(Iteration 34301 / 61240) loss: 1.607297\n",
      "(Iteration 34401 / 61240) loss: 1.407339\n",
      "(Iteration 34501 / 61240) loss: 1.360418\n",
      "(Iteration 34601 / 61240) loss: 1.384897\n",
      "(Iteration 34701 / 61240) loss: 1.500890\n",
      "(Iteration 34801 / 61240) loss: 1.374851\n",
      "(Iteration 34901 / 61240) loss: 1.453159\n",
      "(Iteration 35001 / 61240) loss: 1.650108\n",
      "(Iteration 35101 / 61240) loss: 1.663177\n",
      "(Iteration 35201 / 61240) loss: 1.240831\n",
      "(Epoch 23 / 40) train acc: 0.558000; val_acc: 0.520000\n",
      "(Iteration 35301 / 61240) loss: 1.654030\n",
      "(Iteration 35401 / 61240) loss: 1.294222\n",
      "(Iteration 35501 / 61240) loss: 1.346127\n",
      "(Iteration 35601 / 61240) loss: 1.478129\n",
      "(Iteration 35701 / 61240) loss: 1.707059\n",
      "(Iteration 35801 / 61240) loss: 1.900363\n",
      "(Iteration 35901 / 61240) loss: 1.546511\n",
      "(Iteration 36001 / 61240) loss: 1.732245\n",
      "(Iteration 36101 / 61240) loss: 1.425508\n",
      "(Iteration 36201 / 61240) loss: 1.282065\n",
      "(Iteration 36301 / 61240) loss: 1.598905\n",
      "(Iteration 36401 / 61240) loss: 1.313869\n",
      "(Iteration 36501 / 61240) loss: 1.331238\n",
      "(Iteration 36601 / 61240) loss: 1.322547\n",
      "(Iteration 36701 / 61240) loss: 1.447186\n",
      "(Epoch 24 / 40) train acc: 0.527000; val_acc: 0.523000\n",
      "(Iteration 36801 / 61240) loss: 1.516456\n",
      "(Iteration 36901 / 61240) loss: 1.207679\n",
      "(Iteration 37001 / 61240) loss: 1.491839\n",
      "(Iteration 37101 / 61240) loss: 1.272956\n",
      "(Iteration 37201 / 61240) loss: 1.358714\n",
      "(Iteration 37301 / 61240) loss: 1.448636\n",
      "(Iteration 37401 / 61240) loss: 1.222409\n",
      "(Iteration 37501 / 61240) loss: 1.113553\n",
      "(Iteration 37601 / 61240) loss: 1.307712\n",
      "(Iteration 37701 / 61240) loss: 1.611525\n",
      "(Iteration 37801 / 61240) loss: 1.101570\n",
      "(Iteration 37901 / 61240) loss: 1.343258\n",
      "(Iteration 38001 / 61240) loss: 1.236263\n",
      "(Iteration 38101 / 61240) loss: 1.601044\n",
      "(Iteration 38201 / 61240) loss: 1.451654\n",
      "(Epoch 25 / 40) train acc: 0.517000; val_acc: 0.519000\n",
      "(Iteration 38301 / 61240) loss: 1.647725\n",
      "(Iteration 38401 / 61240) loss: 1.627535\n",
      "(Iteration 38501 / 61240) loss: 1.477891\n",
      "(Iteration 38601 / 61240) loss: 1.370138\n",
      "(Iteration 38701 / 61240) loss: 1.209587\n",
      "(Iteration 38801 / 61240) loss: 1.824293\n",
      "(Iteration 38901 / 61240) loss: 1.176321\n",
      "(Iteration 39001 / 61240) loss: 1.479018\n",
      "(Iteration 39101 / 61240) loss: 1.426831\n",
      "(Iteration 39201 / 61240) loss: 1.282931\n",
      "(Iteration 39301 / 61240) loss: 1.383202\n",
      "(Iteration 39401 / 61240) loss: 1.485015\n",
      "(Iteration 39501 / 61240) loss: 1.398524\n",
      "(Iteration 39601 / 61240) loss: 1.296195\n",
      "(Iteration 39701 / 61240) loss: 1.850839\n",
      "(Iteration 39801 / 61240) loss: 1.560010\n",
      "(Epoch 26 / 40) train acc: 0.550000; val_acc: 0.523000\n",
      "(Iteration 39901 / 61240) loss: 1.463402\n",
      "(Iteration 40001 / 61240) loss: 1.624804\n",
      "(Iteration 40101 / 61240) loss: 1.523964\n",
      "(Iteration 40201 / 61240) loss: 1.290709\n",
      "(Iteration 40301 / 61240) loss: 1.530946\n",
      "(Iteration 40401 / 61240) loss: 1.555021\n",
      "(Iteration 40501 / 61240) loss: 1.317135\n",
      "(Iteration 40601 / 61240) loss: 1.460738\n",
      "(Iteration 40701 / 61240) loss: 1.367620\n",
      "(Iteration 40801 / 61240) loss: 1.414150\n",
      "(Iteration 40901 / 61240) loss: 1.430945\n",
      "(Iteration 41001 / 61240) loss: 1.393787\n",
      "(Iteration 41101 / 61240) loss: 1.298216\n",
      "(Iteration 41201 / 61240) loss: 1.477510\n",
      "(Iteration 41301 / 61240) loss: 1.313196\n",
      "(Epoch 27 / 40) train acc: 0.542000; val_acc: 0.518000\n",
      "(Iteration 41401 / 61240) loss: 1.615799\n",
      "(Iteration 41501 / 61240) loss: 1.447968\n",
      "(Iteration 41601 / 61240) loss: 1.582867\n",
      "(Iteration 41701 / 61240) loss: 1.385063\n",
      "(Iteration 41801 / 61240) loss: 1.615685\n",
      "(Iteration 41901 / 61240) loss: 1.153343\n",
      "(Iteration 42001 / 61240) loss: 1.534455\n",
      "(Iteration 42101 / 61240) loss: 1.476283\n",
      "(Iteration 42201 / 61240) loss: 1.435634\n",
      "(Iteration 42301 / 61240) loss: 1.320977\n",
      "(Iteration 42401 / 61240) loss: 1.299622\n",
      "(Iteration 42501 / 61240) loss: 1.395715\n",
      "(Iteration 42601 / 61240) loss: 1.450617\n",
      "(Iteration 42701 / 61240) loss: 1.530064\n",
      "(Iteration 42801 / 61240) loss: 1.507662\n",
      "(Epoch 28 / 40) train acc: 0.546000; val_acc: 0.523000\n",
      "(Iteration 42901 / 61240) loss: 1.535937\n",
      "(Iteration 43001 / 61240) loss: 1.400153\n",
      "(Iteration 43101 / 61240) loss: 1.507326\n",
      "(Iteration 43201 / 61240) loss: 1.506792\n",
      "(Iteration 43301 / 61240) loss: 1.700200\n",
      "(Iteration 43401 / 61240) loss: 1.638997\n",
      "(Iteration 43501 / 61240) loss: 1.638091\n",
      "(Iteration 43601 / 61240) loss: 1.421505\n",
      "(Iteration 43701 / 61240) loss: 1.561196\n",
      "(Iteration 43801 / 61240) loss: 1.305741\n",
      "(Iteration 43901 / 61240) loss: 1.557910\n",
      "(Iteration 44001 / 61240) loss: 1.112664\n",
      "(Iteration 44101 / 61240) loss: 1.433569\n",
      "(Iteration 44201 / 61240) loss: 1.502278\n",
      "(Iteration 44301 / 61240) loss: 1.413682\n",
      "(Epoch 29 / 40) train acc: 0.548000; val_acc: 0.523000\n",
      "(Iteration 44401 / 61240) loss: 1.423001\n",
      "(Iteration 44501 / 61240) loss: 1.498981\n",
      "(Iteration 44601 / 61240) loss: 1.617149\n",
      "(Iteration 44701 / 61240) loss: 1.572942\n",
      "(Iteration 44801 / 61240) loss: 1.307135\n",
      "(Iteration 44901 / 61240) loss: 1.401983\n",
      "(Iteration 45001 / 61240) loss: 1.339339\n",
      "(Iteration 45101 / 61240) loss: 1.275596\n",
      "(Iteration 45201 / 61240) loss: 1.001144\n",
      "(Iteration 45301 / 61240) loss: 1.983680\n",
      "(Iteration 45401 / 61240) loss: 1.298982\n",
      "(Iteration 45501 / 61240) loss: 1.552241\n",
      "(Iteration 45601 / 61240) loss: 1.429965\n",
      "(Iteration 45701 / 61240) loss: 1.262982\n",
      "(Iteration 45801 / 61240) loss: 1.345071\n",
      "(Iteration 45901 / 61240) loss: 1.199666\n",
      "(Epoch 30 / 40) train acc: 0.558000; val_acc: 0.524000\n",
      "(Iteration 46001 / 61240) loss: 1.503880\n",
      "(Iteration 46101 / 61240) loss: 1.674086\n",
      "(Iteration 46201 / 61240) loss: 1.235472\n",
      "(Iteration 46301 / 61240) loss: 1.247216\n",
      "(Iteration 46401 / 61240) loss: 1.344655\n",
      "(Iteration 46501 / 61240) loss: 1.319189\n",
      "(Iteration 46601 / 61240) loss: 1.793551\n",
      "(Iteration 46701 / 61240) loss: 1.329014\n",
      "(Iteration 46801 / 61240) loss: 1.260407\n",
      "(Iteration 46901 / 61240) loss: 1.413542\n",
      "(Iteration 47001 / 61240) loss: 1.523791\n",
      "(Iteration 47101 / 61240) loss: 1.623552\n",
      "(Iteration 47201 / 61240) loss: 1.460930\n",
      "(Iteration 47301 / 61240) loss: 1.589863\n",
      "(Iteration 47401 / 61240) loss: 1.372113\n",
      "(Epoch 31 / 40) train acc: 0.543000; val_acc: 0.525000\n",
      "(Iteration 47501 / 61240) loss: 1.881930\n",
      "(Iteration 47601 / 61240) loss: 1.793398\n",
      "(Iteration 47701 / 61240) loss: 1.262106\n",
      "(Iteration 47801 / 61240) loss: 1.550886\n",
      "(Iteration 47901 / 61240) loss: 1.825888\n",
      "(Iteration 48001 / 61240) loss: 1.686199\n",
      "(Iteration 48101 / 61240) loss: 1.648816\n",
      "(Iteration 48201 / 61240) loss: 1.469428\n",
      "(Iteration 48301 / 61240) loss: 1.536478\n",
      "(Iteration 48401 / 61240) loss: 1.394838\n",
      "(Iteration 48501 / 61240) loss: 1.545488\n",
      "(Iteration 48601 / 61240) loss: 1.549195\n",
      "(Iteration 48701 / 61240) loss: 1.293862\n",
      "(Iteration 48801 / 61240) loss: 1.749425\n",
      "(Iteration 48901 / 61240) loss: 1.293304\n",
      "(Epoch 32 / 40) train acc: 0.530000; val_acc: 0.521000\n",
      "(Iteration 49001 / 61240) loss: 1.600063\n",
      "(Iteration 49101 / 61240) loss: 1.651073\n",
      "(Iteration 49201 / 61240) loss: 1.021329\n",
      "(Iteration 49301 / 61240) loss: 1.389120\n",
      "(Iteration 49401 / 61240) loss: 1.223207\n",
      "(Iteration 49501 / 61240) loss: 1.434769\n",
      "(Iteration 49601 / 61240) loss: 1.219574\n",
      "(Iteration 49701 / 61240) loss: 1.298622\n",
      "(Iteration 49801 / 61240) loss: 1.202990\n",
      "(Iteration 49901 / 61240) loss: 1.254903\n",
      "(Iteration 50001 / 61240) loss: 1.238527\n",
      "(Iteration 50101 / 61240) loss: 1.339208\n",
      "(Iteration 50201 / 61240) loss: 1.385759\n",
      "(Iteration 50301 / 61240) loss: 1.463745\n",
      "(Iteration 50401 / 61240) loss: 1.235645\n",
      "(Iteration 50501 / 61240) loss: 1.374945\n",
      "(Epoch 33 / 40) train acc: 0.546000; val_acc: 0.525000\n",
      "(Iteration 50601 / 61240) loss: 1.256859\n",
      "(Iteration 50701 / 61240) loss: 1.225644\n",
      "(Iteration 50801 / 61240) loss: 1.694143\n",
      "(Iteration 50901 / 61240) loss: 1.150494\n",
      "(Iteration 51001 / 61240) loss: 1.399554\n",
      "(Iteration 51101 / 61240) loss: 1.360163\n",
      "(Iteration 51201 / 61240) loss: 1.197234\n",
      "(Iteration 51301 / 61240) loss: 1.331453\n",
      "(Iteration 51401 / 61240) loss: 1.445804\n",
      "(Iteration 51501 / 61240) loss: 1.666307\n",
      "(Iteration 51601 / 61240) loss: 1.463241\n",
      "(Iteration 51701 / 61240) loss: 1.448969\n",
      "(Iteration 51801 / 61240) loss: 1.225952\n",
      "(Iteration 51901 / 61240) loss: 1.431075\n",
      "(Iteration 52001 / 61240) loss: 1.440958\n",
      "(Epoch 34 / 40) train acc: 0.548000; val_acc: 0.523000\n",
      "(Iteration 52101 / 61240) loss: 1.446131\n",
      "(Iteration 52201 / 61240) loss: 1.772497\n",
      "(Iteration 52301 / 61240) loss: 1.676401\n",
      "(Iteration 52401 / 61240) loss: 1.397179\n",
      "(Iteration 52501 / 61240) loss: 1.210627\n",
      "(Iteration 52601 / 61240) loss: 1.880090\n",
      "(Iteration 52701 / 61240) loss: 1.374780\n",
      "(Iteration 52801 / 61240) loss: 1.445468\n",
      "(Iteration 52901 / 61240) loss: 1.577632\n",
      "(Iteration 53001 / 61240) loss: 1.280008\n",
      "(Iteration 53101 / 61240) loss: 1.452712\n",
      "(Iteration 53201 / 61240) loss: 1.667214\n",
      "(Iteration 53301 / 61240) loss: 1.495574\n",
      "(Iteration 53401 / 61240) loss: 1.504647\n",
      "(Iteration 53501 / 61240) loss: 1.479230\n",
      "(Epoch 35 / 40) train acc: 0.543000; val_acc: 0.523000\n",
      "(Iteration 53601 / 61240) loss: 1.597689\n",
      "(Iteration 53701 / 61240) loss: 1.463210\n",
      "(Iteration 53801 / 61240) loss: 1.721180\n",
      "(Iteration 53901 / 61240) loss: 1.704567\n",
      "(Iteration 54001 / 61240) loss: 1.512845\n",
      "(Iteration 54101 / 61240) loss: 1.387499\n",
      "(Iteration 54201 / 61240) loss: 1.742354\n",
      "(Iteration 54301 / 61240) loss: 1.403392\n",
      "(Iteration 54401 / 61240) loss: 1.663550\n",
      "(Iteration 54501 / 61240) loss: 1.255636\n",
      "(Iteration 54601 / 61240) loss: 1.349299\n",
      "(Iteration 54701 / 61240) loss: 1.186651\n",
      "(Iteration 54801 / 61240) loss: 1.556908\n",
      "(Iteration 54901 / 61240) loss: 1.340936\n",
      "(Iteration 55001 / 61240) loss: 1.769349\n",
      "(Iteration 55101 / 61240) loss: 1.810704\n",
      "(Epoch 36 / 40) train acc: 0.545000; val_acc: 0.525000\n",
      "(Iteration 55201 / 61240) loss: 1.408978\n",
      "(Iteration 55301 / 61240) loss: 1.195139\n",
      "(Iteration 55401 / 61240) loss: 1.232030\n",
      "(Iteration 55501 / 61240) loss: 1.287538\n",
      "(Iteration 55601 / 61240) loss: 1.324308\n",
      "(Iteration 55701 / 61240) loss: 1.373204\n",
      "(Iteration 55801 / 61240) loss: 1.441459\n",
      "(Iteration 55901 / 61240) loss: 1.314042\n",
      "(Iteration 56001 / 61240) loss: 1.385484\n",
      "(Iteration 56101 / 61240) loss: 1.157283\n",
      "(Iteration 56201 / 61240) loss: 1.344912\n",
      "(Iteration 56301 / 61240) loss: 1.487621\n",
      "(Iteration 56401 / 61240) loss: 1.383563\n",
      "(Iteration 56501 / 61240) loss: 1.401406\n",
      "(Iteration 56601 / 61240) loss: 1.420656\n",
      "(Epoch 37 / 40) train acc: 0.546000; val_acc: 0.522000\n",
      "(Iteration 56701 / 61240) loss: 1.444128\n",
      "(Iteration 56801 / 61240) loss: 1.193524\n",
      "(Iteration 56901 / 61240) loss: 1.427326\n",
      "(Iteration 57001 / 61240) loss: 1.433417\n",
      "(Iteration 57101 / 61240) loss: 1.034979\n",
      "(Iteration 57201 / 61240) loss: 1.552617\n",
      "(Iteration 57301 / 61240) loss: 1.200352\n",
      "(Iteration 57401 / 61240) loss: 1.408406\n",
      "(Iteration 57501 / 61240) loss: 1.386660\n",
      "(Iteration 57601 / 61240) loss: 1.568569\n",
      "(Iteration 57701 / 61240) loss: 1.577159\n",
      "(Iteration 57801 / 61240) loss: 1.322447\n",
      "(Iteration 57901 / 61240) loss: 1.123901\n",
      "(Iteration 58001 / 61240) loss: 1.520327\n",
      "(Iteration 58101 / 61240) loss: 1.681246\n",
      "(Epoch 38 / 40) train acc: 0.538000; val_acc: 0.523000\n",
      "(Iteration 58201 / 61240) loss: 1.714092\n",
      "(Iteration 58301 / 61240) loss: 1.495083\n",
      "(Iteration 58401 / 61240) loss: 1.586173\n",
      "(Iteration 58501 / 61240) loss: 1.405870\n",
      "(Iteration 58601 / 61240) loss: 1.437921\n",
      "(Iteration 58701 / 61240) loss: 1.207698\n",
      "(Iteration 58801 / 61240) loss: 1.628259\n",
      "(Iteration 58901 / 61240) loss: 1.523123\n",
      "(Iteration 59001 / 61240) loss: 1.237415\n",
      "(Iteration 59101 / 61240) loss: 1.459725\n",
      "(Iteration 59201 / 61240) loss: 1.572341\n",
      "(Iteration 59301 / 61240) loss: 1.812422\n",
      "(Iteration 59401 / 61240) loss: 1.465351\n",
      "(Iteration 59501 / 61240) loss: 1.504205\n",
      "(Iteration 59601 / 61240) loss: 1.679815\n",
      "(Iteration 59701 / 61240) loss: 1.364343\n",
      "(Epoch 39 / 40) train acc: 0.548000; val_acc: 0.524000\n",
      "(Iteration 59801 / 61240) loss: 1.415110\n",
      "(Iteration 59901 / 61240) loss: 1.227423\n",
      "(Iteration 60001 / 61240) loss: 1.437790\n",
      "(Iteration 60101 / 61240) loss: 1.365080\n",
      "(Iteration 60201 / 61240) loss: 1.626976\n",
      "(Iteration 60301 / 61240) loss: 1.483859\n",
      "(Iteration 60401 / 61240) loss: 1.401341\n",
      "(Iteration 60501 / 61240) loss: 1.529056\n",
      "(Iteration 60601 / 61240) loss: 1.369828\n",
      "(Iteration 60701 / 61240) loss: 1.238030\n",
      "(Iteration 60801 / 61240) loss: 1.843886\n",
      "(Iteration 60901 / 61240) loss: 1.448390\n",
      "(Iteration 61001 / 61240) loss: 1.608460\n",
      "(Iteration 61101 / 61240) loss: 1.366490\n",
      "(Iteration 61201 / 61240) loss: 1.299661\n",
      "(Epoch 40 / 40) train acc: 0.504000; val_acc: 0.522000\n",
      "New best model found with validation accuracy: 0.4988\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.001, 'num_epochs': 20, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 15300) loss: 2.309112\n",
      "(Epoch 0 / 20) train acc: 0.086000; val_acc: 0.098000\n",
      "(Iteration 101 / 15300) loss: 2.309026\n",
      "(Iteration 201 / 15300) loss: 2.308825\n",
      "(Iteration 301 / 15300) loss: 2.308759\n",
      "(Iteration 401 / 15300) loss: 2.308491\n",
      "(Iteration 501 / 15300) loss: 2.308472\n",
      "(Iteration 601 / 15300) loss: 2.308017\n",
      "(Iteration 701 / 15300) loss: 2.308095\n",
      "(Epoch 1 / 20) train acc: 0.219000; val_acc: 0.167000\n",
      "(Iteration 801 / 15300) loss: 2.307955\n",
      "(Iteration 901 / 15300) loss: 2.307889\n",
      "(Iteration 1001 / 15300) loss: 2.307890\n",
      "(Iteration 1101 / 15300) loss: 2.307648\n",
      "(Iteration 1201 / 15300) loss: 2.307526\n",
      "(Iteration 1301 / 15300) loss: 2.307467\n",
      "(Iteration 1401 / 15300) loss: 2.307680\n",
      "(Iteration 1501 / 15300) loss: 2.307103\n",
      "(Epoch 2 / 20) train acc: 0.135000; val_acc: 0.129000\n",
      "(Iteration 1601 / 15300) loss: 2.306988\n",
      "(Iteration 1701 / 15300) loss: 2.307020\n",
      "(Iteration 1801 / 15300) loss: 2.306774\n",
      "(Iteration 1901 / 15300) loss: 2.306876\n",
      "(Iteration 2001 / 15300) loss: 2.306728\n",
      "(Iteration 2101 / 15300) loss: 2.306754\n",
      "(Iteration 2201 / 15300) loss: 2.306572\n",
      "(Epoch 3 / 20) train acc: 0.153000; val_acc: 0.171000\n",
      "(Iteration 2301 / 15300) loss: 2.306400\n",
      "(Iteration 2401 / 15300) loss: 2.306395\n",
      "(Iteration 2501 / 15300) loss: 2.306284\n",
      "(Iteration 2601 / 15300) loss: 2.306337\n",
      "(Iteration 2701 / 15300) loss: 2.305908\n",
      "(Iteration 2801 / 15300) loss: 2.305993\n",
      "(Iteration 2901 / 15300) loss: 2.306396\n",
      "(Iteration 3001 / 15300) loss: 2.305902\n",
      "(Epoch 4 / 20) train acc: 0.243000; val_acc: 0.242000\n",
      "(Iteration 3101 / 15300) loss: 2.305802\n",
      "(Iteration 3201 / 15300) loss: 2.305887\n",
      "(Iteration 3301 / 15300) loss: 2.305640\n",
      "(Iteration 3401 / 15300) loss: 2.305463\n",
      "(Iteration 3501 / 15300) loss: 2.305668\n",
      "(Iteration 3601 / 15300) loss: 2.305472\n",
      "(Iteration 3701 / 15300) loss: 2.306094\n",
      "(Iteration 3801 / 15300) loss: 2.305691\n",
      "(Epoch 5 / 20) train acc: 0.163000; val_acc: 0.157000\n",
      "(Iteration 3901 / 15300) loss: 2.305627\n",
      "(Iteration 4001 / 15300) loss: 2.305154\n",
      "(Iteration 4101 / 15300) loss: 2.305361\n",
      "(Iteration 4201 / 15300) loss: 2.305456\n",
      "(Iteration 4301 / 15300) loss: 2.305472\n",
      "(Iteration 4401 / 15300) loss: 2.305236\n",
      "(Iteration 4501 / 15300) loss: 2.304984\n",
      "(Epoch 6 / 20) train acc: 0.195000; val_acc: 0.168000\n",
      "(Iteration 4601 / 15300) loss: 2.305331\n",
      "(Iteration 4701 / 15300) loss: 2.305349\n",
      "(Iteration 4801 / 15300) loss: 2.305117\n",
      "(Iteration 4901 / 15300) loss: 2.305184\n",
      "(Iteration 5001 / 15300) loss: 2.304489\n",
      "(Iteration 5101 / 15300) loss: 2.304662\n",
      "(Iteration 5201 / 15300) loss: 2.305250\n",
      "(Iteration 5301 / 15300) loss: 2.304986\n",
      "(Epoch 7 / 20) train acc: 0.227000; val_acc: 0.201000\n",
      "(Iteration 5401 / 15300) loss: 2.304418\n",
      "(Iteration 5501 / 15300) loss: 2.304585\n",
      "(Iteration 5601 / 15300) loss: 2.304435\n",
      "(Iteration 5701 / 15300) loss: 2.304401\n",
      "(Iteration 5801 / 15300) loss: 2.304463\n",
      "(Iteration 5901 / 15300) loss: 2.304094\n",
      "(Iteration 6001 / 15300) loss: 2.305532\n",
      "(Iteration 6101 / 15300) loss: 2.304283\n",
      "(Epoch 8 / 20) train acc: 0.271000; val_acc: 0.240000\n",
      "(Iteration 6201 / 15300) loss: 2.304516\n",
      "(Iteration 6301 / 15300) loss: 2.305055\n",
      "(Iteration 6401 / 15300) loss: 2.304410\n",
      "(Iteration 6501 / 15300) loss: 2.303868\n",
      "(Iteration 6601 / 15300) loss: 2.304402\n",
      "(Iteration 6701 / 15300) loss: 2.303892\n",
      "(Iteration 6801 / 15300) loss: 2.303494\n",
      "(Epoch 9 / 20) train acc: 0.212000; val_acc: 0.213000\n",
      "(Iteration 6901 / 15300) loss: 2.304185\n",
      "(Iteration 7001 / 15300) loss: 2.303674\n",
      "(Iteration 7101 / 15300) loss: 2.304058\n",
      "(Iteration 7201 / 15300) loss: 2.304694\n",
      "(Iteration 7301 / 15300) loss: 2.304458\n",
      "(Iteration 7401 / 15300) loss: 2.304063\n",
      "(Iteration 7501 / 15300) loss: 2.303566\n",
      "(Iteration 7601 / 15300) loss: 2.303692\n",
      "(Epoch 10 / 20) train acc: 0.235000; val_acc: 0.216000\n",
      "(Iteration 7701 / 15300) loss: 2.303876\n",
      "(Iteration 7801 / 15300) loss: 2.303618\n",
      "(Iteration 7901 / 15300) loss: 2.304197\n",
      "(Iteration 8001 / 15300) loss: 2.303306\n",
      "(Iteration 8101 / 15300) loss: 2.303662\n",
      "(Iteration 8201 / 15300) loss: 2.303666\n",
      "(Iteration 8301 / 15300) loss: 2.303592\n",
      "(Iteration 8401 / 15300) loss: 2.303999\n",
      "(Epoch 11 / 20) train acc: 0.214000; val_acc: 0.221000\n",
      "(Iteration 8501 / 15300) loss: 2.303014\n",
      "(Iteration 8601 / 15300) loss: 2.302775\n",
      "(Iteration 8701 / 15300) loss: 2.303175\n",
      "(Iteration 8801 / 15300) loss: 2.302575\n",
      "(Iteration 8901 / 15300) loss: 2.303447\n",
      "(Iteration 9001 / 15300) loss: 2.303369\n",
      "(Iteration 9101 / 15300) loss: 2.303372\n",
      "(Epoch 12 / 20) train acc: 0.206000; val_acc: 0.206000\n",
      "(Iteration 9201 / 15300) loss: 2.303054\n",
      "(Iteration 9301 / 15300) loss: 2.303294\n",
      "(Iteration 9401 / 15300) loss: 2.302841\n",
      "(Iteration 9501 / 15300) loss: 2.303039\n",
      "(Iteration 9601 / 15300) loss: 2.304005\n",
      "(Iteration 9701 / 15300) loss: 2.303453\n",
      "(Iteration 9801 / 15300) loss: 2.303347\n",
      "(Iteration 9901 / 15300) loss: 2.302601\n",
      "(Epoch 13 / 20) train acc: 0.228000; val_acc: 0.201000\n",
      "(Iteration 10001 / 15300) loss: 2.302112\n",
      "(Iteration 10101 / 15300) loss: 2.303105\n",
      "(Iteration 10201 / 15300) loss: 2.302836\n",
      "(Iteration 10301 / 15300) loss: 2.302634\n",
      "(Iteration 10401 / 15300) loss: 2.302609\n",
      "(Iteration 10501 / 15300) loss: 2.303636\n",
      "(Iteration 10601 / 15300) loss: 2.303181\n",
      "(Iteration 10701 / 15300) loss: 2.303117\n",
      "(Epoch 14 / 20) train acc: 0.238000; val_acc: 0.202000\n",
      "(Iteration 10801 / 15300) loss: 2.303308\n",
      "(Iteration 10901 / 15300) loss: 2.302125\n",
      "(Iteration 11001 / 15300) loss: 2.302227\n",
      "(Iteration 11101 / 15300) loss: 2.302750\n",
      "(Iteration 11201 / 15300) loss: 2.303234\n",
      "(Iteration 11301 / 15300) loss: 2.302474\n",
      "(Iteration 11401 / 15300) loss: 2.302479\n",
      "(Epoch 15 / 20) train acc: 0.232000; val_acc: 0.204000\n",
      "(Iteration 11501 / 15300) loss: 2.302594\n",
      "(Iteration 11601 / 15300) loss: 2.302481\n",
      "(Iteration 11701 / 15300) loss: 2.302853\n",
      "(Iteration 11801 / 15300) loss: 2.302596\n",
      "(Iteration 11901 / 15300) loss: 2.302789\n",
      "(Iteration 12001 / 15300) loss: 2.302036\n",
      "(Iteration 12101 / 15300) loss: 2.302113\n",
      "(Iteration 12201 / 15300) loss: 2.303218\n",
      "(Epoch 16 / 20) train acc: 0.237000; val_acc: 0.206000\n",
      "(Iteration 12301 / 15300) loss: 2.302419\n",
      "(Iteration 12401 / 15300) loss: 2.301777\n",
      "(Iteration 12501 / 15300) loss: 2.302300\n",
      "(Iteration 12601 / 15300) loss: 2.301726\n",
      "(Iteration 12701 / 15300) loss: 2.301529\n",
      "(Iteration 12801 / 15300) loss: 2.301769\n",
      "(Iteration 12901 / 15300) loss: 2.302392\n",
      "(Iteration 13001 / 15300) loss: 2.302788\n",
      "(Epoch 17 / 20) train acc: 0.190000; val_acc: 0.208000\n",
      "(Iteration 13101 / 15300) loss: 2.303007\n",
      "(Iteration 13201 / 15300) loss: 2.302300\n",
      "(Iteration 13301 / 15300) loss: 2.301228\n",
      "(Iteration 13401 / 15300) loss: 2.301632\n",
      "(Iteration 13501 / 15300) loss: 2.301685\n",
      "(Iteration 13601 / 15300) loss: 2.301045\n",
      "(Iteration 13701 / 15300) loss: 2.302289\n",
      "(Epoch 18 / 20) train acc: 0.248000; val_acc: 0.206000\n",
      "(Iteration 13801 / 15300) loss: 2.301600\n",
      "(Iteration 13901 / 15300) loss: 2.300710\n",
      "(Iteration 14001 / 15300) loss: 2.301641\n",
      "(Iteration 14101 / 15300) loss: 2.301922\n",
      "(Iteration 14201 / 15300) loss: 2.300644\n",
      "(Iteration 14301 / 15300) loss: 2.300358\n",
      "(Iteration 14401 / 15300) loss: 2.301037\n",
      "(Iteration 14501 / 15300) loss: 2.302387\n",
      "(Epoch 19 / 20) train acc: 0.232000; val_acc: 0.203000\n",
      "(Iteration 14601 / 15300) loss: 2.302482\n",
      "(Iteration 14701 / 15300) loss: 2.302032\n",
      "(Iteration 14801 / 15300) loss: 2.302104\n",
      "(Iteration 14901 / 15300) loss: 2.300474\n",
      "(Iteration 15001 / 15300) loss: 2.302470\n",
      "(Iteration 15101 / 15300) loss: 2.301587\n",
      "(Iteration 15201 / 15300) loss: 2.302227\n",
      "(Epoch 20 / 20) train acc: 0.204000; val_acc: 0.202000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.001, 'num_epochs': 20, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 30620) loss: 2.309100\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.111000\n",
      "(Iteration 101 / 30620) loss: 2.308992\n",
      "(Iteration 201 / 30620) loss: 2.308733\n",
      "(Iteration 301 / 30620) loss: 2.308768\n",
      "(Iteration 401 / 30620) loss: 2.308372\n",
      "(Iteration 501 / 30620) loss: 2.308589\n",
      "(Iteration 601 / 30620) loss: 2.307960\n",
      "(Iteration 701 / 30620) loss: 2.308407\n",
      "(Iteration 801 / 30620) loss: 2.308253\n",
      "(Iteration 901 / 30620) loss: 2.307759\n",
      "(Iteration 1001 / 30620) loss: 2.307761\n",
      "(Iteration 1101 / 30620) loss: 2.307780\n",
      "(Iteration 1201 / 30620) loss: 2.307440\n",
      "(Iteration 1301 / 30620) loss: 2.307254\n",
      "(Iteration 1401 / 30620) loss: 2.307049\n",
      "(Iteration 1501 / 30620) loss: 2.306959\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.105000\n",
      "(Iteration 1601 / 30620) loss: 2.307120\n",
      "(Iteration 1701 / 30620) loss: 2.307055\n",
      "(Iteration 1801 / 30620) loss: 2.306801\n",
      "(Iteration 1901 / 30620) loss: 2.306918\n",
      "(Iteration 2001 / 30620) loss: 2.306230\n",
      "(Iteration 2101 / 30620) loss: 2.307211\n",
      "(Iteration 2201 / 30620) loss: 2.306223\n",
      "(Iteration 2301 / 30620) loss: 2.307958\n",
      "(Iteration 2401 / 30620) loss: 2.306567\n",
      "(Iteration 2501 / 30620) loss: 2.305838\n",
      "(Iteration 2601 / 30620) loss: 2.305932\n",
      "(Iteration 2701 / 30620) loss: 2.306238\n",
      "(Iteration 2801 / 30620) loss: 2.305544\n",
      "(Iteration 2901 / 30620) loss: 2.305936\n",
      "(Iteration 3001 / 30620) loss: 2.306171\n",
      "(Epoch 2 / 20) train acc: 0.147000; val_acc: 0.136000\n",
      "(Iteration 3101 / 30620) loss: 2.304931\n",
      "(Iteration 3201 / 30620) loss: 2.305239\n",
      "(Iteration 3301 / 30620) loss: 2.305222\n",
      "(Iteration 3401 / 30620) loss: 2.305470\n",
      "(Iteration 3501 / 30620) loss: 2.305539\n",
      "(Iteration 3601 / 30620) loss: 2.305218\n",
      "(Iteration 3701 / 30620) loss: 2.304588\n",
      "(Iteration 3801 / 30620) loss: 2.305280\n",
      "(Iteration 3901 / 30620) loss: 2.304548\n",
      "(Iteration 4001 / 30620) loss: 2.305363\n",
      "(Iteration 4101 / 30620) loss: 2.305499\n",
      "(Iteration 4201 / 30620) loss: 2.304459\n",
      "(Iteration 4301 / 30620) loss: 2.304909\n",
      "(Iteration 4401 / 30620) loss: 2.305117\n",
      "(Iteration 4501 / 30620) loss: 2.304069\n",
      "(Epoch 3 / 20) train acc: 0.215000; val_acc: 0.203000\n",
      "(Iteration 4601 / 30620) loss: 2.304007\n",
      "(Iteration 4701 / 30620) loss: 2.304477\n",
      "(Iteration 4801 / 30620) loss: 2.304627\n",
      "(Iteration 4901 / 30620) loss: 2.304622\n",
      "(Iteration 5001 / 30620) loss: 2.304182\n",
      "(Iteration 5101 / 30620) loss: 2.303788\n",
      "(Iteration 5201 / 30620) loss: 2.303857\n",
      "(Iteration 5301 / 30620) loss: 2.304651\n",
      "(Iteration 5401 / 30620) loss: 2.303988\n",
      "(Iteration 5501 / 30620) loss: 2.304577\n",
      "(Iteration 5601 / 30620) loss: 2.304915\n",
      "(Iteration 5701 / 30620) loss: 2.303518\n",
      "(Iteration 5801 / 30620) loss: 2.303086\n",
      "(Iteration 5901 / 30620) loss: 2.303495\n",
      "(Iteration 6001 / 30620) loss: 2.303413\n",
      "(Iteration 6101 / 30620) loss: 2.303772\n",
      "(Epoch 4 / 20) train acc: 0.263000; val_acc: 0.268000\n",
      "(Iteration 6201 / 30620) loss: 2.303759\n",
      "(Iteration 6301 / 30620) loss: 2.303729\n",
      "(Iteration 6401 / 30620) loss: 2.303376\n",
      "(Iteration 6501 / 30620) loss: 2.302525\n",
      "(Iteration 6601 / 30620) loss: 2.303302\n",
      "(Iteration 6701 / 30620) loss: 2.302388\n",
      "(Iteration 6801 / 30620) loss: 2.303120\n",
      "(Iteration 6901 / 30620) loss: 2.303840\n",
      "(Iteration 7001 / 30620) loss: 2.303258\n",
      "(Iteration 7101 / 30620) loss: 2.303115\n",
      "(Iteration 7201 / 30620) loss: 2.302773\n",
      "(Iteration 7301 / 30620) loss: 2.302760\n",
      "(Iteration 7401 / 30620) loss: 2.303149\n",
      "(Iteration 7501 / 30620) loss: 2.302935\n",
      "(Iteration 7601 / 30620) loss: 2.302742\n",
      "(Epoch 5 / 20) train acc: 0.218000; val_acc: 0.233000\n",
      "(Iteration 7701 / 30620) loss: 2.302923\n",
      "(Iteration 7801 / 30620) loss: 2.304233\n",
      "(Iteration 7901 / 30620) loss: 2.303009\n",
      "(Iteration 8001 / 30620) loss: 2.301950\n",
      "(Iteration 8101 / 30620) loss: 2.302214\n",
      "(Iteration 8201 / 30620) loss: 2.301146\n",
      "(Iteration 8301 / 30620) loss: 2.300733\n",
      "(Iteration 8401 / 30620) loss: 2.303716\n",
      "(Iteration 8501 / 30620) loss: 2.301524\n",
      "(Iteration 8601 / 30620) loss: 2.302119\n",
      "(Iteration 8701 / 30620) loss: 2.303191\n",
      "(Iteration 8801 / 30620) loss: 2.298901\n",
      "(Iteration 8901 / 30620) loss: 2.300690\n",
      "(Iteration 9001 / 30620) loss: 2.301701\n",
      "(Iteration 9101 / 30620) loss: 2.300320\n",
      "(Epoch 6 / 20) train acc: 0.299000; val_acc: 0.284000\n",
      "(Iteration 9201 / 30620) loss: 2.302619\n",
      "(Iteration 9301 / 30620) loss: 2.300820\n",
      "(Iteration 9401 / 30620) loss: 2.302119\n",
      "(Iteration 9501 / 30620) loss: 2.302203\n",
      "(Iteration 9601 / 30620) loss: 2.301457\n",
      "(Iteration 9701 / 30620) loss: 2.300953\n",
      "(Iteration 9801 / 30620) loss: 2.298430\n",
      "(Iteration 9901 / 30620) loss: 2.298420\n",
      "(Iteration 10001 / 30620) loss: 2.300971\n",
      "(Iteration 10101 / 30620) loss: 2.301583\n",
      "(Iteration 10201 / 30620) loss: 2.300388\n",
      "(Iteration 10301 / 30620) loss: 2.300362\n",
      "(Iteration 10401 / 30620) loss: 2.300979\n",
      "(Iteration 10501 / 30620) loss: 2.301090\n",
      "(Iteration 10601 / 30620) loss: 2.299356\n",
      "(Iteration 10701 / 30620) loss: 2.300968\n",
      "(Epoch 7 / 20) train acc: 0.274000; val_acc: 0.270000\n",
      "(Iteration 10801 / 30620) loss: 2.297160\n",
      "(Iteration 10901 / 30620) loss: 2.300501\n",
      "(Iteration 11001 / 30620) loss: 2.297891\n",
      "(Iteration 11101 / 30620) loss: 2.297988\n",
      "(Iteration 11201 / 30620) loss: 2.298391\n",
      "(Iteration 11301 / 30620) loss: 2.297835\n",
      "(Iteration 11401 / 30620) loss: 2.297271\n",
      "(Iteration 11501 / 30620) loss: 2.298382\n",
      "(Iteration 11601 / 30620) loss: 2.293780\n",
      "(Iteration 11701 / 30620) loss: 2.301709\n",
      "(Iteration 11801 / 30620) loss: 2.298874\n",
      "(Iteration 11901 / 30620) loss: 2.300719\n",
      "(Iteration 12001 / 30620) loss: 2.300348\n",
      "(Iteration 12101 / 30620) loss: 2.297985\n",
      "(Iteration 12201 / 30620) loss: 2.300004\n",
      "(Epoch 8 / 20) train acc: 0.300000; val_acc: 0.302000\n",
      "(Iteration 12301 / 30620) loss: 2.297079\n",
      "(Iteration 12401 / 30620) loss: 2.298523\n",
      "(Iteration 12501 / 30620) loss: 2.295951\n",
      "(Iteration 12601 / 30620) loss: 2.291055\n",
      "(Iteration 12701 / 30620) loss: 2.299859\n",
      "(Iteration 12801 / 30620) loss: 2.299309\n",
      "(Iteration 12901 / 30620) loss: 2.298433\n",
      "(Iteration 13001 / 30620) loss: 2.298268\n",
      "(Iteration 13101 / 30620) loss: 2.291354\n",
      "(Iteration 13201 / 30620) loss: 2.296986\n",
      "(Iteration 13301 / 30620) loss: 2.290881\n",
      "(Iteration 13401 / 30620) loss: 2.291353\n",
      "(Iteration 13501 / 30620) loss: 2.290456\n",
      "(Iteration 13601 / 30620) loss: 2.292184\n",
      "(Iteration 13701 / 30620) loss: 2.290986\n",
      "(Epoch 9 / 20) train acc: 0.273000; val_acc: 0.284000\n",
      "(Iteration 13801 / 30620) loss: 2.293932\n",
      "(Iteration 13901 / 30620) loss: 2.298952\n",
      "(Iteration 14001 / 30620) loss: 2.292928\n",
      "(Iteration 14101 / 30620) loss: 2.291929\n",
      "(Iteration 14201 / 30620) loss: 2.293002\n",
      "(Iteration 14301 / 30620) loss: 2.298050\n",
      "(Iteration 14401 / 30620) loss: 2.285257\n",
      "(Iteration 14501 / 30620) loss: 2.297240\n",
      "(Iteration 14601 / 30620) loss: 2.298557\n",
      "(Iteration 14701 / 30620) loss: 2.292097\n",
      "(Iteration 14801 / 30620) loss: 2.289823\n",
      "(Iteration 14901 / 30620) loss: 2.289820\n",
      "(Iteration 15001 / 30620) loss: 2.290238\n",
      "(Iteration 15101 / 30620) loss: 2.287060\n",
      "(Iteration 15201 / 30620) loss: 2.301037\n",
      "(Iteration 15301 / 30620) loss: 2.289744\n",
      "(Epoch 10 / 20) train acc: 0.257000; val_acc: 0.271000\n",
      "(Iteration 15401 / 30620) loss: 2.299775\n",
      "(Iteration 15501 / 30620) loss: 2.294092\n",
      "(Iteration 15601 / 30620) loss: 2.280943\n",
      "(Iteration 15701 / 30620) loss: 2.282109\n",
      "(Iteration 15801 / 30620) loss: 2.285489\n",
      "(Iteration 15901 / 30620) loss: 2.287560\n",
      "(Iteration 16001 / 30620) loss: 2.299217\n",
      "(Iteration 16101 / 30620) loss: 2.291094\n",
      "(Iteration 16201 / 30620) loss: 2.290336\n",
      "(Iteration 16301 / 30620) loss: 2.290164\n",
      "(Iteration 16401 / 30620) loss: 2.289090\n",
      "(Iteration 16501 / 30620) loss: 2.300963\n",
      "(Iteration 16601 / 30620) loss: 2.288892\n",
      "(Iteration 16701 / 30620) loss: 2.285303\n",
      "(Iteration 16801 / 30620) loss: 2.300274\n",
      "(Epoch 11 / 20) train acc: 0.261000; val_acc: 0.268000\n",
      "(Iteration 16901 / 30620) loss: 2.288203\n",
      "(Iteration 17001 / 30620) loss: 2.289377\n",
      "(Iteration 17101 / 30620) loss: 2.293594\n",
      "(Iteration 17201 / 30620) loss: 2.277358\n",
      "(Iteration 17301 / 30620) loss: 2.273874\n",
      "(Iteration 17401 / 30620) loss: 2.283764\n",
      "(Iteration 17501 / 30620) loss: 2.278383\n",
      "(Iteration 17601 / 30620) loss: 2.286986\n",
      "(Iteration 17701 / 30620) loss: 2.269425\n",
      "(Iteration 17801 / 30620) loss: 2.300020\n",
      "(Iteration 17901 / 30620) loss: 2.281825\n",
      "(Iteration 18001 / 30620) loss: 2.285919\n",
      "(Iteration 18101 / 30620) loss: 2.288439\n",
      "(Iteration 18201 / 30620) loss: 2.286095\n",
      "(Iteration 18301 / 30620) loss: 2.286909\n",
      "(Epoch 12 / 20) train acc: 0.259000; val_acc: 0.271000\n",
      "(Iteration 18401 / 30620) loss: 2.283050\n",
      "(Iteration 18501 / 30620) loss: 2.286407\n",
      "(Iteration 18601 / 30620) loss: 2.284616\n",
      "(Iteration 18701 / 30620) loss: 2.274483\n",
      "(Iteration 18801 / 30620) loss: 2.281290\n",
      "(Iteration 18901 / 30620) loss: 2.289463\n",
      "(Iteration 19001 / 30620) loss: 2.292149\n",
      "(Iteration 19101 / 30620) loss: 2.293061\n",
      "(Iteration 19201 / 30620) loss: 2.283696\n",
      "(Iteration 19301 / 30620) loss: 2.290945\n",
      "(Iteration 19401 / 30620) loss: 2.260114\n",
      "(Iteration 19501 / 30620) loss: 2.284384\n",
      "(Iteration 19601 / 30620) loss: 2.270593\n",
      "(Iteration 19701 / 30620) loss: 2.278496\n",
      "(Iteration 19801 / 30620) loss: 2.273422\n",
      "(Iteration 19901 / 30620) loss: 2.286851\n",
      "(Epoch 13 / 20) train acc: 0.265000; val_acc: 0.266000\n",
      "(Iteration 20001 / 30620) loss: 2.275501\n",
      "(Iteration 20101 / 30620) loss: 2.278228\n",
      "(Iteration 20201 / 30620) loss: 2.277403\n",
      "(Iteration 20301 / 30620) loss: 2.285540\n",
      "(Iteration 20401 / 30620) loss: 2.278462\n",
      "(Iteration 20501 / 30620) loss: 2.270284\n",
      "(Iteration 20601 / 30620) loss: 2.243875\n",
      "(Iteration 20701 / 30620) loss: 2.273528\n",
      "(Iteration 20801 / 30620) loss: 2.274632\n",
      "(Iteration 20901 / 30620) loss: 2.280182\n",
      "(Iteration 21001 / 30620) loss: 2.277615\n",
      "(Iteration 21101 / 30620) loss: 2.270204\n",
      "(Iteration 21201 / 30620) loss: 2.289189\n",
      "(Iteration 21301 / 30620) loss: 2.282794\n",
      "(Iteration 21401 / 30620) loss: 2.281874\n",
      "(Epoch 14 / 20) train acc: 0.261000; val_acc: 0.268000\n",
      "(Iteration 21501 / 30620) loss: 2.248646\n",
      "(Iteration 21601 / 30620) loss: 2.253407\n",
      "(Iteration 21701 / 30620) loss: 2.271579\n",
      "(Iteration 21801 / 30620) loss: 2.270757\n",
      "(Iteration 21901 / 30620) loss: 2.288800\n",
      "(Iteration 22001 / 30620) loss: 2.289575\n",
      "(Iteration 22101 / 30620) loss: 2.259423\n",
      "(Iteration 22201 / 30620) loss: 2.300775\n",
      "(Iteration 22301 / 30620) loss: 2.283367\n",
      "(Iteration 22401 / 30620) loss: 2.283055\n",
      "(Iteration 22501 / 30620) loss: 2.256886\n",
      "(Iteration 22601 / 30620) loss: 2.299865\n",
      "(Iteration 22701 / 30620) loss: 2.281869\n",
      "(Iteration 22801 / 30620) loss: 2.273257\n",
      "(Iteration 22901 / 30620) loss: 2.293329\n",
      "(Epoch 15 / 20) train acc: 0.286000; val_acc: 0.262000\n",
      "(Iteration 23001 / 30620) loss: 2.277072\n",
      "(Iteration 23101 / 30620) loss: 2.262963\n",
      "(Iteration 23201 / 30620) loss: 2.275295\n",
      "(Iteration 23301 / 30620) loss: 2.246582\n",
      "(Iteration 23401 / 30620) loss: 2.269853\n",
      "(Iteration 23501 / 30620) loss: 2.282785\n",
      "(Iteration 23601 / 30620) loss: 2.253866\n",
      "(Iteration 23701 / 30620) loss: 2.250781\n",
      "(Iteration 23801 / 30620) loss: 2.258960\n",
      "(Iteration 23901 / 30620) loss: 2.265984\n",
      "(Iteration 24001 / 30620) loss: 2.264730\n",
      "(Iteration 24101 / 30620) loss: 2.256220\n",
      "(Iteration 24201 / 30620) loss: 2.252940\n",
      "(Iteration 24301 / 30620) loss: 2.275430\n",
      "(Iteration 24401 / 30620) loss: 2.273477\n",
      "(Epoch 16 / 20) train acc: 0.272000; val_acc: 0.265000\n",
      "(Iteration 24501 / 30620) loss: 2.282550\n",
      "(Iteration 24601 / 30620) loss: 2.257254\n",
      "(Iteration 24701 / 30620) loss: 2.254839\n",
      "(Iteration 24801 / 30620) loss: 2.246107\n",
      "(Iteration 24901 / 30620) loss: 2.280515\n",
      "(Iteration 25001 / 30620) loss: 2.272425\n",
      "(Iteration 25101 / 30620) loss: 2.280232\n",
      "(Iteration 25201 / 30620) loss: 2.276931\n",
      "(Iteration 25301 / 30620) loss: 2.220189\n",
      "(Iteration 25401 / 30620) loss: 2.246942\n",
      "(Iteration 25501 / 30620) loss: 2.261910\n",
      "(Iteration 25601 / 30620) loss: 2.273933\n",
      "(Iteration 25701 / 30620) loss: 2.275129\n",
      "(Iteration 25801 / 30620) loss: 2.244543\n",
      "(Iteration 25901 / 30620) loss: 2.261908\n",
      "(Iteration 26001 / 30620) loss: 2.258260\n",
      "(Epoch 17 / 20) train acc: 0.259000; val_acc: 0.273000\n",
      "(Iteration 26101 / 30620) loss: 2.245132\n",
      "(Iteration 26201 / 30620) loss: 2.270358\n",
      "(Iteration 26301 / 30620) loss: 2.262328\n",
      "(Iteration 26401 / 30620) loss: 2.251076\n",
      "(Iteration 26501 / 30620) loss: 2.253675\n",
      "(Iteration 26601 / 30620) loss: 2.254947\n",
      "(Iteration 26701 / 30620) loss: 2.292462\n",
      "(Iteration 26801 / 30620) loss: 2.237596\n",
      "(Iteration 26901 / 30620) loss: 2.254260\n",
      "(Iteration 27001 / 30620) loss: 2.255474\n",
      "(Iteration 27101 / 30620) loss: 2.253572\n",
      "(Iteration 27201 / 30620) loss: 2.263835\n",
      "(Iteration 27301 / 30620) loss: 2.229241\n",
      "(Iteration 27401 / 30620) loss: 2.245433\n",
      "(Iteration 27501 / 30620) loss: 2.250941\n",
      "(Epoch 18 / 20) train acc: 0.277000; val_acc: 0.267000\n",
      "(Iteration 27601 / 30620) loss: 2.205337\n",
      "(Iteration 27701 / 30620) loss: 2.258857\n",
      "(Iteration 27801 / 30620) loss: 2.275064\n",
      "(Iteration 27901 / 30620) loss: 2.235543\n",
      "(Iteration 28001 / 30620) loss: 2.257156\n",
      "(Iteration 28101 / 30620) loss: 2.246199\n",
      "(Iteration 28201 / 30620) loss: 2.273532\n",
      "(Iteration 28301 / 30620) loss: 2.235239\n",
      "(Iteration 28401 / 30620) loss: 2.271091\n",
      "(Iteration 28501 / 30620) loss: 2.216051\n",
      "(Iteration 28601 / 30620) loss: 2.262505\n",
      "(Iteration 28701 / 30620) loss: 2.243252\n",
      "(Iteration 28801 / 30620) loss: 2.292205\n",
      "(Iteration 28901 / 30620) loss: 2.275357\n",
      "(Iteration 29001 / 30620) loss: 2.296624\n",
      "(Epoch 19 / 20) train acc: 0.279000; val_acc: 0.268000\n",
      "(Iteration 29101 / 30620) loss: 2.220846\n",
      "(Iteration 29201 / 30620) loss: 2.224347\n",
      "(Iteration 29301 / 30620) loss: 2.277476\n",
      "(Iteration 29401 / 30620) loss: 2.225780\n",
      "(Iteration 29501 / 30620) loss: 2.241055\n",
      "(Iteration 29601 / 30620) loss: 2.237441\n",
      "(Iteration 29701 / 30620) loss: 2.221125\n",
      "(Iteration 29801 / 30620) loss: 2.224201\n",
      "(Iteration 29901 / 30620) loss: 2.301710\n",
      "(Iteration 30001 / 30620) loss: 2.256744\n",
      "(Iteration 30101 / 30620) loss: 2.277528\n",
      "(Iteration 30201 / 30620) loss: 2.278609\n",
      "(Iteration 30301 / 30620) loss: 2.228778\n",
      "(Iteration 30401 / 30620) loss: 2.242651\n",
      "(Iteration 30501 / 30620) loss: 2.267262\n",
      "(Iteration 30601 / 30620) loss: 2.207158\n",
      "(Epoch 20 / 20) train acc: 0.280000; val_acc: 0.265000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.001, 'num_epochs': 20, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 15300) loss: 2.303247\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.134000\n",
      "(Iteration 101 / 15300) loss: 2.303179\n",
      "(Iteration 201 / 15300) loss: 2.303052\n",
      "(Iteration 301 / 15300) loss: 2.303023\n",
      "(Iteration 401 / 15300) loss: 2.303030\n",
      "(Iteration 501 / 15300) loss: 2.303018\n",
      "(Iteration 601 / 15300) loss: 2.303220\n",
      "(Iteration 701 / 15300) loss: 2.303151\n",
      "(Epoch 1 / 20) train acc: 0.133000; val_acc: 0.138000\n",
      "(Iteration 801 / 15300) loss: 2.303083\n",
      "(Iteration 901 / 15300) loss: 2.302679\n",
      "(Iteration 1001 / 15300) loss: 2.303073\n",
      "(Iteration 1101 / 15300) loss: 2.302934\n",
      "(Iteration 1201 / 15300) loss: 2.302557\n",
      "(Iteration 1301 / 15300) loss: 2.302961\n",
      "(Iteration 1401 / 15300) loss: 2.303079\n",
      "(Iteration 1501 / 15300) loss: 2.302836\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.103000\n",
      "(Iteration 1601 / 15300) loss: 2.302489\n",
      "(Iteration 1701 / 15300) loss: 2.302614\n",
      "(Iteration 1801 / 15300) loss: 2.302841\n",
      "(Iteration 1901 / 15300) loss: 2.302704\n",
      "(Iteration 2001 / 15300) loss: 2.302596\n",
      "(Iteration 2101 / 15300) loss: 2.302345\n",
      "(Iteration 2201 / 15300) loss: 2.302627\n",
      "(Epoch 3 / 20) train acc: 0.165000; val_acc: 0.186000\n",
      "(Iteration 2301 / 15300) loss: 2.302348\n",
      "(Iteration 2401 / 15300) loss: 2.302507\n",
      "(Iteration 2501 / 15300) loss: 2.302628\n",
      "(Iteration 2601 / 15300) loss: 2.302320\n",
      "(Iteration 2701 / 15300) loss: 2.302309\n",
      "(Iteration 2801 / 15300) loss: 2.302621\n",
      "(Iteration 2901 / 15300) loss: 2.301954\n",
      "(Iteration 3001 / 15300) loss: 2.301873\n",
      "(Epoch 4 / 20) train acc: 0.209000; val_acc: 0.201000\n",
      "(Iteration 3101 / 15300) loss: 2.302227\n",
      "(Iteration 3201 / 15300) loss: 2.302474\n",
      "(Iteration 3301 / 15300) loss: 2.301681\n",
      "(Iteration 3401 / 15300) loss: 2.302051\n",
      "(Iteration 3501 / 15300) loss: 2.302061\n",
      "(Iteration 3601 / 15300) loss: 2.301905\n",
      "(Iteration 3701 / 15300) loss: 2.302255\n",
      "(Iteration 3801 / 15300) loss: 2.302016\n",
      "(Epoch 5 / 20) train acc: 0.184000; val_acc: 0.185000\n",
      "(Iteration 3901 / 15300) loss: 2.301758\n",
      "(Iteration 4001 / 15300) loss: 2.301734\n",
      "(Iteration 4101 / 15300) loss: 2.301217\n",
      "(Iteration 4201 / 15300) loss: 2.301464\n",
      "(Iteration 4301 / 15300) loss: 2.301422\n",
      "(Iteration 4401 / 15300) loss: 2.301618\n",
      "(Iteration 4501 / 15300) loss: 2.301395\n",
      "(Epoch 6 / 20) train acc: 0.228000; val_acc: 0.211000\n",
      "(Iteration 4601 / 15300) loss: 2.301892\n",
      "(Iteration 4701 / 15300) loss: 2.300825\n",
      "(Iteration 4801 / 15300) loss: 2.301571\n",
      "(Iteration 4901 / 15300) loss: 2.301160\n",
      "(Iteration 5001 / 15300) loss: 2.300925\n",
      "(Iteration 5101 / 15300) loss: 2.301094\n",
      "(Iteration 5201 / 15300) loss: 2.301076\n",
      "(Iteration 5301 / 15300) loss: 2.301041\n",
      "(Epoch 7 / 20) train acc: 0.255000; val_acc: 0.233000\n",
      "(Iteration 5401 / 15300) loss: 2.301106\n",
      "(Iteration 5501 / 15300) loss: 2.300205\n",
      "(Iteration 5601 / 15300) loss: 2.300844\n",
      "(Iteration 5701 / 15300) loss: 2.300030\n",
      "(Iteration 5801 / 15300) loss: 2.300177\n",
      "(Iteration 5901 / 15300) loss: 2.300356\n",
      "(Iteration 6001 / 15300) loss: 2.300408\n",
      "(Iteration 6101 / 15300) loss: 2.300666\n",
      "(Epoch 8 / 20) train acc: 0.237000; val_acc: 0.210000\n",
      "(Iteration 6201 / 15300) loss: 2.299853\n",
      "(Iteration 6301 / 15300) loss: 2.299260\n",
      "(Iteration 6401 / 15300) loss: 2.299715\n",
      "(Iteration 6501 / 15300) loss: 2.300189\n",
      "(Iteration 6601 / 15300) loss: 2.300346\n",
      "(Iteration 6701 / 15300) loss: 2.299294\n",
      "(Iteration 6801 / 15300) loss: 2.300015\n",
      "(Epoch 9 / 20) train acc: 0.256000; val_acc: 0.252000\n",
      "(Iteration 6901 / 15300) loss: 2.299757\n",
      "(Iteration 7001 / 15300) loss: 2.299784\n",
      "(Iteration 7101 / 15300) loss: 2.298546\n",
      "(Iteration 7201 / 15300) loss: 2.300705\n",
      "(Iteration 7301 / 15300) loss: 2.299575\n",
      "(Iteration 7401 / 15300) loss: 2.298457\n",
      "(Iteration 7501 / 15300) loss: 2.298635\n",
      "(Iteration 7601 / 15300) loss: 2.299114\n",
      "(Epoch 10 / 20) train acc: 0.262000; val_acc: 0.251000\n",
      "(Iteration 7701 / 15300) loss: 2.298765\n",
      "(Iteration 7801 / 15300) loss: 2.298756\n",
      "(Iteration 7901 / 15300) loss: 2.298892\n",
      "(Iteration 8001 / 15300) loss: 2.298228\n",
      "(Iteration 8101 / 15300) loss: 2.299191\n",
      "(Iteration 8201 / 15300) loss: 2.297714\n",
      "(Iteration 8301 / 15300) loss: 2.297803\n",
      "(Iteration 8401 / 15300) loss: 2.298897\n",
      "(Epoch 11 / 20) train acc: 0.292000; val_acc: 0.264000\n",
      "(Iteration 8501 / 15300) loss: 2.299600\n",
      "(Iteration 8601 / 15300) loss: 2.298470\n",
      "(Iteration 8701 / 15300) loss: 2.299045\n",
      "(Iteration 8801 / 15300) loss: 2.299356\n",
      "(Iteration 8901 / 15300) loss: 2.299059\n",
      "(Iteration 9001 / 15300) loss: 2.297101\n",
      "(Iteration 9101 / 15300) loss: 2.299236\n",
      "(Epoch 12 / 20) train acc: 0.264000; val_acc: 0.278000\n",
      "(Iteration 9201 / 15300) loss: 2.297563\n",
      "(Iteration 9301 / 15300) loss: 2.298352\n",
      "(Iteration 9401 / 15300) loss: 2.298572\n",
      "(Iteration 9501 / 15300) loss: 2.296991\n",
      "(Iteration 9601 / 15300) loss: 2.298275\n",
      "(Iteration 9701 / 15300) loss: 2.297181\n",
      "(Iteration 9801 / 15300) loss: 2.295571\n",
      "(Iteration 9901 / 15300) loss: 2.298469\n",
      "(Epoch 13 / 20) train acc: 0.290000; val_acc: 0.283000\n",
      "(Iteration 10001 / 15300) loss: 2.296236\n",
      "(Iteration 10101 / 15300) loss: 2.295972\n",
      "(Iteration 10201 / 15300) loss: 2.297234\n",
      "(Iteration 10301 / 15300) loss: 2.295503\n",
      "(Iteration 10401 / 15300) loss: 2.296462\n",
      "(Iteration 10501 / 15300) loss: 2.296225\n",
      "(Iteration 10601 / 15300) loss: 2.296182\n",
      "(Iteration 10701 / 15300) loss: 2.295668\n",
      "(Epoch 14 / 20) train acc: 0.285000; val_acc: 0.286000\n",
      "(Iteration 10801 / 15300) loss: 2.299162\n",
      "(Iteration 10901 / 15300) loss: 2.297549\n",
      "(Iteration 11001 / 15300) loss: 2.296944\n",
      "(Iteration 11101 / 15300) loss: 2.296462\n",
      "(Iteration 11201 / 15300) loss: 2.297582\n",
      "(Iteration 11301 / 15300) loss: 2.296265\n",
      "(Iteration 11401 / 15300) loss: 2.294831\n",
      "(Epoch 15 / 20) train acc: 0.293000; val_acc: 0.293000\n",
      "(Iteration 11501 / 15300) loss: 2.295761\n",
      "(Iteration 11601 / 15300) loss: 2.295381\n",
      "(Iteration 11701 / 15300) loss: 2.293705\n",
      "(Iteration 11801 / 15300) loss: 2.295253\n",
      "(Iteration 11901 / 15300) loss: 2.293111\n",
      "(Iteration 12001 / 15300) loss: 2.293928\n",
      "(Iteration 12101 / 15300) loss: 2.296517\n",
      "(Iteration 12201 / 15300) loss: 2.293141\n",
      "(Epoch 16 / 20) train acc: 0.306000; val_acc: 0.290000\n",
      "(Iteration 12301 / 15300) loss: 2.294507\n",
      "(Iteration 12401 / 15300) loss: 2.293950\n",
      "(Iteration 12501 / 15300) loss: 2.293147\n",
      "(Iteration 12601 / 15300) loss: 2.295089\n",
      "(Iteration 12701 / 15300) loss: 2.295082\n",
      "(Iteration 12801 / 15300) loss: 2.294743\n",
      "(Iteration 12901 / 15300) loss: 2.295106\n",
      "(Iteration 13001 / 15300) loss: 2.292848\n",
      "(Epoch 17 / 20) train acc: 0.275000; val_acc: 0.294000\n",
      "(Iteration 13101 / 15300) loss: 2.292637\n",
      "(Iteration 13201 / 15300) loss: 2.293681\n",
      "(Iteration 13301 / 15300) loss: 2.292432\n",
      "(Iteration 13401 / 15300) loss: 2.293031\n",
      "(Iteration 13501 / 15300) loss: 2.290845\n",
      "(Iteration 13601 / 15300) loss: 2.292755\n",
      "(Iteration 13701 / 15300) loss: 2.291097\n",
      "(Epoch 18 / 20) train acc: 0.298000; val_acc: 0.295000\n",
      "(Iteration 13801 / 15300) loss: 2.293476\n",
      "(Iteration 13901 / 15300) loss: 2.293265\n",
      "(Iteration 14001 / 15300) loss: 2.296298\n",
      "(Iteration 14101 / 15300) loss: 2.289679\n",
      "(Iteration 14201 / 15300) loss: 2.294236\n",
      "(Iteration 14301 / 15300) loss: 2.296363\n",
      "(Iteration 14401 / 15300) loss: 2.289910\n",
      "(Iteration 14501 / 15300) loss: 2.290699\n",
      "(Epoch 19 / 20) train acc: 0.329000; val_acc: 0.307000\n",
      "(Iteration 14601 / 15300) loss: 2.289462\n",
      "(Iteration 14701 / 15300) loss: 2.291805\n",
      "(Iteration 14801 / 15300) loss: 2.293475\n",
      "(Iteration 14901 / 15300) loss: 2.293000\n",
      "(Iteration 15001 / 15300) loss: 2.290803\n",
      "(Iteration 15101 / 15300) loss: 2.292595\n",
      "(Iteration 15201 / 15300) loss: 2.289626\n",
      "(Epoch 20 / 20) train acc: 0.296000; val_acc: 0.308000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.001, 'num_epochs': 20, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 30620) loss: 2.303236\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.111000\n",
      "(Iteration 101 / 30620) loss: 2.302959\n",
      "(Iteration 201 / 30620) loss: 2.303120\n",
      "(Iteration 301 / 30620) loss: 2.303319\n",
      "(Iteration 401 / 30620) loss: 2.302960\n",
      "(Iteration 501 / 30620) loss: 2.302966\n",
      "(Iteration 601 / 30620) loss: 2.303294\n",
      "(Iteration 701 / 30620) loss: 2.302679\n",
      "(Iteration 801 / 30620) loss: 2.303346\n",
      "(Iteration 901 / 30620) loss: 2.303370\n",
      "(Iteration 1001 / 30620) loss: 2.303076\n",
      "(Iteration 1101 / 30620) loss: 2.302070\n",
      "(Iteration 1201 / 30620) loss: 2.303178\n",
      "(Iteration 1301 / 30620) loss: 2.301783\n",
      "(Iteration 1401 / 30620) loss: 2.302663\n",
      "(Iteration 1501 / 30620) loss: 2.302834\n",
      "(Epoch 1 / 20) train acc: 0.275000; val_acc: 0.282000\n",
      "(Iteration 1601 / 30620) loss: 2.303247\n",
      "(Iteration 1701 / 30620) loss: 2.302411\n",
      "(Iteration 1801 / 30620) loss: 2.303618\n",
      "(Iteration 1901 / 30620) loss: 2.302215\n",
      "(Iteration 2001 / 30620) loss: 2.302274\n",
      "(Iteration 2101 / 30620) loss: 2.302907\n",
      "(Iteration 2201 / 30620) loss: 2.302156\n",
      "(Iteration 2301 / 30620) loss: 2.303347\n",
      "(Iteration 2401 / 30620) loss: 2.302062\n",
      "(Iteration 2501 / 30620) loss: 2.301760\n",
      "(Iteration 2601 / 30620) loss: 2.302362\n",
      "(Iteration 2701 / 30620) loss: 2.302686\n",
      "(Iteration 2801 / 30620) loss: 2.301917\n",
      "(Iteration 2901 / 30620) loss: 2.302536\n",
      "(Iteration 3001 / 30620) loss: 2.301326\n",
      "(Epoch 2 / 20) train acc: 0.176000; val_acc: 0.177000\n",
      "(Iteration 3101 / 30620) loss: 2.302204\n",
      "(Iteration 3201 / 30620) loss: 2.301678\n",
      "(Iteration 3301 / 30620) loss: 2.302591\n",
      "(Iteration 3401 / 30620) loss: 2.302114\n",
      "(Iteration 3501 / 30620) loss: 2.300463\n",
      "(Iteration 3601 / 30620) loss: 2.300858\n",
      "(Iteration 3701 / 30620) loss: 2.302022\n",
      "(Iteration 3801 / 30620) loss: 2.301967\n",
      "(Iteration 3901 / 30620) loss: 2.301873\n",
      "(Iteration 4001 / 30620) loss: 2.301475\n",
      "(Iteration 4101 / 30620) loss: 2.301134\n",
      "(Iteration 4201 / 30620) loss: 2.300959\n",
      "(Iteration 4301 / 30620) loss: 2.300916\n",
      "(Iteration 4401 / 30620) loss: 2.301057\n",
      "(Iteration 4501 / 30620) loss: 2.301495\n",
      "(Epoch 3 / 20) train acc: 0.268000; val_acc: 0.299000\n",
      "(Iteration 4601 / 30620) loss: 2.301000\n",
      "(Iteration 4701 / 30620) loss: 2.299577\n",
      "(Iteration 4801 / 30620) loss: 2.300525\n",
      "(Iteration 4901 / 30620) loss: 2.300626\n",
      "(Iteration 5001 / 30620) loss: 2.299931\n",
      "(Iteration 5101 / 30620) loss: 2.300638\n",
      "(Iteration 5201 / 30620) loss: 2.299867\n",
      "(Iteration 5301 / 30620) loss: 2.300090\n",
      "(Iteration 5401 / 30620) loss: 2.300912\n",
      "(Iteration 5501 / 30620) loss: 2.298906\n",
      "(Iteration 5601 / 30620) loss: 2.298120\n",
      "(Iteration 5701 / 30620) loss: 2.299534\n",
      "(Iteration 5801 / 30620) loss: 2.299586\n",
      "(Iteration 5901 / 30620) loss: 2.297880\n",
      "(Iteration 6001 / 30620) loss: 2.300162\n",
      "(Iteration 6101 / 30620) loss: 2.299090\n",
      "(Epoch 4 / 20) train acc: 0.269000; val_acc: 0.291000\n",
      "(Iteration 6201 / 30620) loss: 2.298140\n",
      "(Iteration 6301 / 30620) loss: 2.300090\n",
      "(Iteration 6401 / 30620) loss: 2.295800\n",
      "(Iteration 6501 / 30620) loss: 2.298797\n",
      "(Iteration 6601 / 30620) loss: 2.297829\n",
      "(Iteration 6701 / 30620) loss: 2.297538\n",
      "(Iteration 6801 / 30620) loss: 2.298318\n",
      "(Iteration 6901 / 30620) loss: 2.297704\n",
      "(Iteration 7001 / 30620) loss: 2.295547\n",
      "(Iteration 7101 / 30620) loss: 2.295529\n",
      "(Iteration 7201 / 30620) loss: 2.292904\n",
      "(Iteration 7301 / 30620) loss: 2.293064\n",
      "(Iteration 7401 / 30620) loss: 2.293839\n",
      "(Iteration 7501 / 30620) loss: 2.296673\n",
      "(Iteration 7601 / 30620) loss: 2.295139\n",
      "(Epoch 5 / 20) train acc: 0.276000; val_acc: 0.286000\n",
      "(Iteration 7701 / 30620) loss: 2.297255\n",
      "(Iteration 7801 / 30620) loss: 2.295362\n",
      "(Iteration 7901 / 30620) loss: 2.292032\n",
      "(Iteration 8001 / 30620) loss: 2.292584\n",
      "(Iteration 8101 / 30620) loss: 2.295493\n",
      "(Iteration 8201 / 30620) loss: 2.291349\n",
      "(Iteration 8301 / 30620) loss: 2.289643\n",
      "(Iteration 8401 / 30620) loss: 2.296188\n",
      "(Iteration 8501 / 30620) loss: 2.297810\n",
      "(Iteration 8601 / 30620) loss: 2.291059\n",
      "(Iteration 8701 / 30620) loss: 2.287028\n",
      "(Iteration 8801 / 30620) loss: 2.286527\n",
      "(Iteration 8901 / 30620) loss: 2.290919\n",
      "(Iteration 9001 / 30620) loss: 2.288113\n",
      "(Iteration 9101 / 30620) loss: 2.293632\n",
      "(Epoch 6 / 20) train acc: 0.279000; val_acc: 0.298000\n",
      "(Iteration 9201 / 30620) loss: 2.274487\n",
      "(Iteration 9301 / 30620) loss: 2.290459\n",
      "(Iteration 9401 / 30620) loss: 2.287845\n",
      "(Iteration 9501 / 30620) loss: 2.282478\n",
      "(Iteration 9601 / 30620) loss: 2.285326\n",
      "(Iteration 9701 / 30620) loss: 2.279942\n",
      "(Iteration 9801 / 30620) loss: 2.286756\n",
      "(Iteration 9901 / 30620) loss: 2.286238\n",
      "(Iteration 10001 / 30620) loss: 2.275976\n",
      "(Iteration 10101 / 30620) loss: 2.284000\n",
      "(Iteration 10201 / 30620) loss: 2.278899\n",
      "(Iteration 10301 / 30620) loss: 2.288972\n",
      "(Iteration 10401 / 30620) loss: 2.286533\n",
      "(Iteration 10501 / 30620) loss: 2.283048\n",
      "(Iteration 10601 / 30620) loss: 2.270058\n",
      "(Iteration 10701 / 30620) loss: 2.269971\n",
      "(Epoch 7 / 20) train acc: 0.274000; val_acc: 0.290000\n",
      "(Iteration 10801 / 30620) loss: 2.281231\n",
      "(Iteration 10901 / 30620) loss: 2.277982\n",
      "(Iteration 11001 / 30620) loss: 2.262772\n",
      "(Iteration 11101 / 30620) loss: 2.282855\n",
      "(Iteration 11201 / 30620) loss: 2.270029\n",
      "(Iteration 11301 / 30620) loss: 2.285470\n",
      "(Iteration 11401 / 30620) loss: 2.267200\n",
      "(Iteration 11501 / 30620) loss: 2.281106\n",
      "(Iteration 11601 / 30620) loss: 2.247234\n",
      "(Iteration 11701 / 30620) loss: 2.265980\n",
      "(Iteration 11801 / 30620) loss: 2.272738\n",
      "(Iteration 11901 / 30620) loss: 2.260325\n",
      "(Iteration 12001 / 30620) loss: 2.275889\n",
      "(Iteration 12101 / 30620) loss: 2.264233\n",
      "(Iteration 12201 / 30620) loss: 2.280664\n",
      "(Epoch 8 / 20) train acc: 0.246000; val_acc: 0.292000\n",
      "(Iteration 12301 / 30620) loss: 2.287867\n",
      "(Iteration 12401 / 30620) loss: 2.273508\n",
      "(Iteration 12501 / 30620) loss: 2.242155\n",
      "(Iteration 12601 / 30620) loss: 2.228440\n",
      "(Iteration 12701 / 30620) loss: 2.233406\n",
      "(Iteration 12801 / 30620) loss: 2.281882\n",
      "(Iteration 12901 / 30620) loss: 2.275802\n",
      "(Iteration 13001 / 30620) loss: 2.234894\n",
      "(Iteration 13101 / 30620) loss: 2.282629\n",
      "(Iteration 13201 / 30620) loss: 2.242548\n",
      "(Iteration 13301 / 30620) loss: 2.239994\n",
      "(Iteration 13401 / 30620) loss: 2.252322\n",
      "(Iteration 13501 / 30620) loss: 2.239599\n",
      "(Iteration 13601 / 30620) loss: 2.211676\n",
      "(Iteration 13701 / 30620) loss: 2.246774\n",
      "(Epoch 9 / 20) train acc: 0.273000; val_acc: 0.287000\n",
      "(Iteration 13801 / 30620) loss: 2.259376\n",
      "(Iteration 13901 / 30620) loss: 2.238027\n",
      "(Iteration 14001 / 30620) loss: 2.227848\n",
      "(Iteration 14101 / 30620) loss: 2.232380\n",
      "(Iteration 14201 / 30620) loss: 2.210768\n",
      "(Iteration 14301 / 30620) loss: 2.220631\n",
      "(Iteration 14401 / 30620) loss: 2.241012\n",
      "(Iteration 14501 / 30620) loss: 2.264826\n",
      "(Iteration 14601 / 30620) loss: 2.230600\n",
      "(Iteration 14701 / 30620) loss: 2.241299\n",
      "(Iteration 14801 / 30620) loss: 2.205841\n",
      "(Iteration 14901 / 30620) loss: 2.220788\n",
      "(Iteration 15001 / 30620) loss: 2.266624\n",
      "(Iteration 15101 / 30620) loss: 2.249080\n",
      "(Iteration 15201 / 30620) loss: 2.241420\n",
      "(Iteration 15301 / 30620) loss: 2.233727\n",
      "(Epoch 10 / 20) train acc: 0.262000; val_acc: 0.285000\n",
      "(Iteration 15401 / 30620) loss: 2.179104\n",
      "(Iteration 15501 / 30620) loss: 2.211945\n",
      "(Iteration 15601 / 30620) loss: 2.245169\n",
      "(Iteration 15701 / 30620) loss: 2.204966\n",
      "(Iteration 15801 / 30620) loss: 2.221312\n",
      "(Iteration 15901 / 30620) loss: 2.206358\n",
      "(Iteration 16001 / 30620) loss: 2.204864\n",
      "(Iteration 16101 / 30620) loss: 2.139422\n",
      "(Iteration 16201 / 30620) loss: 2.233391\n",
      "(Iteration 16301 / 30620) loss: 2.202803\n",
      "(Iteration 16401 / 30620) loss: 2.208607\n",
      "(Iteration 16501 / 30620) loss: 2.214566\n",
      "(Iteration 16601 / 30620) loss: 2.150562\n",
      "(Iteration 16701 / 30620) loss: 2.187178\n",
      "(Iteration 16801 / 30620) loss: 2.170379\n",
      "(Epoch 11 / 20) train acc: 0.269000; val_acc: 0.282000\n",
      "(Iteration 16901 / 30620) loss: 2.232846\n",
      "(Iteration 17001 / 30620) loss: 2.210250\n",
      "(Iteration 17101 / 30620) loss: 2.234241\n",
      "(Iteration 17201 / 30620) loss: 2.183460\n",
      "(Iteration 17301 / 30620) loss: 2.159513\n",
      "(Iteration 17401 / 30620) loss: 2.092274\n",
      "(Iteration 17501 / 30620) loss: 2.196898\n",
      "(Iteration 17601 / 30620) loss: 2.166304\n",
      "(Iteration 17701 / 30620) loss: 2.177005\n",
      "(Iteration 17801 / 30620) loss: 2.172322\n",
      "(Iteration 17901 / 30620) loss: 2.192101\n",
      "(Iteration 18001 / 30620) loss: 2.136495\n",
      "(Iteration 18101 / 30620) loss: 2.176192\n",
      "(Iteration 18201 / 30620) loss: 2.197962\n",
      "(Iteration 18301 / 30620) loss: 2.178574\n",
      "(Epoch 12 / 20) train acc: 0.273000; val_acc: 0.280000\n",
      "(Iteration 18401 / 30620) loss: 2.211260\n",
      "(Iteration 18501 / 30620) loss: 2.219696\n",
      "(Iteration 18601 / 30620) loss: 2.163607\n",
      "(Iteration 18701 / 30620) loss: 2.184087\n",
      "(Iteration 18801 / 30620) loss: 2.198407\n",
      "(Iteration 18901 / 30620) loss: 2.212530\n",
      "(Iteration 19001 / 30620) loss: 2.166298\n",
      "(Iteration 19101 / 30620) loss: 2.159503\n",
      "(Iteration 19201 / 30620) loss: 2.073331\n",
      "(Iteration 19301 / 30620) loss: 2.133369\n",
      "(Iteration 19401 / 30620) loss: 2.275958\n",
      "(Iteration 19501 / 30620) loss: 2.178736\n",
      "(Iteration 19601 / 30620) loss: 2.096605\n",
      "(Iteration 19701 / 30620) loss: 2.222207\n",
      "(Iteration 19801 / 30620) loss: 2.167319\n",
      "(Iteration 19901 / 30620) loss: 2.155622\n",
      "(Epoch 13 / 20) train acc: 0.261000; val_acc: 0.274000\n",
      "(Iteration 20001 / 30620) loss: 2.168583\n",
      "(Iteration 20101 / 30620) loss: 2.176182\n",
      "(Iteration 20201 / 30620) loss: 2.001908\n",
      "(Iteration 20301 / 30620) loss: 2.154380\n",
      "(Iteration 20401 / 30620) loss: 2.117502\n",
      "(Iteration 20501 / 30620) loss: 2.162707\n",
      "(Iteration 20601 / 30620) loss: 2.122255\n",
      "(Iteration 20701 / 30620) loss: 2.196205\n",
      "(Iteration 20801 / 30620) loss: 2.083661\n",
      "(Iteration 20901 / 30620) loss: 2.130067\n",
      "(Iteration 21001 / 30620) loss: 2.194400\n",
      "(Iteration 21101 / 30620) loss: 2.213745\n",
      "(Iteration 21201 / 30620) loss: 2.117351\n",
      "(Iteration 21301 / 30620) loss: 2.162520\n",
      "(Iteration 21401 / 30620) loss: 2.187888\n",
      "(Epoch 14 / 20) train acc: 0.268000; val_acc: 0.277000\n",
      "(Iteration 21501 / 30620) loss: 2.115660\n",
      "(Iteration 21601 / 30620) loss: 2.065793\n",
      "(Iteration 21701 / 30620) loss: 2.042017\n",
      "(Iteration 21801 / 30620) loss: 2.173428\n",
      "(Iteration 21901 / 30620) loss: 2.244684\n",
      "(Iteration 22001 / 30620) loss: 2.137609\n",
      "(Iteration 22101 / 30620) loss: 1.995412\n",
      "(Iteration 22201 / 30620) loss: 2.216537\n",
      "(Iteration 22301 / 30620) loss: 2.078508\n",
      "(Iteration 22401 / 30620) loss: 1.967415\n",
      "(Iteration 22501 / 30620) loss: 2.055674\n",
      "(Iteration 22601 / 30620) loss: 2.097157\n",
      "(Iteration 22701 / 30620) loss: 2.108042\n",
      "(Iteration 22801 / 30620) loss: 2.208612\n",
      "(Iteration 22901 / 30620) loss: 2.079396\n",
      "(Epoch 15 / 20) train acc: 0.266000; val_acc: 0.274000\n",
      "(Iteration 23001 / 30620) loss: 2.123470\n",
      "(Iteration 23101 / 30620) loss: 2.102748\n",
      "(Iteration 23201 / 30620) loss: 2.199559\n",
      "(Iteration 23301 / 30620) loss: 2.134100\n",
      "(Iteration 23401 / 30620) loss: 2.187558\n",
      "(Iteration 23501 / 30620) loss: 2.207867\n",
      "(Iteration 23601 / 30620) loss: 2.144131\n",
      "(Iteration 23701 / 30620) loss: 2.045431\n",
      "(Iteration 23801 / 30620) loss: 2.179624\n",
      "(Iteration 23901 / 30620) loss: 2.042988\n",
      "(Iteration 24001 / 30620) loss: 2.037969\n",
      "(Iteration 24101 / 30620) loss: 1.985854\n",
      "(Iteration 24201 / 30620) loss: 1.984695\n",
      "(Iteration 24301 / 30620) loss: 2.134951\n",
      "(Iteration 24401 / 30620) loss: 2.070117\n",
      "(Epoch 16 / 20) train acc: 0.270000; val_acc: 0.276000\n",
      "(Iteration 24501 / 30620) loss: 2.119981\n",
      "(Iteration 24601 / 30620) loss: 2.067597\n",
      "(Iteration 24701 / 30620) loss: 2.124949\n",
      "(Iteration 24801 / 30620) loss: 2.107738\n",
      "(Iteration 24901 / 30620) loss: 2.030554\n",
      "(Iteration 25001 / 30620) loss: 2.072973\n",
      "(Iteration 25101 / 30620) loss: 2.143232\n",
      "(Iteration 25201 / 30620) loss: 2.067881\n",
      "(Iteration 25301 / 30620) loss: 2.196597\n",
      "(Iteration 25401 / 30620) loss: 2.016075\n",
      "(Iteration 25501 / 30620) loss: 2.085746\n",
      "(Iteration 25601 / 30620) loss: 2.069137\n",
      "(Iteration 25701 / 30620) loss: 1.983662\n",
      "(Iteration 25801 / 30620) loss: 2.034424\n",
      "(Iteration 25901 / 30620) loss: 2.148415\n",
      "(Iteration 26001 / 30620) loss: 2.188529\n",
      "(Epoch 17 / 20) train acc: 0.265000; val_acc: 0.279000\n",
      "(Iteration 26101 / 30620) loss: 2.074358\n",
      "(Iteration 26201 / 30620) loss: 2.024608\n",
      "(Iteration 26301 / 30620) loss: 1.976075\n",
      "(Iteration 26401 / 30620) loss: 2.033348\n",
      "(Iteration 26501 / 30620) loss: 2.045470\n",
      "(Iteration 26601 / 30620) loss: 2.013560\n",
      "(Iteration 26701 / 30620) loss: 2.037834\n",
      "(Iteration 26801 / 30620) loss: 2.064127\n",
      "(Iteration 26901 / 30620) loss: 2.028111\n",
      "(Iteration 27001 / 30620) loss: 2.116873\n",
      "(Iteration 27101 / 30620) loss: 2.052317\n",
      "(Iteration 27201 / 30620) loss: 2.110432\n",
      "(Iteration 27301 / 30620) loss: 2.044147\n",
      "(Iteration 27401 / 30620) loss: 2.182131\n",
      "(Iteration 27501 / 30620) loss: 2.107590\n",
      "(Epoch 18 / 20) train acc: 0.281000; val_acc: 0.282000\n",
      "(Iteration 27601 / 30620) loss: 2.031335\n",
      "(Iteration 27701 / 30620) loss: 1.991999\n",
      "(Iteration 27801 / 30620) loss: 2.108110\n",
      "(Iteration 27901 / 30620) loss: 2.048232\n",
      "(Iteration 28001 / 30620) loss: 2.041472\n",
      "(Iteration 28101 / 30620) loss: 2.157462\n",
      "(Iteration 28201 / 30620) loss: 2.031722\n",
      "(Iteration 28301 / 30620) loss: 1.931122\n",
      "(Iteration 28401 / 30620) loss: 2.061885\n",
      "(Iteration 28501 / 30620) loss: 2.073522\n",
      "(Iteration 28601 / 30620) loss: 2.139779\n",
      "(Iteration 28701 / 30620) loss: 2.089212\n",
      "(Iteration 28801 / 30620) loss: 2.178849\n",
      "(Iteration 28901 / 30620) loss: 2.118343\n",
      "(Iteration 29001 / 30620) loss: 2.033915\n",
      "(Epoch 19 / 20) train acc: 0.297000; val_acc: 0.284000\n",
      "(Iteration 29101 / 30620) loss: 1.990223\n",
      "(Iteration 29201 / 30620) loss: 2.102520\n",
      "(Iteration 29301 / 30620) loss: 2.117294\n",
      "(Iteration 29401 / 30620) loss: 2.050289\n",
      "(Iteration 29501 / 30620) loss: 2.106045\n",
      "(Iteration 29601 / 30620) loss: 2.081055\n",
      "(Iteration 29701 / 30620) loss: 2.166513\n",
      "(Iteration 29801 / 30620) loss: 2.020384\n",
      "(Iteration 29901 / 30620) loss: 2.039931\n",
      "(Iteration 30001 / 30620) loss: 2.107850\n",
      "(Iteration 30101 / 30620) loss: 2.083739\n",
      "(Iteration 30201 / 30620) loss: 2.022532\n",
      "(Iteration 30301 / 30620) loss: 2.013160\n",
      "(Iteration 30401 / 30620) loss: 1.990408\n",
      "(Iteration 30501 / 30620) loss: 2.081737\n",
      "(Iteration 30601 / 30620) loss: 2.037205\n",
      "(Epoch 20 / 20) train acc: 0.299000; val_acc: 0.291000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.001, 'num_epochs': 30, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 22950) loss: 2.309185\n",
      "(Epoch 0 / 30) train acc: 0.072000; val_acc: 0.093000\n",
      "(Iteration 101 / 22950) loss: 2.309058\n",
      "(Iteration 201 / 22950) loss: 2.308846\n",
      "(Iteration 301 / 22950) loss: 2.308769\n",
      "(Iteration 401 / 22950) loss: 2.308604\n",
      "(Iteration 501 / 22950) loss: 2.308539\n",
      "(Iteration 601 / 22950) loss: 2.308294\n",
      "(Iteration 701 / 22950) loss: 2.308150\n",
      "(Epoch 1 / 30) train acc: 0.133000; val_acc: 0.127000\n",
      "(Iteration 801 / 22950) loss: 2.308143\n",
      "(Iteration 901 / 22950) loss: 2.307884\n",
      "(Iteration 1001 / 22950) loss: 2.307756\n",
      "(Iteration 1101 / 22950) loss: 2.307666\n",
      "(Iteration 1201 / 22950) loss: 2.307752\n",
      "(Iteration 1301 / 22950) loss: 2.307343\n",
      "(Iteration 1401 / 22950) loss: 2.307503\n",
      "(Iteration 1501 / 22950) loss: 2.307458\n",
      "(Epoch 2 / 30) train acc: 0.116000; val_acc: 0.085000\n",
      "(Iteration 1601 / 22950) loss: 2.306985\n",
      "(Iteration 1701 / 22950) loss: 2.307339\n",
      "(Iteration 1801 / 22950) loss: 2.306943\n",
      "(Iteration 1901 / 22950) loss: 2.306870\n",
      "(Iteration 2001 / 22950) loss: 2.306614\n",
      "(Iteration 2101 / 22950) loss: 2.306875\n",
      "(Iteration 2201 / 22950) loss: 2.306740\n",
      "(Epoch 3 / 30) train acc: 0.099000; val_acc: 0.086000\n",
      "(Iteration 2301 / 22950) loss: 2.306809\n",
      "(Iteration 2401 / 22950) loss: 2.306563\n",
      "(Iteration 2501 / 22950) loss: 2.306500\n",
      "(Iteration 2601 / 22950) loss: 2.306408\n",
      "(Iteration 2701 / 22950) loss: 2.305902\n",
      "(Iteration 2801 / 22950) loss: 2.306220\n",
      "(Iteration 2901 / 22950) loss: 2.306664\n",
      "(Iteration 3001 / 22950) loss: 2.306116\n",
      "(Epoch 4 / 30) train acc: 0.120000; val_acc: 0.085000\n",
      "(Iteration 3101 / 22950) loss: 2.305677\n",
      "(Iteration 3201 / 22950) loss: 2.305774\n",
      "(Iteration 3301 / 22950) loss: 2.305713\n",
      "(Iteration 3401 / 22950) loss: 2.305750\n",
      "(Iteration 3501 / 22950) loss: 2.305809\n",
      "(Iteration 3601 / 22950) loss: 2.305869\n",
      "(Iteration 3701 / 22950) loss: 2.305856\n",
      "(Iteration 3801 / 22950) loss: 2.305559\n",
      "(Epoch 5 / 30) train acc: 0.122000; val_acc: 0.103000\n",
      "(Iteration 3901 / 22950) loss: 2.305304\n",
      "(Iteration 4001 / 22950) loss: 2.305460\n",
      "(Iteration 4101 / 22950) loss: 2.304895\n",
      "(Iteration 4201 / 22950) loss: 2.305069\n",
      "(Iteration 4301 / 22950) loss: 2.305296\n",
      "(Iteration 4401 / 22950) loss: 2.305258\n",
      "(Iteration 4501 / 22950) loss: 2.305167\n",
      "(Epoch 6 / 30) train acc: 0.205000; val_acc: 0.145000\n",
      "(Iteration 4601 / 22950) loss: 2.305240\n",
      "(Iteration 4701 / 22950) loss: 2.305422\n",
      "(Iteration 4801 / 22950) loss: 2.304940\n",
      "(Iteration 4901 / 22950) loss: 2.304863\n",
      "(Iteration 5001 / 22950) loss: 2.304958\n",
      "(Iteration 5101 / 22950) loss: 2.304691\n",
      "(Iteration 5201 / 22950) loss: 2.304940\n",
      "(Iteration 5301 / 22950) loss: 2.304843\n",
      "(Epoch 7 / 30) train acc: 0.244000; val_acc: 0.204000\n",
      "(Iteration 5401 / 22950) loss: 2.305024\n",
      "(Iteration 5501 / 22950) loss: 2.304789\n",
      "(Iteration 5601 / 22950) loss: 2.304362\n",
      "(Iteration 5701 / 22950) loss: 2.304450\n",
      "(Iteration 5801 / 22950) loss: 2.304258\n",
      "(Iteration 5901 / 22950) loss: 2.304671\n",
      "(Iteration 6001 / 22950) loss: 2.304924\n",
      "(Iteration 6101 / 22950) loss: 2.304078\n",
      "(Epoch 8 / 30) train acc: 0.236000; val_acc: 0.211000\n",
      "(Iteration 6201 / 22950) loss: 2.304820\n",
      "(Iteration 6301 / 22950) loss: 2.304954\n",
      "(Iteration 6401 / 22950) loss: 2.304379\n",
      "(Iteration 6501 / 22950) loss: 2.304648\n",
      "(Iteration 6601 / 22950) loss: 2.304009\n",
      "(Iteration 6701 / 22950) loss: 2.304711\n",
      "(Iteration 6801 / 22950) loss: 2.304394\n",
      "(Epoch 9 / 30) train acc: 0.242000; val_acc: 0.221000\n",
      "(Iteration 6901 / 22950) loss: 2.304269\n",
      "(Iteration 7001 / 22950) loss: 2.304242\n",
      "(Iteration 7101 / 22950) loss: 2.304266\n",
      "(Iteration 7201 / 22950) loss: 2.304004\n",
      "(Iteration 7301 / 22950) loss: 2.304412\n",
      "(Iteration 7401 / 22950) loss: 2.304157\n",
      "(Iteration 7501 / 22950) loss: 2.303724\n",
      "(Iteration 7601 / 22950) loss: 2.304555\n",
      "(Epoch 10 / 30) train acc: 0.229000; val_acc: 0.198000\n",
      "(Iteration 7701 / 22950) loss: 2.304005\n",
      "(Iteration 7801 / 22950) loss: 2.303773\n",
      "(Iteration 7901 / 22950) loss: 2.303336\n",
      "(Iteration 8001 / 22950) loss: 2.303921\n",
      "(Iteration 8101 / 22950) loss: 2.303561\n",
      "(Iteration 8201 / 22950) loss: 2.303586\n",
      "(Iteration 8301 / 22950) loss: 2.303940\n",
      "(Iteration 8401 / 22950) loss: 2.303320\n",
      "(Epoch 11 / 30) train acc: 0.234000; val_acc: 0.203000\n",
      "(Iteration 8501 / 22950) loss: 2.303602\n",
      "(Iteration 8601 / 22950) loss: 2.304040\n",
      "(Iteration 8701 / 22950) loss: 2.303300\n",
      "(Iteration 8801 / 22950) loss: 2.303387\n",
      "(Iteration 8901 / 22950) loss: 2.303093\n",
      "(Iteration 9001 / 22950) loss: 2.303590\n",
      "(Iteration 9101 / 22950) loss: 2.303072\n",
      "(Epoch 12 / 30) train acc: 0.219000; val_acc: 0.200000\n",
      "(Iteration 9201 / 22950) loss: 2.303355\n",
      "(Iteration 9301 / 22950) loss: 2.303602\n",
      "(Iteration 9401 / 22950) loss: 2.303186\n",
      "(Iteration 9501 / 22950) loss: 2.303008\n",
      "(Iteration 9601 / 22950) loss: 2.302868\n",
      "(Iteration 9701 / 22950) loss: 2.302908\n",
      "(Iteration 9801 / 22950) loss: 2.303300\n",
      "(Iteration 9901 / 22950) loss: 2.303322\n",
      "(Epoch 13 / 30) train acc: 0.222000; val_acc: 0.201000\n",
      "(Iteration 10001 / 22950) loss: 2.302809\n",
      "(Iteration 10101 / 22950) loss: 2.302807\n",
      "(Iteration 10201 / 22950) loss: 2.302506\n",
      "(Iteration 10301 / 22950) loss: 2.303056\n",
      "(Iteration 10401 / 22950) loss: 2.302485\n",
      "(Iteration 10501 / 22950) loss: 2.302981\n",
      "(Iteration 10601 / 22950) loss: 2.303162\n",
      "(Iteration 10701 / 22950) loss: 2.303341\n",
      "(Epoch 14 / 30) train acc: 0.222000; val_acc: 0.214000\n",
      "(Iteration 10801 / 22950) loss: 2.303016\n",
      "(Iteration 10901 / 22950) loss: 2.302255\n",
      "(Iteration 11001 / 22950) loss: 2.303323\n",
      "(Iteration 11101 / 22950) loss: 2.302794\n",
      "(Iteration 11201 / 22950) loss: 2.302956\n",
      "(Iteration 11301 / 22950) loss: 2.303427\n",
      "(Iteration 11401 / 22950) loss: 2.302272\n",
      "(Epoch 15 / 30) train acc: 0.238000; val_acc: 0.216000\n",
      "(Iteration 11501 / 22950) loss: 2.303068\n",
      "(Iteration 11601 / 22950) loss: 2.302279\n",
      "(Iteration 11701 / 22950) loss: 2.302598\n",
      "(Iteration 11801 / 22950) loss: 2.303126\n",
      "(Iteration 11901 / 22950) loss: 2.302792\n",
      "(Iteration 12001 / 22950) loss: 2.303682\n",
      "(Iteration 12101 / 22950) loss: 2.302729\n",
      "(Iteration 12201 / 22950) loss: 2.302460\n",
      "(Epoch 16 / 30) train acc: 0.220000; val_acc: 0.219000\n",
      "(Iteration 12301 / 22950) loss: 2.302987\n",
      "(Iteration 12401 / 22950) loss: 2.301687\n",
      "(Iteration 12501 / 22950) loss: 2.302640\n",
      "(Iteration 12601 / 22950) loss: 2.302497\n",
      "(Iteration 12701 / 22950) loss: 2.302151\n",
      "(Iteration 12801 / 22950) loss: 2.301531\n",
      "(Iteration 12901 / 22950) loss: 2.302697\n",
      "(Iteration 13001 / 22950) loss: 2.301944\n",
      "(Epoch 17 / 30) train acc: 0.249000; val_acc: 0.223000\n",
      "(Iteration 13101 / 22950) loss: 2.301472\n",
      "(Iteration 13201 / 22950) loss: 2.302265\n",
      "(Iteration 13301 / 22950) loss: 2.302631\n",
      "(Iteration 13401 / 22950) loss: 2.301743\n",
      "(Iteration 13501 / 22950) loss: 2.302157\n",
      "(Iteration 13601 / 22950) loss: 2.302224\n",
      "(Iteration 13701 / 22950) loss: 2.301971\n",
      "(Epoch 18 / 30) train acc: 0.229000; val_acc: 0.222000\n",
      "(Iteration 13801 / 22950) loss: 2.302557\n",
      "(Iteration 13901 / 22950) loss: 2.301824\n",
      "(Iteration 14001 / 22950) loss: 2.302851\n",
      "(Iteration 14101 / 22950) loss: 2.302778\n",
      "(Iteration 14201 / 22950) loss: 2.302323\n",
      "(Iteration 14301 / 22950) loss: 2.301363\n",
      "(Iteration 14401 / 22950) loss: 2.301979\n",
      "(Iteration 14501 / 22950) loss: 2.302421\n",
      "(Epoch 19 / 30) train acc: 0.243000; val_acc: 0.221000\n",
      "(Iteration 14601 / 22950) loss: 2.302065\n",
      "(Iteration 14701 / 22950) loss: 2.302701\n",
      "(Iteration 14801 / 22950) loss: 2.301697\n",
      "(Iteration 14901 / 22950) loss: 2.301768\n",
      "(Iteration 15001 / 22950) loss: 2.302642\n",
      "(Iteration 15101 / 22950) loss: 2.302408\n",
      "(Iteration 15201 / 22950) loss: 2.301579\n",
      "(Epoch 20 / 30) train acc: 0.257000; val_acc: 0.224000\n",
      "(Iteration 15301 / 22950) loss: 2.302435\n",
      "(Iteration 15401 / 22950) loss: 2.301483\n",
      "(Iteration 15501 / 22950) loss: 2.300849\n",
      "(Iteration 15601 / 22950) loss: 2.301908\n",
      "(Iteration 15701 / 22950) loss: 2.300686\n",
      "(Iteration 15801 / 22950) loss: 2.300804\n",
      "(Iteration 15901 / 22950) loss: 2.301710\n",
      "(Iteration 16001 / 22950) loss: 2.301815\n",
      "(Epoch 21 / 30) train acc: 0.252000; val_acc: 0.226000\n",
      "(Iteration 16101 / 22950) loss: 2.301304\n",
      "(Iteration 16201 / 22950) loss: 2.302298\n",
      "(Iteration 16301 / 22950) loss: 2.301983\n",
      "(Iteration 16401 / 22950) loss: 2.302576\n",
      "(Iteration 16501 / 22950) loss: 2.301128\n",
      "(Iteration 16601 / 22950) loss: 2.300999\n",
      "(Iteration 16701 / 22950) loss: 2.302116\n",
      "(Iteration 16801 / 22950) loss: 2.301670\n",
      "(Epoch 22 / 30) train acc: 0.233000; val_acc: 0.234000\n",
      "(Iteration 16901 / 22950) loss: 2.301786\n",
      "(Iteration 17001 / 22950) loss: 2.302278\n",
      "(Iteration 17101 / 22950) loss: 2.300066\n",
      "(Iteration 17201 / 22950) loss: 2.300907\n",
      "(Iteration 17301 / 22950) loss: 2.301156\n",
      "(Iteration 17401 / 22950) loss: 2.301089\n",
      "(Iteration 17501 / 22950) loss: 2.300677\n",
      "(Epoch 23 / 30) train acc: 0.234000; val_acc: 0.231000\n",
      "(Iteration 17601 / 22950) loss: 2.302170\n",
      "(Iteration 17701 / 22950) loss: 2.301073\n",
      "(Iteration 17801 / 22950) loss: 2.302184\n",
      "(Iteration 17901 / 22950) loss: 2.301567\n",
      "(Iteration 18001 / 22950) loss: 2.301372\n",
      "(Iteration 18101 / 22950) loss: 2.301524\n",
      "(Iteration 18201 / 22950) loss: 2.301482\n",
      "(Iteration 18301 / 22950) loss: 2.300340\n",
      "(Epoch 24 / 30) train acc: 0.273000; val_acc: 0.226000\n",
      "(Iteration 18401 / 22950) loss: 2.300517\n",
      "(Iteration 18501 / 22950) loss: 2.301418\n",
      "(Iteration 18601 / 22950) loss: 2.300323\n",
      "(Iteration 18701 / 22950) loss: 2.299938\n",
      "(Iteration 18801 / 22950) loss: 2.300773\n",
      "(Iteration 18901 / 22950) loss: 2.301342\n",
      "(Iteration 19001 / 22950) loss: 2.301848\n",
      "(Iteration 19101 / 22950) loss: 2.300960\n",
      "(Epoch 25 / 30) train acc: 0.233000; val_acc: 0.234000\n",
      "(Iteration 19201 / 22950) loss: 2.302474\n",
      "(Iteration 19301 / 22950) loss: 2.300007\n",
      "(Iteration 19401 / 22950) loss: 2.302223\n",
      "(Iteration 19501 / 22950) loss: 2.300508\n",
      "(Iteration 19601 / 22950) loss: 2.301067\n",
      "(Iteration 19701 / 22950) loss: 2.302630\n",
      "(Iteration 19801 / 22950) loss: 2.301281\n",
      "(Epoch 26 / 30) train acc: 0.229000; val_acc: 0.242000\n",
      "(Iteration 19901 / 22950) loss: 2.300598\n",
      "(Iteration 20001 / 22950) loss: 2.302084\n",
      "(Iteration 20101 / 22950) loss: 2.302508\n",
      "(Iteration 20201 / 22950) loss: 2.299880\n",
      "(Iteration 20301 / 22950) loss: 2.301134\n",
      "(Iteration 20401 / 22950) loss: 2.301023\n",
      "(Iteration 20501 / 22950) loss: 2.300147\n",
      "(Iteration 20601 / 22950) loss: 2.300564\n",
      "(Epoch 27 / 30) train acc: 0.247000; val_acc: 0.244000\n",
      "(Iteration 20701 / 22950) loss: 2.300424\n",
      "(Iteration 20801 / 22950) loss: 2.301900\n",
      "(Iteration 20901 / 22950) loss: 2.301352\n",
      "(Iteration 21001 / 22950) loss: 2.301612\n",
      "(Iteration 21101 / 22950) loss: 2.300988\n",
      "(Iteration 21201 / 22950) loss: 2.301339\n",
      "(Iteration 21301 / 22950) loss: 2.302625\n",
      "(Iteration 21401 / 22950) loss: 2.301368\n",
      "(Epoch 28 / 30) train acc: 0.239000; val_acc: 0.243000\n",
      "(Iteration 21501 / 22950) loss: 2.300969\n",
      "(Iteration 21601 / 22950) loss: 2.300171\n",
      "(Iteration 21701 / 22950) loss: 2.301480\n",
      "(Iteration 21801 / 22950) loss: 2.300448\n",
      "(Iteration 21901 / 22950) loss: 2.300959\n",
      "(Iteration 22001 / 22950) loss: 2.301288\n",
      "(Iteration 22101 / 22950) loss: 2.301102\n",
      "(Epoch 29 / 30) train acc: 0.268000; val_acc: 0.241000\n",
      "(Iteration 22201 / 22950) loss: 2.300370\n",
      "(Iteration 22301 / 22950) loss: 2.300611\n",
      "(Iteration 22401 / 22950) loss: 2.300308\n",
      "(Iteration 22501 / 22950) loss: 2.301457\n",
      "(Iteration 22601 / 22950) loss: 2.301658\n",
      "(Iteration 22701 / 22950) loss: 2.301873\n",
      "(Iteration 22801 / 22950) loss: 2.300701\n",
      "(Iteration 22901 / 22950) loss: 2.300846\n",
      "(Epoch 30 / 30) train acc: 0.229000; val_acc: 0.239000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.001, 'num_epochs': 30, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 45930) loss: 2.309176\n",
      "(Epoch 0 / 30) train acc: 0.101000; val_acc: 0.090000\n",
      "(Iteration 101 / 45930) loss: 2.308933\n",
      "(Iteration 201 / 45930) loss: 2.308870\n",
      "(Iteration 301 / 45930) loss: 2.308717\n",
      "(Iteration 401 / 45930) loss: 2.308877\n",
      "(Iteration 501 / 45930) loss: 2.307882\n",
      "(Iteration 601 / 45930) loss: 2.308650\n",
      "(Iteration 701 / 45930) loss: 2.308247\n",
      "(Iteration 801 / 45930) loss: 2.308045\n",
      "(Iteration 901 / 45930) loss: 2.307752\n",
      "(Iteration 1001 / 45930) loss: 2.307763\n",
      "(Iteration 1101 / 45930) loss: 2.307010\n",
      "(Iteration 1201 / 45930) loss: 2.307001\n",
      "(Iteration 1301 / 45930) loss: 2.307210\n",
      "(Iteration 1401 / 45930) loss: 2.307236\n",
      "(Iteration 1501 / 45930) loss: 2.306986\n",
      "(Epoch 1 / 30) train acc: 0.174000; val_acc: 0.119000\n",
      "(Iteration 1601 / 45930) loss: 2.307329\n",
      "(Iteration 1701 / 45930) loss: 2.307042\n",
      "(Iteration 1801 / 45930) loss: 2.305798\n",
      "(Iteration 1901 / 45930) loss: 2.305732\n",
      "(Iteration 2001 / 45930) loss: 2.306721\n",
      "(Iteration 2101 / 45930) loss: 2.306314\n",
      "(Iteration 2201 / 45930) loss: 2.305988\n",
      "(Iteration 2301 / 45930) loss: 2.305740\n",
      "(Iteration 2401 / 45930) loss: 2.306021\n",
      "(Iteration 2501 / 45930) loss: 2.306428\n",
      "(Iteration 2601 / 45930) loss: 2.306164\n",
      "(Iteration 2701 / 45930) loss: 2.305582\n",
      "(Iteration 2801 / 45930) loss: 2.305845\n",
      "(Iteration 2901 / 45930) loss: 2.305335\n",
      "(Iteration 3001 / 45930) loss: 2.305289\n",
      "(Epoch 2 / 30) train acc: 0.107000; val_acc: 0.090000\n",
      "(Iteration 3101 / 45930) loss: 2.306117\n",
      "(Iteration 3201 / 45930) loss: 2.305636\n",
      "(Iteration 3301 / 45930) loss: 2.305490\n",
      "(Iteration 3401 / 45930) loss: 2.304407\n",
      "(Iteration 3501 / 45930) loss: 2.304864\n",
      "(Iteration 3601 / 45930) loss: 2.305247\n",
      "(Iteration 3701 / 45930) loss: 2.305098\n",
      "(Iteration 3801 / 45930) loss: 2.305213\n",
      "(Iteration 3901 / 45930) loss: 2.305214\n",
      "(Iteration 4001 / 45930) loss: 2.304798\n",
      "(Iteration 4101 / 45930) loss: 2.304503\n",
      "(Iteration 4201 / 45930) loss: 2.304786\n",
      "(Iteration 4301 / 45930) loss: 2.304763\n",
      "(Iteration 4401 / 45930) loss: 2.304157\n",
      "(Iteration 4501 / 45930) loss: 2.304327\n",
      "(Epoch 3 / 30) train acc: 0.147000; val_acc: 0.142000\n",
      "(Iteration 4601 / 45930) loss: 2.304657\n",
      "(Iteration 4701 / 45930) loss: 2.304436\n",
      "(Iteration 4801 / 45930) loss: 2.304306\n",
      "(Iteration 4901 / 45930) loss: 2.304912\n",
      "(Iteration 5001 / 45930) loss: 2.304006\n",
      "(Iteration 5101 / 45930) loss: 2.303863\n",
      "(Iteration 5201 / 45930) loss: 2.304486\n",
      "(Iteration 5301 / 45930) loss: 2.303683\n",
      "(Iteration 5401 / 45930) loss: 2.304568\n",
      "(Iteration 5501 / 45930) loss: 2.303938\n",
      "(Iteration 5601 / 45930) loss: 2.304315\n",
      "(Iteration 5701 / 45930) loss: 2.303003\n",
      "(Iteration 5801 / 45930) loss: 2.303555\n",
      "(Iteration 5901 / 45930) loss: 2.304176\n",
      "(Iteration 6001 / 45930) loss: 2.302387\n",
      "(Iteration 6101 / 45930) loss: 2.303394\n",
      "(Epoch 4 / 30) train acc: 0.176000; val_acc: 0.168000\n",
      "(Iteration 6201 / 45930) loss: 2.303846\n",
      "(Iteration 6301 / 45930) loss: 2.302875\n",
      "(Iteration 6401 / 45930) loss: 2.303300\n",
      "(Iteration 6501 / 45930) loss: 2.303167\n",
      "(Iteration 6601 / 45930) loss: 2.303431\n",
      "(Iteration 6701 / 45930) loss: 2.303849\n",
      "(Iteration 6801 / 45930) loss: 2.303388\n",
      "(Iteration 6901 / 45930) loss: 2.303617\n",
      "(Iteration 7001 / 45930) loss: 2.302389\n",
      "(Iteration 7101 / 45930) loss: 2.303138\n",
      "(Iteration 7201 / 45930) loss: 2.302858\n",
      "(Iteration 7301 / 45930) loss: 2.303473\n",
      "(Iteration 7401 / 45930) loss: 2.303083\n",
      "(Iteration 7501 / 45930) loss: 2.303185\n",
      "(Iteration 7601 / 45930) loss: 2.303462\n",
      "(Epoch 5 / 30) train acc: 0.168000; val_acc: 0.151000\n",
      "(Iteration 7701 / 45930) loss: 2.302833\n",
      "(Iteration 7801 / 45930) loss: 2.303805\n",
      "(Iteration 7901 / 45930) loss: 2.302971\n",
      "(Iteration 8001 / 45930) loss: 2.302641\n",
      "(Iteration 8101 / 45930) loss: 2.302971\n",
      "(Iteration 8201 / 45930) loss: 2.303190\n",
      "(Iteration 8301 / 45930) loss: 2.300270\n",
      "(Iteration 8401 / 45930) loss: 2.300349\n",
      "(Iteration 8501 / 45930) loss: 2.300926\n",
      "(Iteration 8601 / 45930) loss: 2.303067\n",
      "(Iteration 8701 / 45930) loss: 2.300898\n",
      "(Iteration 8801 / 45930) loss: 2.304706\n",
      "(Iteration 8901 / 45930) loss: 2.301051\n",
      "(Iteration 9001 / 45930) loss: 2.302221\n",
      "(Iteration 9101 / 45930) loss: 2.299640\n",
      "(Epoch 6 / 30) train acc: 0.164000; val_acc: 0.148000\n",
      "(Iteration 9201 / 45930) loss: 2.300151\n",
      "(Iteration 9301 / 45930) loss: 2.299277\n",
      "(Iteration 9401 / 45930) loss: 2.301397\n",
      "(Iteration 9501 / 45930) loss: 2.300015\n",
      "(Iteration 9601 / 45930) loss: 2.300008\n",
      "(Iteration 9701 / 45930) loss: 2.300916\n",
      "(Iteration 9801 / 45930) loss: 2.301100\n",
      "(Iteration 9901 / 45930) loss: 2.299226\n",
      "(Iteration 10001 / 45930) loss: 2.299521\n",
      "(Iteration 10101 / 45930) loss: 2.301890\n",
      "(Iteration 10201 / 45930) loss: 2.299207\n",
      "(Iteration 10301 / 45930) loss: 2.296768\n",
      "(Iteration 10401 / 45930) loss: 2.296882\n",
      "(Iteration 10501 / 45930) loss: 2.304718\n",
      "(Iteration 10601 / 45930) loss: 2.299489\n",
      "(Iteration 10701 / 45930) loss: 2.299337\n",
      "(Epoch 7 / 30) train acc: 0.197000; val_acc: 0.176000\n",
      "(Iteration 10801 / 45930) loss: 2.299863\n",
      "(Iteration 10901 / 45930) loss: 2.295340\n",
      "(Iteration 11001 / 45930) loss: 2.300275\n",
      "(Iteration 11101 / 45930) loss: 2.301276\n",
      "(Iteration 11201 / 45930) loss: 2.296094\n",
      "(Iteration 11301 / 45930) loss: 2.296984\n",
      "(Iteration 11401 / 45930) loss: 2.300953\n",
      "(Iteration 11501 / 45930) loss: 2.295931\n",
      "(Iteration 11601 / 45930) loss: 2.299784\n",
      "(Iteration 11701 / 45930) loss: 2.296323\n",
      "(Iteration 11801 / 45930) loss: 2.300939\n",
      "(Iteration 11901 / 45930) loss: 2.292979\n",
      "(Iteration 12001 / 45930) loss: 2.298315\n",
      "(Iteration 12101 / 45930) loss: 2.294878\n",
      "(Iteration 12201 / 45930) loss: 2.299610\n",
      "(Epoch 8 / 30) train acc: 0.197000; val_acc: 0.183000\n",
      "(Iteration 12301 / 45930) loss: 2.294967\n",
      "(Iteration 12401 / 45930) loss: 2.299304\n",
      "(Iteration 12501 / 45930) loss: 2.290115\n",
      "(Iteration 12601 / 45930) loss: 2.290780\n",
      "(Iteration 12701 / 45930) loss: 2.298607\n",
      "(Iteration 12801 / 45930) loss: 2.293401\n",
      "(Iteration 12901 / 45930) loss: 2.293036\n",
      "(Iteration 13001 / 45930) loss: 2.296301\n",
      "(Iteration 13101 / 45930) loss: 2.299263\n",
      "(Iteration 13201 / 45930) loss: 2.295661\n",
      "(Iteration 13301 / 45930) loss: 2.294023\n",
      "(Iteration 13401 / 45930) loss: 2.291471\n",
      "(Iteration 13501 / 45930) loss: 2.294319\n",
      "(Iteration 13601 / 45930) loss: 2.296232\n",
      "(Iteration 13701 / 45930) loss: 2.299556\n",
      "(Epoch 9 / 30) train acc: 0.202000; val_acc: 0.194000\n",
      "(Iteration 13801 / 45930) loss: 2.297249\n",
      "(Iteration 13901 / 45930) loss: 2.291226\n",
      "(Iteration 14001 / 45930) loss: 2.285740\n",
      "(Iteration 14101 / 45930) loss: 2.291825\n",
      "(Iteration 14201 / 45930) loss: 2.296088\n",
      "(Iteration 14301 / 45930) loss: 2.296864\n",
      "(Iteration 14401 / 45930) loss: 2.285504\n",
      "(Iteration 14501 / 45930) loss: 2.300155\n",
      "(Iteration 14601 / 45930) loss: 2.294204\n",
      "(Iteration 14701 / 45930) loss: 2.292980\n",
      "(Iteration 14801 / 45930) loss: 2.288215\n",
      "(Iteration 14901 / 45930) loss: 2.281307\n",
      "(Iteration 15001 / 45930) loss: 2.287944\n",
      "(Iteration 15101 / 45930) loss: 2.290394\n",
      "(Iteration 15201 / 45930) loss: 2.287518\n",
      "(Iteration 15301 / 45930) loss: 2.296011\n",
      "(Epoch 10 / 30) train acc: 0.216000; val_acc: 0.196000\n",
      "(Iteration 15401 / 45930) loss: 2.299362\n",
      "(Iteration 15501 / 45930) loss: 2.296671\n",
      "(Iteration 15601 / 45930) loss: 2.280369\n",
      "(Iteration 15701 / 45930) loss: 2.297408\n",
      "(Iteration 15801 / 45930) loss: 2.296349\n",
      "(Iteration 15901 / 45930) loss: 2.284571\n",
      "(Iteration 16001 / 45930) loss: 2.279349\n",
      "(Iteration 16101 / 45930) loss: 2.289328\n",
      "(Iteration 16201 / 45930) loss: 2.290294\n",
      "(Iteration 16301 / 45930) loss: 2.292101\n",
      "(Iteration 16401 / 45930) loss: 2.289899\n",
      "(Iteration 16501 / 45930) loss: 2.285040\n",
      "(Iteration 16601 / 45930) loss: 2.279800\n",
      "(Iteration 16701 / 45930) loss: 2.280426\n",
      "(Iteration 16801 / 45930) loss: 2.290573\n",
      "(Epoch 11 / 30) train acc: 0.216000; val_acc: 0.204000\n",
      "(Iteration 16901 / 45930) loss: 2.294257\n",
      "(Iteration 17001 / 45930) loss: 2.297693\n",
      "(Iteration 17101 / 45930) loss: 2.296132\n",
      "(Iteration 17201 / 45930) loss: 2.295600\n",
      "(Iteration 17301 / 45930) loss: 2.282187\n",
      "(Iteration 17401 / 45930) loss: 2.288052\n",
      "(Iteration 17501 / 45930) loss: 2.271848\n",
      "(Iteration 17601 / 45930) loss: 2.273189\n",
      "(Iteration 17701 / 45930) loss: 2.302325\n",
      "(Iteration 17801 / 45930) loss: 2.285777\n",
      "(Iteration 17901 / 45930) loss: 2.278920\n",
      "(Iteration 18001 / 45930) loss: 2.285530\n",
      "(Iteration 18101 / 45930) loss: 2.290413\n",
      "(Iteration 18201 / 45930) loss: 2.302039\n",
      "(Iteration 18301 / 45930) loss: 2.290804\n",
      "(Epoch 12 / 30) train acc: 0.233000; val_acc: 0.226000\n",
      "(Iteration 18401 / 45930) loss: 2.270818\n",
      "(Iteration 18501 / 45930) loss: 2.290860\n",
      "(Iteration 18601 / 45930) loss: 2.279011\n",
      "(Iteration 18701 / 45930) loss: 2.271598\n",
      "(Iteration 18801 / 45930) loss: 2.280367\n",
      "(Iteration 18901 / 45930) loss: 2.262989\n",
      "(Iteration 19001 / 45930) loss: 2.273069\n",
      "(Iteration 19101 / 45930) loss: 2.284393\n",
      "(Iteration 19201 / 45930) loss: 2.265953\n",
      "(Iteration 19301 / 45930) loss: 2.282611\n",
      "(Iteration 19401 / 45930) loss: 2.274802\n",
      "(Iteration 19501 / 45930) loss: 2.268944\n",
      "(Iteration 19601 / 45930) loss: 2.275394\n",
      "(Iteration 19701 / 45930) loss: 2.291855\n",
      "(Iteration 19801 / 45930) loss: 2.289983\n",
      "(Iteration 19901 / 45930) loss: 2.273249\n",
      "(Epoch 13 / 30) train acc: 0.263000; val_acc: 0.242000\n",
      "(Iteration 20001 / 45930) loss: 2.284538\n",
      "(Iteration 20101 / 45930) loss: 2.277266\n",
      "(Iteration 20201 / 45930) loss: 2.279384\n",
      "(Iteration 20301 / 45930) loss: 2.281417\n",
      "(Iteration 20401 / 45930) loss: 2.257484\n",
      "(Iteration 20501 / 45930) loss: 2.254051\n",
      "(Iteration 20601 / 45930) loss: 2.242955\n",
      "(Iteration 20701 / 45930) loss: 2.290453\n",
      "(Iteration 20801 / 45930) loss: 2.264102\n",
      "(Iteration 20901 / 45930) loss: 2.264062\n",
      "(Iteration 21001 / 45930) loss: 2.260430\n",
      "(Iteration 21101 / 45930) loss: 2.272310\n",
      "(Iteration 21201 / 45930) loss: 2.263472\n",
      "(Iteration 21301 / 45930) loss: 2.268819\n",
      "(Iteration 21401 / 45930) loss: 2.282263\n",
      "(Epoch 14 / 30) train acc: 0.243000; val_acc: 0.242000\n",
      "(Iteration 21501 / 45930) loss: 2.270544\n",
      "(Iteration 21601 / 45930) loss: 2.240565\n",
      "(Iteration 21701 / 45930) loss: 2.291972\n",
      "(Iteration 21801 / 45930) loss: 2.255691\n",
      "(Iteration 21901 / 45930) loss: 2.285488\n",
      "(Iteration 22001 / 45930) loss: 2.264706\n",
      "(Iteration 22101 / 45930) loss: 2.269128\n",
      "(Iteration 22201 / 45930) loss: 2.256365\n",
      "(Iteration 22301 / 45930) loss: 2.270331\n",
      "(Iteration 22401 / 45930) loss: 2.286532\n",
      "(Iteration 22501 / 45930) loss: 2.293670\n",
      "(Iteration 22601 / 45930) loss: 2.248355\n",
      "(Iteration 22701 / 45930) loss: 2.275197\n",
      "(Iteration 22801 / 45930) loss: 2.253586\n",
      "(Iteration 22901 / 45930) loss: 2.278558\n",
      "(Epoch 15 / 30) train acc: 0.215000; val_acc: 0.240000\n",
      "(Iteration 23001 / 45930) loss: 2.267396\n",
      "(Iteration 23101 / 45930) loss: 2.289944\n",
      "(Iteration 23201 / 45930) loss: 2.280869\n",
      "(Iteration 23301 / 45930) loss: 2.245070\n",
      "(Iteration 23401 / 45930) loss: 2.292930\n",
      "(Iteration 23501 / 45930) loss: 2.247099\n",
      "(Iteration 23601 / 45930) loss: 2.248959\n",
      "(Iteration 23701 / 45930) loss: 2.265780\n",
      "(Iteration 23801 / 45930) loss: 2.260744\n",
      "(Iteration 23901 / 45930) loss: 2.273881\n",
      "(Iteration 24001 / 45930) loss: 2.281689\n",
      "(Iteration 24101 / 45930) loss: 2.263245\n",
      "(Iteration 24201 / 45930) loss: 2.251595\n",
      "(Iteration 24301 / 45930) loss: 2.251945\n",
      "(Iteration 24401 / 45930) loss: 2.283909\n",
      "(Epoch 16 / 30) train acc: 0.227000; val_acc: 0.241000\n",
      "(Iteration 24501 / 45930) loss: 2.256335\n",
      "(Iteration 24601 / 45930) loss: 2.235398\n",
      "(Iteration 24701 / 45930) loss: 2.296607\n",
      "(Iteration 24801 / 45930) loss: 2.271960\n",
      "(Iteration 24901 / 45930) loss: 2.290208\n",
      "(Iteration 25001 / 45930) loss: 2.281994\n",
      "(Iteration 25101 / 45930) loss: 2.241206\n",
      "(Iteration 25201 / 45930) loss: 2.256589\n",
      "(Iteration 25301 / 45930) loss: 2.287957\n",
      "(Iteration 25401 / 45930) loss: 2.287280\n",
      "(Iteration 25501 / 45930) loss: 2.263411\n",
      "(Iteration 25601 / 45930) loss: 2.263433\n",
      "(Iteration 25701 / 45930) loss: 2.232571\n",
      "(Iteration 25801 / 45930) loss: 2.256498\n",
      "(Iteration 25901 / 45930) loss: 2.259597\n",
      "(Iteration 26001 / 45930) loss: 2.269102\n",
      "(Epoch 17 / 30) train acc: 0.240000; val_acc: 0.238000\n",
      "(Iteration 26101 / 45930) loss: 2.253023\n",
      "(Iteration 26201 / 45930) loss: 2.269837\n",
      "(Iteration 26301 / 45930) loss: 2.260505\n",
      "(Iteration 26401 / 45930) loss: 2.274023\n",
      "(Iteration 26501 / 45930) loss: 2.186092\n",
      "(Iteration 26601 / 45930) loss: 2.238210\n",
      "(Iteration 26701 / 45930) loss: 2.270917\n",
      "(Iteration 26801 / 45930) loss: 2.241269\n",
      "(Iteration 26901 / 45930) loss: 2.257169\n",
      "(Iteration 27001 / 45930) loss: 2.294708\n",
      "(Iteration 27101 / 45930) loss: 2.286096\n",
      "(Iteration 27201 / 45930) loss: 2.241923\n",
      "(Iteration 27301 / 45930) loss: 2.253724\n",
      "(Iteration 27401 / 45930) loss: 2.254403\n",
      "(Iteration 27501 / 45930) loss: 2.260114\n",
      "(Epoch 18 / 30) train acc: 0.217000; val_acc: 0.238000\n",
      "(Iteration 27601 / 45930) loss: 2.242502\n",
      "(Iteration 27701 / 45930) loss: 2.283794\n",
      "(Iteration 27801 / 45930) loss: 2.278981\n",
      "(Iteration 27901 / 45930) loss: 2.248493\n",
      "(Iteration 28001 / 45930) loss: 2.276718\n",
      "(Iteration 28101 / 45930) loss: 2.231013\n",
      "(Iteration 28201 / 45930) loss: 2.252929\n",
      "(Iteration 28301 / 45930) loss: 2.253093\n",
      "(Iteration 28401 / 45930) loss: 2.268038\n",
      "(Iteration 28501 / 45930) loss: 2.263679\n",
      "(Iteration 28601 / 45930) loss: 2.271350\n",
      "(Iteration 28701 / 45930) loss: 2.256435\n",
      "(Iteration 28801 / 45930) loss: 2.307292\n",
      "(Iteration 28901 / 45930) loss: 2.230271\n",
      "(Iteration 29001 / 45930) loss: 2.274247\n",
      "(Epoch 19 / 30) train acc: 0.224000; val_acc: 0.236000\n",
      "(Iteration 29101 / 45930) loss: 2.292552\n",
      "(Iteration 29201 / 45930) loss: 2.179847\n",
      "(Iteration 29301 / 45930) loss: 2.287660\n",
      "(Iteration 29401 / 45930) loss: 2.249138\n",
      "(Iteration 29501 / 45930) loss: 2.286616\n",
      "(Iteration 29601 / 45930) loss: 2.240272\n",
      "(Iteration 29701 / 45930) loss: 2.221880\n",
      "(Iteration 29801 / 45930) loss: 2.259892\n",
      "(Iteration 29901 / 45930) loss: 2.244854\n",
      "(Iteration 30001 / 45930) loss: 2.278240\n",
      "(Iteration 30101 / 45930) loss: 2.241356\n",
      "(Iteration 30201 / 45930) loss: 2.227945\n",
      "(Iteration 30301 / 45930) loss: 2.250937\n",
      "(Iteration 30401 / 45930) loss: 2.285775\n",
      "(Iteration 30501 / 45930) loss: 2.223408\n",
      "(Iteration 30601 / 45930) loss: 2.273634\n",
      "(Epoch 20 / 30) train acc: 0.225000; val_acc: 0.234000\n",
      "(Iteration 30701 / 45930) loss: 2.216020\n",
      "(Iteration 30801 / 45930) loss: 2.260946\n",
      "(Iteration 30901 / 45930) loss: 2.265940\n",
      "(Iteration 31001 / 45930) loss: 2.219098\n",
      "(Iteration 31101 / 45930) loss: 2.260599\n",
      "(Iteration 31201 / 45930) loss: 2.199314\n",
      "(Iteration 31301 / 45930) loss: 2.191605\n",
      "(Iteration 31401 / 45930) loss: 2.222055\n",
      "(Iteration 31501 / 45930) loss: 2.258693\n",
      "(Iteration 31601 / 45930) loss: 2.255452\n",
      "(Iteration 31701 / 45930) loss: 2.251352\n",
      "(Iteration 31801 / 45930) loss: 2.196245\n",
      "(Iteration 31901 / 45930) loss: 2.232291\n",
      "(Iteration 32001 / 45930) loss: 2.278273\n",
      "(Iteration 32101 / 45930) loss: 2.213043\n",
      "(Epoch 21 / 30) train acc: 0.211000; val_acc: 0.236000\n",
      "(Iteration 32201 / 45930) loss: 2.265734\n",
      "(Iteration 32301 / 45930) loss: 2.218068\n",
      "(Iteration 32401 / 45930) loss: 2.257468\n",
      "(Iteration 32501 / 45930) loss: 2.249570\n",
      "(Iteration 32601 / 45930) loss: 2.195786\n",
      "(Iteration 32701 / 45930) loss: 2.239653\n",
      "(Iteration 32801 / 45930) loss: 2.200371\n",
      "(Iteration 32901 / 45930) loss: 2.161604\n",
      "(Iteration 33001 / 45930) loss: 2.180136\n",
      "(Iteration 33101 / 45930) loss: 2.255320\n",
      "(Iteration 33201 / 45930) loss: 2.233162\n",
      "(Iteration 33301 / 45930) loss: 2.258959\n",
      "(Iteration 33401 / 45930) loss: 2.274846\n",
      "(Iteration 33501 / 45930) loss: 2.267150\n",
      "(Iteration 33601 / 45930) loss: 2.247365\n",
      "(Epoch 22 / 30) train acc: 0.235000; val_acc: 0.236000\n",
      "(Iteration 33701 / 45930) loss: 2.252152\n",
      "(Iteration 33801 / 45930) loss: 2.238436\n",
      "(Iteration 33901 / 45930) loss: 2.266708\n",
      "(Iteration 34001 / 45930) loss: 2.247016\n",
      "(Iteration 34101 / 45930) loss: 2.249658\n",
      "(Iteration 34201 / 45930) loss: 2.174150\n",
      "(Iteration 34301 / 45930) loss: 2.208964\n",
      "(Iteration 34401 / 45930) loss: 2.299182\n",
      "(Iteration 34501 / 45930) loss: 2.250629\n",
      "(Iteration 34601 / 45930) loss: 2.202484\n",
      "(Iteration 34701 / 45930) loss: 2.285664\n",
      "(Iteration 34801 / 45930) loss: 2.238794\n",
      "(Iteration 34901 / 45930) loss: 2.223471\n",
      "(Iteration 35001 / 45930) loss: 2.257401\n",
      "(Iteration 35101 / 45930) loss: 2.209347\n",
      "(Iteration 35201 / 45930) loss: 2.248030\n",
      "(Epoch 23 / 30) train acc: 0.223000; val_acc: 0.232000\n",
      "(Iteration 35301 / 45930) loss: 2.256774\n",
      "(Iteration 35401 / 45930) loss: 2.244525\n",
      "(Iteration 35501 / 45930) loss: 2.215422\n",
      "(Iteration 35601 / 45930) loss: 2.203719\n",
      "(Iteration 35701 / 45930) loss: 2.254421\n",
      "(Iteration 35801 / 45930) loss: 2.249215\n",
      "(Iteration 35901 / 45930) loss: 2.279370\n",
      "(Iteration 36001 / 45930) loss: 2.254740\n",
      "(Iteration 36101 / 45930) loss: 2.199795\n",
      "(Iteration 36201 / 45930) loss: 2.218567\n",
      "(Iteration 36301 / 45930) loss: 2.269645\n",
      "(Iteration 36401 / 45930) loss: 2.246788\n",
      "(Iteration 36501 / 45930) loss: 2.237444\n",
      "(Iteration 36601 / 45930) loss: 2.266848\n",
      "(Iteration 36701 / 45930) loss: 2.257771\n",
      "(Epoch 24 / 30) train acc: 0.256000; val_acc: 0.231000\n",
      "(Iteration 36801 / 45930) loss: 2.250464\n",
      "(Iteration 36901 / 45930) loss: 2.203195\n",
      "(Iteration 37001 / 45930) loss: 2.202164\n",
      "(Iteration 37101 / 45930) loss: 2.292284\n",
      "(Iteration 37201 / 45930) loss: 2.246194\n",
      "(Iteration 37301 / 45930) loss: 2.221677\n",
      "(Iteration 37401 / 45930) loss: 2.261158\n",
      "(Iteration 37501 / 45930) loss: 2.282142\n",
      "(Iteration 37601 / 45930) loss: 2.245142\n",
      "(Iteration 37701 / 45930) loss: 2.242697\n",
      "(Iteration 37801 / 45930) loss: 2.311060\n",
      "(Iteration 37901 / 45930) loss: 2.271216\n",
      "(Iteration 38001 / 45930) loss: 2.235414\n",
      "(Iteration 38101 / 45930) loss: 2.233527\n",
      "(Iteration 38201 / 45930) loss: 2.246118\n",
      "(Epoch 25 / 30) train acc: 0.242000; val_acc: 0.231000\n",
      "(Iteration 38301 / 45930) loss: 2.246763\n",
      "(Iteration 38401 / 45930) loss: 2.207526\n",
      "(Iteration 38501 / 45930) loss: 2.180240\n",
      "(Iteration 38601 / 45930) loss: 2.252815\n",
      "(Iteration 38701 / 45930) loss: 2.239334\n",
      "(Iteration 38801 / 45930) loss: 2.214615\n",
      "(Iteration 38901 / 45930) loss: 2.279661\n",
      "(Iteration 39001 / 45930) loss: 2.205571\n",
      "(Iteration 39101 / 45930) loss: 2.270039\n",
      "(Iteration 39201 / 45930) loss: 2.261815\n",
      "(Iteration 39301 / 45930) loss: 2.251548\n",
      "(Iteration 39401 / 45930) loss: 2.192595\n",
      "(Iteration 39501 / 45930) loss: 2.228896\n",
      "(Iteration 39601 / 45930) loss: 2.269463\n",
      "(Iteration 39701 / 45930) loss: 2.196031\n",
      "(Iteration 39801 / 45930) loss: 2.173419\n",
      "(Epoch 26 / 30) train acc: 0.234000; val_acc: 0.234000\n",
      "(Iteration 39901 / 45930) loss: 2.226566\n",
      "(Iteration 40001 / 45930) loss: 2.232402\n",
      "(Iteration 40101 / 45930) loss: 2.199028\n",
      "(Iteration 40201 / 45930) loss: 2.282238\n",
      "(Iteration 40301 / 45930) loss: 2.263960\n",
      "(Iteration 40401 / 45930) loss: 2.241076\n",
      "(Iteration 40501 / 45930) loss: 2.240410\n",
      "(Iteration 40601 / 45930) loss: 2.208077\n",
      "(Iteration 40701 / 45930) loss: 2.234247\n",
      "(Iteration 40801 / 45930) loss: 2.259087\n",
      "(Iteration 40901 / 45930) loss: 2.215082\n",
      "(Iteration 41001 / 45930) loss: 2.191108\n",
      "(Iteration 41101 / 45930) loss: 2.236730\n",
      "(Iteration 41201 / 45930) loss: 2.211528\n",
      "(Iteration 41301 / 45930) loss: 2.183009\n",
      "(Epoch 27 / 30) train acc: 0.202000; val_acc: 0.235000\n",
      "(Iteration 41401 / 45930) loss: 2.243442\n",
      "(Iteration 41501 / 45930) loss: 2.238294\n",
      "(Iteration 41601 / 45930) loss: 2.209436\n",
      "(Iteration 41701 / 45930) loss: 2.204358\n",
      "(Iteration 41801 / 45930) loss: 2.253841\n",
      "(Iteration 41901 / 45930) loss: 2.193955\n",
      "(Iteration 42001 / 45930) loss: 2.229489\n",
      "(Iteration 42101 / 45930) loss: 2.177092\n",
      "(Iteration 42201 / 45930) loss: 2.205021\n",
      "(Iteration 42301 / 45930) loss: 2.266331\n",
      "(Iteration 42401 / 45930) loss: 2.222406\n",
      "(Iteration 42501 / 45930) loss: 2.221948\n",
      "(Iteration 42601 / 45930) loss: 2.233685\n",
      "(Iteration 42701 / 45930) loss: 2.240445\n",
      "(Iteration 42801 / 45930) loss: 2.259570\n",
      "(Epoch 28 / 30) train acc: 0.209000; val_acc: 0.235000\n",
      "(Iteration 42901 / 45930) loss: 2.178587\n",
      "(Iteration 43001 / 45930) loss: 2.182439\n",
      "(Iteration 43101 / 45930) loss: 2.231991\n",
      "(Iteration 43201 / 45930) loss: 2.195714\n",
      "(Iteration 43301 / 45930) loss: 2.249005\n",
      "(Iteration 43401 / 45930) loss: 2.275637\n",
      "(Iteration 43501 / 45930) loss: 2.200923\n",
      "(Iteration 43601 / 45930) loss: 2.214114\n",
      "(Iteration 43701 / 45930) loss: 2.231350\n",
      "(Iteration 43801 / 45930) loss: 2.228206\n",
      "(Iteration 43901 / 45930) loss: 2.206310\n",
      "(Iteration 44001 / 45930) loss: 2.231530\n",
      "(Iteration 44101 / 45930) loss: 2.225170\n",
      "(Iteration 44201 / 45930) loss: 2.253759\n",
      "(Iteration 44301 / 45930) loss: 2.254731\n",
      "(Epoch 29 / 30) train acc: 0.246000; val_acc: 0.234000\n",
      "(Iteration 44401 / 45930) loss: 2.242105\n",
      "(Iteration 44501 / 45930) loss: 2.225571\n",
      "(Iteration 44601 / 45930) loss: 2.213439\n",
      "(Iteration 44701 / 45930) loss: 2.205993\n",
      "(Iteration 44801 / 45930) loss: 2.217183\n",
      "(Iteration 44901 / 45930) loss: 2.159504\n",
      "(Iteration 45001 / 45930) loss: 2.203859\n",
      "(Iteration 45101 / 45930) loss: 2.190096\n",
      "(Iteration 45201 / 45930) loss: 2.197351\n",
      "(Iteration 45301 / 45930) loss: 2.279881\n",
      "(Iteration 45401 / 45930) loss: 2.217991\n",
      "(Iteration 45501 / 45930) loss: 2.148636\n",
      "(Iteration 45601 / 45930) loss: 2.213662\n",
      "(Iteration 45701 / 45930) loss: 2.251697\n",
      "(Iteration 45801 / 45930) loss: 2.221017\n",
      "(Iteration 45901 / 45930) loss: 2.289706\n",
      "(Epoch 30 / 30) train acc: 0.227000; val_acc: 0.234000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.001, 'num_epochs': 30, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 22950) loss: 2.303287\n",
      "(Epoch 0 / 30) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 101 / 22950) loss: 2.303314\n",
      "(Iteration 201 / 22950) loss: 2.303151\n",
      "(Iteration 301 / 22950) loss: 2.303114\n",
      "(Iteration 401 / 22950) loss: 2.303415\n",
      "(Iteration 501 / 22950) loss: 2.302766\n",
      "(Iteration 601 / 22950) loss: 2.303396\n",
      "(Iteration 701 / 22950) loss: 2.302796\n",
      "(Epoch 1 / 30) train acc: 0.155000; val_acc: 0.158000\n",
      "(Iteration 801 / 22950) loss: 2.303181\n",
      "(Iteration 901 / 22950) loss: 2.303182\n",
      "(Iteration 1001 / 22950) loss: 2.302900\n",
      "(Iteration 1101 / 22950) loss: 2.302897\n",
      "(Iteration 1201 / 22950) loss: 2.303117\n",
      "(Iteration 1301 / 22950) loss: 2.302881\n",
      "(Iteration 1401 / 22950) loss: 2.302941\n",
      "(Iteration 1501 / 22950) loss: 2.302800\n",
      "(Epoch 2 / 30) train acc: 0.172000; val_acc: 0.169000\n",
      "(Iteration 1601 / 22950) loss: 2.302477\n",
      "(Iteration 1701 / 22950) loss: 2.302922\n",
      "(Iteration 1801 / 22950) loss: 2.301865\n",
      "(Iteration 1901 / 22950) loss: 2.302861\n",
      "(Iteration 2001 / 22950) loss: 2.302977\n",
      "(Iteration 2101 / 22950) loss: 2.302499\n",
      "(Iteration 2201 / 22950) loss: 2.302603\n",
      "(Epoch 3 / 30) train acc: 0.196000; val_acc: 0.210000\n",
      "(Iteration 2301 / 22950) loss: 2.302245\n",
      "(Iteration 2401 / 22950) loss: 2.302409\n",
      "(Iteration 2501 / 22950) loss: 2.302458\n",
      "(Iteration 2601 / 22950) loss: 2.302674\n",
      "(Iteration 2701 / 22950) loss: 2.302721\n",
      "(Iteration 2801 / 22950) loss: 2.302218\n",
      "(Iteration 2901 / 22950) loss: 2.302499\n",
      "(Iteration 3001 / 22950) loss: 2.302420\n",
      "(Epoch 4 / 30) train acc: 0.204000; val_acc: 0.208000\n",
      "(Iteration 3101 / 22950) loss: 2.301963\n",
      "(Iteration 3201 / 22950) loss: 2.302188\n",
      "(Iteration 3301 / 22950) loss: 2.302124\n",
      "(Iteration 3401 / 22950) loss: 2.302315\n",
      "(Iteration 3501 / 22950) loss: 2.301712\n",
      "(Iteration 3601 / 22950) loss: 2.302230\n",
      "(Iteration 3701 / 22950) loss: 2.301920\n",
      "(Iteration 3801 / 22950) loss: 2.301871\n",
      "(Epoch 5 / 30) train acc: 0.218000; val_acc: 0.188000\n",
      "(Iteration 3901 / 22950) loss: 2.302051\n",
      "(Iteration 4001 / 22950) loss: 2.302185\n",
      "(Iteration 4101 / 22950) loss: 2.301575\n",
      "(Iteration 4201 / 22950) loss: 2.301600\n",
      "(Iteration 4301 / 22950) loss: 2.301637\n",
      "(Iteration 4401 / 22950) loss: 2.301539\n",
      "(Iteration 4501 / 22950) loss: 2.301333\n",
      "(Epoch 6 / 30) train acc: 0.218000; val_acc: 0.199000\n",
      "(Iteration 4601 / 22950) loss: 2.301677\n",
      "(Iteration 4701 / 22950) loss: 2.301913\n",
      "(Iteration 4801 / 22950) loss: 2.302079\n",
      "(Iteration 4901 / 22950) loss: 2.301241\n",
      "(Iteration 5001 / 22950) loss: 2.301521\n",
      "(Iteration 5101 / 22950) loss: 2.301141\n",
      "(Iteration 5201 / 22950) loss: 2.301469\n",
      "(Iteration 5301 / 22950) loss: 2.300830\n",
      "(Epoch 7 / 30) train acc: 0.221000; val_acc: 0.196000\n",
      "(Iteration 5401 / 22950) loss: 2.301105\n",
      "(Iteration 5501 / 22950) loss: 2.300606\n",
      "(Iteration 5601 / 22950) loss: 2.300593\n",
      "(Iteration 5701 / 22950) loss: 2.301044\n",
      "(Iteration 5801 / 22950) loss: 2.301658\n",
      "(Iteration 5901 / 22950) loss: 2.301158\n",
      "(Iteration 6001 / 22950) loss: 2.300599\n",
      "(Iteration 6101 / 22950) loss: 2.300117\n",
      "(Epoch 8 / 30) train acc: 0.200000; val_acc: 0.192000\n",
      "(Iteration 6201 / 22950) loss: 2.301559\n",
      "(Iteration 6301 / 22950) loss: 2.300577\n",
      "(Iteration 6401 / 22950) loss: 2.300423\n",
      "(Iteration 6501 / 22950) loss: 2.300564\n",
      "(Iteration 6601 / 22950) loss: 2.300482\n",
      "(Iteration 6701 / 22950) loss: 2.299206\n",
      "(Iteration 6801 / 22950) loss: 2.300067\n",
      "(Epoch 9 / 30) train acc: 0.209000; val_acc: 0.204000\n",
      "(Iteration 6901 / 22950) loss: 2.299936\n",
      "(Iteration 7001 / 22950) loss: 2.300530\n",
      "(Iteration 7101 / 22950) loss: 2.298939\n",
      "(Iteration 7201 / 22950) loss: 2.299329\n",
      "(Iteration 7301 / 22950) loss: 2.299393\n",
      "(Iteration 7401 / 22950) loss: 2.300286\n",
      "(Iteration 7501 / 22950) loss: 2.299162\n",
      "(Iteration 7601 / 22950) loss: 2.299291\n",
      "(Epoch 10 / 30) train acc: 0.218000; val_acc: 0.199000\n",
      "(Iteration 7701 / 22950) loss: 2.299479\n",
      "(Iteration 7801 / 22950) loss: 2.298890\n",
      "(Iteration 7901 / 22950) loss: 2.300030\n",
      "(Iteration 8001 / 22950) loss: 2.298063\n",
      "(Iteration 8101 / 22950) loss: 2.298700\n",
      "(Iteration 8201 / 22950) loss: 2.298781\n",
      "(Iteration 8301 / 22950) loss: 2.299455\n",
      "(Iteration 8401 / 22950) loss: 2.299171\n",
      "(Epoch 11 / 30) train acc: 0.238000; val_acc: 0.208000\n",
      "(Iteration 8501 / 22950) loss: 2.298061\n",
      "(Iteration 8601 / 22950) loss: 2.298896\n",
      "(Iteration 8701 / 22950) loss: 2.298583\n",
      "(Iteration 8801 / 22950) loss: 2.298280\n",
      "(Iteration 8901 / 22950) loss: 2.299400\n",
      "(Iteration 9001 / 22950) loss: 2.298013\n",
      "(Iteration 9101 / 22950) loss: 2.299379\n",
      "(Epoch 12 / 30) train acc: 0.236000; val_acc: 0.213000\n",
      "(Iteration 9201 / 22950) loss: 2.298105\n",
      "(Iteration 9301 / 22950) loss: 2.298875\n",
      "(Iteration 9401 / 22950) loss: 2.297724\n",
      "(Iteration 9501 / 22950) loss: 2.298548\n",
      "(Iteration 9601 / 22950) loss: 2.297143\n",
      "(Iteration 9701 / 22950) loss: 2.298330\n",
      "(Iteration 9801 / 22950) loss: 2.298734\n",
      "(Iteration 9901 / 22950) loss: 2.297603\n",
      "(Epoch 13 / 30) train acc: 0.232000; val_acc: 0.221000\n",
      "(Iteration 10001 / 22950) loss: 2.298245\n",
      "(Iteration 10101 / 22950) loss: 2.297639\n",
      "(Iteration 10201 / 22950) loss: 2.296121\n",
      "(Iteration 10301 / 22950) loss: 2.297761\n",
      "(Iteration 10401 / 22950) loss: 2.293182\n",
      "(Iteration 10501 / 22950) loss: 2.296719\n",
      "(Iteration 10601 / 22950) loss: 2.295992\n",
      "(Iteration 10701 / 22950) loss: 2.294789\n",
      "(Epoch 14 / 30) train acc: 0.266000; val_acc: 0.218000\n",
      "(Iteration 10801 / 22950) loss: 2.297126\n",
      "(Iteration 10901 / 22950) loss: 2.296070\n",
      "(Iteration 11001 / 22950) loss: 2.296459\n",
      "(Iteration 11101 / 22950) loss: 2.294941\n",
      "(Iteration 11201 / 22950) loss: 2.296105\n",
      "(Iteration 11301 / 22950) loss: 2.297442\n",
      "(Iteration 11401 / 22950) loss: 2.295060\n",
      "(Epoch 15 / 30) train acc: 0.284000; val_acc: 0.242000\n",
      "(Iteration 11501 / 22950) loss: 2.296089\n",
      "(Iteration 11601 / 22950) loss: 2.296688\n",
      "(Iteration 11701 / 22950) loss: 2.294952\n",
      "(Iteration 11801 / 22950) loss: 2.294024\n",
      "(Iteration 11901 / 22950) loss: 2.292652\n",
      "(Iteration 12001 / 22950) loss: 2.293690\n",
      "(Iteration 12101 / 22950) loss: 2.294927\n",
      "(Iteration 12201 / 22950) loss: 2.295597\n",
      "(Epoch 16 / 30) train acc: 0.262000; val_acc: 0.246000\n",
      "(Iteration 12301 / 22950) loss: 2.296628\n",
      "(Iteration 12401 / 22950) loss: 2.293408\n",
      "(Iteration 12501 / 22950) loss: 2.292536\n",
      "(Iteration 12601 / 22950) loss: 2.295588\n",
      "(Iteration 12701 / 22950) loss: 2.294269\n",
      "(Iteration 12801 / 22950) loss: 2.297113\n",
      "(Iteration 12901 / 22950) loss: 2.294377\n",
      "(Iteration 13001 / 22950) loss: 2.294035\n",
      "(Epoch 17 / 30) train acc: 0.255000; val_acc: 0.248000\n",
      "(Iteration 13101 / 22950) loss: 2.294468\n",
      "(Iteration 13201 / 22950) loss: 2.295451\n",
      "(Iteration 13301 / 22950) loss: 2.295106\n",
      "(Iteration 13401 / 22950) loss: 2.293223\n",
      "(Iteration 13501 / 22950) loss: 2.292257\n",
      "(Iteration 13601 / 22950) loss: 2.290925\n",
      "(Iteration 13701 / 22950) loss: 2.295752\n",
      "(Epoch 18 / 30) train acc: 0.236000; val_acc: 0.250000\n",
      "(Iteration 13801 / 22950) loss: 2.296680\n",
      "(Iteration 13901 / 22950) loss: 2.294199\n",
      "(Iteration 14001 / 22950) loss: 2.293494\n",
      "(Iteration 14101 / 22950) loss: 2.292513\n",
      "(Iteration 14201 / 22950) loss: 2.292307\n",
      "(Iteration 14301 / 22950) loss: 2.290381\n",
      "(Iteration 14401 / 22950) loss: 2.292258\n",
      "(Iteration 14501 / 22950) loss: 2.290107\n",
      "(Epoch 19 / 30) train acc: 0.247000; val_acc: 0.251000\n",
      "(Iteration 14601 / 22950) loss: 2.291534\n",
      "(Iteration 14701 / 22950) loss: 2.290309\n",
      "(Iteration 14801 / 22950) loss: 2.292828\n",
      "(Iteration 14901 / 22950) loss: 2.287536\n",
      "(Iteration 15001 / 22950) loss: 2.288927\n",
      "(Iteration 15101 / 22950) loss: 2.288321\n",
      "(Iteration 15201 / 22950) loss: 2.288285\n",
      "(Epoch 20 / 30) train acc: 0.273000; val_acc: 0.258000\n",
      "(Iteration 15301 / 22950) loss: 2.291730\n",
      "(Iteration 15401 / 22950) loss: 2.287770\n",
      "(Iteration 15501 / 22950) loss: 2.293986\n",
      "(Iteration 15601 / 22950) loss: 2.291808\n",
      "(Iteration 15701 / 22950) loss: 2.290144\n",
      "(Iteration 15801 / 22950) loss: 2.289149\n",
      "(Iteration 15901 / 22950) loss: 2.292635\n",
      "(Iteration 16001 / 22950) loss: 2.293894\n",
      "(Epoch 21 / 30) train acc: 0.252000; val_acc: 0.260000\n",
      "(Iteration 16101 / 22950) loss: 2.292301\n",
      "(Iteration 16201 / 22950) loss: 2.292952\n",
      "(Iteration 16301 / 22950) loss: 2.290282\n",
      "(Iteration 16401 / 22950) loss: 2.291345\n",
      "(Iteration 16501 / 22950) loss: 2.288460\n",
      "(Iteration 16601 / 22950) loss: 2.288902\n",
      "(Iteration 16701 / 22950) loss: 2.289518\n",
      "(Iteration 16801 / 22950) loss: 2.291694\n",
      "(Epoch 22 / 30) train acc: 0.256000; val_acc: 0.259000\n",
      "(Iteration 16901 / 22950) loss: 2.292645\n",
      "(Iteration 17001 / 22950) loss: 2.291603\n",
      "(Iteration 17101 / 22950) loss: 2.290542\n",
      "(Iteration 17201 / 22950) loss: 2.292305\n",
      "(Iteration 17301 / 22950) loss: 2.288119\n",
      "(Iteration 17401 / 22950) loss: 2.289672\n",
      "(Iteration 17501 / 22950) loss: 2.290856\n",
      "(Epoch 23 / 30) train acc: 0.247000; val_acc: 0.256000\n",
      "(Iteration 17601 / 22950) loss: 2.289415\n",
      "(Iteration 17701 / 22950) loss: 2.290383\n",
      "(Iteration 17801 / 22950) loss: 2.289804\n",
      "(Iteration 17901 / 22950) loss: 2.293026\n",
      "(Iteration 18001 / 22950) loss: 2.288506\n",
      "(Iteration 18101 / 22950) loss: 2.287148\n",
      "(Iteration 18201 / 22950) loss: 2.286098\n",
      "(Iteration 18301 / 22950) loss: 2.292647\n",
      "(Epoch 24 / 30) train acc: 0.253000; val_acc: 0.256000\n",
      "(Iteration 18401 / 22950) loss: 2.293543\n",
      "(Iteration 18501 / 22950) loss: 2.287175\n",
      "(Iteration 18601 / 22950) loss: 2.291485\n",
      "(Iteration 18701 / 22950) loss: 2.292754\n",
      "(Iteration 18801 / 22950) loss: 2.284092\n",
      "(Iteration 18901 / 22950) loss: 2.286660\n",
      "(Iteration 19001 / 22950) loss: 2.290973\n",
      "(Iteration 19101 / 22950) loss: 2.290764\n",
      "(Epoch 25 / 30) train acc: 0.263000; val_acc: 0.258000\n",
      "(Iteration 19201 / 22950) loss: 2.289032\n",
      "(Iteration 19301 / 22950) loss: 2.286596\n",
      "(Iteration 19401 / 22950) loss: 2.291000\n",
      "(Iteration 19501 / 22950) loss: 2.286504\n",
      "(Iteration 19601 / 22950) loss: 2.294546\n",
      "(Iteration 19701 / 22950) loss: 2.287914\n",
      "(Iteration 19801 / 22950) loss: 2.291396\n",
      "(Epoch 26 / 30) train acc: 0.260000; val_acc: 0.259000\n",
      "(Iteration 19901 / 22950) loss: 2.284759\n",
      "(Iteration 20001 / 22950) loss: 2.288124\n",
      "(Iteration 20101 / 22950) loss: 2.283978\n",
      "(Iteration 20201 / 22950) loss: 2.284590\n",
      "(Iteration 20301 / 22950) loss: 2.287417\n",
      "(Iteration 20401 / 22950) loss: 2.284069\n",
      "(Iteration 20501 / 22950) loss: 2.289557\n",
      "(Iteration 20601 / 22950) loss: 2.285577\n",
      "(Epoch 27 / 30) train acc: 0.264000; val_acc: 0.261000\n",
      "(Iteration 20701 / 22950) loss: 2.286966\n",
      "(Iteration 20801 / 22950) loss: 2.291281\n",
      "(Iteration 20901 / 22950) loss: 2.292134\n",
      "(Iteration 21001 / 22950) loss: 2.288649\n",
      "(Iteration 21101 / 22950) loss: 2.290144\n",
      "(Iteration 21201 / 22950) loss: 2.286301\n",
      "(Iteration 21301 / 22950) loss: 2.290725\n",
      "(Iteration 21401 / 22950) loss: 2.286801\n",
      "(Epoch 28 / 30) train acc: 0.259000; val_acc: 0.262000\n",
      "(Iteration 21501 / 22950) loss: 2.287448\n",
      "(Iteration 21601 / 22950) loss: 2.285462\n",
      "(Iteration 21701 / 22950) loss: 2.286775\n",
      "(Iteration 21801 / 22950) loss: 2.279859\n",
      "(Iteration 21901 / 22950) loss: 2.283617\n",
      "(Iteration 22001 / 22950) loss: 2.286542\n",
      "(Iteration 22101 / 22950) loss: 2.285896\n",
      "(Epoch 29 / 30) train acc: 0.270000; val_acc: 0.261000\n",
      "(Iteration 22201 / 22950) loss: 2.284942\n",
      "(Iteration 22301 / 22950) loss: 2.290420\n",
      "(Iteration 22401 / 22950) loss: 2.285406\n",
      "(Iteration 22501 / 22950) loss: 2.289165\n",
      "(Iteration 22601 / 22950) loss: 2.288222\n",
      "(Iteration 22701 / 22950) loss: 2.289029\n",
      "(Iteration 22801 / 22950) loss: 2.293264\n",
      "(Iteration 22901 / 22950) loss: 2.285205\n",
      "(Epoch 30 / 30) train acc: 0.277000; val_acc: 0.262000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.001, 'num_epochs': 30, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 45930) loss: 2.303248\n",
      "(Epoch 0 / 30) train acc: 0.093000; val_acc: 0.106000\n",
      "(Iteration 101 / 45930) loss: 2.303182\n",
      "(Iteration 201 / 45930) loss: 2.303145\n",
      "(Iteration 301 / 45930) loss: 2.303037\n",
      "(Iteration 401 / 45930) loss: 2.303346\n",
      "(Iteration 501 / 45930) loss: 2.303276\n",
      "(Iteration 601 / 45930) loss: 2.302686\n",
      "(Iteration 701 / 45930) loss: 2.303248\n",
      "(Iteration 801 / 45930) loss: 2.302873\n",
      "(Iteration 901 / 45930) loss: 2.303406\n",
      "(Iteration 1001 / 45930) loss: 2.302739\n",
      "(Iteration 1101 / 45930) loss: 2.303529\n",
      "(Iteration 1201 / 45930) loss: 2.302478\n",
      "(Iteration 1301 / 45930) loss: 2.302949\n",
      "(Iteration 1401 / 45930) loss: 2.303117\n",
      "(Iteration 1501 / 45930) loss: 2.302401\n",
      "(Epoch 1 / 30) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 1601 / 45930) loss: 2.303202\n",
      "(Iteration 1701 / 45930) loss: 2.302193\n",
      "(Iteration 1801 / 45930) loss: 2.302779\n",
      "(Iteration 1901 / 45930) loss: 2.302530\n",
      "(Iteration 2001 / 45930) loss: 2.302850\n",
      "(Iteration 2101 / 45930) loss: 2.302273\n",
      "(Iteration 2201 / 45930) loss: 2.302571\n",
      "(Iteration 2301 / 45930) loss: 2.302601\n",
      "(Iteration 2401 / 45930) loss: 2.302811\n",
      "(Iteration 2501 / 45930) loss: 2.301502\n",
      "(Iteration 2601 / 45930) loss: 2.302701\n",
      "(Iteration 2701 / 45930) loss: 2.301840\n",
      "(Iteration 2801 / 45930) loss: 2.302584\n",
      "(Iteration 2901 / 45930) loss: 2.302392\n",
      "(Iteration 3001 / 45930) loss: 2.301696\n",
      "(Epoch 2 / 30) train acc: 0.223000; val_acc: 0.221000\n",
      "(Iteration 3101 / 45930) loss: 2.302100\n",
      "(Iteration 3201 / 45930) loss: 2.301979\n",
      "(Iteration 3301 / 45930) loss: 2.302106\n",
      "(Iteration 3401 / 45930) loss: 2.301901\n",
      "(Iteration 3501 / 45930) loss: 2.301769\n",
      "(Iteration 3601 / 45930) loss: 2.302209\n",
      "(Iteration 3701 / 45930) loss: 2.302198\n",
      "(Iteration 3801 / 45930) loss: 2.301599\n",
      "(Iteration 3901 / 45930) loss: 2.301758\n",
      "(Iteration 4001 / 45930) loss: 2.302079\n",
      "(Iteration 4101 / 45930) loss: 2.301993\n",
      "(Iteration 4201 / 45930) loss: 2.302139\n",
      "(Iteration 4301 / 45930) loss: 2.301841\n",
      "(Iteration 4401 / 45930) loss: 2.300621\n",
      "(Iteration 4501 / 45930) loss: 2.300286\n",
      "(Epoch 3 / 30) train acc: 0.244000; val_acc: 0.265000\n",
      "(Iteration 4601 / 45930) loss: 2.300527\n",
      "(Iteration 4701 / 45930) loss: 2.300016\n",
      "(Iteration 4801 / 45930) loss: 2.299040\n",
      "(Iteration 4901 / 45930) loss: 2.299902\n",
      "(Iteration 5001 / 45930) loss: 2.300327\n",
      "(Iteration 5101 / 45930) loss: 2.300587\n",
      "(Iteration 5201 / 45930) loss: 2.299196\n",
      "(Iteration 5301 / 45930) loss: 2.301250\n",
      "(Iteration 5401 / 45930) loss: 2.299909\n",
      "(Iteration 5501 / 45930) loss: 2.299198\n",
      "(Iteration 5601 / 45930) loss: 2.300238\n",
      "(Iteration 5701 / 45930) loss: 2.300255\n",
      "(Iteration 5801 / 45930) loss: 2.300590\n",
      "(Iteration 5901 / 45930) loss: 2.300347\n",
      "(Iteration 6001 / 45930) loss: 2.296974\n",
      "(Iteration 6101 / 45930) loss: 2.299857\n",
      "(Epoch 4 / 30) train acc: 0.258000; val_acc: 0.247000\n",
      "(Iteration 6201 / 45930) loss: 2.299089\n",
      "(Iteration 6301 / 45930) loss: 2.296053\n",
      "(Iteration 6401 / 45930) loss: 2.298850\n",
      "(Iteration 6501 / 45930) loss: 2.295465\n",
      "(Iteration 6601 / 45930) loss: 2.298345\n",
      "(Iteration 6701 / 45930) loss: 2.293638\n",
      "(Iteration 6801 / 45930) loss: 2.296274\n",
      "(Iteration 6901 / 45930) loss: 2.296375\n",
      "(Iteration 7001 / 45930) loss: 2.297073\n",
      "(Iteration 7101 / 45930) loss: 2.290889\n",
      "(Iteration 7201 / 45930) loss: 2.300453\n",
      "(Iteration 7301 / 45930) loss: 2.293510\n",
      "(Iteration 7401 / 45930) loss: 2.297145\n",
      "(Iteration 7501 / 45930) loss: 2.294765\n",
      "(Iteration 7601 / 45930) loss: 2.294884\n",
      "(Epoch 5 / 30) train acc: 0.337000; val_acc: 0.310000\n",
      "(Iteration 7701 / 45930) loss: 2.297866\n",
      "(Iteration 7801 / 45930) loss: 2.290962\n",
      "(Iteration 7901 / 45930) loss: 2.292384\n",
      "(Iteration 8001 / 45930) loss: 2.299819\n",
      "(Iteration 8101 / 45930) loss: 2.292538\n",
      "(Iteration 8201 / 45930) loss: 2.288550\n",
      "(Iteration 8301 / 45930) loss: 2.293615\n",
      "(Iteration 8401 / 45930) loss: 2.297066\n",
      "(Iteration 8501 / 45930) loss: 2.293936\n",
      "(Iteration 8601 / 45930) loss: 2.293693\n",
      "(Iteration 8701 / 45930) loss: 2.285345\n",
      "(Iteration 8801 / 45930) loss: 2.291648\n",
      "(Iteration 8901 / 45930) loss: 2.283408\n",
      "(Iteration 9001 / 45930) loss: 2.293630\n",
      "(Iteration 9101 / 45930) loss: 2.285256\n",
      "(Epoch 6 / 30) train acc: 0.319000; val_acc: 0.322000\n",
      "(Iteration 9201 / 45930) loss: 2.288868\n",
      "(Iteration 9301 / 45930) loss: 2.288316\n",
      "(Iteration 9401 / 45930) loss: 2.290862\n",
      "(Iteration 9501 / 45930) loss: 2.287310\n",
      "(Iteration 9601 / 45930) loss: 2.274914\n",
      "(Iteration 9701 / 45930) loss: 2.280064\n",
      "(Iteration 9801 / 45930) loss: 2.289884\n",
      "(Iteration 9901 / 45930) loss: 2.284155\n",
      "(Iteration 10001 / 45930) loss: 2.287437\n",
      "(Iteration 10101 / 45930) loss: 2.283556\n",
      "(Iteration 10201 / 45930) loss: 2.288186\n",
      "(Iteration 10301 / 45930) loss: 2.277077\n",
      "(Iteration 10401 / 45930) loss: 2.283839\n",
      "(Iteration 10501 / 45930) loss: 2.286853\n",
      "(Iteration 10601 / 45930) loss: 2.284747\n",
      "(Iteration 10701 / 45930) loss: 2.284371\n",
      "(Epoch 7 / 30) train acc: 0.272000; val_acc: 0.290000\n",
      "(Iteration 10801 / 45930) loss: 2.274903\n",
      "(Iteration 10901 / 45930) loss: 2.283123\n",
      "(Iteration 11001 / 45930) loss: 2.281297\n",
      "(Iteration 11101 / 45930) loss: 2.280199\n",
      "(Iteration 11201 / 45930) loss: 2.281420\n",
      "(Iteration 11301 / 45930) loss: 2.281964\n",
      "(Iteration 11401 / 45930) loss: 2.268890\n",
      "(Iteration 11501 / 45930) loss: 2.268159\n",
      "(Iteration 11601 / 45930) loss: 2.273647\n",
      "(Iteration 11701 / 45930) loss: 2.270508\n",
      "(Iteration 11801 / 45930) loss: 2.246463\n",
      "(Iteration 11901 / 45930) loss: 2.234647\n",
      "(Iteration 12001 / 45930) loss: 2.276749\n",
      "(Iteration 12101 / 45930) loss: 2.239805\n",
      "(Iteration 12201 / 45930) loss: 2.262731\n",
      "(Epoch 8 / 30) train acc: 0.299000; val_acc: 0.282000\n",
      "(Iteration 12301 / 45930) loss: 2.275677\n",
      "(Iteration 12401 / 45930) loss: 2.273976\n",
      "(Iteration 12501 / 45930) loss: 2.258886\n",
      "(Iteration 12601 / 45930) loss: 2.259849\n",
      "(Iteration 12701 / 45930) loss: 2.251208\n",
      "(Iteration 12801 / 45930) loss: 2.276931\n",
      "(Iteration 12901 / 45930) loss: 2.280292\n",
      "(Iteration 13001 / 45930) loss: 2.269484\n",
      "(Iteration 13101 / 45930) loss: 2.241779\n",
      "(Iteration 13201 / 45930) loss: 2.227704\n",
      "(Iteration 13301 / 45930) loss: 2.231376\n",
      "(Iteration 13401 / 45930) loss: 2.256060\n",
      "(Iteration 13501 / 45930) loss: 2.255821\n",
      "(Iteration 13601 / 45930) loss: 2.236780\n",
      "(Iteration 13701 / 45930) loss: 2.264596\n",
      "(Epoch 9 / 30) train acc: 0.295000; val_acc: 0.287000\n",
      "(Iteration 13801 / 45930) loss: 2.239681\n",
      "(Iteration 13901 / 45930) loss: 2.230369\n",
      "(Iteration 14001 / 45930) loss: 2.287539\n",
      "(Iteration 14101 / 45930) loss: 2.242661\n",
      "(Iteration 14201 / 45930) loss: 2.254359\n",
      "(Iteration 14301 / 45930) loss: 2.230877\n",
      "(Iteration 14401 / 45930) loss: 2.239601\n",
      "(Iteration 14501 / 45930) loss: 2.224849\n",
      "(Iteration 14601 / 45930) loss: 2.230308\n",
      "(Iteration 14701 / 45930) loss: 2.256427\n",
      "(Iteration 14801 / 45930) loss: 2.227523\n",
      "(Iteration 14901 / 45930) loss: 2.268111\n",
      "(Iteration 15001 / 45930) loss: 2.246785\n",
      "(Iteration 15101 / 45930) loss: 2.241210\n",
      "(Iteration 15201 / 45930) loss: 2.267403\n",
      "(Iteration 15301 / 45930) loss: 2.145016\n",
      "(Epoch 10 / 30) train acc: 0.324000; val_acc: 0.285000\n",
      "(Iteration 15401 / 45930) loss: 2.261218\n",
      "(Iteration 15501 / 45930) loss: 2.224100\n",
      "(Iteration 15601 / 45930) loss: 2.202332\n",
      "(Iteration 15701 / 45930) loss: 2.243776\n",
      "(Iteration 15801 / 45930) loss: 2.268223\n",
      "(Iteration 15901 / 45930) loss: 2.221883\n",
      "(Iteration 16001 / 45930) loss: 2.240893\n",
      "(Iteration 16101 / 45930) loss: 2.205112\n",
      "(Iteration 16201 / 45930) loss: 2.226005\n",
      "(Iteration 16301 / 45930) loss: 2.203287\n",
      "(Iteration 16401 / 45930) loss: 2.180444\n",
      "(Iteration 16501 / 45930) loss: 2.248800\n",
      "(Iteration 16601 / 45930) loss: 2.239515\n",
      "(Iteration 16701 / 45930) loss: 2.208142\n",
      "(Iteration 16801 / 45930) loss: 2.208837\n",
      "(Epoch 11 / 30) train acc: 0.306000; val_acc: 0.291000\n",
      "(Iteration 16901 / 45930) loss: 2.256002\n",
      "(Iteration 17001 / 45930) loss: 2.190089\n",
      "(Iteration 17101 / 45930) loss: 2.098109\n",
      "(Iteration 17201 / 45930) loss: 2.197453\n",
      "(Iteration 17301 / 45930) loss: 2.209759\n",
      "(Iteration 17401 / 45930) loss: 2.131681\n",
      "(Iteration 17501 / 45930) loss: 2.178746\n",
      "(Iteration 17601 / 45930) loss: 2.240274\n",
      "(Iteration 17701 / 45930) loss: 2.229608\n",
      "(Iteration 17801 / 45930) loss: 2.167171\n",
      "(Iteration 17901 / 45930) loss: 2.235980\n",
      "(Iteration 18001 / 45930) loss: 2.227721\n",
      "(Iteration 18101 / 45930) loss: 2.196807\n",
      "(Iteration 18201 / 45930) loss: 2.193427\n",
      "(Iteration 18301 / 45930) loss: 2.155649\n",
      "(Epoch 12 / 30) train acc: 0.261000; val_acc: 0.286000\n",
      "(Iteration 18401 / 45930) loss: 2.102660\n",
      "(Iteration 18501 / 45930) loss: 2.224221\n",
      "(Iteration 18601 / 45930) loss: 2.201460\n",
      "(Iteration 18701 / 45930) loss: 2.202586\n",
      "(Iteration 18801 / 45930) loss: 2.187227\n",
      "(Iteration 18901 / 45930) loss: 2.215401\n",
      "(Iteration 19001 / 45930) loss: 2.148602\n",
      "(Iteration 19101 / 45930) loss: 2.186241\n",
      "(Iteration 19201 / 45930) loss: 2.230257\n",
      "(Iteration 19301 / 45930) loss: 2.244262\n",
      "(Iteration 19401 / 45930) loss: 2.075200\n",
      "(Iteration 19501 / 45930) loss: 2.135325\n",
      "(Iteration 19601 / 45930) loss: 2.136285\n",
      "(Iteration 19701 / 45930) loss: 2.166998\n",
      "(Iteration 19801 / 45930) loss: 2.100784\n",
      "(Iteration 19901 / 45930) loss: 2.110609\n",
      "(Epoch 13 / 30) train acc: 0.295000; val_acc: 0.290000\n",
      "(Iteration 20001 / 45930) loss: 2.188409\n",
      "(Iteration 20101 / 45930) loss: 2.152415\n",
      "(Iteration 20201 / 45930) loss: 2.126071\n",
      "(Iteration 20301 / 45930) loss: 2.191253\n",
      "(Iteration 20401 / 45930) loss: 2.150446\n",
      "(Iteration 20501 / 45930) loss: 2.173036\n",
      "(Iteration 20601 / 45930) loss: 2.045034\n",
      "(Iteration 20701 / 45930) loss: 2.198516\n",
      "(Iteration 20801 / 45930) loss: 2.088285\n",
      "(Iteration 20901 / 45930) loss: 2.168977\n",
      "(Iteration 21001 / 45930) loss: 2.109268\n",
      "(Iteration 21101 / 45930) loss: 2.114470\n",
      "(Iteration 21201 / 45930) loss: 2.090063\n",
      "(Iteration 21301 / 45930) loss: 2.134968\n",
      "(Iteration 21401 / 45930) loss: 2.087670\n",
      "(Epoch 14 / 30) train acc: 0.277000; val_acc: 0.290000\n",
      "(Iteration 21501 / 45930) loss: 2.178432\n",
      "(Iteration 21601 / 45930) loss: 2.057681\n",
      "(Iteration 21701 / 45930) loss: 2.129865\n",
      "(Iteration 21801 / 45930) loss: 2.144173\n",
      "(Iteration 21901 / 45930) loss: 2.150321\n",
      "(Iteration 22001 / 45930) loss: 2.148257\n",
      "(Iteration 22101 / 45930) loss: 2.169088\n",
      "(Iteration 22201 / 45930) loss: 2.125345\n",
      "(Iteration 22301 / 45930) loss: 2.068955\n",
      "(Iteration 22401 / 45930) loss: 2.152217\n",
      "(Iteration 22501 / 45930) loss: 2.135336\n",
      "(Iteration 22601 / 45930) loss: 2.086535\n",
      "(Iteration 22701 / 45930) loss: 2.180138\n",
      "(Iteration 22801 / 45930) loss: 2.157555\n",
      "(Iteration 22901 / 45930) loss: 2.042830\n",
      "(Epoch 15 / 30) train acc: 0.287000; val_acc: 0.292000\n",
      "(Iteration 23001 / 45930) loss: 2.139467\n",
      "(Iteration 23101 / 45930) loss: 2.139305\n",
      "(Iteration 23201 / 45930) loss: 2.123352\n",
      "(Iteration 23301 / 45930) loss: 2.114653\n",
      "(Iteration 23401 / 45930) loss: 2.082296\n",
      "(Iteration 23501 / 45930) loss: 2.183807\n",
      "(Iteration 23601 / 45930) loss: 2.141442\n",
      "(Iteration 23701 / 45930) loss: 2.167851\n",
      "(Iteration 23801 / 45930) loss: 2.055617\n",
      "(Iteration 23901 / 45930) loss: 2.105672\n",
      "(Iteration 24001 / 45930) loss: 2.116220\n",
      "(Iteration 24101 / 45930) loss: 2.093091\n",
      "(Iteration 24201 / 45930) loss: 2.019557\n",
      "(Iteration 24301 / 45930) loss: 1.982978\n",
      "(Iteration 24401 / 45930) loss: 2.091349\n",
      "(Epoch 16 / 30) train acc: 0.274000; val_acc: 0.293000\n",
      "(Iteration 24501 / 45930) loss: 2.183368\n",
      "(Iteration 24601 / 45930) loss: 2.055907\n",
      "(Iteration 24701 / 45930) loss: 1.978073\n",
      "(Iteration 24801 / 45930) loss: 2.057595\n",
      "(Iteration 24901 / 45930) loss: 2.143424\n",
      "(Iteration 25001 / 45930) loss: 2.004432\n",
      "(Iteration 25101 / 45930) loss: 2.062936\n",
      "(Iteration 25201 / 45930) loss: 2.139019\n",
      "(Iteration 25301 / 45930) loss: 2.113824\n",
      "(Iteration 25401 / 45930) loss: 2.096132\n",
      "(Iteration 25501 / 45930) loss: 2.065076\n",
      "(Iteration 25601 / 45930) loss: 2.020780\n",
      "(Iteration 25701 / 45930) loss: 2.087677\n",
      "(Iteration 25801 / 45930) loss: 2.091875\n",
      "(Iteration 25901 / 45930) loss: 2.081189\n",
      "(Iteration 26001 / 45930) loss: 2.007605\n",
      "(Epoch 17 / 30) train acc: 0.264000; val_acc: 0.293000\n",
      "(Iteration 26101 / 45930) loss: 2.109256\n",
      "(Iteration 26201 / 45930) loss: 2.123089\n",
      "(Iteration 26301 / 45930) loss: 2.102500\n",
      "(Iteration 26401 / 45930) loss: 2.021650\n",
      "(Iteration 26501 / 45930) loss: 2.156234\n",
      "(Iteration 26601 / 45930) loss: 2.077877\n",
      "(Iteration 26701 / 45930) loss: 2.049137\n",
      "(Iteration 26801 / 45930) loss: 2.144016\n",
      "(Iteration 26901 / 45930) loss: 2.183781\n",
      "(Iteration 27001 / 45930) loss: 1.898734\n",
      "(Iteration 27101 / 45930) loss: 2.107080\n",
      "(Iteration 27201 / 45930) loss: 2.140323\n",
      "(Iteration 27301 / 45930) loss: 2.221733\n",
      "(Iteration 27401 / 45930) loss: 2.167876\n",
      "(Iteration 27501 / 45930) loss: 1.903789\n",
      "(Epoch 18 / 30) train acc: 0.261000; val_acc: 0.295000\n",
      "(Iteration 27601 / 45930) loss: 2.092569\n",
      "(Iteration 27701 / 45930) loss: 2.077675\n",
      "(Iteration 27801 / 45930) loss: 2.060635\n",
      "(Iteration 27901 / 45930) loss: 2.010667\n",
      "(Iteration 28001 / 45930) loss: 2.090869\n",
      "(Iteration 28101 / 45930) loss: 2.013031\n",
      "(Iteration 28201 / 45930) loss: 2.074914\n",
      "(Iteration 28301 / 45930) loss: 2.055084\n",
      "(Iteration 28401 / 45930) loss: 2.131535\n",
      "(Iteration 28501 / 45930) loss: 2.036323\n",
      "(Iteration 28601 / 45930) loss: 2.042842\n",
      "(Iteration 28701 / 45930) loss: 2.124643\n",
      "(Iteration 28801 / 45930) loss: 2.055441\n",
      "(Iteration 28901 / 45930) loss: 1.969519\n",
      "(Iteration 29001 / 45930) loss: 2.195223\n",
      "(Epoch 19 / 30) train acc: 0.284000; val_acc: 0.289000\n",
      "(Iteration 29101 / 45930) loss: 2.164089\n",
      "(Iteration 29201 / 45930) loss: 2.020922\n",
      "(Iteration 29301 / 45930) loss: 2.104529\n",
      "(Iteration 29401 / 45930) loss: 1.991421\n",
      "(Iteration 29501 / 45930) loss: 1.990162\n",
      "(Iteration 29601 / 45930) loss: 1.901802\n",
      "(Iteration 29701 / 45930) loss: 2.137293\n",
      "(Iteration 29801 / 45930) loss: 2.103731\n",
      "(Iteration 29901 / 45930) loss: 2.074726\n",
      "(Iteration 30001 / 45930) loss: 1.934057\n",
      "(Iteration 30101 / 45930) loss: 2.022726\n",
      "(Iteration 30201 / 45930) loss: 1.993335\n",
      "(Iteration 30301 / 45930) loss: 2.140288\n",
      "(Iteration 30401 / 45930) loss: 1.931717\n",
      "(Iteration 30501 / 45930) loss: 2.101988\n",
      "(Iteration 30601 / 45930) loss: 2.035620\n",
      "(Epoch 20 / 30) train acc: 0.275000; val_acc: 0.294000\n",
      "(Iteration 30701 / 45930) loss: 2.205467\n",
      "(Iteration 30801 / 45930) loss: 2.112912\n",
      "(Iteration 30901 / 45930) loss: 2.072030\n",
      "(Iteration 31001 / 45930) loss: 2.023265\n",
      "(Iteration 31101 / 45930) loss: 2.106637\n",
      "(Iteration 31201 / 45930) loss: 2.068066\n",
      "(Iteration 31301 / 45930) loss: 2.075064\n",
      "(Iteration 31401 / 45930) loss: 1.957893\n",
      "(Iteration 31501 / 45930) loss: 2.105002\n",
      "(Iteration 31601 / 45930) loss: 2.003203\n",
      "(Iteration 31701 / 45930) loss: 2.103095\n",
      "(Iteration 31801 / 45930) loss: 2.018212\n",
      "(Iteration 31901 / 45930) loss: 2.125675\n",
      "(Iteration 32001 / 45930) loss: 2.029217\n",
      "(Iteration 32101 / 45930) loss: 1.917330\n",
      "(Epoch 21 / 30) train acc: 0.289000; val_acc: 0.297000\n",
      "(Iteration 32201 / 45930) loss: 2.051824\n",
      "(Iteration 32301 / 45930) loss: 2.188682\n",
      "(Iteration 32401 / 45930) loss: 1.998610\n",
      "(Iteration 32501 / 45930) loss: 2.146556\n",
      "(Iteration 32601 / 45930) loss: 2.114439\n",
      "(Iteration 32701 / 45930) loss: 2.003360\n",
      "(Iteration 32801 / 45930) loss: 2.154379\n",
      "(Iteration 32901 / 45930) loss: 1.895417\n",
      "(Iteration 33001 / 45930) loss: 2.140866\n",
      "(Iteration 33101 / 45930) loss: 2.012245\n",
      "(Iteration 33201 / 45930) loss: 2.008988\n",
      "(Iteration 33301 / 45930) loss: 1.954867\n",
      "(Iteration 33401 / 45930) loss: 2.085681\n",
      "(Iteration 33501 / 45930) loss: 2.088818\n",
      "(Iteration 33601 / 45930) loss: 2.019131\n",
      "(Epoch 22 / 30) train acc: 0.330000; val_acc: 0.300000\n",
      "(Iteration 33701 / 45930) loss: 1.998470\n",
      "(Iteration 33801 / 45930) loss: 2.089550\n",
      "(Iteration 33901 / 45930) loss: 2.031900\n",
      "(Iteration 34001 / 45930) loss: 1.995177\n",
      "(Iteration 34101 / 45930) loss: 2.068081\n",
      "(Iteration 34201 / 45930) loss: 1.995258\n",
      "(Iteration 34301 / 45930) loss: 1.914290\n",
      "(Iteration 34401 / 45930) loss: 2.041920\n",
      "(Iteration 34501 / 45930) loss: 2.186660\n",
      "(Iteration 34601 / 45930) loss: 1.992165\n",
      "(Iteration 34701 / 45930) loss: 1.968141\n",
      "(Iteration 34801 / 45930) loss: 2.062467\n",
      "(Iteration 34901 / 45930) loss: 1.917084\n",
      "(Iteration 35001 / 45930) loss: 2.082014\n",
      "(Iteration 35101 / 45930) loss: 1.998086\n",
      "(Iteration 35201 / 45930) loss: 2.060942\n",
      "(Epoch 23 / 30) train acc: 0.298000; val_acc: 0.303000\n",
      "(Iteration 35301 / 45930) loss: 1.879716\n",
      "(Iteration 35401 / 45930) loss: 1.900838\n",
      "(Iteration 35501 / 45930) loss: 1.974419\n",
      "(Iteration 35601 / 45930) loss: 1.974675\n",
      "(Iteration 35701 / 45930) loss: 2.111245\n",
      "(Iteration 35801 / 45930) loss: 2.128025\n",
      "(Iteration 35901 / 45930) loss: 2.052385\n",
      "(Iteration 36001 / 45930) loss: 2.072484\n",
      "(Iteration 36101 / 45930) loss: 1.991813\n",
      "(Iteration 36201 / 45930) loss: 2.110249\n",
      "(Iteration 36301 / 45930) loss: 1.961862\n",
      "(Iteration 36401 / 45930) loss: 2.061390\n",
      "(Iteration 36501 / 45930) loss: 1.981088\n",
      "(Iteration 36601 / 45930) loss: 2.080895\n",
      "(Iteration 36701 / 45930) loss: 1.974434\n",
      "(Epoch 24 / 30) train acc: 0.294000; val_acc: 0.304000\n",
      "(Iteration 36801 / 45930) loss: 2.028069\n",
      "(Iteration 36901 / 45930) loss: 2.033701\n",
      "(Iteration 37001 / 45930) loss: 2.095205\n",
      "(Iteration 37101 / 45930) loss: 1.965852\n",
      "(Iteration 37201 / 45930) loss: 2.036253\n",
      "(Iteration 37301 / 45930) loss: 1.965529\n",
      "(Iteration 37401 / 45930) loss: 2.008232\n",
      "(Iteration 37501 / 45930) loss: 1.920345\n",
      "(Iteration 37601 / 45930) loss: 2.019842\n",
      "(Iteration 37701 / 45930) loss: 2.086598\n",
      "(Iteration 37801 / 45930) loss: 2.007935\n",
      "(Iteration 37901 / 45930) loss: 2.008689\n",
      "(Iteration 38001 / 45930) loss: 1.980471\n",
      "(Iteration 38101 / 45930) loss: 2.045824\n",
      "(Iteration 38201 / 45930) loss: 2.170548\n",
      "(Epoch 25 / 30) train acc: 0.306000; val_acc: 0.305000\n",
      "(Iteration 38301 / 45930) loss: 1.905992\n",
      "(Iteration 38401 / 45930) loss: 1.983339\n",
      "(Iteration 38501 / 45930) loss: 2.004872\n",
      "(Iteration 38601 / 45930) loss: 2.005607\n",
      "(Iteration 38701 / 45930) loss: 2.071557\n",
      "(Iteration 38801 / 45930) loss: 2.012816\n",
      "(Iteration 38901 / 45930) loss: 2.021574\n",
      "(Iteration 39001 / 45930) loss: 2.035560\n",
      "(Iteration 39101 / 45930) loss: 2.057183\n",
      "(Iteration 39201 / 45930) loss: 1.942860\n",
      "(Iteration 39301 / 45930) loss: 1.925542\n",
      "(Iteration 39401 / 45930) loss: 1.930722\n",
      "(Iteration 39501 / 45930) loss: 2.070377\n",
      "(Iteration 39601 / 45930) loss: 2.042410\n",
      "(Iteration 39701 / 45930) loss: 2.064544\n",
      "(Iteration 39801 / 45930) loss: 2.000375\n",
      "(Epoch 26 / 30) train acc: 0.294000; val_acc: 0.306000\n",
      "(Iteration 39901 / 45930) loss: 2.000820\n",
      "(Iteration 40001 / 45930) loss: 2.024776\n",
      "(Iteration 40101 / 45930) loss: 1.935610\n",
      "(Iteration 40201 / 45930) loss: 1.892338\n",
      "(Iteration 40301 / 45930) loss: 1.920895\n",
      "(Iteration 40401 / 45930) loss: 2.036552\n",
      "(Iteration 40501 / 45930) loss: 2.101105\n",
      "(Iteration 40601 / 45930) loss: 1.935892\n",
      "(Iteration 40701 / 45930) loss: 1.968870\n",
      "(Iteration 40801 / 45930) loss: 2.010668\n",
      "(Iteration 40901 / 45930) loss: 2.125900\n",
      "(Iteration 41001 / 45930) loss: 1.980628\n",
      "(Iteration 41101 / 45930) loss: 1.886923\n",
      "(Iteration 41201 / 45930) loss: 2.042294\n",
      "(Iteration 41301 / 45930) loss: 2.047994\n",
      "(Epoch 27 / 30) train acc: 0.321000; val_acc: 0.300000\n",
      "(Iteration 41401 / 45930) loss: 2.120688\n",
      "(Iteration 41501 / 45930) loss: 2.047889\n",
      "(Iteration 41601 / 45930) loss: 2.057189\n",
      "(Iteration 41701 / 45930) loss: 2.050605\n",
      "(Iteration 41801 / 45930) loss: 1.957659\n",
      "(Iteration 41901 / 45930) loss: 2.046761\n",
      "(Iteration 42001 / 45930) loss: 1.970937\n",
      "(Iteration 42101 / 45930) loss: 2.076242\n",
      "(Iteration 42201 / 45930) loss: 1.997541\n",
      "(Iteration 42301 / 45930) loss: 2.000658\n",
      "(Iteration 42401 / 45930) loss: 1.947275\n",
      "(Iteration 42501 / 45930) loss: 1.942933\n",
      "(Iteration 42601 / 45930) loss: 2.068648\n",
      "(Iteration 42701 / 45930) loss: 2.033815\n",
      "(Iteration 42801 / 45930) loss: 1.946585\n",
      "(Epoch 28 / 30) train acc: 0.281000; val_acc: 0.302000\n",
      "(Iteration 42901 / 45930) loss: 2.112843\n",
      "(Iteration 43001 / 45930) loss: 1.868297\n",
      "(Iteration 43101 / 45930) loss: 2.075523\n",
      "(Iteration 43201 / 45930) loss: 1.988446\n",
      "(Iteration 43301 / 45930) loss: 2.020598\n",
      "(Iteration 43401 / 45930) loss: 1.866301\n",
      "(Iteration 43501 / 45930) loss: 2.038969\n",
      "(Iteration 43601 / 45930) loss: 1.970797\n",
      "(Iteration 43701 / 45930) loss: 1.914079\n",
      "(Iteration 43801 / 45930) loss: 1.990334\n",
      "(Iteration 43901 / 45930) loss: 1.957095\n",
      "(Iteration 44001 / 45930) loss: 2.106136\n",
      "(Iteration 44101 / 45930) loss: 1.948226\n",
      "(Iteration 44201 / 45930) loss: 1.943695\n",
      "(Iteration 44301 / 45930) loss: 2.035496\n",
      "(Epoch 29 / 30) train acc: 0.307000; val_acc: 0.307000\n",
      "(Iteration 44401 / 45930) loss: 2.014839\n",
      "(Iteration 44501 / 45930) loss: 2.165450\n",
      "(Iteration 44601 / 45930) loss: 1.955979\n",
      "(Iteration 44701 / 45930) loss: 2.039171\n",
      "(Iteration 44801 / 45930) loss: 2.140894\n",
      "(Iteration 44901 / 45930) loss: 2.058379\n",
      "(Iteration 45001 / 45930) loss: 1.948706\n",
      "(Iteration 45101 / 45930) loss: 2.026135\n",
      "(Iteration 45201 / 45930) loss: 1.971816\n",
      "(Iteration 45301 / 45930) loss: 1.950742\n",
      "(Iteration 45401 / 45930) loss: 2.066685\n",
      "(Iteration 45501 / 45930) loss: 2.073471\n",
      "(Iteration 45601 / 45930) loss: 2.031449\n",
      "(Iteration 45701 / 45930) loss: 1.918631\n",
      "(Iteration 45801 / 45930) loss: 2.084749\n",
      "(Iteration 45901 / 45930) loss: 2.031967\n",
      "(Epoch 30 / 30) train acc: 0.297000; val_acc: 0.305000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.001, 'num_epochs': 40, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 30600) loss: 2.309058\n",
      "(Epoch 0 / 40) train acc: 0.088000; val_acc: 0.100000\n",
      "(Iteration 101 / 30600) loss: 2.308922\n",
      "(Iteration 201 / 30600) loss: 2.308748\n",
      "(Iteration 301 / 30600) loss: 2.308685\n",
      "(Iteration 401 / 30600) loss: 2.308438\n",
      "(Iteration 501 / 30600) loss: 2.308364\n",
      "(Iteration 601 / 30600) loss: 2.308231\n",
      "(Iteration 701 / 30600) loss: 2.308090\n",
      "(Epoch 1 / 40) train acc: 0.178000; val_acc: 0.155000\n",
      "(Iteration 801 / 30600) loss: 2.307904\n",
      "(Iteration 901 / 30600) loss: 2.307858\n",
      "(Iteration 1001 / 30600) loss: 2.307598\n",
      "(Iteration 1101 / 30600) loss: 2.307860\n",
      "(Iteration 1201 / 30600) loss: 2.307588\n",
      "(Iteration 1301 / 30600) loss: 2.307457\n",
      "(Iteration 1401 / 30600) loss: 2.307412\n",
      "(Iteration 1501 / 30600) loss: 2.307283\n",
      "(Epoch 2 / 40) train acc: 0.187000; val_acc: 0.186000\n",
      "(Iteration 1601 / 30600) loss: 2.307105\n",
      "(Iteration 1701 / 30600) loss: 2.307103\n",
      "(Iteration 1801 / 30600) loss: 2.307109\n",
      "(Iteration 1901 / 30600) loss: 2.307098\n",
      "(Iteration 2001 / 30600) loss: 2.306793\n",
      "(Iteration 2101 / 30600) loss: 2.306952\n",
      "(Iteration 2201 / 30600) loss: 2.306458\n",
      "(Epoch 3 / 40) train acc: 0.174000; val_acc: 0.146000\n",
      "(Iteration 2301 / 30600) loss: 2.306631\n",
      "(Iteration 2401 / 30600) loss: 2.306322\n",
      "(Iteration 2501 / 30600) loss: 2.306528\n",
      "(Iteration 2601 / 30600) loss: 2.306233\n",
      "(Iteration 2701 / 30600) loss: 2.306275\n",
      "(Iteration 2801 / 30600) loss: 2.306267\n",
      "(Iteration 2901 / 30600) loss: 2.305747\n",
      "(Iteration 3001 / 30600) loss: 2.306100\n",
      "(Epoch 4 / 40) train acc: 0.172000; val_acc: 0.151000\n",
      "(Iteration 3101 / 30600) loss: 2.305865\n",
      "(Iteration 3201 / 30600) loss: 2.305766\n",
      "(Iteration 3301 / 30600) loss: 2.305618\n",
      "(Iteration 3401 / 30600) loss: 2.305837\n",
      "(Iteration 3501 / 30600) loss: 2.305581\n",
      "(Iteration 3601 / 30600) loss: 2.305675\n",
      "(Iteration 3701 / 30600) loss: 2.305508\n",
      "(Iteration 3801 / 30600) loss: 2.305669\n",
      "(Epoch 5 / 40) train acc: 0.215000; val_acc: 0.185000\n",
      "(Iteration 3901 / 30600) loss: 2.305307\n",
      "(Iteration 4001 / 30600) loss: 2.305801\n",
      "(Iteration 4101 / 30600) loss: 2.305831\n",
      "(Iteration 4201 / 30600) loss: 2.305084\n",
      "(Iteration 4301 / 30600) loss: 2.305481\n",
      "(Iteration 4401 / 30600) loss: 2.305461\n",
      "(Iteration 4501 / 30600) loss: 2.305083\n",
      "(Epoch 6 / 40) train acc: 0.191000; val_acc: 0.162000\n",
      "(Iteration 4601 / 30600) loss: 2.305146\n",
      "(Iteration 4701 / 30600) loss: 2.305069\n",
      "(Iteration 4801 / 30600) loss: 2.304928\n",
      "(Iteration 4901 / 30600) loss: 2.305074\n",
      "(Iteration 5001 / 30600) loss: 2.304889\n",
      "(Iteration 5101 / 30600) loss: 2.304840\n",
      "(Iteration 5201 / 30600) loss: 2.304811\n",
      "(Iteration 5301 / 30600) loss: 2.304225\n",
      "(Epoch 7 / 40) train acc: 0.187000; val_acc: 0.159000\n",
      "(Iteration 5401 / 30600) loss: 2.304924\n",
      "(Iteration 5501 / 30600) loss: 2.304732\n",
      "(Iteration 5601 / 30600) loss: 2.304788\n",
      "(Iteration 5701 / 30600) loss: 2.304785\n",
      "(Iteration 5801 / 30600) loss: 2.304543\n",
      "(Iteration 5901 / 30600) loss: 2.304036\n",
      "(Iteration 6001 / 30600) loss: 2.304116\n",
      "(Iteration 6101 / 30600) loss: 2.304326\n",
      "(Epoch 8 / 40) train acc: 0.208000; val_acc: 0.176000\n",
      "(Iteration 6201 / 30600) loss: 2.304355\n",
      "(Iteration 6301 / 30600) loss: 2.303915\n",
      "(Iteration 6401 / 30600) loss: 2.304244\n",
      "(Iteration 6501 / 30600) loss: 2.304552\n",
      "(Iteration 6601 / 30600) loss: 2.303565\n",
      "(Iteration 6701 / 30600) loss: 2.304163\n",
      "(Iteration 6801 / 30600) loss: 2.304327\n",
      "(Epoch 9 / 40) train acc: 0.212000; val_acc: 0.169000\n",
      "(Iteration 6901 / 30600) loss: 2.303858\n",
      "(Iteration 7001 / 30600) loss: 2.303791\n",
      "(Iteration 7101 / 30600) loss: 2.304275\n",
      "(Iteration 7201 / 30600) loss: 2.304321\n",
      "(Iteration 7301 / 30600) loss: 2.304078\n",
      "(Iteration 7401 / 30600) loss: 2.303844\n",
      "(Iteration 7501 / 30600) loss: 2.303887\n",
      "(Iteration 7601 / 30600) loss: 2.304054\n",
      "(Epoch 10 / 40) train acc: 0.199000; val_acc: 0.185000\n",
      "(Iteration 7701 / 30600) loss: 2.303954\n",
      "(Iteration 7801 / 30600) loss: 2.303784\n",
      "(Iteration 7901 / 30600) loss: 2.303981\n",
      "(Iteration 8001 / 30600) loss: 2.302520\n",
      "(Iteration 8101 / 30600) loss: 2.304052\n",
      "(Iteration 8201 / 30600) loss: 2.303861\n",
      "(Iteration 8301 / 30600) loss: 2.303513\n",
      "(Iteration 8401 / 30600) loss: 2.303112\n",
      "(Epoch 11 / 40) train acc: 0.207000; val_acc: 0.192000\n",
      "(Iteration 8501 / 30600) loss: 2.303521\n",
      "(Iteration 8601 / 30600) loss: 2.304261\n",
      "(Iteration 8701 / 30600) loss: 2.303418\n",
      "(Iteration 8801 / 30600) loss: 2.302618\n",
      "(Iteration 8901 / 30600) loss: 2.303590\n",
      "(Iteration 9001 / 30600) loss: 2.303246\n",
      "(Iteration 9101 / 30600) loss: 2.303080\n",
      "(Epoch 12 / 40) train acc: 0.213000; val_acc: 0.204000\n",
      "(Iteration 9201 / 30600) loss: 2.302875\n",
      "(Iteration 9301 / 30600) loss: 2.303827\n",
      "(Iteration 9401 / 30600) loss: 2.303464\n",
      "(Iteration 9501 / 30600) loss: 2.303287\n",
      "(Iteration 9601 / 30600) loss: 2.303849\n",
      "(Iteration 9701 / 30600) loss: 2.303604\n",
      "(Iteration 9801 / 30600) loss: 2.303797\n",
      "(Iteration 9901 / 30600) loss: 2.302927\n",
      "(Epoch 13 / 40) train acc: 0.208000; val_acc: 0.196000\n",
      "(Iteration 10001 / 30600) loss: 2.303109\n",
      "(Iteration 10101 / 30600) loss: 2.303536\n",
      "(Iteration 10201 / 30600) loss: 2.302957\n",
      "(Iteration 10301 / 30600) loss: 2.303539\n",
      "(Iteration 10401 / 30600) loss: 2.303472\n",
      "(Iteration 10501 / 30600) loss: 2.302855\n",
      "(Iteration 10601 / 30600) loss: 2.303057\n",
      "(Iteration 10701 / 30600) loss: 2.302746\n",
      "(Epoch 14 / 40) train acc: 0.208000; val_acc: 0.193000\n",
      "(Iteration 10801 / 30600) loss: 2.302889\n",
      "(Iteration 10901 / 30600) loss: 2.302860\n",
      "(Iteration 11001 / 30600) loss: 2.302722\n",
      "(Iteration 11101 / 30600) loss: 2.303217\n",
      "(Iteration 11201 / 30600) loss: 2.302835\n",
      "(Iteration 11301 / 30600) loss: 2.303014\n",
      "(Iteration 11401 / 30600) loss: 2.302873\n",
      "(Epoch 15 / 40) train acc: 0.200000; val_acc: 0.196000\n",
      "(Iteration 11501 / 30600) loss: 2.302754\n",
      "(Iteration 11601 / 30600) loss: 2.302436\n",
      "(Iteration 11701 / 30600) loss: 2.302873\n",
      "(Iteration 11801 / 30600) loss: 2.302835\n",
      "(Iteration 11901 / 30600) loss: 2.303254\n",
      "(Iteration 12001 / 30600) loss: 2.302963\n",
      "(Iteration 12101 / 30600) loss: 2.302759\n",
      "(Iteration 12201 / 30600) loss: 2.302501\n",
      "(Epoch 16 / 40) train acc: 0.204000; val_acc: 0.185000\n",
      "(Iteration 12301 / 30600) loss: 2.302735\n",
      "(Iteration 12401 / 30600) loss: 2.302217\n",
      "(Iteration 12501 / 30600) loss: 2.303341\n",
      "(Iteration 12601 / 30600) loss: 2.301728\n",
      "(Iteration 12701 / 30600) loss: 2.301712\n",
      "(Iteration 12801 / 30600) loss: 2.302707\n",
      "(Iteration 12901 / 30600) loss: 2.302011\n",
      "(Iteration 13001 / 30600) loss: 2.303155\n",
      "(Epoch 17 / 40) train acc: 0.219000; val_acc: 0.174000\n",
      "(Iteration 13101 / 30600) loss: 2.302634\n",
      "(Iteration 13201 / 30600) loss: 2.302349\n",
      "(Iteration 13301 / 30600) loss: 2.302972\n",
      "(Iteration 13401 / 30600) loss: 2.302067\n",
      "(Iteration 13501 / 30600) loss: 2.301906\n",
      "(Iteration 13601 / 30600) loss: 2.302240\n",
      "(Iteration 13701 / 30600) loss: 2.300167\n",
      "(Epoch 18 / 40) train acc: 0.189000; val_acc: 0.175000\n",
      "(Iteration 13801 / 30600) loss: 2.301056\n",
      "(Iteration 13901 / 30600) loss: 2.302218\n",
      "(Iteration 14001 / 30600) loss: 2.303072\n",
      "(Iteration 14101 / 30600) loss: 2.301285\n",
      "(Iteration 14201 / 30600) loss: 2.302221\n",
      "(Iteration 14301 / 30600) loss: 2.301778\n",
      "(Iteration 14401 / 30600) loss: 2.301202\n",
      "(Iteration 14501 / 30600) loss: 2.301881\n",
      "(Epoch 19 / 40) train acc: 0.203000; val_acc: 0.180000\n",
      "(Iteration 14601 / 30600) loss: 2.300745\n",
      "(Iteration 14701 / 30600) loss: 2.301436\n",
      "(Iteration 14801 / 30600) loss: 2.301447\n",
      "(Iteration 14901 / 30600) loss: 2.300832\n",
      "(Iteration 15001 / 30600) loss: 2.302352\n",
      "(Iteration 15101 / 30600) loss: 2.301613\n",
      "(Iteration 15201 / 30600) loss: 2.301273\n",
      "(Epoch 20 / 40) train acc: 0.186000; val_acc: 0.190000\n",
      "(Iteration 15301 / 30600) loss: 2.300220\n",
      "(Iteration 15401 / 30600) loss: 2.302470\n",
      "(Iteration 15501 / 30600) loss: 2.302311\n",
      "(Iteration 15601 / 30600) loss: 2.300327\n",
      "(Iteration 15701 / 30600) loss: 2.301521\n",
      "(Iteration 15801 / 30600) loss: 2.302375\n",
      "(Iteration 15901 / 30600) loss: 2.300757\n",
      "(Iteration 16001 / 30600) loss: 2.302279\n",
      "(Epoch 21 / 40) train acc: 0.209000; val_acc: 0.183000\n",
      "(Iteration 16101 / 30600) loss: 2.301813\n",
      "(Iteration 16201 / 30600) loss: 2.300706\n",
      "(Iteration 16301 / 30600) loss: 2.301995\n",
      "(Iteration 16401 / 30600) loss: 2.301671\n",
      "(Iteration 16501 / 30600) loss: 2.302240\n",
      "(Iteration 16601 / 30600) loss: 2.302535\n",
      "(Iteration 16701 / 30600) loss: 2.301462\n",
      "(Iteration 16801 / 30600) loss: 2.301180\n",
      "(Epoch 22 / 40) train acc: 0.211000; val_acc: 0.182000\n",
      "(Iteration 16901 / 30600) loss: 2.301299\n",
      "(Iteration 17001 / 30600) loss: 2.302118\n",
      "(Iteration 17101 / 30600) loss: 2.300224\n",
      "(Iteration 17201 / 30600) loss: 2.302364\n",
      "(Iteration 17301 / 30600) loss: 2.302242\n",
      "(Iteration 17401 / 30600) loss: 2.301385\n",
      "(Iteration 17501 / 30600) loss: 2.301515\n",
      "(Epoch 23 / 40) train acc: 0.213000; val_acc: 0.180000\n",
      "(Iteration 17601 / 30600) loss: 2.302981\n",
      "(Iteration 17701 / 30600) loss: 2.300669\n",
      "(Iteration 17801 / 30600) loss: 2.301467\n",
      "(Iteration 17901 / 30600) loss: 2.302139\n",
      "(Iteration 18001 / 30600) loss: 2.300946\n",
      "(Iteration 18101 / 30600) loss: 2.302315\n",
      "(Iteration 18201 / 30600) loss: 2.301256\n",
      "(Iteration 18301 / 30600) loss: 2.301010\n",
      "(Epoch 24 / 40) train acc: 0.228000; val_acc: 0.186000\n",
      "(Iteration 18401 / 30600) loss: 2.301814\n",
      "(Iteration 18501 / 30600) loss: 2.301082\n",
      "(Iteration 18601 / 30600) loss: 2.302400\n",
      "(Iteration 18701 / 30600) loss: 2.301055\n",
      "(Iteration 18801 / 30600) loss: 2.300060\n",
      "(Iteration 18901 / 30600) loss: 2.302071\n",
      "(Iteration 19001 / 30600) loss: 2.302161\n",
      "(Iteration 19101 / 30600) loss: 2.301123\n",
      "(Epoch 25 / 40) train acc: 0.214000; val_acc: 0.182000\n",
      "(Iteration 19201 / 30600) loss: 2.299309\n",
      "(Iteration 19301 / 30600) loss: 2.302606\n",
      "(Iteration 19401 / 30600) loss: 2.301142\n",
      "(Iteration 19501 / 30600) loss: 2.300808\n",
      "(Iteration 19601 / 30600) loss: 2.299950\n",
      "(Iteration 19701 / 30600) loss: 2.300907\n",
      "(Iteration 19801 / 30600) loss: 2.299823\n",
      "(Epoch 26 / 40) train acc: 0.211000; val_acc: 0.182000\n",
      "(Iteration 19901 / 30600) loss: 2.301243\n",
      "(Iteration 20001 / 30600) loss: 2.300022\n",
      "(Iteration 20101 / 30600) loss: 2.301244\n",
      "(Iteration 20201 / 30600) loss: 2.301661\n",
      "(Iteration 20301 / 30600) loss: 2.301491\n",
      "(Iteration 20401 / 30600) loss: 2.300266\n",
      "(Iteration 20501 / 30600) loss: 2.300398\n",
      "(Iteration 20601 / 30600) loss: 2.301409\n",
      "(Epoch 27 / 40) train acc: 0.221000; val_acc: 0.186000\n",
      "(Iteration 20701 / 30600) loss: 2.301894\n",
      "(Iteration 20801 / 30600) loss: 2.301397\n",
      "(Iteration 20901 / 30600) loss: 2.300951\n",
      "(Iteration 21001 / 30600) loss: 2.299452\n",
      "(Iteration 21101 / 30600) loss: 2.302200\n",
      "(Iteration 21201 / 30600) loss: 2.300226\n",
      "(Iteration 21301 / 30600) loss: 2.300010\n",
      "(Iteration 21401 / 30600) loss: 2.302772\n",
      "(Epoch 28 / 40) train acc: 0.211000; val_acc: 0.187000\n",
      "(Iteration 21501 / 30600) loss: 2.301984\n",
      "(Iteration 21601 / 30600) loss: 2.298822\n",
      "(Iteration 21701 / 30600) loss: 2.298968\n",
      "(Iteration 21801 / 30600) loss: 2.300505\n",
      "(Iteration 21901 / 30600) loss: 2.301127\n",
      "(Iteration 22001 / 30600) loss: 2.301908\n",
      "(Iteration 22101 / 30600) loss: 2.300460\n",
      "(Epoch 29 / 40) train acc: 0.236000; val_acc: 0.187000\n",
      "(Iteration 22201 / 30600) loss: 2.300379\n",
      "(Iteration 22301 / 30600) loss: 2.300408\n",
      "(Iteration 22401 / 30600) loss: 2.301000\n",
      "(Iteration 22501 / 30600) loss: 2.301439\n",
      "(Iteration 22601 / 30600) loss: 2.300170\n",
      "(Iteration 22701 / 30600) loss: 2.301792\n",
      "(Iteration 22801 / 30600) loss: 2.299547\n",
      "(Iteration 22901 / 30600) loss: 2.301451\n",
      "(Epoch 30 / 40) train acc: 0.231000; val_acc: 0.186000\n",
      "(Iteration 23001 / 30600) loss: 2.301061\n",
      "(Iteration 23101 / 30600) loss: 2.302057\n",
      "(Iteration 23201 / 30600) loss: 2.300005\n",
      "(Iteration 23301 / 30600) loss: 2.299502\n",
      "(Iteration 23401 / 30600) loss: 2.301945\n",
      "(Iteration 23501 / 30600) loss: 2.300891\n",
      "(Iteration 23601 / 30600) loss: 2.300462\n",
      "(Iteration 23701 / 30600) loss: 2.302336\n",
      "(Epoch 31 / 40) train acc: 0.228000; val_acc: 0.188000\n",
      "(Iteration 23801 / 30600) loss: 2.301854\n",
      "(Iteration 23901 / 30600) loss: 2.300091\n",
      "(Iteration 24001 / 30600) loss: 2.301149\n",
      "(Iteration 24101 / 30600) loss: 2.301628\n",
      "(Iteration 24201 / 30600) loss: 2.300372\n",
      "(Iteration 24301 / 30600) loss: 2.299789\n",
      "(Iteration 24401 / 30600) loss: 2.299311\n",
      "(Epoch 32 / 40) train acc: 0.218000; val_acc: 0.187000\n",
      "(Iteration 24501 / 30600) loss: 2.301155\n",
      "(Iteration 24601 / 30600) loss: 2.300609\n",
      "(Iteration 24701 / 30600) loss: 2.300930\n",
      "(Iteration 24801 / 30600) loss: 2.301133\n",
      "(Iteration 24901 / 30600) loss: 2.301049\n",
      "(Iteration 25001 / 30600) loss: 2.299898\n",
      "(Iteration 25101 / 30600) loss: 2.302311\n",
      "(Iteration 25201 / 30600) loss: 2.301040\n",
      "(Epoch 33 / 40) train acc: 0.223000; val_acc: 0.188000\n",
      "(Iteration 25301 / 30600) loss: 2.301248\n",
      "(Iteration 25401 / 30600) loss: 2.301811\n",
      "(Iteration 25501 / 30600) loss: 2.300361\n",
      "(Iteration 25601 / 30600) loss: 2.300450\n",
      "(Iteration 25701 / 30600) loss: 2.300474\n",
      "(Iteration 25801 / 30600) loss: 2.299222\n",
      "(Iteration 25901 / 30600) loss: 2.301042\n",
      "(Iteration 26001 / 30600) loss: 2.300239\n",
      "(Epoch 34 / 40) train acc: 0.225000; val_acc: 0.188000\n",
      "(Iteration 26101 / 30600) loss: 2.300300\n",
      "(Iteration 26201 / 30600) loss: 2.300343\n",
      "(Iteration 26301 / 30600) loss: 2.300594\n",
      "(Iteration 26401 / 30600) loss: 2.300937\n",
      "(Iteration 26501 / 30600) loss: 2.299567\n",
      "(Iteration 26601 / 30600) loss: 2.300868\n",
      "(Iteration 26701 / 30600) loss: 2.300501\n",
      "(Epoch 35 / 40) train acc: 0.226000; val_acc: 0.189000\n",
      "(Iteration 26801 / 30600) loss: 2.300196\n",
      "(Iteration 26901 / 30600) loss: 2.301322\n",
      "(Iteration 27001 / 30600) loss: 2.301647\n",
      "(Iteration 27101 / 30600) loss: 2.301386\n",
      "(Iteration 27201 / 30600) loss: 2.300486\n",
      "(Iteration 27301 / 30600) loss: 2.302609\n",
      "(Iteration 27401 / 30600) loss: 2.298442\n",
      "(Iteration 27501 / 30600) loss: 2.300090\n",
      "(Epoch 36 / 40) train acc: 0.219000; val_acc: 0.191000\n",
      "(Iteration 27601 / 30600) loss: 2.301029\n",
      "(Iteration 27701 / 30600) loss: 2.301366\n",
      "(Iteration 27801 / 30600) loss: 2.300461\n",
      "(Iteration 27901 / 30600) loss: 2.297906\n",
      "(Iteration 28001 / 30600) loss: 2.300127\n",
      "(Iteration 28101 / 30600) loss: 2.300519\n",
      "(Iteration 28201 / 30600) loss: 2.300638\n",
      "(Iteration 28301 / 30600) loss: 2.300176\n",
      "(Epoch 37 / 40) train acc: 0.230000; val_acc: 0.192000\n",
      "(Iteration 28401 / 30600) loss: 2.300811\n",
      "(Iteration 28501 / 30600) loss: 2.300087\n",
      "(Iteration 28601 / 30600) loss: 2.301619\n",
      "(Iteration 28701 / 30600) loss: 2.302215\n",
      "(Iteration 28801 / 30600) loss: 2.299143\n",
      "(Iteration 28901 / 30600) loss: 2.298529\n",
      "(Iteration 29001 / 30600) loss: 2.300447\n",
      "(Epoch 38 / 40) train acc: 0.240000; val_acc: 0.194000\n",
      "(Iteration 29101 / 30600) loss: 2.301079\n",
      "(Iteration 29201 / 30600) loss: 2.299449\n",
      "(Iteration 29301 / 30600) loss: 2.300162\n",
      "(Iteration 29401 / 30600) loss: 2.302130\n",
      "(Iteration 29501 / 30600) loss: 2.300905\n",
      "(Iteration 29601 / 30600) loss: 2.301196\n",
      "(Iteration 29701 / 30600) loss: 2.301163\n",
      "(Iteration 29801 / 30600) loss: 2.301498\n",
      "(Epoch 39 / 40) train acc: 0.197000; val_acc: 0.194000\n",
      "(Iteration 29901 / 30600) loss: 2.300618\n",
      "(Iteration 30001 / 30600) loss: 2.299899\n",
      "(Iteration 30101 / 30600) loss: 2.300820\n",
      "(Iteration 30201 / 30600) loss: 2.300126\n",
      "(Iteration 30301 / 30600) loss: 2.299210\n",
      "(Iteration 30401 / 30600) loss: 2.299546\n",
      "(Iteration 30501 / 30600) loss: 2.302021\n",
      "(Epoch 40 / 40) train acc: 0.211000; val_acc: 0.195000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.001, 'num_epochs': 40, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 61240) loss: 2.309130\n",
      "(Epoch 0 / 40) train acc: 0.097000; val_acc: 0.093000\n",
      "(Iteration 101 / 61240) loss: 2.308978\n",
      "(Iteration 201 / 61240) loss: 2.308820\n",
      "(Iteration 301 / 61240) loss: 2.308500\n",
      "(Iteration 401 / 61240) loss: 2.308534\n",
      "(Iteration 501 / 61240) loss: 2.308319\n",
      "(Iteration 601 / 61240) loss: 2.308387\n",
      "(Iteration 701 / 61240) loss: 2.308031\n",
      "(Iteration 801 / 61240) loss: 2.307415\n",
      "(Iteration 901 / 61240) loss: 2.307749\n",
      "(Iteration 1001 / 61240) loss: 2.307670\n",
      "(Iteration 1101 / 61240) loss: 2.307682\n",
      "(Iteration 1201 / 61240) loss: 2.307508\n",
      "(Iteration 1301 / 61240) loss: 2.307198\n",
      "(Iteration 1401 / 61240) loss: 2.307590\n",
      "(Iteration 1501 / 61240) loss: 2.307283\n",
      "(Epoch 1 / 40) train acc: 0.192000; val_acc: 0.206000\n",
      "(Iteration 1601 / 61240) loss: 2.307054\n",
      "(Iteration 1701 / 61240) loss: 2.307436\n",
      "(Iteration 1801 / 61240) loss: 2.306871\n",
      "(Iteration 1901 / 61240) loss: 2.306628\n",
      "(Iteration 2001 / 61240) loss: 2.307140\n",
      "(Iteration 2101 / 61240) loss: 2.306270\n",
      "(Iteration 2201 / 61240) loss: 2.306113\n",
      "(Iteration 2301 / 61240) loss: 2.306453\n",
      "(Iteration 2401 / 61240) loss: 2.306497\n",
      "(Iteration 2501 / 61240) loss: 2.306390\n",
      "(Iteration 2601 / 61240) loss: 2.306013\n",
      "(Iteration 2701 / 61240) loss: 2.305687\n",
      "(Iteration 2801 / 61240) loss: 2.306028\n",
      "(Iteration 2901 / 61240) loss: 2.305904\n",
      "(Iteration 3001 / 61240) loss: 2.305954\n",
      "(Epoch 2 / 40) train acc: 0.173000; val_acc: 0.170000\n",
      "(Iteration 3101 / 61240) loss: 2.305446\n",
      "(Iteration 3201 / 61240) loss: 2.305729\n",
      "(Iteration 3301 / 61240) loss: 2.305753\n",
      "(Iteration 3401 / 61240) loss: 2.305675\n",
      "(Iteration 3501 / 61240) loss: 2.305521\n",
      "(Iteration 3601 / 61240) loss: 2.305243\n",
      "(Iteration 3701 / 61240) loss: 2.305237\n",
      "(Iteration 3801 / 61240) loss: 2.305467\n",
      "(Iteration 3901 / 61240) loss: 2.305038\n",
      "(Iteration 4001 / 61240) loss: 2.305693\n",
      "(Iteration 4101 / 61240) loss: 2.304588\n",
      "(Iteration 4201 / 61240) loss: 2.304966\n",
      "(Iteration 4301 / 61240) loss: 2.304995\n",
      "(Iteration 4401 / 61240) loss: 2.304926\n",
      "(Iteration 4501 / 61240) loss: 2.304824\n",
      "(Epoch 3 / 40) train acc: 0.178000; val_acc: 0.189000\n",
      "(Iteration 4601 / 61240) loss: 2.305061\n",
      "(Iteration 4701 / 61240) loss: 2.305039\n",
      "(Iteration 4801 / 61240) loss: 2.304850\n",
      "(Iteration 4901 / 61240) loss: 2.304113\n",
      "(Iteration 5001 / 61240) loss: 2.303451\n",
      "(Iteration 5101 / 61240) loss: 2.304220\n",
      "(Iteration 5201 / 61240) loss: 2.304565\n",
      "(Iteration 5301 / 61240) loss: 2.304326\n",
      "(Iteration 5401 / 61240) loss: 2.303691\n",
      "(Iteration 5501 / 61240) loss: 2.304273\n",
      "(Iteration 5601 / 61240) loss: 2.303327\n",
      "(Iteration 5701 / 61240) loss: 2.303629\n",
      "(Iteration 5801 / 61240) loss: 2.304375\n",
      "(Iteration 5901 / 61240) loss: 2.303882\n",
      "(Iteration 6001 / 61240) loss: 2.303285\n",
      "(Iteration 6101 / 61240) loss: 2.304172\n",
      "(Epoch 4 / 40) train acc: 0.264000; val_acc: 0.263000\n",
      "(Iteration 6201 / 61240) loss: 2.302923\n",
      "(Iteration 6301 / 61240) loss: 2.303092\n",
      "(Iteration 6401 / 61240) loss: 2.303736\n",
      "(Iteration 6501 / 61240) loss: 2.303962\n",
      "(Iteration 6601 / 61240) loss: 2.303559\n",
      "(Iteration 6701 / 61240) loss: 2.302664\n",
      "(Iteration 6801 / 61240) loss: 2.302877\n",
      "(Iteration 6901 / 61240) loss: 2.303214\n",
      "(Iteration 7001 / 61240) loss: 2.303389\n",
      "(Iteration 7101 / 61240) loss: 2.301821\n",
      "(Iteration 7201 / 61240) loss: 2.302654\n",
      "(Iteration 7301 / 61240) loss: 2.302652\n",
      "(Iteration 7401 / 61240) loss: 2.302411\n",
      "(Iteration 7501 / 61240) loss: 2.302795\n",
      "(Iteration 7601 / 61240) loss: 2.301120\n",
      "(Epoch 5 / 40) train acc: 0.295000; val_acc: 0.297000\n",
      "(Iteration 7701 / 61240) loss: 2.302240\n",
      "(Iteration 7801 / 61240) loss: 2.300273\n",
      "(Iteration 7901 / 61240) loss: 2.302930\n",
      "(Iteration 8001 / 61240) loss: 2.301590\n",
      "(Iteration 8101 / 61240) loss: 2.302136\n",
      "(Iteration 8201 / 61240) loss: 2.302284\n",
      "(Iteration 8301 / 61240) loss: 2.301930\n",
      "(Iteration 8401 / 61240) loss: 2.301299\n",
      "(Iteration 8501 / 61240) loss: 2.302764\n",
      "(Iteration 8601 / 61240) loss: 2.300837\n",
      "(Iteration 8701 / 61240) loss: 2.301228\n",
      "(Iteration 8801 / 61240) loss: 2.300783\n",
      "(Iteration 8901 / 61240) loss: 2.299634\n",
      "(Iteration 9001 / 61240) loss: 2.301621\n",
      "(Iteration 9101 / 61240) loss: 2.300288\n",
      "(Epoch 6 / 40) train acc: 0.257000; val_acc: 0.274000\n",
      "(Iteration 9201 / 61240) loss: 2.301184\n",
      "(Iteration 9301 / 61240) loss: 2.300440\n",
      "(Iteration 9401 / 61240) loss: 2.300644\n",
      "(Iteration 9501 / 61240) loss: 2.301629\n",
      "(Iteration 9601 / 61240) loss: 2.303094\n",
      "(Iteration 9701 / 61240) loss: 2.299413\n",
      "(Iteration 9801 / 61240) loss: 2.301794\n",
      "(Iteration 9901 / 61240) loss: 2.300733\n",
      "(Iteration 10001 / 61240) loss: 2.302657\n",
      "(Iteration 10101 / 61240) loss: 2.298924\n",
      "(Iteration 10201 / 61240) loss: 2.302741\n",
      "(Iteration 10301 / 61240) loss: 2.300473\n",
      "(Iteration 10401 / 61240) loss: 2.296149\n",
      "(Iteration 10501 / 61240) loss: 2.300570\n",
      "(Iteration 10601 / 61240) loss: 2.300319\n",
      "(Iteration 10701 / 61240) loss: 2.298110\n",
      "(Epoch 7 / 40) train acc: 0.259000; val_acc: 0.259000\n",
      "(Iteration 10801 / 61240) loss: 2.297237\n",
      "(Iteration 10901 / 61240) loss: 2.297724\n",
      "(Iteration 11001 / 61240) loss: 2.298586\n",
      "(Iteration 11101 / 61240) loss: 2.299139\n",
      "(Iteration 11201 / 61240) loss: 2.298322\n",
      "(Iteration 11301 / 61240) loss: 2.298924\n",
      "(Iteration 11401 / 61240) loss: 2.297437\n",
      "(Iteration 11501 / 61240) loss: 2.296772\n",
      "(Iteration 11601 / 61240) loss: 2.299739\n",
      "(Iteration 11701 / 61240) loss: 2.295802\n",
      "(Iteration 11801 / 61240) loss: 2.294040\n",
      "(Iteration 11901 / 61240) loss: 2.298578\n",
      "(Iteration 12001 / 61240) loss: 2.301152\n",
      "(Iteration 12101 / 61240) loss: 2.295909\n",
      "(Iteration 12201 / 61240) loss: 2.300805\n",
      "(Epoch 8 / 40) train acc: 0.275000; val_acc: 0.272000\n",
      "(Iteration 12301 / 61240) loss: 2.298941\n",
      "(Iteration 12401 / 61240) loss: 2.296343\n",
      "(Iteration 12501 / 61240) loss: 2.292646\n",
      "(Iteration 12601 / 61240) loss: 2.301303\n",
      "(Iteration 12701 / 61240) loss: 2.297150\n",
      "(Iteration 12801 / 61240) loss: 2.299326\n",
      "(Iteration 12901 / 61240) loss: 2.298408\n",
      "(Iteration 13001 / 61240) loss: 2.295111\n",
      "(Iteration 13101 / 61240) loss: 2.297973\n",
      "(Iteration 13201 / 61240) loss: 2.297466\n",
      "(Iteration 13301 / 61240) loss: 2.296612\n",
      "(Iteration 13401 / 61240) loss: 2.293851\n",
      "(Iteration 13501 / 61240) loss: 2.293834\n",
      "(Iteration 13601 / 61240) loss: 2.292085\n",
      "(Iteration 13701 / 61240) loss: 2.283467\n",
      "(Epoch 9 / 40) train acc: 0.253000; val_acc: 0.239000\n",
      "(Iteration 13801 / 61240) loss: 2.289549\n",
      "(Iteration 13901 / 61240) loss: 2.299757\n",
      "(Iteration 14001 / 61240) loss: 2.299292\n",
      "(Iteration 14101 / 61240) loss: 2.288070\n",
      "(Iteration 14201 / 61240) loss: 2.296327\n",
      "(Iteration 14301 / 61240) loss: 2.293659\n",
      "(Iteration 14401 / 61240) loss: 2.295981\n",
      "(Iteration 14501 / 61240) loss: 2.294757\n",
      "(Iteration 14601 / 61240) loss: 2.294889\n",
      "(Iteration 14701 / 61240) loss: 2.296963\n",
      "(Iteration 14801 / 61240) loss: 2.285587\n",
      "(Iteration 14901 / 61240) loss: 2.291904\n",
      "(Iteration 15001 / 61240) loss: 2.298163\n",
      "(Iteration 15101 / 61240) loss: 2.296827\n",
      "(Iteration 15201 / 61240) loss: 2.295944\n",
      "(Iteration 15301 / 61240) loss: 2.288705\n",
      "(Epoch 10 / 40) train acc: 0.266000; val_acc: 0.244000\n",
      "(Iteration 15401 / 61240) loss: 2.286970\n",
      "(Iteration 15501 / 61240) loss: 2.289803\n",
      "(Iteration 15601 / 61240) loss: 2.284191\n",
      "(Iteration 15701 / 61240) loss: 2.289685\n",
      "(Iteration 15801 / 61240) loss: 2.298703\n",
      "(Iteration 15901 / 61240) loss: 2.289311\n",
      "(Iteration 16001 / 61240) loss: 2.294585\n",
      "(Iteration 16101 / 61240) loss: 2.289693\n",
      "(Iteration 16201 / 61240) loss: 2.283462\n",
      "(Iteration 16301 / 61240) loss: 2.287732\n",
      "(Iteration 16401 / 61240) loss: 2.278705\n",
      "(Iteration 16501 / 61240) loss: 2.291903\n",
      "(Iteration 16601 / 61240) loss: 2.276811\n",
      "(Iteration 16701 / 61240) loss: 2.289069\n",
      "(Iteration 16801 / 61240) loss: 2.293446\n",
      "(Epoch 11 / 40) train acc: 0.243000; val_acc: 0.253000\n",
      "(Iteration 16901 / 61240) loss: 2.292342\n",
      "(Iteration 17001 / 61240) loss: 2.291205\n",
      "(Iteration 17101 / 61240) loss: 2.283052\n",
      "(Iteration 17201 / 61240) loss: 2.289449\n",
      "(Iteration 17301 / 61240) loss: 2.281980\n",
      "(Iteration 17401 / 61240) loss: 2.301634\n",
      "(Iteration 17501 / 61240) loss: 2.284333\n",
      "(Iteration 17601 / 61240) loss: 2.287926\n",
      "(Iteration 17701 / 61240) loss: 2.280052\n",
      "(Iteration 17801 / 61240) loss: 2.286375\n",
      "(Iteration 17901 / 61240) loss: 2.289748\n",
      "(Iteration 18001 / 61240) loss: 2.287083\n",
      "(Iteration 18101 / 61240) loss: 2.301087\n",
      "(Iteration 18201 / 61240) loss: 2.289121\n",
      "(Iteration 18301 / 61240) loss: 2.263550\n",
      "(Epoch 12 / 40) train acc: 0.265000; val_acc: 0.254000\n",
      "(Iteration 18401 / 61240) loss: 2.288330\n",
      "(Iteration 18501 / 61240) loss: 2.270079\n",
      "(Iteration 18601 / 61240) loss: 2.299013\n",
      "(Iteration 18701 / 61240) loss: 2.278362\n",
      "(Iteration 18801 / 61240) loss: 2.286967\n",
      "(Iteration 18901 / 61240) loss: 2.281196\n",
      "(Iteration 19001 / 61240) loss: 2.288137\n",
      "(Iteration 19101 / 61240) loss: 2.276787\n",
      "(Iteration 19201 / 61240) loss: 2.285238\n",
      "(Iteration 19301 / 61240) loss: 2.281153\n",
      "(Iteration 19401 / 61240) loss: 2.289537\n",
      "(Iteration 19501 / 61240) loss: 2.283349\n",
      "(Iteration 19601 / 61240) loss: 2.277355\n",
      "(Iteration 19701 / 61240) loss: 2.279758\n",
      "(Iteration 19801 / 61240) loss: 2.288164\n",
      "(Iteration 19901 / 61240) loss: 2.289508\n",
      "(Epoch 13 / 40) train acc: 0.253000; val_acc: 0.247000\n",
      "(Iteration 20001 / 61240) loss: 2.276575\n",
      "(Iteration 20101 / 61240) loss: 2.286874\n",
      "(Iteration 20201 / 61240) loss: 2.281259\n",
      "(Iteration 20301 / 61240) loss: 2.287944\n",
      "(Iteration 20401 / 61240) loss: 2.273694\n",
      "(Iteration 20501 / 61240) loss: 2.268594\n",
      "(Iteration 20601 / 61240) loss: 2.269709\n",
      "(Iteration 20701 / 61240) loss: 2.293345\n",
      "(Iteration 20801 / 61240) loss: 2.268820\n",
      "(Iteration 20901 / 61240) loss: 2.276275\n",
      "(Iteration 21001 / 61240) loss: 2.287454\n",
      "(Iteration 21101 / 61240) loss: 2.284249\n",
      "(Iteration 21201 / 61240) loss: 2.250374\n",
      "(Iteration 21301 / 61240) loss: 2.286971\n",
      "(Iteration 21401 / 61240) loss: 2.230859\n",
      "(Epoch 14 / 40) train acc: 0.250000; val_acc: 0.245000\n",
      "(Iteration 21501 / 61240) loss: 2.278974\n",
      "(Iteration 21601 / 61240) loss: 2.282058\n",
      "(Iteration 21701 / 61240) loss: 2.280309\n",
      "(Iteration 21801 / 61240) loss: 2.274924\n",
      "(Iteration 21901 / 61240) loss: 2.233260\n",
      "(Iteration 22001 / 61240) loss: 2.282779\n",
      "(Iteration 22101 / 61240) loss: 2.267726\n",
      "(Iteration 22201 / 61240) loss: 2.283954\n",
      "(Iteration 22301 / 61240) loss: 2.262000\n",
      "(Iteration 22401 / 61240) loss: 2.270146\n",
      "(Iteration 22501 / 61240) loss: 2.252404\n",
      "(Iteration 22601 / 61240) loss: 2.285505\n",
      "(Iteration 22701 / 61240) loss: 2.303814\n",
      "(Iteration 22801 / 61240) loss: 2.275324\n",
      "(Iteration 22901 / 61240) loss: 2.264457\n",
      "(Epoch 15 / 40) train acc: 0.265000; val_acc: 0.246000\n",
      "(Iteration 23001 / 61240) loss: 2.283447\n",
      "(Iteration 23101 / 61240) loss: 2.271474\n",
      "(Iteration 23201 / 61240) loss: 2.252970\n",
      "(Iteration 23301 / 61240) loss: 2.275227\n",
      "(Iteration 23401 / 61240) loss: 2.268449\n",
      "(Iteration 23501 / 61240) loss: 2.240639\n",
      "(Iteration 23601 / 61240) loss: 2.258036\n",
      "(Iteration 23701 / 61240) loss: 2.285122\n",
      "(Iteration 23801 / 61240) loss: 2.279054\n",
      "(Iteration 23901 / 61240) loss: 2.266927\n",
      "(Iteration 24001 / 61240) loss: 2.256445\n",
      "(Iteration 24101 / 61240) loss: 2.274109\n",
      "(Iteration 24201 / 61240) loss: 2.269504\n",
      "(Iteration 24301 / 61240) loss: 2.273536\n",
      "(Iteration 24401 / 61240) loss: 2.222080\n",
      "(Epoch 16 / 40) train acc: 0.260000; val_acc: 0.245000\n",
      "(Iteration 24501 / 61240) loss: 2.264786\n",
      "(Iteration 24601 / 61240) loss: 2.282588\n",
      "(Iteration 24701 / 61240) loss: 2.269080\n",
      "(Iteration 24801 / 61240) loss: 2.266178\n",
      "(Iteration 24901 / 61240) loss: 2.272549\n",
      "(Iteration 25001 / 61240) loss: 2.277154\n",
      "(Iteration 25101 / 61240) loss: 2.253234\n",
      "(Iteration 25201 / 61240) loss: 2.258946\n",
      "(Iteration 25301 / 61240) loss: 2.251967\n",
      "(Iteration 25401 / 61240) loss: 2.289006\n",
      "(Iteration 25501 / 61240) loss: 2.268843\n",
      "(Iteration 25601 / 61240) loss: 2.252360\n",
      "(Iteration 25701 / 61240) loss: 2.256639\n",
      "(Iteration 25801 / 61240) loss: 2.257678\n",
      "(Iteration 25901 / 61240) loss: 2.246368\n",
      "(Iteration 26001 / 61240) loss: 2.277122\n",
      "(Epoch 17 / 40) train acc: 0.228000; val_acc: 0.252000\n",
      "(Iteration 26101 / 61240) loss: 2.271688\n",
      "(Iteration 26201 / 61240) loss: 2.264443\n",
      "(Iteration 26301 / 61240) loss: 2.280983\n",
      "(Iteration 26401 / 61240) loss: 2.249295\n",
      "(Iteration 26501 / 61240) loss: 2.284817\n",
      "(Iteration 26601 / 61240) loss: 2.253735\n",
      "(Iteration 26701 / 61240) loss: 2.275860\n",
      "(Iteration 26801 / 61240) loss: 2.250251\n",
      "(Iteration 26901 / 61240) loss: 2.240467\n",
      "(Iteration 27001 / 61240) loss: 2.232852\n",
      "(Iteration 27101 / 61240) loss: 2.263828\n",
      "(Iteration 27201 / 61240) loss: 2.257195\n",
      "(Iteration 27301 / 61240) loss: 2.265284\n",
      "(Iteration 27401 / 61240) loss: 2.217915\n",
      "(Iteration 27501 / 61240) loss: 2.240616\n",
      "(Epoch 18 / 40) train acc: 0.251000; val_acc: 0.245000\n",
      "(Iteration 27601 / 61240) loss: 2.255218\n",
      "(Iteration 27701 / 61240) loss: 2.268047\n",
      "(Iteration 27801 / 61240) loss: 2.253714\n",
      "(Iteration 27901 / 61240) loss: 2.248351\n",
      "(Iteration 28001 / 61240) loss: 2.216090\n",
      "(Iteration 28101 / 61240) loss: 2.266895\n",
      "(Iteration 28201 / 61240) loss: 2.243005\n",
      "(Iteration 28301 / 61240) loss: 2.268171\n",
      "(Iteration 28401 / 61240) loss: 2.248115\n",
      "(Iteration 28501 / 61240) loss: 2.234254\n",
      "(Iteration 28601 / 61240) loss: 2.269142\n",
      "(Iteration 28701 / 61240) loss: 2.276504\n",
      "(Iteration 28801 / 61240) loss: 2.256924\n",
      "(Iteration 28901 / 61240) loss: 2.254721\n",
      "(Iteration 29001 / 61240) loss: 2.232548\n",
      "(Epoch 19 / 40) train acc: 0.243000; val_acc: 0.247000\n",
      "(Iteration 29101 / 61240) loss: 2.246346\n",
      "(Iteration 29201 / 61240) loss: 2.230717\n",
      "(Iteration 29301 / 61240) loss: 2.273838\n",
      "(Iteration 29401 / 61240) loss: 2.239413\n",
      "(Iteration 29501 / 61240) loss: 2.293273\n",
      "(Iteration 29601 / 61240) loss: 2.209457\n",
      "(Iteration 29701 / 61240) loss: 2.213801\n",
      "(Iteration 29801 / 61240) loss: 2.236318\n",
      "(Iteration 29901 / 61240) loss: 2.263321\n",
      "(Iteration 30001 / 61240) loss: 2.281464\n",
      "(Iteration 30101 / 61240) loss: 2.250633\n",
      "(Iteration 30201 / 61240) loss: 2.261209\n",
      "(Iteration 30301 / 61240) loss: 2.238652\n",
      "(Iteration 30401 / 61240) loss: 2.245677\n",
      "(Iteration 30501 / 61240) loss: 2.219264\n",
      "(Iteration 30601 / 61240) loss: 2.236864\n",
      "(Epoch 20 / 40) train acc: 0.240000; val_acc: 0.248000\n",
      "(Iteration 30701 / 61240) loss: 2.203253\n",
      "(Iteration 30801 / 61240) loss: 2.271209\n",
      "(Iteration 30901 / 61240) loss: 2.265007\n",
      "(Iteration 31001 / 61240) loss: 2.255538\n",
      "(Iteration 31101 / 61240) loss: 2.266978\n",
      "(Iteration 31201 / 61240) loss: 2.244226\n",
      "(Iteration 31301 / 61240) loss: 2.239647\n",
      "(Iteration 31401 / 61240) loss: 2.260048\n",
      "(Iteration 31501 / 61240) loss: 2.268161\n",
      "(Iteration 31601 / 61240) loss: 2.223197\n",
      "(Iteration 31701 / 61240) loss: 2.263005\n",
      "(Iteration 31801 / 61240) loss: 2.212973\n",
      "(Iteration 31901 / 61240) loss: 2.203638\n",
      "(Iteration 32001 / 61240) loss: 2.252711\n",
      "(Iteration 32101 / 61240) loss: 2.204168\n",
      "(Epoch 21 / 40) train acc: 0.263000; val_acc: 0.245000\n",
      "(Iteration 32201 / 61240) loss: 2.242844\n",
      "(Iteration 32301 / 61240) loss: 2.263972\n",
      "(Iteration 32401 / 61240) loss: 2.247269\n",
      "(Iteration 32501 / 61240) loss: 2.237509\n",
      "(Iteration 32601 / 61240) loss: 2.265414\n",
      "(Iteration 32701 / 61240) loss: 2.195639\n",
      "(Iteration 32801 / 61240) loss: 2.194607\n",
      "(Iteration 32901 / 61240) loss: 2.217996\n",
      "(Iteration 33001 / 61240) loss: 2.215008\n",
      "(Iteration 33101 / 61240) loss: 2.208280\n",
      "(Iteration 33201 / 61240) loss: 2.241075\n",
      "(Iteration 33301 / 61240) loss: 2.276986\n",
      "(Iteration 33401 / 61240) loss: 2.232010\n",
      "(Iteration 33501 / 61240) loss: 2.220157\n",
      "(Iteration 33601 / 61240) loss: 2.204905\n",
      "(Epoch 22 / 40) train acc: 0.222000; val_acc: 0.246000\n",
      "(Iteration 33701 / 61240) loss: 2.187740\n",
      "(Iteration 33801 / 61240) loss: 2.231677\n",
      "(Iteration 33901 / 61240) loss: 2.226394\n",
      "(Iteration 34001 / 61240) loss: 2.249900\n",
      "(Iteration 34101 / 61240) loss: 2.264072\n",
      "(Iteration 34201 / 61240) loss: 2.229825\n",
      "(Iteration 34301 / 61240) loss: 2.242463\n",
      "(Iteration 34401 / 61240) loss: 2.228291\n",
      "(Iteration 34501 / 61240) loss: 2.263232\n",
      "(Iteration 34601 / 61240) loss: 2.252031\n",
      "(Iteration 34701 / 61240) loss: 2.249351\n",
      "(Iteration 34801 / 61240) loss: 2.286859\n",
      "(Iteration 34901 / 61240) loss: 2.274796\n",
      "(Iteration 35001 / 61240) loss: 2.273177\n",
      "(Iteration 35101 / 61240) loss: 2.288134\n",
      "(Iteration 35201 / 61240) loss: 2.276264\n",
      "(Epoch 23 / 40) train acc: 0.247000; val_acc: 0.246000\n",
      "(Iteration 35301 / 61240) loss: 2.240658\n",
      "(Iteration 35401 / 61240) loss: 2.240503\n",
      "(Iteration 35501 / 61240) loss: 2.266450\n",
      "(Iteration 35601 / 61240) loss: 2.227827\n",
      "(Iteration 35701 / 61240) loss: 2.222166\n",
      "(Iteration 35801 / 61240) loss: 2.237151\n",
      "(Iteration 35901 / 61240) loss: 2.264419\n",
      "(Iteration 36001 / 61240) loss: 2.306767\n",
      "(Iteration 36101 / 61240) loss: 2.203175\n",
      "(Iteration 36201 / 61240) loss: 2.201992\n",
      "(Iteration 36301 / 61240) loss: 2.274644\n",
      "(Iteration 36401 / 61240) loss: 2.231514\n",
      "(Iteration 36501 / 61240) loss: 2.250986\n",
      "(Iteration 36601 / 61240) loss: 2.264690\n",
      "(Iteration 36701 / 61240) loss: 2.225658\n",
      "(Epoch 24 / 40) train acc: 0.217000; val_acc: 0.244000\n",
      "(Iteration 36801 / 61240) loss: 2.243439\n",
      "(Iteration 36901 / 61240) loss: 2.268843\n",
      "(Iteration 37001 / 61240) loss: 2.262516\n",
      "(Iteration 37101 / 61240) loss: 2.251436\n",
      "(Iteration 37201 / 61240) loss: 2.195134\n",
      "(Iteration 37301 / 61240) loss: 2.271138\n",
      "(Iteration 37401 / 61240) loss: 2.242095\n",
      "(Iteration 37501 / 61240) loss: 2.201163\n",
      "(Iteration 37601 / 61240) loss: 2.147711\n",
      "(Iteration 37701 / 61240) loss: 2.263070\n",
      "(Iteration 37801 / 61240) loss: 2.211206\n",
      "(Iteration 37901 / 61240) loss: 2.186865\n",
      "(Iteration 38001 / 61240) loss: 2.238874\n",
      "(Iteration 38101 / 61240) loss: 2.199496\n",
      "(Iteration 38201 / 61240) loss: 2.221486\n",
      "(Epoch 25 / 40) train acc: 0.211000; val_acc: 0.244000\n",
      "(Iteration 38301 / 61240) loss: 2.242502\n",
      "(Iteration 38401 / 61240) loss: 2.278302\n",
      "(Iteration 38501 / 61240) loss: 2.220700\n",
      "(Iteration 38601 / 61240) loss: 2.250262\n",
      "(Iteration 38701 / 61240) loss: 2.263369\n",
      "(Iteration 38801 / 61240) loss: 2.244828\n",
      "(Iteration 38901 / 61240) loss: 2.253220\n",
      "(Iteration 39001 / 61240) loss: 2.245585\n",
      "(Iteration 39101 / 61240) loss: 2.219278\n",
      "(Iteration 39201 / 61240) loss: 2.198849\n",
      "(Iteration 39301 / 61240) loss: 2.265827\n",
      "(Iteration 39401 / 61240) loss: 2.196327\n",
      "(Iteration 39501 / 61240) loss: 2.268852\n",
      "(Iteration 39601 / 61240) loss: 2.235458\n",
      "(Iteration 39701 / 61240) loss: 2.270794\n",
      "(Iteration 39801 / 61240) loss: 2.206519\n",
      "(Epoch 26 / 40) train acc: 0.214000; val_acc: 0.243000\n",
      "(Iteration 39901 / 61240) loss: 2.265274\n",
      "(Iteration 40001 / 61240) loss: 2.281168\n",
      "(Iteration 40101 / 61240) loss: 2.167841\n",
      "(Iteration 40201 / 61240) loss: 2.272700\n",
      "(Iteration 40301 / 61240) loss: 2.231354\n",
      "(Iteration 40401 / 61240) loss: 2.214077\n",
      "(Iteration 40501 / 61240) loss: 2.258411\n",
      "(Iteration 40601 / 61240) loss: 2.225639\n",
      "(Iteration 40701 / 61240) loss: 2.141935\n",
      "(Iteration 40801 / 61240) loss: 2.242398\n",
      "(Iteration 40901 / 61240) loss: 2.142898\n",
      "(Iteration 41001 / 61240) loss: 2.225206\n",
      "(Iteration 41101 / 61240) loss: 2.277107\n",
      "(Iteration 41201 / 61240) loss: 2.268972\n",
      "(Iteration 41301 / 61240) loss: 2.286195\n",
      "(Epoch 27 / 40) train acc: 0.249000; val_acc: 0.242000\n",
      "(Iteration 41401 / 61240) loss: 2.175027\n",
      "(Iteration 41501 / 61240) loss: 2.164927\n",
      "(Iteration 41601 / 61240) loss: 2.261068\n",
      "(Iteration 41701 / 61240) loss: 2.276165\n",
      "(Iteration 41801 / 61240) loss: 2.279092\n",
      "(Iteration 41901 / 61240) loss: 2.247050\n",
      "(Iteration 42001 / 61240) loss: 2.269547\n",
      "(Iteration 42101 / 61240) loss: 2.290381\n",
      "(Iteration 42201 / 61240) loss: 2.239303\n",
      "(Iteration 42301 / 61240) loss: 2.170467\n",
      "(Iteration 42401 / 61240) loss: 2.181656\n",
      "(Iteration 42501 / 61240) loss: 2.227154\n",
      "(Iteration 42601 / 61240) loss: 2.252751\n",
      "(Iteration 42701 / 61240) loss: 2.183685\n",
      "(Iteration 42801 / 61240) loss: 2.215536\n",
      "(Epoch 28 / 40) train acc: 0.226000; val_acc: 0.240000\n",
      "(Iteration 42901 / 61240) loss: 2.241821\n",
      "(Iteration 43001 / 61240) loss: 2.235560\n",
      "(Iteration 43101 / 61240) loss: 2.228249\n",
      "(Iteration 43201 / 61240) loss: 2.213076\n",
      "(Iteration 43301 / 61240) loss: 2.194354\n",
      "(Iteration 43401 / 61240) loss: 2.254927\n",
      "(Iteration 43501 / 61240) loss: 2.218904\n",
      "(Iteration 43601 / 61240) loss: 2.235249\n",
      "(Iteration 43701 / 61240) loss: 2.198664\n",
      "(Iteration 43801 / 61240) loss: 2.229559\n",
      "(Iteration 43901 / 61240) loss: 2.231988\n",
      "(Iteration 44001 / 61240) loss: 2.214164\n",
      "(Iteration 44101 / 61240) loss: 2.227090\n",
      "(Iteration 44201 / 61240) loss: 2.263936\n",
      "(Iteration 44301 / 61240) loss: 2.254920\n",
      "(Epoch 29 / 40) train acc: 0.229000; val_acc: 0.241000\n",
      "(Iteration 44401 / 61240) loss: 2.224126\n",
      "(Iteration 44501 / 61240) loss: 2.297694\n",
      "(Iteration 44601 / 61240) loss: 2.156990\n",
      "(Iteration 44701 / 61240) loss: 2.265651\n",
      "(Iteration 44801 / 61240) loss: 2.263192\n",
      "(Iteration 44901 / 61240) loss: 2.291911\n",
      "(Iteration 45001 / 61240) loss: 2.271925\n",
      "(Iteration 45101 / 61240) loss: 2.239333\n",
      "(Iteration 45201 / 61240) loss: 2.191824\n",
      "(Iteration 45301 / 61240) loss: 2.188060\n",
      "(Iteration 45401 / 61240) loss: 2.244482\n",
      "(Iteration 45501 / 61240) loss: 2.185771\n",
      "(Iteration 45601 / 61240) loss: 2.214860\n",
      "(Iteration 45701 / 61240) loss: 2.270012\n",
      "(Iteration 45801 / 61240) loss: 2.241451\n",
      "(Iteration 45901 / 61240) loss: 2.205519\n",
      "(Epoch 30 / 40) train acc: 0.238000; val_acc: 0.242000\n",
      "(Iteration 46001 / 61240) loss: 2.301997\n",
      "(Iteration 46101 / 61240) loss: 2.267532\n",
      "(Iteration 46201 / 61240) loss: 2.199387\n",
      "(Iteration 46301 / 61240) loss: 2.153674\n",
      "(Iteration 46401 / 61240) loss: 2.238713\n",
      "(Iteration 46501 / 61240) loss: 2.245631\n",
      "(Iteration 46601 / 61240) loss: 2.233856\n",
      "(Iteration 46701 / 61240) loss: 2.165329\n",
      "(Iteration 46801 / 61240) loss: 2.194496\n",
      "(Iteration 46901 / 61240) loss: 2.138084\n",
      "(Iteration 47001 / 61240) loss: 2.231800\n",
      "(Iteration 47101 / 61240) loss: 2.253583\n",
      "(Iteration 47201 / 61240) loss: 2.298025\n",
      "(Iteration 47301 / 61240) loss: 2.262447\n",
      "(Iteration 47401 / 61240) loss: 2.179558\n",
      "(Epoch 31 / 40) train acc: 0.244000; val_acc: 0.241000\n",
      "(Iteration 47501 / 61240) loss: 2.225227\n",
      "(Iteration 47601 / 61240) loss: 2.225427\n",
      "(Iteration 47701 / 61240) loss: 2.268014\n",
      "(Iteration 47801 / 61240) loss: 2.190933\n",
      "(Iteration 47901 / 61240) loss: 2.215746\n",
      "(Iteration 48001 / 61240) loss: 2.173328\n",
      "(Iteration 48101 / 61240) loss: 2.213753\n",
      "(Iteration 48201 / 61240) loss: 2.219586\n",
      "(Iteration 48301 / 61240) loss: 2.244156\n",
      "(Iteration 48401 / 61240) loss: 2.274786\n",
      "(Iteration 48501 / 61240) loss: 2.276344\n",
      "(Iteration 48601 / 61240) loss: 2.184984\n",
      "(Iteration 48701 / 61240) loss: 2.153782\n",
      "(Iteration 48801 / 61240) loss: 2.254455\n",
      "(Iteration 48901 / 61240) loss: 2.225293\n",
      "(Epoch 32 / 40) train acc: 0.245000; val_acc: 0.242000\n",
      "(Iteration 49001 / 61240) loss: 2.242030\n",
      "(Iteration 49101 / 61240) loss: 2.273628\n",
      "(Iteration 49201 / 61240) loss: 2.203549\n",
      "(Iteration 49301 / 61240) loss: 2.246494\n",
      "(Iteration 49401 / 61240) loss: 2.140142\n",
      "(Iteration 49501 / 61240) loss: 2.234383\n",
      "(Iteration 49601 / 61240) loss: 2.161101\n",
      "(Iteration 49701 / 61240) loss: 2.221772\n",
      "(Iteration 49801 / 61240) loss: 2.169380\n",
      "(Iteration 49901 / 61240) loss: 2.205144\n",
      "(Iteration 50001 / 61240) loss: 2.208199\n",
      "(Iteration 50101 / 61240) loss: 2.211176\n",
      "(Iteration 50201 / 61240) loss: 2.254328\n",
      "(Iteration 50301 / 61240) loss: 2.219104\n",
      "(Iteration 50401 / 61240) loss: 2.227264\n",
      "(Iteration 50501 / 61240) loss: 2.175812\n",
      "(Epoch 33 / 40) train acc: 0.243000; val_acc: 0.242000\n",
      "(Iteration 50601 / 61240) loss: 2.269420\n",
      "(Iteration 50701 / 61240) loss: 2.259421\n",
      "(Iteration 50801 / 61240) loss: 2.183553\n",
      "(Iteration 50901 / 61240) loss: 2.171901\n",
      "(Iteration 51001 / 61240) loss: 2.277041\n",
      "(Iteration 51101 / 61240) loss: 2.243052\n",
      "(Iteration 51201 / 61240) loss: 2.270900\n",
      "(Iteration 51301 / 61240) loss: 2.301657\n",
      "(Iteration 51401 / 61240) loss: 2.221044\n",
      "(Iteration 51501 / 61240) loss: 2.247267\n",
      "(Iteration 51601 / 61240) loss: 2.246608\n",
      "(Iteration 51701 / 61240) loss: 2.203114\n",
      "(Iteration 51801 / 61240) loss: 2.207140\n",
      "(Iteration 51901 / 61240) loss: 2.239882\n",
      "(Iteration 52001 / 61240) loss: 2.267565\n",
      "(Epoch 34 / 40) train acc: 0.222000; val_acc: 0.242000\n",
      "(Iteration 52101 / 61240) loss: 2.215131\n",
      "(Iteration 52201 / 61240) loss: 2.218641\n",
      "(Iteration 52301 / 61240) loss: 2.203252\n",
      "(Iteration 52401 / 61240) loss: 2.152881\n",
      "(Iteration 52501 / 61240) loss: 2.241636\n",
      "(Iteration 52601 / 61240) loss: 2.225491\n",
      "(Iteration 52701 / 61240) loss: 2.272774\n",
      "(Iteration 52801 / 61240) loss: 2.209683\n",
      "(Iteration 52901 / 61240) loss: 2.200815\n",
      "(Iteration 53001 / 61240) loss: 2.170268\n",
      "(Iteration 53101 / 61240) loss: 2.278419\n",
      "(Iteration 53201 / 61240) loss: 2.149060\n",
      "(Iteration 53301 / 61240) loss: 2.243737\n",
      "(Iteration 53401 / 61240) loss: 2.213277\n",
      "(Iteration 53501 / 61240) loss: 2.179043\n",
      "(Epoch 35 / 40) train acc: 0.247000; val_acc: 0.242000\n",
      "(Iteration 53601 / 61240) loss: 2.249532\n",
      "(Iteration 53701 / 61240) loss: 2.186658\n",
      "(Iteration 53801 / 61240) loss: 2.247927\n",
      "(Iteration 53901 / 61240) loss: 2.142403\n",
      "(Iteration 54001 / 61240) loss: 2.161704\n",
      "(Iteration 54101 / 61240) loss: 2.220663\n",
      "(Iteration 54201 / 61240) loss: 2.253967\n",
      "(Iteration 54301 / 61240) loss: 2.223679\n",
      "(Iteration 54401 / 61240) loss: 2.206314\n",
      "(Iteration 54501 / 61240) loss: 2.214908\n",
      "(Iteration 54601 / 61240) loss: 2.266448\n",
      "(Iteration 54701 / 61240) loss: 2.230060\n",
      "(Iteration 54801 / 61240) loss: 2.184850\n",
      "(Iteration 54901 / 61240) loss: 2.219693\n",
      "(Iteration 55001 / 61240) loss: 2.200617\n",
      "(Iteration 55101 / 61240) loss: 2.259042\n",
      "(Epoch 36 / 40) train acc: 0.205000; val_acc: 0.242000\n",
      "(Iteration 55201 / 61240) loss: 2.187518\n",
      "(Iteration 55301 / 61240) loss: 2.216739\n",
      "(Iteration 55401 / 61240) loss: 2.249610\n",
      "(Iteration 55501 / 61240) loss: 2.197792\n",
      "(Iteration 55601 / 61240) loss: 2.207819\n",
      "(Iteration 55701 / 61240) loss: 2.267725\n",
      "(Iteration 55801 / 61240) loss: 2.183888\n",
      "(Iteration 55901 / 61240) loss: 2.269774\n",
      "(Iteration 56001 / 61240) loss: 2.233534\n",
      "(Iteration 56101 / 61240) loss: 2.190567\n",
      "(Iteration 56201 / 61240) loss: 2.167312\n",
      "(Iteration 56301 / 61240) loss: 2.167510\n",
      "(Iteration 56401 / 61240) loss: 2.251047\n",
      "(Iteration 56501 / 61240) loss: 2.228869\n",
      "(Iteration 56601 / 61240) loss: 2.205976\n",
      "(Epoch 37 / 40) train acc: 0.247000; val_acc: 0.242000\n",
      "(Iteration 56701 / 61240) loss: 2.230007\n",
      "(Iteration 56801 / 61240) loss: 2.244535\n",
      "(Iteration 56901 / 61240) loss: 2.120429\n",
      "(Iteration 57001 / 61240) loss: 2.118789\n",
      "(Iteration 57101 / 61240) loss: 2.221893\n",
      "(Iteration 57201 / 61240) loss: 2.241200\n",
      "(Iteration 57301 / 61240) loss: 2.161934\n",
      "(Iteration 57401 / 61240) loss: 2.169366\n",
      "(Iteration 57501 / 61240) loss: 2.235249\n",
      "(Iteration 57601 / 61240) loss: 2.163817\n",
      "(Iteration 57701 / 61240) loss: 2.221888\n",
      "(Iteration 57801 / 61240) loss: 2.249970\n",
      "(Iteration 57901 / 61240) loss: 2.209313\n",
      "(Iteration 58001 / 61240) loss: 2.220500\n",
      "(Iteration 58101 / 61240) loss: 2.246326\n",
      "(Epoch 38 / 40) train acc: 0.231000; val_acc: 0.241000\n",
      "(Iteration 58201 / 61240) loss: 2.217846\n",
      "(Iteration 58301 / 61240) loss: 2.148419\n",
      "(Iteration 58401 / 61240) loss: 2.187499\n",
      "(Iteration 58501 / 61240) loss: 2.220046\n",
      "(Iteration 58601 / 61240) loss: 2.242105\n",
      "(Iteration 58701 / 61240) loss: 2.225955\n",
      "(Iteration 58801 / 61240) loss: 2.241694\n",
      "(Iteration 58901 / 61240) loss: 2.262351\n",
      "(Iteration 59001 / 61240) loss: 2.257522\n",
      "(Iteration 59101 / 61240) loss: 2.078091\n",
      "(Iteration 59201 / 61240) loss: 2.247141\n",
      "(Iteration 59301 / 61240) loss: 2.220414\n",
      "(Iteration 59401 / 61240) loss: 2.161115\n",
      "(Iteration 59501 / 61240) loss: 2.191487\n",
      "(Iteration 59601 / 61240) loss: 2.222305\n",
      "(Iteration 59701 / 61240) loss: 2.251987\n",
      "(Epoch 39 / 40) train acc: 0.259000; val_acc: 0.241000\n",
      "(Iteration 59801 / 61240) loss: 2.204536\n",
      "(Iteration 59901 / 61240) loss: 2.315347\n",
      "(Iteration 60001 / 61240) loss: 2.160194\n",
      "(Iteration 60101 / 61240) loss: 2.278125\n",
      "(Iteration 60201 / 61240) loss: 2.207941\n",
      "(Iteration 60301 / 61240) loss: 2.231602\n",
      "(Iteration 60401 / 61240) loss: 2.170961\n",
      "(Iteration 60501 / 61240) loss: 2.263783\n",
      "(Iteration 60601 / 61240) loss: 2.206371\n",
      "(Iteration 60701 / 61240) loss: 2.145016\n",
      "(Iteration 60801 / 61240) loss: 2.172356\n",
      "(Iteration 60901 / 61240) loss: 2.229513\n",
      "(Iteration 61001 / 61240) loss: 2.294576\n",
      "(Iteration 61101 / 61240) loss: 2.194401\n",
      "(Iteration 61201 / 61240) loss: 2.198082\n",
      "(Epoch 40 / 40) train acc: 0.258000; val_acc: 0.241000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.001, 'num_epochs': 40, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 30600) loss: 2.303256\n",
      "(Epoch 0 / 40) train acc: 0.110000; val_acc: 0.101000\n",
      "(Iteration 101 / 30600) loss: 2.303138\n",
      "(Iteration 201 / 30600) loss: 2.303273\n",
      "(Iteration 301 / 30600) loss: 2.303142\n",
      "(Iteration 401 / 30600) loss: 2.302901\n",
      "(Iteration 501 / 30600) loss: 2.302907\n",
      "(Iteration 601 / 30600) loss: 2.302979\n",
      "(Iteration 701 / 30600) loss: 2.302868\n",
      "(Epoch 1 / 40) train acc: 0.090000; val_acc: 0.114000\n",
      "(Iteration 801 / 30600) loss: 2.303160\n",
      "(Iteration 901 / 30600) loss: 2.302986\n",
      "(Iteration 1001 / 30600) loss: 2.302726\n",
      "(Iteration 1101 / 30600) loss: 2.303051\n",
      "(Iteration 1201 / 30600) loss: 2.302798\n",
      "(Iteration 1301 / 30600) loss: 2.303187\n",
      "(Iteration 1401 / 30600) loss: 2.302668\n",
      "(Iteration 1501 / 30600) loss: 2.302906\n",
      "(Epoch 2 / 40) train acc: 0.098000; val_acc: 0.115000\n",
      "(Iteration 1601 / 30600) loss: 2.302922\n",
      "(Iteration 1701 / 30600) loss: 2.302894\n",
      "(Iteration 1801 / 30600) loss: 2.302659\n",
      "(Iteration 1901 / 30600) loss: 2.302428\n",
      "(Iteration 2001 / 30600) loss: 2.302389\n",
      "(Iteration 2101 / 30600) loss: 2.302528\n",
      "(Iteration 2201 / 30600) loss: 2.302102\n",
      "(Epoch 3 / 40) train acc: 0.180000; val_acc: 0.169000\n",
      "(Iteration 2301 / 30600) loss: 2.302403\n",
      "(Iteration 2401 / 30600) loss: 2.302284\n",
      "(Iteration 2501 / 30600) loss: 2.302421\n",
      "(Iteration 2601 / 30600) loss: 2.302182\n",
      "(Iteration 2701 / 30600) loss: 2.302220\n",
      "(Iteration 2801 / 30600) loss: 2.302321\n",
      "(Iteration 2901 / 30600) loss: 2.301887\n",
      "(Iteration 3001 / 30600) loss: 2.302131\n",
      "(Epoch 4 / 40) train acc: 0.243000; val_acc: 0.240000\n",
      "(Iteration 3101 / 30600) loss: 2.301976\n",
      "(Iteration 3201 / 30600) loss: 2.302037\n",
      "(Iteration 3301 / 30600) loss: 2.301973\n",
      "(Iteration 3401 / 30600) loss: 2.301933\n",
      "(Iteration 3501 / 30600) loss: 2.301802\n",
      "(Iteration 3601 / 30600) loss: 2.301813\n",
      "(Iteration 3701 / 30600) loss: 2.301231\n",
      "(Iteration 3801 / 30600) loss: 2.301602\n",
      "(Epoch 5 / 40) train acc: 0.234000; val_acc: 0.237000\n",
      "(Iteration 3901 / 30600) loss: 2.302192\n",
      "(Iteration 4001 / 30600) loss: 2.301736\n",
      "(Iteration 4101 / 30600) loss: 2.301772\n",
      "(Iteration 4201 / 30600) loss: 2.301714\n",
      "(Iteration 4301 / 30600) loss: 2.301401\n",
      "(Iteration 4401 / 30600) loss: 2.301561\n",
      "(Iteration 4501 / 30600) loss: 2.301320\n",
      "(Epoch 6 / 40) train acc: 0.240000; val_acc: 0.231000\n",
      "(Iteration 4601 / 30600) loss: 2.301436\n",
      "(Iteration 4701 / 30600) loss: 2.301617\n",
      "(Iteration 4801 / 30600) loss: 2.300863\n",
      "(Iteration 4901 / 30600) loss: 2.300118\n",
      "(Iteration 5001 / 30600) loss: 2.301097\n",
      "(Iteration 5101 / 30600) loss: 2.301256\n",
      "(Iteration 5201 / 30600) loss: 2.299895\n",
      "(Iteration 5301 / 30600) loss: 2.301158\n",
      "(Epoch 7 / 40) train acc: 0.231000; val_acc: 0.221000\n",
      "(Iteration 5401 / 30600) loss: 2.300727\n",
      "(Iteration 5501 / 30600) loss: 2.300083\n",
      "(Iteration 5601 / 30600) loss: 2.300722\n",
      "(Iteration 5701 / 30600) loss: 2.300157\n",
      "(Iteration 5801 / 30600) loss: 2.300966\n",
      "(Iteration 5901 / 30600) loss: 2.300205\n",
      "(Iteration 6001 / 30600) loss: 2.300465\n",
      "(Iteration 6101 / 30600) loss: 2.300265\n",
      "(Epoch 8 / 40) train acc: 0.290000; val_acc: 0.257000\n",
      "(Iteration 6201 / 30600) loss: 2.300703\n",
      "(Iteration 6301 / 30600) loss: 2.299677\n",
      "(Iteration 6401 / 30600) loss: 2.299555\n",
      "(Iteration 6501 / 30600) loss: 2.301094\n",
      "(Iteration 6601 / 30600) loss: 2.300296\n",
      "(Iteration 6701 / 30600) loss: 2.299425\n",
      "(Iteration 6801 / 30600) loss: 2.299736\n",
      "(Epoch 9 / 40) train acc: 0.284000; val_acc: 0.277000\n",
      "(Iteration 6901 / 30600) loss: 2.299610\n",
      "(Iteration 7001 / 30600) loss: 2.298995\n",
      "(Iteration 7101 / 30600) loss: 2.298176\n",
      "(Iteration 7201 / 30600) loss: 2.299478\n",
      "(Iteration 7301 / 30600) loss: 2.298598\n",
      "(Iteration 7401 / 30600) loss: 2.298796\n",
      "(Iteration 7501 / 30600) loss: 2.298811\n",
      "(Iteration 7601 / 30600) loss: 2.299201\n",
      "(Epoch 10 / 40) train acc: 0.277000; val_acc: 0.263000\n",
      "(Iteration 7701 / 30600) loss: 2.299170\n",
      "(Iteration 7801 / 30600) loss: 2.298077\n",
      "(Iteration 7901 / 30600) loss: 2.298746\n",
      "(Iteration 8001 / 30600) loss: 2.298246\n",
      "(Iteration 8101 / 30600) loss: 2.298862\n",
      "(Iteration 8201 / 30600) loss: 2.298989\n",
      "(Iteration 8301 / 30600) loss: 2.299071\n",
      "(Iteration 8401 / 30600) loss: 2.297077\n",
      "(Epoch 11 / 40) train acc: 0.301000; val_acc: 0.267000\n",
      "(Iteration 8501 / 30600) loss: 2.298904\n",
      "(Iteration 8601 / 30600) loss: 2.297531\n",
      "(Iteration 8701 / 30600) loss: 2.299726\n",
      "(Iteration 8801 / 30600) loss: 2.298250\n",
      "(Iteration 8901 / 30600) loss: 2.297985\n",
      "(Iteration 9001 / 30600) loss: 2.297871\n",
      "(Iteration 9101 / 30600) loss: 2.296820\n",
      "(Epoch 12 / 40) train acc: 0.283000; val_acc: 0.275000\n",
      "(Iteration 9201 / 30600) loss: 2.297467\n",
      "(Iteration 9301 / 30600) loss: 2.297312\n",
      "(Iteration 9401 / 30600) loss: 2.294455\n",
      "(Iteration 9501 / 30600) loss: 2.297947\n",
      "(Iteration 9601 / 30600) loss: 2.296153\n",
      "(Iteration 9701 / 30600) loss: 2.296391\n",
      "(Iteration 9801 / 30600) loss: 2.298503\n",
      "(Iteration 9901 / 30600) loss: 2.295336\n",
      "(Epoch 13 / 40) train acc: 0.260000; val_acc: 0.283000\n",
      "(Iteration 10001 / 30600) loss: 2.297171\n",
      "(Iteration 10101 / 30600) loss: 2.297971\n",
      "(Iteration 10201 / 30600) loss: 2.297604\n",
      "(Iteration 10301 / 30600) loss: 2.296565\n",
      "(Iteration 10401 / 30600) loss: 2.296617\n",
      "(Iteration 10501 / 30600) loss: 2.298520\n",
      "(Iteration 10601 / 30600) loss: 2.296924\n",
      "(Iteration 10701 / 30600) loss: 2.295790\n",
      "(Epoch 14 / 40) train acc: 0.307000; val_acc: 0.307000\n",
      "(Iteration 10801 / 30600) loss: 2.295631\n",
      "(Iteration 10901 / 30600) loss: 2.293735\n",
      "(Iteration 11001 / 30600) loss: 2.293570\n",
      "(Iteration 11101 / 30600) loss: 2.296617\n",
      "(Iteration 11201 / 30600) loss: 2.294941\n",
      "(Iteration 11301 / 30600) loss: 2.293350\n",
      "(Iteration 11401 / 30600) loss: 2.294241\n",
      "(Epoch 15 / 40) train acc: 0.305000; val_acc: 0.304000\n",
      "(Iteration 11501 / 30600) loss: 2.295696\n",
      "(Iteration 11601 / 30600) loss: 2.296241\n",
      "(Iteration 11701 / 30600) loss: 2.293917\n",
      "(Iteration 11801 / 30600) loss: 2.294841\n",
      "(Iteration 11901 / 30600) loss: 2.292458\n",
      "(Iteration 12001 / 30600) loss: 2.294819\n",
      "(Iteration 12101 / 30600) loss: 2.294585\n",
      "(Iteration 12201 / 30600) loss: 2.289650\n",
      "(Epoch 16 / 40) train acc: 0.320000; val_acc: 0.305000\n",
      "(Iteration 12301 / 30600) loss: 2.295569\n",
      "(Iteration 12401 / 30600) loss: 2.293704\n",
      "(Iteration 12501 / 30600) loss: 2.293868\n",
      "(Iteration 12601 / 30600) loss: 2.292418\n",
      "(Iteration 12701 / 30600) loss: 2.294236\n",
      "(Iteration 12801 / 30600) loss: 2.291433\n",
      "(Iteration 12901 / 30600) loss: 2.288681\n",
      "(Iteration 13001 / 30600) loss: 2.293752\n",
      "(Epoch 17 / 40) train acc: 0.280000; val_acc: 0.301000\n",
      "(Iteration 13101 / 30600) loss: 2.294577\n",
      "(Iteration 13201 / 30600) loss: 2.288884\n",
      "(Iteration 13301 / 30600) loss: 2.292896\n",
      "(Iteration 13401 / 30600) loss: 2.291014\n",
      "(Iteration 13501 / 30600) loss: 2.294414\n",
      "(Iteration 13601 / 30600) loss: 2.292616\n",
      "(Iteration 13701 / 30600) loss: 2.293176\n",
      "(Epoch 18 / 40) train acc: 0.303000; val_acc: 0.299000\n",
      "(Iteration 13801 / 30600) loss: 2.293363\n",
      "(Iteration 13901 / 30600) loss: 2.292870\n",
      "(Iteration 14001 / 30600) loss: 2.286944\n",
      "(Iteration 14101 / 30600) loss: 2.292909\n",
      "(Iteration 14201 / 30600) loss: 2.292702\n",
      "(Iteration 14301 / 30600) loss: 2.291847\n",
      "(Iteration 14401 / 30600) loss: 2.293318\n",
      "(Iteration 14501 / 30600) loss: 2.287989\n",
      "(Epoch 19 / 40) train acc: 0.273000; val_acc: 0.287000\n",
      "(Iteration 14601 / 30600) loss: 2.286239\n",
      "(Iteration 14701 / 30600) loss: 2.291344\n",
      "(Iteration 14801 / 30600) loss: 2.291471\n",
      "(Iteration 14901 / 30600) loss: 2.291033\n",
      "(Iteration 15001 / 30600) loss: 2.288711\n",
      "(Iteration 15101 / 30600) loss: 2.294750\n",
      "(Iteration 15201 / 30600) loss: 2.292486\n",
      "(Epoch 20 / 40) train acc: 0.306000; val_acc: 0.293000\n",
      "(Iteration 15301 / 30600) loss: 2.291763\n",
      "(Iteration 15401 / 30600) loss: 2.289745\n",
      "(Iteration 15501 / 30600) loss: 2.292068\n",
      "(Iteration 15601 / 30600) loss: 2.293696\n",
      "(Iteration 15701 / 30600) loss: 2.290457\n",
      "(Iteration 15801 / 30600) loss: 2.291338\n",
      "(Iteration 15901 / 30600) loss: 2.290534\n",
      "(Iteration 16001 / 30600) loss: 2.294764\n",
      "(Epoch 21 / 40) train acc: 0.331000; val_acc: 0.293000\n",
      "(Iteration 16101 / 30600) loss: 2.291739\n",
      "(Iteration 16201 / 30600) loss: 2.293164\n",
      "(Iteration 16301 / 30600) loss: 2.289737\n",
      "(Iteration 16401 / 30600) loss: 2.289643\n",
      "(Iteration 16501 / 30600) loss: 2.289046\n",
      "(Iteration 16601 / 30600) loss: 2.286151\n",
      "(Iteration 16701 / 30600) loss: 2.291881\n",
      "(Iteration 16801 / 30600) loss: 2.289977\n",
      "(Epoch 22 / 40) train acc: 0.297000; val_acc: 0.291000\n",
      "(Iteration 16901 / 30600) loss: 2.290603\n",
      "(Iteration 17001 / 30600) loss: 2.286986\n",
      "(Iteration 17101 / 30600) loss: 2.286412\n",
      "(Iteration 17201 / 30600) loss: 2.282202\n",
      "(Iteration 17301 / 30600) loss: 2.290720\n",
      "(Iteration 17401 / 30600) loss: 2.285510\n",
      "(Iteration 17501 / 30600) loss: 2.290004\n",
      "(Epoch 23 / 40) train acc: 0.314000; val_acc: 0.286000\n",
      "(Iteration 17601 / 30600) loss: 2.287713\n",
      "(Iteration 17701 / 30600) loss: 2.285691\n",
      "(Iteration 17801 / 30600) loss: 2.290014\n",
      "(Iteration 17901 / 30600) loss: 2.289487\n",
      "(Iteration 18001 / 30600) loss: 2.290763\n",
      "(Iteration 18101 / 30600) loss: 2.291239\n",
      "(Iteration 18201 / 30600) loss: 2.285503\n",
      "(Iteration 18301 / 30600) loss: 2.286106\n",
      "(Epoch 24 / 40) train acc: 0.293000; val_acc: 0.293000\n",
      "(Iteration 18401 / 30600) loss: 2.293137\n",
      "(Iteration 18501 / 30600) loss: 2.288955\n",
      "(Iteration 18601 / 30600) loss: 2.286695\n",
      "(Iteration 18701 / 30600) loss: 2.282154\n",
      "(Iteration 18801 / 30600) loss: 2.283965\n",
      "(Iteration 18901 / 30600) loss: 2.286053\n",
      "(Iteration 19001 / 30600) loss: 2.290060\n",
      "(Iteration 19101 / 30600) loss: 2.289214\n",
      "(Epoch 25 / 40) train acc: 0.290000; val_acc: 0.284000\n",
      "(Iteration 19201 / 30600) loss: 2.283147\n",
      "(Iteration 19301 / 30600) loss: 2.282944\n",
      "(Iteration 19401 / 30600) loss: 2.284791\n",
      "(Iteration 19501 / 30600) loss: 2.286050\n",
      "(Iteration 19601 / 30600) loss: 2.281173\n",
      "(Iteration 19701 / 30600) loss: 2.285750\n",
      "(Iteration 19801 / 30600) loss: 2.288088\n",
      "(Epoch 26 / 40) train acc: 0.307000; val_acc: 0.285000\n",
      "(Iteration 19901 / 30600) loss: 2.283162\n",
      "(Iteration 20001 / 30600) loss: 2.283964\n",
      "(Iteration 20101 / 30600) loss: 2.284836\n",
      "(Iteration 20201 / 30600) loss: 2.286570\n",
      "(Iteration 20301 / 30600) loss: 2.287208\n",
      "(Iteration 20401 / 30600) loss: 2.284959\n",
      "(Iteration 20501 / 30600) loss: 2.285829\n",
      "(Iteration 20601 / 30600) loss: 2.285507\n",
      "(Epoch 27 / 40) train acc: 0.303000; val_acc: 0.287000\n",
      "(Iteration 20701 / 30600) loss: 2.284661\n",
      "(Iteration 20801 / 30600) loss: 2.284388\n",
      "(Iteration 20901 / 30600) loss: 2.287607\n",
      "(Iteration 21001 / 30600) loss: 2.280971\n",
      "(Iteration 21101 / 30600) loss: 2.287590\n",
      "(Iteration 21201 / 30600) loss: 2.282937\n",
      "(Iteration 21301 / 30600) loss: 2.284674\n",
      "(Iteration 21401 / 30600) loss: 2.281313\n",
      "(Epoch 28 / 40) train acc: 0.327000; val_acc: 0.287000\n",
      "(Iteration 21501 / 30600) loss: 2.281448\n",
      "(Iteration 21601 / 30600) loss: 2.276045\n",
      "(Iteration 21701 / 30600) loss: 2.291565\n",
      "(Iteration 21801 / 30600) loss: 2.281628\n",
      "(Iteration 21901 / 30600) loss: 2.285734\n",
      "(Iteration 22001 / 30600) loss: 2.290221\n",
      "(Iteration 22101 / 30600) loss: 2.291723\n",
      "(Epoch 29 / 40) train acc: 0.293000; val_acc: 0.286000\n",
      "(Iteration 22201 / 30600) loss: 2.280344\n",
      "(Iteration 22301 / 30600) loss: 2.290130\n",
      "(Iteration 22401 / 30600) loss: 2.284912\n",
      "(Iteration 22501 / 30600) loss: 2.278598\n",
      "(Iteration 22601 / 30600) loss: 2.282230\n",
      "(Iteration 22701 / 30600) loss: 2.290186\n",
      "(Iteration 22801 / 30600) loss: 2.291942\n",
      "(Iteration 22901 / 30600) loss: 2.286300\n",
      "(Epoch 30 / 40) train acc: 0.292000; val_acc: 0.284000\n",
      "(Iteration 23001 / 30600) loss: 2.282313\n",
      "(Iteration 23101 / 30600) loss: 2.282925\n",
      "(Iteration 23201 / 30600) loss: 2.280538\n",
      "(Iteration 23301 / 30600) loss: 2.283006\n",
      "(Iteration 23401 / 30600) loss: 2.287373\n",
      "(Iteration 23501 / 30600) loss: 2.271481\n",
      "(Iteration 23601 / 30600) loss: 2.284187\n",
      "(Iteration 23701 / 30600) loss: 2.288566\n",
      "(Epoch 31 / 40) train acc: 0.309000; val_acc: 0.287000\n",
      "(Iteration 23801 / 30600) loss: 2.287107\n",
      "(Iteration 23901 / 30600) loss: 2.278492\n",
      "(Iteration 24001 / 30600) loss: 2.280177\n",
      "(Iteration 24101 / 30600) loss: 2.287306\n",
      "(Iteration 24201 / 30600) loss: 2.276763\n",
      "(Iteration 24301 / 30600) loss: 2.287643\n",
      "(Iteration 24401 / 30600) loss: 2.284988\n",
      "(Epoch 32 / 40) train acc: 0.288000; val_acc: 0.286000\n",
      "(Iteration 24501 / 30600) loss: 2.281733\n",
      "(Iteration 24601 / 30600) loss: 2.283427\n",
      "(Iteration 24701 / 30600) loss: 2.278571\n",
      "(Iteration 24801 / 30600) loss: 2.287261\n",
      "(Iteration 24901 / 30600) loss: 2.284187\n",
      "(Iteration 25001 / 30600) loss: 2.278760\n",
      "(Iteration 25101 / 30600) loss: 2.286359\n",
      "(Iteration 25201 / 30600) loss: 2.285268\n",
      "(Epoch 33 / 40) train acc: 0.287000; val_acc: 0.286000\n",
      "(Iteration 25301 / 30600) loss: 2.280167\n",
      "(Iteration 25401 / 30600) loss: 2.284422\n",
      "(Iteration 25501 / 30600) loss: 2.279578\n",
      "(Iteration 25601 / 30600) loss: 2.280708\n",
      "(Iteration 25701 / 30600) loss: 2.281812\n",
      "(Iteration 25801 / 30600) loss: 2.280461\n",
      "(Iteration 25901 / 30600) loss: 2.279737\n",
      "(Iteration 26001 / 30600) loss: 2.277039\n",
      "(Epoch 34 / 40) train acc: 0.280000; val_acc: 0.286000\n",
      "(Iteration 26101 / 30600) loss: 2.284267\n",
      "(Iteration 26201 / 30600) loss: 2.280778\n",
      "(Iteration 26301 / 30600) loss: 2.286580\n",
      "(Iteration 26401 / 30600) loss: 2.278681\n",
      "(Iteration 26501 / 30600) loss: 2.288243\n",
      "(Iteration 26601 / 30600) loss: 2.282920\n",
      "(Iteration 26701 / 30600) loss: 2.283177\n",
      "(Epoch 35 / 40) train acc: 0.312000; val_acc: 0.286000\n",
      "(Iteration 26801 / 30600) loss: 2.281945\n",
      "(Iteration 26901 / 30600) loss: 2.280729\n",
      "(Iteration 27001 / 30600) loss: 2.284239\n",
      "(Iteration 27101 / 30600) loss: 2.282829\n",
      "(Iteration 27201 / 30600) loss: 2.289671\n",
      "(Iteration 27301 / 30600) loss: 2.276362\n",
      "(Iteration 27401 / 30600) loss: 2.284664\n",
      "(Iteration 27501 / 30600) loss: 2.285705\n",
      "(Epoch 36 / 40) train acc: 0.288000; val_acc: 0.285000\n",
      "(Iteration 27601 / 30600) loss: 2.283627\n",
      "(Iteration 27701 / 30600) loss: 2.286225\n",
      "(Iteration 27801 / 30600) loss: 2.276670\n",
      "(Iteration 27901 / 30600) loss: 2.284686\n",
      "(Iteration 28001 / 30600) loss: 2.282618\n",
      "(Iteration 28101 / 30600) loss: 2.284485\n",
      "(Iteration 28201 / 30600) loss: 2.281567\n",
      "(Iteration 28301 / 30600) loss: 2.294424\n",
      "(Epoch 37 / 40) train acc: 0.292000; val_acc: 0.285000\n",
      "(Iteration 28401 / 30600) loss: 2.275821\n",
      "(Iteration 28501 / 30600) loss: 2.285219\n",
      "(Iteration 28601 / 30600) loss: 2.283877\n",
      "(Iteration 28701 / 30600) loss: 2.278009\n",
      "(Iteration 28801 / 30600) loss: 2.283667\n",
      "(Iteration 28901 / 30600) loss: 2.282555\n",
      "(Iteration 29001 / 30600) loss: 2.287634\n",
      "(Epoch 38 / 40) train acc: 0.307000; val_acc: 0.284000\n",
      "(Iteration 29101 / 30600) loss: 2.281889\n",
      "(Iteration 29201 / 30600) loss: 2.281508\n",
      "(Iteration 29301 / 30600) loss: 2.289781\n",
      "(Iteration 29401 / 30600) loss: 2.274732\n",
      "(Iteration 29501 / 30600) loss: 2.283354\n",
      "(Iteration 29601 / 30600) loss: 2.282530\n",
      "(Iteration 29701 / 30600) loss: 2.276700\n",
      "(Iteration 29801 / 30600) loss: 2.287790\n",
      "(Epoch 39 / 40) train acc: 0.340000; val_acc: 0.285000\n",
      "(Iteration 29901 / 30600) loss: 2.281717\n",
      "(Iteration 30001 / 30600) loss: 2.276069\n",
      "(Iteration 30101 / 30600) loss: 2.286388\n",
      "(Iteration 30201 / 30600) loss: 2.284606\n",
      "(Iteration 30301 / 30600) loss: 2.280178\n",
      "(Iteration 30401 / 30600) loss: 2.287142\n",
      "(Iteration 30501 / 30600) loss: 2.284801\n",
      "(Epoch 40 / 40) train acc: 0.298000; val_acc: 0.284000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.001, 'num_epochs': 40, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 61240) loss: 2.303286\n",
      "(Epoch 0 / 40) train acc: 0.106000; val_acc: 0.095000\n",
      "(Iteration 101 / 61240) loss: 2.303241\n",
      "(Iteration 201 / 61240) loss: 2.303068\n",
      "(Iteration 301 / 61240) loss: 2.303015\n",
      "(Iteration 401 / 61240) loss: 2.303204\n",
      "(Iteration 501 / 61240) loss: 2.302780\n",
      "(Iteration 601 / 61240) loss: 2.303199\n",
      "(Iteration 701 / 61240) loss: 2.303024\n",
      "(Iteration 801 / 61240) loss: 2.302832\n",
      "(Iteration 901 / 61240) loss: 2.303259\n",
      "(Iteration 1001 / 61240) loss: 2.302941\n",
      "(Iteration 1101 / 61240) loss: 2.302464\n",
      "(Iteration 1201 / 61240) loss: 2.302855\n",
      "(Iteration 1301 / 61240) loss: 2.302172\n",
      "(Iteration 1401 / 61240) loss: 2.303071\n",
      "(Iteration 1501 / 61240) loss: 2.303025\n",
      "(Epoch 1 / 40) train acc: 0.109000; val_acc: 0.109000\n",
      "(Iteration 1601 / 61240) loss: 2.302769\n",
      "(Iteration 1701 / 61240) loss: 2.301778\n",
      "(Iteration 1801 / 61240) loss: 2.301521\n",
      "(Iteration 1901 / 61240) loss: 2.302797\n",
      "(Iteration 2001 / 61240) loss: 2.302420\n",
      "(Iteration 2101 / 61240) loss: 2.302159\n",
      "(Iteration 2201 / 61240) loss: 2.302237\n",
      "(Iteration 2301 / 61240) loss: 2.302451\n",
      "(Iteration 2401 / 61240) loss: 2.301914\n",
      "(Iteration 2501 / 61240) loss: 2.302606\n",
      "(Iteration 2601 / 61240) loss: 2.302098\n",
      "(Iteration 2701 / 61240) loss: 2.302359\n",
      "(Iteration 2801 / 61240) loss: 2.301601\n",
      "(Iteration 2901 / 61240) loss: 2.301938\n",
      "(Iteration 3001 / 61240) loss: 2.301945\n",
      "(Epoch 2 / 40) train acc: 0.196000; val_acc: 0.180000\n",
      "(Iteration 3101 / 61240) loss: 2.302411\n",
      "(Iteration 3201 / 61240) loss: 2.301311\n",
      "(Iteration 3301 / 61240) loss: 2.301881\n",
      "(Iteration 3401 / 61240) loss: 2.302337\n",
      "(Iteration 3501 / 61240) loss: 2.302334\n",
      "(Iteration 3601 / 61240) loss: 2.301399\n",
      "(Iteration 3701 / 61240) loss: 2.301513\n",
      "(Iteration 3801 / 61240) loss: 2.301186\n",
      "(Iteration 3901 / 61240) loss: 2.302198\n",
      "(Iteration 4001 / 61240) loss: 2.301618\n",
      "(Iteration 4101 / 61240) loss: 2.301410\n",
      "(Iteration 4201 / 61240) loss: 2.301468\n",
      "(Iteration 4301 / 61240) loss: 2.301351\n",
      "(Iteration 4401 / 61240) loss: 2.300863\n",
      "(Iteration 4501 / 61240) loss: 2.301266\n",
      "(Epoch 3 / 40) train acc: 0.245000; val_acc: 0.269000\n",
      "(Iteration 4601 / 61240) loss: 2.302036\n",
      "(Iteration 4701 / 61240) loss: 2.301499\n",
      "(Iteration 4801 / 61240) loss: 2.300524\n",
      "(Iteration 4901 / 61240) loss: 2.300717\n",
      "(Iteration 5001 / 61240) loss: 2.300533\n",
      "(Iteration 5101 / 61240) loss: 2.300462\n",
      "(Iteration 5201 / 61240) loss: 2.300306\n",
      "(Iteration 5301 / 61240) loss: 2.299519\n",
      "(Iteration 5401 / 61240) loss: 2.299374\n",
      "(Iteration 5501 / 61240) loss: 2.299136\n",
      "(Iteration 5601 / 61240) loss: 2.300952\n",
      "(Iteration 5701 / 61240) loss: 2.299820\n",
      "(Iteration 5801 / 61240) loss: 2.300520\n",
      "(Iteration 5901 / 61240) loss: 2.299681\n",
      "(Iteration 6001 / 61240) loss: 2.300291\n",
      "(Iteration 6101 / 61240) loss: 2.297992\n",
      "(Epoch 4 / 40) train acc: 0.272000; val_acc: 0.318000\n",
      "(Iteration 6201 / 61240) loss: 2.297343\n",
      "(Iteration 6301 / 61240) loss: 2.297976\n",
      "(Iteration 6401 / 61240) loss: 2.296452\n",
      "(Iteration 6501 / 61240) loss: 2.297973\n",
      "(Iteration 6601 / 61240) loss: 2.297168\n",
      "(Iteration 6701 / 61240) loss: 2.298682\n",
      "(Iteration 6801 / 61240) loss: 2.298922\n",
      "(Iteration 6901 / 61240) loss: 2.297318\n",
      "(Iteration 7001 / 61240) loss: 2.296866\n",
      "(Iteration 7101 / 61240) loss: 2.296654\n",
      "(Iteration 7201 / 61240) loss: 2.296873\n",
      "(Iteration 7301 / 61240) loss: 2.296263\n",
      "(Iteration 7401 / 61240) loss: 2.297751\n",
      "(Iteration 7501 / 61240) loss: 2.297222\n",
      "(Iteration 7601 / 61240) loss: 2.292340\n",
      "(Epoch 5 / 40) train acc: 0.278000; val_acc: 0.304000\n",
      "(Iteration 7701 / 61240) loss: 2.294731\n",
      "(Iteration 7801 / 61240) loss: 2.290880\n",
      "(Iteration 7901 / 61240) loss: 2.291807\n",
      "(Iteration 8001 / 61240) loss: 2.292614\n",
      "(Iteration 8101 / 61240) loss: 2.296700\n",
      "(Iteration 8201 / 61240) loss: 2.294160\n",
      "(Iteration 8301 / 61240) loss: 2.294086\n",
      "(Iteration 8401 / 61240) loss: 2.291678\n",
      "(Iteration 8501 / 61240) loss: 2.289497\n",
      "(Iteration 8601 / 61240) loss: 2.294291\n",
      "(Iteration 8701 / 61240) loss: 2.294024\n",
      "(Iteration 8801 / 61240) loss: 2.292977\n",
      "(Iteration 8901 / 61240) loss: 2.292168\n",
      "(Iteration 9001 / 61240) loss: 2.291346\n",
      "(Iteration 9101 / 61240) loss: 2.292247\n",
      "(Epoch 6 / 40) train acc: 0.308000; val_acc: 0.310000\n",
      "(Iteration 9201 / 61240) loss: 2.291999\n",
      "(Iteration 9301 / 61240) loss: 2.287111\n",
      "(Iteration 9401 / 61240) loss: 2.280383\n",
      "(Iteration 9501 / 61240) loss: 2.282683\n",
      "(Iteration 9601 / 61240) loss: 2.285448\n",
      "(Iteration 9701 / 61240) loss: 2.283851\n",
      "(Iteration 9801 / 61240) loss: 2.279211\n",
      "(Iteration 9901 / 61240) loss: 2.284173\n",
      "(Iteration 10001 / 61240) loss: 2.284491\n",
      "(Iteration 10101 / 61240) loss: 2.276873\n",
      "(Iteration 10201 / 61240) loss: 2.272937\n",
      "(Iteration 10301 / 61240) loss: 2.283912\n",
      "(Iteration 10401 / 61240) loss: 2.280133\n",
      "(Iteration 10501 / 61240) loss: 2.280302\n",
      "(Iteration 10601 / 61240) loss: 2.274223\n",
      "(Iteration 10701 / 61240) loss: 2.279064\n",
      "(Epoch 7 / 40) train acc: 0.316000; val_acc: 0.305000\n",
      "(Iteration 10801 / 61240) loss: 2.265918\n",
      "(Iteration 10901 / 61240) loss: 2.280967\n",
      "(Iteration 11001 / 61240) loss: 2.277847\n",
      "(Iteration 11101 / 61240) loss: 2.272166\n",
      "(Iteration 11201 / 61240) loss: 2.267899\n",
      "(Iteration 11301 / 61240) loss: 2.271604\n",
      "(Iteration 11401 / 61240) loss: 2.283422\n",
      "(Iteration 11501 / 61240) loss: 2.273713\n",
      "(Iteration 11601 / 61240) loss: 2.255538\n",
      "(Iteration 11701 / 61240) loss: 2.278029\n",
      "(Iteration 11801 / 61240) loss: 2.274161\n",
      "(Iteration 11901 / 61240) loss: 2.271219\n",
      "(Iteration 12001 / 61240) loss: 2.260046\n",
      "(Iteration 12101 / 61240) loss: 2.265503\n",
      "(Iteration 12201 / 61240) loss: 2.265062\n",
      "(Epoch 8 / 40) train acc: 0.326000; val_acc: 0.306000\n",
      "(Iteration 12301 / 61240) loss: 2.259277\n",
      "(Iteration 12401 / 61240) loss: 2.272097\n",
      "(Iteration 12501 / 61240) loss: 2.270408\n",
      "(Iteration 12601 / 61240) loss: 2.270754\n",
      "(Iteration 12701 / 61240) loss: 2.254507\n",
      "(Iteration 12801 / 61240) loss: 2.247688\n",
      "(Iteration 12901 / 61240) loss: 2.260936\n",
      "(Iteration 13001 / 61240) loss: 2.271570\n",
      "(Iteration 13101 / 61240) loss: 2.255814\n",
      "(Iteration 13201 / 61240) loss: 2.272341\n",
      "(Iteration 13301 / 61240) loss: 2.244861\n",
      "(Iteration 13401 / 61240) loss: 2.277170\n",
      "(Iteration 13501 / 61240) loss: 2.244859\n",
      "(Iteration 13601 / 61240) loss: 2.214212\n",
      "(Iteration 13701 / 61240) loss: 2.211586\n",
      "(Epoch 9 / 40) train acc: 0.309000; val_acc: 0.299000\n",
      "(Iteration 13801 / 61240) loss: 2.260682\n",
      "(Iteration 13901 / 61240) loss: 2.214701\n",
      "(Iteration 14001 / 61240) loss: 2.245239\n",
      "(Iteration 14101 / 61240) loss: 2.229613\n",
      "(Iteration 14201 / 61240) loss: 2.261098\n",
      "(Iteration 14301 / 61240) loss: 2.254762\n",
      "(Iteration 14401 / 61240) loss: 2.235439\n",
      "(Iteration 14501 / 61240) loss: 2.260149\n",
      "(Iteration 14601 / 61240) loss: 2.234659\n",
      "(Iteration 14701 / 61240) loss: 2.220870\n",
      "(Iteration 14801 / 61240) loss: 2.263515\n",
      "(Iteration 14901 / 61240) loss: 2.253240\n",
      "(Iteration 15001 / 61240) loss: 2.254291\n",
      "(Iteration 15101 / 61240) loss: 2.236925\n",
      "(Iteration 15201 / 61240) loss: 2.247981\n",
      "(Iteration 15301 / 61240) loss: 2.215621\n",
      "(Epoch 10 / 40) train acc: 0.288000; val_acc: 0.289000\n",
      "(Iteration 15401 / 61240) loss: 2.258443\n",
      "(Iteration 15501 / 61240) loss: 2.225258\n",
      "(Iteration 15601 / 61240) loss: 2.202468\n",
      "(Iteration 15701 / 61240) loss: 2.226036\n",
      "(Iteration 15801 / 61240) loss: 2.230104\n",
      "(Iteration 15901 / 61240) loss: 2.237847\n",
      "(Iteration 16001 / 61240) loss: 2.216634\n",
      "(Iteration 16101 / 61240) loss: 2.228874\n",
      "(Iteration 16201 / 61240) loss: 2.183881\n",
      "(Iteration 16301 / 61240) loss: 2.248373\n",
      "(Iteration 16401 / 61240) loss: 2.189168\n",
      "(Iteration 16501 / 61240) loss: 2.189293\n",
      "(Iteration 16601 / 61240) loss: 2.135577\n",
      "(Iteration 16701 / 61240) loss: 2.193205\n",
      "(Iteration 16801 / 61240) loss: 2.199215\n",
      "(Epoch 11 / 40) train acc: 0.263000; val_acc: 0.285000\n",
      "(Iteration 16901 / 61240) loss: 2.261365\n",
      "(Iteration 17001 / 61240) loss: 2.209462\n",
      "(Iteration 17101 / 61240) loss: 2.197274\n",
      "(Iteration 17201 / 61240) loss: 2.207693\n",
      "(Iteration 17301 / 61240) loss: 2.202044\n",
      "(Iteration 17401 / 61240) loss: 2.194911\n",
      "(Iteration 17501 / 61240) loss: 2.237473\n",
      "(Iteration 17601 / 61240) loss: 2.195491\n",
      "(Iteration 17701 / 61240) loss: 2.153613\n",
      "(Iteration 17801 / 61240) loss: 2.169411\n",
      "(Iteration 17901 / 61240) loss: 2.205551\n",
      "(Iteration 18001 / 61240) loss: 2.197219\n",
      "(Iteration 18101 / 61240) loss: 2.237424\n",
      "(Iteration 18201 / 61240) loss: 2.115701\n",
      "(Iteration 18301 / 61240) loss: 2.166083\n",
      "(Epoch 12 / 40) train acc: 0.269000; val_acc: 0.285000\n",
      "(Iteration 18401 / 61240) loss: 2.167311\n",
      "(Iteration 18501 / 61240) loss: 2.146942\n",
      "(Iteration 18601 / 61240) loss: 2.211224\n",
      "(Iteration 18701 / 61240) loss: 2.163691\n",
      "(Iteration 18801 / 61240) loss: 2.171751\n",
      "(Iteration 18901 / 61240) loss: 2.122617\n",
      "(Iteration 19001 / 61240) loss: 2.233085\n",
      "(Iteration 19101 / 61240) loss: 2.151035\n",
      "(Iteration 19201 / 61240) loss: 2.131288\n",
      "(Iteration 19301 / 61240) loss: 2.130590\n",
      "(Iteration 19401 / 61240) loss: 2.112076\n",
      "(Iteration 19501 / 61240) loss: 2.203180\n",
      "(Iteration 19601 / 61240) loss: 2.175129\n",
      "(Iteration 19701 / 61240) loss: 2.138440\n",
      "(Iteration 19801 / 61240) loss: 2.132242\n",
      "(Iteration 19901 / 61240) loss: 2.115186\n",
      "(Epoch 13 / 40) train acc: 0.299000; val_acc: 0.290000\n",
      "(Iteration 20001 / 61240) loss: 2.183427\n",
      "(Iteration 20101 / 61240) loss: 2.184547\n",
      "(Iteration 20201 / 61240) loss: 2.218814\n",
      "(Iteration 20301 / 61240) loss: 2.157191\n",
      "(Iteration 20401 / 61240) loss: 2.117084\n",
      "(Iteration 20501 / 61240) loss: 2.145445\n",
      "(Iteration 20601 / 61240) loss: 2.163316\n",
      "(Iteration 20701 / 61240) loss: 2.126735\n",
      "(Iteration 20801 / 61240) loss: 2.095004\n",
      "(Iteration 20901 / 61240) loss: 2.164600\n",
      "(Iteration 21001 / 61240) loss: 2.091016\n",
      "(Iteration 21101 / 61240) loss: 2.099668\n",
      "(Iteration 21201 / 61240) loss: 2.164646\n",
      "(Iteration 21301 / 61240) loss: 2.131160\n",
      "(Iteration 21401 / 61240) loss: 2.108661\n",
      "(Epoch 14 / 40) train acc: 0.257000; val_acc: 0.291000\n",
      "(Iteration 21501 / 61240) loss: 2.125522\n",
      "(Iteration 21601 / 61240) loss: 2.103889\n",
      "(Iteration 21701 / 61240) loss: 2.102818\n",
      "(Iteration 21801 / 61240) loss: 2.078337\n",
      "(Iteration 21901 / 61240) loss: 2.092106\n",
      "(Iteration 22001 / 61240) loss: 2.146025\n",
      "(Iteration 22101 / 61240) loss: 2.103816\n",
      "(Iteration 22201 / 61240) loss: 2.099729\n",
      "(Iteration 22301 / 61240) loss: 2.147518\n",
      "(Iteration 22401 / 61240) loss: 2.130374\n",
      "(Iteration 22501 / 61240) loss: 2.093845\n",
      "(Iteration 22601 / 61240) loss: 2.175014\n",
      "(Iteration 22701 / 61240) loss: 2.138693\n",
      "(Iteration 22801 / 61240) loss: 2.009718\n",
      "(Iteration 22901 / 61240) loss: 2.140588\n",
      "(Epoch 15 / 40) train acc: 0.264000; val_acc: 0.290000\n",
      "(Iteration 23001 / 61240) loss: 2.174168\n",
      "(Iteration 23101 / 61240) loss: 2.000258\n",
      "(Iteration 23201 / 61240) loss: 2.093324\n",
      "(Iteration 23301 / 61240) loss: 1.998361\n",
      "(Iteration 23401 / 61240) loss: 2.098130\n",
      "(Iteration 23501 / 61240) loss: 2.086817\n",
      "(Iteration 23601 / 61240) loss: 2.104071\n",
      "(Iteration 23701 / 61240) loss: 2.127823\n",
      "(Iteration 23801 / 61240) loss: 1.932282\n",
      "(Iteration 23901 / 61240) loss: 2.153324\n",
      "(Iteration 24001 / 61240) loss: 2.125300\n",
      "(Iteration 24101 / 61240) loss: 2.184775\n",
      "(Iteration 24201 / 61240) loss: 2.145887\n",
      "(Iteration 24301 / 61240) loss: 2.025682\n",
      "(Iteration 24401 / 61240) loss: 2.117980\n",
      "(Epoch 16 / 40) train acc: 0.272000; val_acc: 0.289000\n",
      "(Iteration 24501 / 61240) loss: 2.075728\n",
      "(Iteration 24601 / 61240) loss: 2.075333\n",
      "(Iteration 24701 / 61240) loss: 2.104869\n",
      "(Iteration 24801 / 61240) loss: 2.083970\n",
      "(Iteration 24901 / 61240) loss: 2.103383\n",
      "(Iteration 25001 / 61240) loss: 2.157653\n",
      "(Iteration 25101 / 61240) loss: 2.122335\n",
      "(Iteration 25201 / 61240) loss: 2.162136\n",
      "(Iteration 25301 / 61240) loss: 2.128768\n",
      "(Iteration 25401 / 61240) loss: 2.107313\n",
      "(Iteration 25501 / 61240) loss: 2.115933\n",
      "(Iteration 25601 / 61240) loss: 2.111581\n",
      "(Iteration 25701 / 61240) loss: 2.074610\n",
      "(Iteration 25801 / 61240) loss: 2.097813\n",
      "(Iteration 25901 / 61240) loss: 2.000825\n",
      "(Iteration 26001 / 61240) loss: 2.108644\n",
      "(Epoch 17 / 40) train acc: 0.292000; val_acc: 0.288000\n",
      "(Iteration 26101 / 61240) loss: 1.974251\n",
      "(Iteration 26201 / 61240) loss: 2.059550\n",
      "(Iteration 26301 / 61240) loss: 2.077036\n",
      "(Iteration 26401 / 61240) loss: 2.015563\n",
      "(Iteration 26501 / 61240) loss: 2.076624\n",
      "(Iteration 26601 / 61240) loss: 1.929826\n",
      "(Iteration 26701 / 61240) loss: 2.150017\n",
      "(Iteration 26801 / 61240) loss: 2.148396\n",
      "(Iteration 26901 / 61240) loss: 2.154788\n",
      "(Iteration 27001 / 61240) loss: 2.102929\n",
      "(Iteration 27101 / 61240) loss: 2.157982\n",
      "(Iteration 27201 / 61240) loss: 2.116083\n",
      "(Iteration 27301 / 61240) loss: 2.081624\n",
      "(Iteration 27401 / 61240) loss: 1.978314\n",
      "(Iteration 27501 / 61240) loss: 2.123170\n",
      "(Epoch 18 / 40) train acc: 0.267000; val_acc: 0.288000\n",
      "(Iteration 27601 / 61240) loss: 2.077562\n",
      "(Iteration 27701 / 61240) loss: 2.097244\n",
      "(Iteration 27801 / 61240) loss: 2.164185\n",
      "(Iteration 27901 / 61240) loss: 2.189807\n",
      "(Iteration 28001 / 61240) loss: 2.149383\n",
      "(Iteration 28101 / 61240) loss: 2.022222\n",
      "(Iteration 28201 / 61240) loss: 2.112292\n",
      "(Iteration 28301 / 61240) loss: 2.063894\n",
      "(Iteration 28401 / 61240) loss: 2.058100\n",
      "(Iteration 28501 / 61240) loss: 1.998764\n",
      "(Iteration 28601 / 61240) loss: 2.160538\n",
      "(Iteration 28701 / 61240) loss: 2.067451\n",
      "(Iteration 28801 / 61240) loss: 2.034140\n",
      "(Iteration 28901 / 61240) loss: 2.107506\n",
      "(Iteration 29001 / 61240) loss: 1.974023\n",
      "(Epoch 19 / 40) train acc: 0.279000; val_acc: 0.289000\n",
      "(Iteration 29101 / 61240) loss: 2.089322\n",
      "(Iteration 29201 / 61240) loss: 2.081244\n",
      "(Iteration 29301 / 61240) loss: 2.071249\n",
      "(Iteration 29401 / 61240) loss: 2.108921\n",
      "(Iteration 29501 / 61240) loss: 2.038874\n",
      "(Iteration 29601 / 61240) loss: 2.055040\n",
      "(Iteration 29701 / 61240) loss: 2.044828\n",
      "(Iteration 29801 / 61240) loss: 2.084310\n",
      "(Iteration 29901 / 61240) loss: 1.881894\n",
      "(Iteration 30001 / 61240) loss: 2.152024\n",
      "(Iteration 30101 / 61240) loss: 2.050887\n",
      "(Iteration 30201 / 61240) loss: 2.123087\n",
      "(Iteration 30301 / 61240) loss: 2.068464\n",
      "(Iteration 30401 / 61240) loss: 2.040611\n",
      "(Iteration 30501 / 61240) loss: 2.071154\n",
      "(Iteration 30601 / 61240) loss: 2.133962\n",
      "(Epoch 20 / 40) train acc: 0.271000; val_acc: 0.293000\n",
      "(Iteration 30701 / 61240) loss: 2.015683\n",
      "(Iteration 30801 / 61240) loss: 2.142387\n",
      "(Iteration 30901 / 61240) loss: 2.016408\n",
      "(Iteration 31001 / 61240) loss: 2.102045\n",
      "(Iteration 31101 / 61240) loss: 1.940110\n",
      "(Iteration 31201 / 61240) loss: 2.121107\n",
      "(Iteration 31301 / 61240) loss: 2.086107\n",
      "(Iteration 31401 / 61240) loss: 2.023515\n",
      "(Iteration 31501 / 61240) loss: 2.124060\n",
      "(Iteration 31601 / 61240) loss: 2.141021\n",
      "(Iteration 31701 / 61240) loss: 2.094495\n",
      "(Iteration 31801 / 61240) loss: 2.035783\n",
      "(Iteration 31901 / 61240) loss: 2.025462\n",
      "(Iteration 32001 / 61240) loss: 1.990032\n",
      "(Iteration 32101 / 61240) loss: 1.995010\n",
      "(Epoch 21 / 40) train acc: 0.280000; val_acc: 0.299000\n",
      "(Iteration 32201 / 61240) loss: 2.103838\n",
      "(Iteration 32301 / 61240) loss: 2.080232\n",
      "(Iteration 32401 / 61240) loss: 2.075022\n",
      "(Iteration 32501 / 61240) loss: 1.902635\n",
      "(Iteration 32601 / 61240) loss: 1.904367\n",
      "(Iteration 32701 / 61240) loss: 2.100296\n",
      "(Iteration 32801 / 61240) loss: 2.030665\n",
      "(Iteration 32901 / 61240) loss: 2.063888\n",
      "(Iteration 33001 / 61240) loss: 2.049346\n",
      "(Iteration 33101 / 61240) loss: 1.941538\n",
      "(Iteration 33201 / 61240) loss: 2.057161\n",
      "(Iteration 33301 / 61240) loss: 1.992219\n",
      "(Iteration 33401 / 61240) loss: 2.053014\n",
      "(Iteration 33501 / 61240) loss: 2.138513\n",
      "(Iteration 33601 / 61240) loss: 1.977930\n",
      "(Epoch 22 / 40) train acc: 0.301000; val_acc: 0.302000\n",
      "(Iteration 33701 / 61240) loss: 2.078623\n",
      "(Iteration 33801 / 61240) loss: 1.968008\n",
      "(Iteration 33901 / 61240) loss: 1.994239\n",
      "(Iteration 34001 / 61240) loss: 2.034413\n",
      "(Iteration 34101 / 61240) loss: 1.977253\n",
      "(Iteration 34201 / 61240) loss: 2.104654\n",
      "(Iteration 34301 / 61240) loss: 1.925056\n",
      "(Iteration 34401 / 61240) loss: 2.033296\n",
      "(Iteration 34501 / 61240) loss: 1.956319\n",
      "(Iteration 34601 / 61240) loss: 2.049561\n",
      "(Iteration 34701 / 61240) loss: 2.012931\n",
      "(Iteration 34801 / 61240) loss: 2.105174\n",
      "(Iteration 34901 / 61240) loss: 2.008717\n",
      "(Iteration 35001 / 61240) loss: 2.055905\n",
      "(Iteration 35101 / 61240) loss: 2.031102\n",
      "(Iteration 35201 / 61240) loss: 1.987726\n",
      "(Epoch 23 / 40) train acc: 0.299000; val_acc: 0.302000\n",
      "(Iteration 35301 / 61240) loss: 2.104138\n",
      "(Iteration 35401 / 61240) loss: 2.098717\n",
      "(Iteration 35501 / 61240) loss: 2.089910\n",
      "(Iteration 35601 / 61240) loss: 2.040061\n",
      "(Iteration 35701 / 61240) loss: 1.979743\n",
      "(Iteration 35801 / 61240) loss: 1.981183\n",
      "(Iteration 35901 / 61240) loss: 2.052495\n",
      "(Iteration 36001 / 61240) loss: 2.029058\n",
      "(Iteration 36101 / 61240) loss: 1.928357\n",
      "(Iteration 36201 / 61240) loss: 2.169018\n",
      "(Iteration 36301 / 61240) loss: 1.924293\n",
      "(Iteration 36401 / 61240) loss: 1.917022\n",
      "(Iteration 36501 / 61240) loss: 1.867874\n",
      "(Iteration 36601 / 61240) loss: 1.945153\n",
      "(Iteration 36701 / 61240) loss: 1.784914\n",
      "(Epoch 24 / 40) train acc: 0.274000; val_acc: 0.305000\n",
      "(Iteration 36801 / 61240) loss: 1.920048\n",
      "(Iteration 36901 / 61240) loss: 2.065993\n",
      "(Iteration 37001 / 61240) loss: 1.947688\n",
      "(Iteration 37101 / 61240) loss: 1.966966\n",
      "(Iteration 37201 / 61240) loss: 2.199633\n",
      "(Iteration 37301 / 61240) loss: 2.067206\n",
      "(Iteration 37401 / 61240) loss: 1.798896\n",
      "(Iteration 37501 / 61240) loss: 1.875542\n",
      "(Iteration 37601 / 61240) loss: 1.882947\n",
      "(Iteration 37701 / 61240) loss: 1.969202\n",
      "(Iteration 37801 / 61240) loss: 1.954212\n",
      "(Iteration 37901 / 61240) loss: 2.109429\n",
      "(Iteration 38001 / 61240) loss: 2.103997\n",
      "(Iteration 38101 / 61240) loss: 1.939078\n",
      "(Iteration 38201 / 61240) loss: 1.965148\n",
      "(Epoch 25 / 40) train acc: 0.269000; val_acc: 0.305000\n",
      "(Iteration 38301 / 61240) loss: 2.019098\n",
      "(Iteration 38401 / 61240) loss: 2.085011\n",
      "(Iteration 38501 / 61240) loss: 1.856747\n",
      "(Iteration 38601 / 61240) loss: 2.025855\n",
      "(Iteration 38701 / 61240) loss: 2.064355\n",
      "(Iteration 38801 / 61240) loss: 2.040705\n",
      "(Iteration 38901 / 61240) loss: 2.155041\n",
      "(Iteration 39001 / 61240) loss: 1.882375\n",
      "(Iteration 39101 / 61240) loss: 2.126769\n",
      "(Iteration 39201 / 61240) loss: 1.978247\n",
      "(Iteration 39301 / 61240) loss: 2.056185\n",
      "(Iteration 39401 / 61240) loss: 2.089068\n",
      "(Iteration 39501 / 61240) loss: 2.142673\n",
      "(Iteration 39601 / 61240) loss: 1.967879\n",
      "(Iteration 39701 / 61240) loss: 2.035068\n",
      "(Iteration 39801 / 61240) loss: 1.977327\n",
      "(Epoch 26 / 40) train acc: 0.280000; val_acc: 0.304000\n",
      "(Iteration 39901 / 61240) loss: 1.953561\n",
      "(Iteration 40001 / 61240) loss: 1.987609\n",
      "(Iteration 40101 / 61240) loss: 1.905825\n",
      "(Iteration 40201 / 61240) loss: 2.052234\n",
      "(Iteration 40301 / 61240) loss: 1.906876\n",
      "(Iteration 40401 / 61240) loss: 2.029481\n",
      "(Iteration 40501 / 61240) loss: 1.945528\n",
      "(Iteration 40601 / 61240) loss: 2.071529\n",
      "(Iteration 40701 / 61240) loss: 1.945819\n",
      "(Iteration 40801 / 61240) loss: 2.000922\n",
      "(Iteration 40901 / 61240) loss: 1.986714\n",
      "(Iteration 41001 / 61240) loss: 2.021305\n",
      "(Iteration 41101 / 61240) loss: 1.974956\n",
      "(Iteration 41201 / 61240) loss: 2.018425\n",
      "(Iteration 41301 / 61240) loss: 1.962823\n",
      "(Epoch 27 / 40) train acc: 0.311000; val_acc: 0.303000\n",
      "(Iteration 41401 / 61240) loss: 1.923561\n",
      "(Iteration 41501 / 61240) loss: 2.043615\n",
      "(Iteration 41601 / 61240) loss: 2.055471\n",
      "(Iteration 41701 / 61240) loss: 1.998780\n",
      "(Iteration 41801 / 61240) loss: 2.038269\n",
      "(Iteration 41901 / 61240) loss: 1.985155\n",
      "(Iteration 42001 / 61240) loss: 2.081843\n",
      "(Iteration 42101 / 61240) loss: 2.044928\n",
      "(Iteration 42201 / 61240) loss: 1.912931\n",
      "(Iteration 42301 / 61240) loss: 1.880151\n",
      "(Iteration 42401 / 61240) loss: 2.015881\n",
      "(Iteration 42501 / 61240) loss: 2.001286\n",
      "(Iteration 42601 / 61240) loss: 1.834827\n",
      "(Iteration 42701 / 61240) loss: 1.987160\n",
      "(Iteration 42801 / 61240) loss: 1.804795\n",
      "(Epoch 28 / 40) train acc: 0.319000; val_acc: 0.304000\n",
      "(Iteration 42901 / 61240) loss: 1.985675\n",
      "(Iteration 43001 / 61240) loss: 1.926283\n",
      "(Iteration 43101 / 61240) loss: 1.799481\n",
      "(Iteration 43201 / 61240) loss: 1.955271\n",
      "(Iteration 43301 / 61240) loss: 2.075673\n",
      "(Iteration 43401 / 61240) loss: 2.116338\n",
      "(Iteration 43501 / 61240) loss: 2.168329\n",
      "(Iteration 43601 / 61240) loss: 2.132050\n",
      "(Iteration 43701 / 61240) loss: 2.032628\n",
      "(Iteration 43801 / 61240) loss: 2.022805\n",
      "(Iteration 43901 / 61240) loss: 2.085303\n",
      "(Iteration 44001 / 61240) loss: 2.060495\n",
      "(Iteration 44101 / 61240) loss: 2.051769\n",
      "(Iteration 44201 / 61240) loss: 2.003569\n",
      "(Iteration 44301 / 61240) loss: 1.853443\n",
      "(Epoch 29 / 40) train acc: 0.280000; val_acc: 0.307000\n",
      "(Iteration 44401 / 61240) loss: 2.116319\n",
      "(Iteration 44501 / 61240) loss: 2.008291\n",
      "(Iteration 44601 / 61240) loss: 1.890762\n",
      "(Iteration 44701 / 61240) loss: 2.057034\n",
      "(Iteration 44801 / 61240) loss: 1.880646\n",
      "(Iteration 44901 / 61240) loss: 2.076483\n",
      "(Iteration 45001 / 61240) loss: 2.072577\n",
      "(Iteration 45101 / 61240) loss: 2.002750\n",
      "(Iteration 45201 / 61240) loss: 2.186107\n",
      "(Iteration 45301 / 61240) loss: 1.979784\n",
      "(Iteration 45401 / 61240) loss: 2.099463\n",
      "(Iteration 45501 / 61240) loss: 2.120461\n",
      "(Iteration 45601 / 61240) loss: 1.997242\n",
      "(Iteration 45701 / 61240) loss: 1.783129\n",
      "(Iteration 45801 / 61240) loss: 1.918161\n",
      "(Iteration 45901 / 61240) loss: 1.885472\n",
      "(Epoch 30 / 40) train acc: 0.304000; val_acc: 0.308000\n",
      "(Iteration 46001 / 61240) loss: 1.969984\n",
      "(Iteration 46101 / 61240) loss: 1.981999\n",
      "(Iteration 46201 / 61240) loss: 1.966042\n",
      "(Iteration 46301 / 61240) loss: 2.163107\n",
      "(Iteration 46401 / 61240) loss: 2.043166\n",
      "(Iteration 46501 / 61240) loss: 1.906476\n",
      "(Iteration 46601 / 61240) loss: 1.905484\n",
      "(Iteration 46701 / 61240) loss: 1.936980\n",
      "(Iteration 46801 / 61240) loss: 1.989562\n",
      "(Iteration 46901 / 61240) loss: 1.878244\n",
      "(Iteration 47001 / 61240) loss: 1.993081\n",
      "(Iteration 47101 / 61240) loss: 2.057675\n",
      "(Iteration 47201 / 61240) loss: 1.900955\n",
      "(Iteration 47301 / 61240) loss: 2.071894\n",
      "(Iteration 47401 / 61240) loss: 2.058936\n",
      "(Epoch 31 / 40) train acc: 0.299000; val_acc: 0.308000\n",
      "(Iteration 47501 / 61240) loss: 2.032554\n",
      "(Iteration 47601 / 61240) loss: 1.920400\n",
      "(Iteration 47701 / 61240) loss: 1.807643\n",
      "(Iteration 47801 / 61240) loss: 1.856135\n",
      "(Iteration 47901 / 61240) loss: 2.071907\n",
      "(Iteration 48001 / 61240) loss: 1.925359\n",
      "(Iteration 48101 / 61240) loss: 1.965404\n",
      "(Iteration 48201 / 61240) loss: 1.898038\n",
      "(Iteration 48301 / 61240) loss: 1.883413\n",
      "(Iteration 48401 / 61240) loss: 2.046230\n",
      "(Iteration 48501 / 61240) loss: 2.129742\n",
      "(Iteration 48601 / 61240) loss: 2.007043\n",
      "(Iteration 48701 / 61240) loss: 1.928977\n",
      "(Iteration 48801 / 61240) loss: 2.007594\n",
      "(Iteration 48901 / 61240) loss: 1.976030\n",
      "(Epoch 32 / 40) train acc: 0.296000; val_acc: 0.312000\n",
      "(Iteration 49001 / 61240) loss: 1.773166\n",
      "(Iteration 49101 / 61240) loss: 2.068024\n",
      "(Iteration 49201 / 61240) loss: 2.131439\n",
      "(Iteration 49301 / 61240) loss: 2.146309\n",
      "(Iteration 49401 / 61240) loss: 1.974744\n",
      "(Iteration 49501 / 61240) loss: 1.857808\n",
      "(Iteration 49601 / 61240) loss: 1.857288\n",
      "(Iteration 49701 / 61240) loss: 2.047598\n",
      "(Iteration 49801 / 61240) loss: 2.022332\n",
      "(Iteration 49901 / 61240) loss: 2.066866\n",
      "(Iteration 50001 / 61240) loss: 1.988990\n",
      "(Iteration 50101 / 61240) loss: 2.000329\n",
      "(Iteration 50201 / 61240) loss: 2.019954\n",
      "(Iteration 50301 / 61240) loss: 2.044892\n",
      "(Iteration 50401 / 61240) loss: 1.989943\n",
      "(Iteration 50501 / 61240) loss: 1.916569\n",
      "(Epoch 33 / 40) train acc: 0.317000; val_acc: 0.312000\n",
      "(Iteration 50601 / 61240) loss: 2.023387\n",
      "(Iteration 50701 / 61240) loss: 2.030024\n",
      "(Iteration 50801 / 61240) loss: 1.990414\n",
      "(Iteration 50901 / 61240) loss: 1.916155\n",
      "(Iteration 51001 / 61240) loss: 1.972133\n",
      "(Iteration 51101 / 61240) loss: 2.041436\n",
      "(Iteration 51201 / 61240) loss: 1.872118\n",
      "(Iteration 51301 / 61240) loss: 2.132400\n",
      "(Iteration 51401 / 61240) loss: 2.016267\n",
      "(Iteration 51501 / 61240) loss: 2.000244\n",
      "(Iteration 51601 / 61240) loss: 1.964529\n",
      "(Iteration 51701 / 61240) loss: 1.978233\n",
      "(Iteration 51801 / 61240) loss: 1.989133\n",
      "(Iteration 51901 / 61240) loss: 2.058360\n",
      "(Iteration 52001 / 61240) loss: 2.030744\n",
      "(Epoch 34 / 40) train acc: 0.282000; val_acc: 0.312000\n",
      "(Iteration 52101 / 61240) loss: 2.080448\n",
      "(Iteration 52201 / 61240) loss: 2.076013\n",
      "(Iteration 52301 / 61240) loss: 1.945510\n",
      "(Iteration 52401 / 61240) loss: 2.075164\n",
      "(Iteration 52501 / 61240) loss: 1.979939\n",
      "(Iteration 52601 / 61240) loss: 1.892951\n",
      "(Iteration 52701 / 61240) loss: 1.968494\n",
      "(Iteration 52801 / 61240) loss: 1.948238\n",
      "(Iteration 52901 / 61240) loss: 1.961789\n",
      "(Iteration 53001 / 61240) loss: 1.938385\n",
      "(Iteration 53101 / 61240) loss: 2.002118\n",
      "(Iteration 53201 / 61240) loss: 1.881019\n",
      "(Iteration 53301 / 61240) loss: 1.795215\n",
      "(Iteration 53401 / 61240) loss: 2.007276\n",
      "(Iteration 53501 / 61240) loss: 1.991080\n",
      "(Epoch 35 / 40) train acc: 0.282000; val_acc: 0.312000\n",
      "(Iteration 53601 / 61240) loss: 2.029031\n",
      "(Iteration 53701 / 61240) loss: 2.072113\n",
      "(Iteration 53801 / 61240) loss: 1.896906\n",
      "(Iteration 53901 / 61240) loss: 1.985760\n",
      "(Iteration 54001 / 61240) loss: 1.962228\n",
      "(Iteration 54101 / 61240) loss: 1.958689\n",
      "(Iteration 54201 / 61240) loss: 1.990007\n",
      "(Iteration 54301 / 61240) loss: 1.954585\n",
      "(Iteration 54401 / 61240) loss: 1.904638\n",
      "(Iteration 54501 / 61240) loss: 1.962143\n",
      "(Iteration 54601 / 61240) loss: 2.075122\n",
      "(Iteration 54701 / 61240) loss: 2.053435\n",
      "(Iteration 54801 / 61240) loss: 1.985274\n",
      "(Iteration 54901 / 61240) loss: 2.099862\n",
      "(Iteration 55001 / 61240) loss: 2.104798\n",
      "(Iteration 55101 / 61240) loss: 1.996051\n",
      "(Epoch 36 / 40) train acc: 0.289000; val_acc: 0.312000\n",
      "(Iteration 55201 / 61240) loss: 2.112074\n",
      "(Iteration 55301 / 61240) loss: 1.884384\n",
      "(Iteration 55401 / 61240) loss: 2.060957\n",
      "(Iteration 55501 / 61240) loss: 2.026229\n",
      "(Iteration 55601 / 61240) loss: 1.954662\n",
      "(Iteration 55701 / 61240) loss: 2.100560\n",
      "(Iteration 55801 / 61240) loss: 1.964724\n",
      "(Iteration 55901 / 61240) loss: 1.957148\n",
      "(Iteration 56001 / 61240) loss: 1.898152\n",
      "(Iteration 56101 / 61240) loss: 1.888277\n",
      "(Iteration 56201 / 61240) loss: 2.058049\n",
      "(Iteration 56301 / 61240) loss: 1.953410\n",
      "(Iteration 56401 / 61240) loss: 2.041167\n",
      "(Iteration 56501 / 61240) loss: 1.895272\n",
      "(Iteration 56601 / 61240) loss: 1.912204\n",
      "(Epoch 37 / 40) train acc: 0.294000; val_acc: 0.312000\n",
      "(Iteration 56701 / 61240) loss: 1.925816\n",
      "(Iteration 56801 / 61240) loss: 2.067666\n",
      "(Iteration 56901 / 61240) loss: 1.890237\n",
      "(Iteration 57001 / 61240) loss: 1.944331\n",
      "(Iteration 57101 / 61240) loss: 1.923717\n",
      "(Iteration 57201 / 61240) loss: 2.016074\n",
      "(Iteration 57301 / 61240) loss: 1.905186\n",
      "(Iteration 57401 / 61240) loss: 1.972271\n",
      "(Iteration 57501 / 61240) loss: 2.003202\n",
      "(Iteration 57601 / 61240) loss: 1.887787\n",
      "(Iteration 57701 / 61240) loss: 1.899300\n",
      "(Iteration 57801 / 61240) loss: 2.092804\n",
      "(Iteration 57901 / 61240) loss: 2.011169\n",
      "(Iteration 58001 / 61240) loss: 2.129614\n",
      "(Iteration 58101 / 61240) loss: 1.931975\n",
      "(Epoch 38 / 40) train acc: 0.283000; val_acc: 0.311000\n",
      "(Iteration 58201 / 61240) loss: 1.945229\n",
      "(Iteration 58301 / 61240) loss: 2.000722\n",
      "(Iteration 58401 / 61240) loss: 1.947894\n",
      "(Iteration 58501 / 61240) loss: 1.744801\n",
      "(Iteration 58601 / 61240) loss: 2.040497\n",
      "(Iteration 58701 / 61240) loss: 1.916365\n",
      "(Iteration 58801 / 61240) loss: 2.046412\n",
      "(Iteration 58901 / 61240) loss: 2.097270\n",
      "(Iteration 59001 / 61240) loss: 1.942914\n",
      "(Iteration 59101 / 61240) loss: 1.977951\n",
      "(Iteration 59201 / 61240) loss: 1.963943\n",
      "(Iteration 59301 / 61240) loss: 2.037425\n",
      "(Iteration 59401 / 61240) loss: 2.011546\n",
      "(Iteration 59501 / 61240) loss: 2.013256\n",
      "(Iteration 59601 / 61240) loss: 2.034608\n",
      "(Iteration 59701 / 61240) loss: 2.012622\n",
      "(Epoch 39 / 40) train acc: 0.306000; val_acc: 0.309000\n",
      "(Iteration 59801 / 61240) loss: 1.865816\n",
      "(Iteration 59901 / 61240) loss: 2.164693\n",
      "(Iteration 60001 / 61240) loss: 1.818976\n",
      "(Iteration 60101 / 61240) loss: 1.768691\n",
      "(Iteration 60201 / 61240) loss: 2.168951\n",
      "(Iteration 60301 / 61240) loss: 2.034669\n",
      "(Iteration 60401 / 61240) loss: 1.960593\n",
      "(Iteration 60501 / 61240) loss: 1.934424\n",
      "(Iteration 60601 / 61240) loss: 1.930015\n",
      "(Iteration 60701 / 61240) loss: 1.834351\n",
      "(Iteration 60801 / 61240) loss: 1.856393\n",
      "(Iteration 60901 / 61240) loss: 2.056033\n",
      "(Iteration 61001 / 61240) loss: 1.975704\n",
      "(Iteration 61101 / 61240) loss: 1.905303\n",
      "(Iteration 61201 / 61240) loss: 2.018928\n",
      "(Epoch 40 / 40) train acc: 0.262000; val_acc: 0.309000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.01, 'num_epochs': 20, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 15300) loss: 2.309143\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.117000\n",
      "(Iteration 101 / 15300) loss: 2.307873\n",
      "(Iteration 201 / 15300) loss: 2.307075\n",
      "(Iteration 301 / 15300) loss: 2.304838\n",
      "(Iteration 401 / 15300) loss: 2.304560\n",
      "(Iteration 501 / 15300) loss: 2.301807\n",
      "(Iteration 601 / 15300) loss: 2.304757\n",
      "(Iteration 701 / 15300) loss: 2.300404\n",
      "(Epoch 1 / 20) train acc: 0.192000; val_acc: 0.216000\n",
      "(Iteration 801 / 15300) loss: 2.298301\n",
      "(Iteration 901 / 15300) loss: 2.294472\n",
      "(Iteration 1001 / 15300) loss: 2.289882\n",
      "(Iteration 1101 / 15300) loss: 2.275337\n",
      "(Iteration 1201 / 15300) loss: 2.275107\n",
      "(Iteration 1301 / 15300) loss: 2.264219\n",
      "(Iteration 1401 / 15300) loss: 2.266213\n",
      "(Iteration 1501 / 15300) loss: 2.253171\n",
      "(Epoch 2 / 20) train acc: 0.208000; val_acc: 0.236000\n",
      "(Iteration 1601 / 15300) loss: 2.172939\n",
      "(Iteration 1701 / 15300) loss: 2.162276\n",
      "(Iteration 1801 / 15300) loss: 2.257095\n",
      "(Iteration 1901 / 15300) loss: 2.091241\n",
      "(Iteration 2001 / 15300) loss: 2.224722\n",
      "(Iteration 2101 / 15300) loss: 2.118895\n",
      "(Iteration 2201 / 15300) loss: 2.207567\n",
      "(Epoch 3 / 20) train acc: 0.248000; val_acc: 0.282000\n",
      "(Iteration 2301 / 15300) loss: 2.083800\n",
      "(Iteration 2401 / 15300) loss: 2.065238\n",
      "(Iteration 2501 / 15300) loss: 2.102165\n",
      "(Iteration 2601 / 15300) loss: 2.071003\n",
      "(Iteration 2701 / 15300) loss: 2.063274\n",
      "(Iteration 2801 / 15300) loss: 2.135922\n",
      "(Iteration 2901 / 15300) loss: 2.225473\n",
      "(Iteration 3001 / 15300) loss: 2.076388\n",
      "(Epoch 4 / 20) train acc: 0.294000; val_acc: 0.297000\n",
      "(Iteration 3101 / 15300) loss: 2.077429\n",
      "(Iteration 3201 / 15300) loss: 2.248969\n",
      "(Iteration 3301 / 15300) loss: 2.102240\n",
      "(Iteration 3401 / 15300) loss: 2.075334\n",
      "(Iteration 3501 / 15300) loss: 2.094512\n",
      "(Iteration 3601 / 15300) loss: 2.067535\n",
      "(Iteration 3701 / 15300) loss: 2.053224\n",
      "(Iteration 3801 / 15300) loss: 2.067945\n",
      "(Epoch 5 / 20) train acc: 0.323000; val_acc: 0.334000\n",
      "(Iteration 3901 / 15300) loss: 2.060674\n",
      "(Iteration 4001 / 15300) loss: 2.093489\n",
      "(Iteration 4101 / 15300) loss: 2.003155\n",
      "(Iteration 4201 / 15300) loss: 1.949639\n",
      "(Iteration 4301 / 15300) loss: 2.007162\n",
      "(Iteration 4401 / 15300) loss: 2.052750\n",
      "(Iteration 4501 / 15300) loss: 2.012734\n",
      "(Epoch 6 / 20) train acc: 0.371000; val_acc: 0.363000\n",
      "(Iteration 4601 / 15300) loss: 1.970203\n",
      "(Iteration 4701 / 15300) loss: 1.969382\n",
      "(Iteration 4801 / 15300) loss: 2.024810\n",
      "(Iteration 4901 / 15300) loss: 2.142304\n",
      "(Iteration 5001 / 15300) loss: 2.082761\n",
      "(Iteration 5101 / 15300) loss: 2.028185\n",
      "(Iteration 5201 / 15300) loss: 2.034831\n",
      "(Iteration 5301 / 15300) loss: 1.977304\n",
      "(Epoch 7 / 20) train acc: 0.365000; val_acc: 0.366000\n",
      "(Iteration 5401 / 15300) loss: 2.105884\n",
      "(Iteration 5501 / 15300) loss: 2.045904\n",
      "(Iteration 5601 / 15300) loss: 2.078561\n",
      "(Iteration 5701 / 15300) loss: 1.916585\n",
      "(Iteration 5801 / 15300) loss: 2.151473\n",
      "(Iteration 5901 / 15300) loss: 1.932153\n",
      "(Iteration 6001 / 15300) loss: 1.963556\n",
      "(Iteration 6101 / 15300) loss: 2.038737\n",
      "(Epoch 8 / 20) train acc: 0.382000; val_acc: 0.381000\n",
      "(Iteration 6201 / 15300) loss: 2.023518\n",
      "(Iteration 6301 / 15300) loss: 1.932884\n",
      "(Iteration 6401 / 15300) loss: 2.090435\n",
      "(Iteration 6501 / 15300) loss: 1.974332\n",
      "(Iteration 6601 / 15300) loss: 1.920839\n",
      "(Iteration 6701 / 15300) loss: 2.095001\n",
      "(Iteration 6801 / 15300) loss: 1.995440\n",
      "(Epoch 9 / 20) train acc: 0.394000; val_acc: 0.395000\n",
      "(Iteration 6901 / 15300) loss: 2.049303\n",
      "(Iteration 7001 / 15300) loss: 2.018661\n",
      "(Iteration 7101 / 15300) loss: 1.957694\n",
      "(Iteration 7201 / 15300) loss: 1.964287\n",
      "(Iteration 7301 / 15300) loss: 1.993863\n",
      "(Iteration 7401 / 15300) loss: 2.013833\n",
      "(Iteration 7501 / 15300) loss: 1.979895\n",
      "(Iteration 7601 / 15300) loss: 1.889525\n",
      "(Epoch 10 / 20) train acc: 0.395000; val_acc: 0.402000\n",
      "(Iteration 7701 / 15300) loss: 2.025574\n",
      "(Iteration 7801 / 15300) loss: 2.050928\n",
      "(Iteration 7901 / 15300) loss: 1.980104\n",
      "(Iteration 8001 / 15300) loss: 2.108404\n",
      "(Iteration 8101 / 15300) loss: 1.978912\n",
      "(Iteration 8201 / 15300) loss: 1.984418\n",
      "(Iteration 8301 / 15300) loss: 2.009984\n",
      "(Iteration 8401 / 15300) loss: 1.944254\n",
      "(Epoch 11 / 20) train acc: 0.419000; val_acc: 0.390000\n",
      "(Iteration 8501 / 15300) loss: 1.961711\n",
      "(Iteration 8601 / 15300) loss: 1.943490\n",
      "(Iteration 8701 / 15300) loss: 1.858113\n",
      "(Iteration 8801 / 15300) loss: 2.110248\n",
      "(Iteration 8901 / 15300) loss: 2.047385\n",
      "(Iteration 9001 / 15300) loss: 1.974245\n",
      "(Iteration 9101 / 15300) loss: 1.898015\n",
      "(Epoch 12 / 20) train acc: 0.447000; val_acc: 0.398000\n",
      "(Iteration 9201 / 15300) loss: 1.894500\n",
      "(Iteration 9301 / 15300) loss: 1.982685\n",
      "(Iteration 9401 / 15300) loss: 2.015305\n",
      "(Iteration 9501 / 15300) loss: 1.934357\n",
      "(Iteration 9601 / 15300) loss: 1.856886\n",
      "(Iteration 9701 / 15300) loss: 2.081151\n",
      "(Iteration 9801 / 15300) loss: 1.992203\n",
      "(Iteration 9901 / 15300) loss: 2.054495\n",
      "(Epoch 13 / 20) train acc: 0.410000; val_acc: 0.401000\n",
      "(Iteration 10001 / 15300) loss: 1.976960\n",
      "(Iteration 10101 / 15300) loss: 1.969270\n",
      "(Iteration 10201 / 15300) loss: 1.966776\n",
      "(Iteration 10301 / 15300) loss: 1.842374\n",
      "(Iteration 10401 / 15300) loss: 2.003929\n",
      "(Iteration 10501 / 15300) loss: 2.053681\n",
      "(Iteration 10601 / 15300) loss: 1.978224\n",
      "(Iteration 10701 / 15300) loss: 2.045288\n",
      "(Epoch 14 / 20) train acc: 0.416000; val_acc: 0.408000\n",
      "(Iteration 10801 / 15300) loss: 1.978674\n",
      "(Iteration 10901 / 15300) loss: 2.003200\n",
      "(Iteration 11001 / 15300) loss: 1.931776\n",
      "(Iteration 11101 / 15300) loss: 1.865884\n",
      "(Iteration 11201 / 15300) loss: 2.019808\n",
      "(Iteration 11301 / 15300) loss: 2.060085\n",
      "(Iteration 11401 / 15300) loss: 2.140241\n",
      "(Epoch 15 / 20) train acc: 0.403000; val_acc: 0.417000\n",
      "(Iteration 11501 / 15300) loss: 2.083566\n",
      "(Iteration 11601 / 15300) loss: 2.011319\n",
      "(Iteration 11701 / 15300) loss: 2.126491\n",
      "(Iteration 11801 / 15300) loss: 2.033778\n",
      "(Iteration 11901 / 15300) loss: 1.989727\n",
      "(Iteration 12001 / 15300) loss: 1.936943\n",
      "(Iteration 12101 / 15300) loss: 1.951231\n",
      "(Iteration 12201 / 15300) loss: 1.903750\n",
      "(Epoch 16 / 20) train acc: 0.422000; val_acc: 0.407000\n",
      "(Iteration 12301 / 15300) loss: 2.058831\n",
      "(Iteration 12401 / 15300) loss: 2.069791\n",
      "(Iteration 12501 / 15300) loss: 1.951591\n",
      "(Iteration 12601 / 15300) loss: 1.960763\n",
      "(Iteration 12701 / 15300) loss: 1.878108\n",
      "(Iteration 12801 / 15300) loss: 2.040229\n",
      "(Iteration 12901 / 15300) loss: 1.970267\n",
      "(Iteration 13001 / 15300) loss: 2.054576\n",
      "(Epoch 17 / 20) train acc: 0.454000; val_acc: 0.412000\n",
      "(Iteration 13101 / 15300) loss: 1.921912\n",
      "(Iteration 13201 / 15300) loss: 1.987125\n",
      "(Iteration 13301 / 15300) loss: 1.907964\n",
      "(Iteration 13401 / 15300) loss: 2.078161\n",
      "(Iteration 13501 / 15300) loss: 2.048388\n",
      "(Iteration 13601 / 15300) loss: 1.974978\n",
      "(Iteration 13701 / 15300) loss: 1.986818\n",
      "(Epoch 18 / 20) train acc: 0.442000; val_acc: 0.417000\n",
      "(Iteration 13801 / 15300) loss: 2.070475\n",
      "(Iteration 13901 / 15300) loss: 2.057455\n",
      "(Iteration 14001 / 15300) loss: 1.834730\n",
      "(Iteration 14101 / 15300) loss: 1.968862\n",
      "(Iteration 14201 / 15300) loss: 1.917524\n",
      "(Iteration 14301 / 15300) loss: 1.889591\n",
      "(Iteration 14401 / 15300) loss: 1.860591\n",
      "(Iteration 14501 / 15300) loss: 1.922637\n",
      "(Epoch 19 / 20) train acc: 0.441000; val_acc: 0.411000\n",
      "(Iteration 14601 / 15300) loss: 1.871099\n",
      "(Iteration 14701 / 15300) loss: 1.848431\n",
      "(Iteration 14801 / 15300) loss: 1.987540\n",
      "(Iteration 14901 / 15300) loss: 1.887962\n",
      "(Iteration 15001 / 15300) loss: 2.014084\n",
      "(Iteration 15101 / 15300) loss: 1.969487\n",
      "(Iteration 15201 / 15300) loss: 2.161070\n",
      "(Epoch 20 / 20) train acc: 0.421000; val_acc: 0.414000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.01, 'num_epochs': 20, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 30620) loss: 2.309114\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.091000\n",
      "(Iteration 101 / 30620) loss: 2.306934\n",
      "(Iteration 201 / 30620) loss: 2.309122\n",
      "(Iteration 301 / 30620) loss: 2.307726\n",
      "(Iteration 401 / 30620) loss: 2.306166\n",
      "(Iteration 501 / 30620) loss: 2.299894\n",
      "(Iteration 601 / 30620) loss: 2.302681\n",
      "(Iteration 701 / 30620) loss: 2.301083\n",
      "(Iteration 801 / 30620) loss: 2.299481\n",
      "(Iteration 901 / 30620) loss: 2.299457\n",
      "(Iteration 1001 / 30620) loss: 2.297443\n",
      "(Iteration 1101 / 30620) loss: 2.277442\n",
      "(Iteration 1201 / 30620) loss: 2.278578\n",
      "(Iteration 1301 / 30620) loss: 2.251566\n",
      "(Iteration 1401 / 30620) loss: 2.252507\n",
      "(Iteration 1501 / 30620) loss: 2.236804\n",
      "(Epoch 1 / 20) train acc: 0.217000; val_acc: 0.230000\n",
      "(Iteration 1601 / 30620) loss: 2.246515\n",
      "(Iteration 1701 / 30620) loss: 2.203025\n",
      "(Iteration 1801 / 30620) loss: 2.165114\n",
      "(Iteration 1901 / 30620) loss: 2.178182\n",
      "(Iteration 2001 / 30620) loss: 2.190226\n",
      "(Iteration 2101 / 30620) loss: 2.164957\n",
      "(Iteration 2201 / 30620) loss: 2.174408\n",
      "(Iteration 2301 / 30620) loss: 2.098608\n",
      "(Iteration 2401 / 30620) loss: 2.232085\n",
      "(Iteration 2501 / 30620) loss: 2.039162\n",
      "(Iteration 2601 / 30620) loss: 1.895989\n",
      "(Iteration 2701 / 30620) loss: 2.079053\n",
      "(Iteration 2801 / 30620) loss: 2.144336\n",
      "(Iteration 2901 / 30620) loss: 2.013662\n",
      "(Iteration 3001 / 30620) loss: 2.204205\n",
      "(Epoch 2 / 20) train acc: 0.305000; val_acc: 0.323000\n",
      "(Iteration 3101 / 30620) loss: 2.066546\n",
      "(Iteration 3201 / 30620) loss: 1.988621\n",
      "(Iteration 3301 / 30620) loss: 2.027271\n",
      "(Iteration 3401 / 30620) loss: 2.114171\n",
      "(Iteration 3501 / 30620) loss: 2.110902\n",
      "(Iteration 3601 / 30620) loss: 2.189225\n",
      "(Iteration 3701 / 30620) loss: 2.149704\n",
      "(Iteration 3801 / 30620) loss: 2.080444\n",
      "(Iteration 3901 / 30620) loss: 2.005190\n",
      "(Iteration 4001 / 30620) loss: 2.007194\n",
      "(Iteration 4101 / 30620) loss: 1.958924\n",
      "(Iteration 4201 / 30620) loss: 2.106211\n",
      "(Iteration 4301 / 30620) loss: 1.969429\n",
      "(Iteration 4401 / 30620) loss: 1.875107\n",
      "(Iteration 4501 / 30620) loss: 1.926625\n",
      "(Epoch 3 / 20) train acc: 0.365000; val_acc: 0.375000\n",
      "(Iteration 4601 / 30620) loss: 2.006832\n",
      "(Iteration 4701 / 30620) loss: 1.931712\n",
      "(Iteration 4801 / 30620) loss: 2.079366\n",
      "(Iteration 4901 / 30620) loss: 1.894160\n",
      "(Iteration 5001 / 30620) loss: 2.055322\n",
      "(Iteration 5101 / 30620) loss: 2.065687\n",
      "(Iteration 5201 / 30620) loss: 1.887495\n",
      "(Iteration 5301 / 30620) loss: 1.972892\n",
      "(Iteration 5401 / 30620) loss: 1.995021\n",
      "(Iteration 5501 / 30620) loss: 2.110011\n",
      "(Iteration 5601 / 30620) loss: 2.056621\n",
      "(Iteration 5701 / 30620) loss: 2.016620\n",
      "(Iteration 5801 / 30620) loss: 1.859044\n",
      "(Iteration 5901 / 30620) loss: 2.033567\n",
      "(Iteration 6001 / 30620) loss: 2.054338\n",
      "(Iteration 6101 / 30620) loss: 1.913544\n",
      "(Epoch 4 / 20) train acc: 0.454000; val_acc: 0.401000\n",
      "(Iteration 6201 / 30620) loss: 1.864214\n",
      "(Iteration 6301 / 30620) loss: 2.033960\n",
      "(Iteration 6401 / 30620) loss: 2.017320\n",
      "(Iteration 6501 / 30620) loss: 2.074159\n",
      "(Iteration 6601 / 30620) loss: 1.855979\n",
      "(Iteration 6701 / 30620) loss: 1.999670\n",
      "(Iteration 6801 / 30620) loss: 1.847825\n",
      "(Iteration 6901 / 30620) loss: 1.961422\n",
      "(Iteration 7001 / 30620) loss: 1.909911\n",
      "(Iteration 7101 / 30620) loss: 2.008724\n",
      "(Iteration 7201 / 30620) loss: 2.037349\n",
      "(Iteration 7301 / 30620) loss: 1.930852\n",
      "(Iteration 7401 / 30620) loss: 2.048377\n",
      "(Iteration 7501 / 30620) loss: 2.000235\n",
      "(Iteration 7601 / 30620) loss: 1.811317\n",
      "(Epoch 5 / 20) train acc: 0.413000; val_acc: 0.399000\n",
      "(Iteration 7701 / 30620) loss: 1.950641\n",
      "(Iteration 7801 / 30620) loss: 2.071527\n",
      "(Iteration 7901 / 30620) loss: 2.062239\n",
      "(Iteration 8001 / 30620) loss: 1.887673\n",
      "(Iteration 8101 / 30620) loss: 2.076061\n",
      "(Iteration 8201 / 30620) loss: 2.078590\n",
      "(Iteration 8301 / 30620) loss: 1.974699\n",
      "(Iteration 8401 / 30620) loss: 2.169699\n",
      "(Iteration 8501 / 30620) loss: 1.944412\n",
      "(Iteration 8601 / 30620) loss: 1.849451\n",
      "(Iteration 8701 / 30620) loss: 2.112574\n",
      "(Iteration 8801 / 30620) loss: 2.027849\n",
      "(Iteration 8901 / 30620) loss: 1.983566\n",
      "(Iteration 9001 / 30620) loss: 2.000559\n",
      "(Iteration 9101 / 30620) loss: 1.985747\n",
      "(Epoch 6 / 20) train acc: 0.421000; val_acc: 0.410000\n",
      "(Iteration 9201 / 30620) loss: 1.856442\n",
      "(Iteration 9301 / 30620) loss: 1.979570\n",
      "(Iteration 9401 / 30620) loss: 2.045447\n",
      "(Iteration 9501 / 30620) loss: 2.070737\n",
      "(Iteration 9601 / 30620) loss: 2.091919\n",
      "(Iteration 9701 / 30620) loss: 2.081572\n",
      "(Iteration 9801 / 30620) loss: 1.981417\n",
      "(Iteration 9901 / 30620) loss: 2.036569\n",
      "(Iteration 10001 / 30620) loss: 2.045800\n",
      "(Iteration 10101 / 30620) loss: 1.886524\n",
      "(Iteration 10201 / 30620) loss: 1.781879\n",
      "(Iteration 10301 / 30620) loss: 2.074874\n",
      "(Iteration 10401 / 30620) loss: 1.988736\n",
      "(Iteration 10501 / 30620) loss: 1.897834\n",
      "(Iteration 10601 / 30620) loss: 2.144203\n",
      "(Iteration 10701 / 30620) loss: 1.754822\n",
      "(Epoch 7 / 20) train acc: 0.427000; val_acc: 0.420000\n",
      "(Iteration 10801 / 30620) loss: 1.830615\n",
      "(Iteration 10901 / 30620) loss: 2.033511\n",
      "(Iteration 11001 / 30620) loss: 1.943026\n",
      "(Iteration 11101 / 30620) loss: 2.034101\n",
      "(Iteration 11201 / 30620) loss: 2.034261\n",
      "(Iteration 11301 / 30620) loss: 1.889434\n",
      "(Iteration 11401 / 30620) loss: 1.833985\n",
      "(Iteration 11501 / 30620) loss: 2.127118\n",
      "(Iteration 11601 / 30620) loss: 1.899150\n",
      "(Iteration 11701 / 30620) loss: 1.824704\n",
      "(Iteration 11801 / 30620) loss: 2.108706\n",
      "(Iteration 11901 / 30620) loss: 2.009074\n",
      "(Iteration 12001 / 30620) loss: 2.072131\n",
      "(Iteration 12101 / 30620) loss: 2.202159\n",
      "(Iteration 12201 / 30620) loss: 1.964862\n",
      "(Epoch 8 / 20) train acc: 0.466000; val_acc: 0.418000\n",
      "(Iteration 12301 / 30620) loss: 1.751001\n",
      "(Iteration 12401 / 30620) loss: 1.928185\n",
      "(Iteration 12501 / 30620) loss: 1.977169\n",
      "(Iteration 12601 / 30620) loss: 1.775308\n",
      "(Iteration 12701 / 30620) loss: 2.051383\n",
      "(Iteration 12801 / 30620) loss: 2.027198\n",
      "(Iteration 12901 / 30620) loss: 1.906643\n",
      "(Iteration 13001 / 30620) loss: 1.862629\n",
      "(Iteration 13101 / 30620) loss: 1.891735\n",
      "(Iteration 13201 / 30620) loss: 1.957416\n",
      "(Iteration 13301 / 30620) loss: 1.919884\n",
      "(Iteration 13401 / 30620) loss: 1.976096\n",
      "(Iteration 13501 / 30620) loss: 1.870390\n",
      "(Iteration 13601 / 30620) loss: 1.793080\n",
      "(Iteration 13701 / 30620) loss: 1.824234\n",
      "(Epoch 9 / 20) train acc: 0.437000; val_acc: 0.420000\n",
      "(Iteration 13801 / 30620) loss: 1.956640\n",
      "(Iteration 13901 / 30620) loss: 1.886765\n",
      "(Iteration 14001 / 30620) loss: 1.900156\n",
      "(Iteration 14101 / 30620) loss: 2.123333\n",
      "(Iteration 14201 / 30620) loss: 1.822767\n",
      "(Iteration 14301 / 30620) loss: 1.976695\n",
      "(Iteration 14401 / 30620) loss: 1.974988\n",
      "(Iteration 14501 / 30620) loss: 2.094382\n",
      "(Iteration 14601 / 30620) loss: 1.763947\n",
      "(Iteration 14701 / 30620) loss: 2.090956\n",
      "(Iteration 14801 / 30620) loss: 1.846972\n",
      "(Iteration 14901 / 30620) loss: 2.080943\n",
      "(Iteration 15001 / 30620) loss: 2.045265\n",
      "(Iteration 15101 / 30620) loss: 1.915027\n",
      "(Iteration 15201 / 30620) loss: 1.942542\n",
      "(Iteration 15301 / 30620) loss: 1.956242\n",
      "(Epoch 10 / 20) train acc: 0.430000; val_acc: 0.430000\n",
      "(Iteration 15401 / 30620) loss: 1.996689\n",
      "(Iteration 15501 / 30620) loss: 1.797317\n",
      "(Iteration 15601 / 30620) loss: 2.090209\n",
      "(Iteration 15701 / 30620) loss: 1.956079\n",
      "(Iteration 15801 / 30620) loss: 1.940135\n",
      "(Iteration 15901 / 30620) loss: 1.865836\n",
      "(Iteration 16001 / 30620) loss: 1.983911\n",
      "(Iteration 16101 / 30620) loss: 1.944343\n",
      "(Iteration 16201 / 30620) loss: 2.236106\n",
      "(Iteration 16301 / 30620) loss: 2.084195\n",
      "(Iteration 16401 / 30620) loss: 1.925299\n",
      "(Iteration 16501 / 30620) loss: 1.879336\n",
      "(Iteration 16601 / 30620) loss: 1.951732\n",
      "(Iteration 16701 / 30620) loss: 1.788027\n",
      "(Iteration 16801 / 30620) loss: 1.980978\n",
      "(Epoch 11 / 20) train acc: 0.444000; val_acc: 0.428000\n",
      "(Iteration 16901 / 30620) loss: 1.961831\n",
      "(Iteration 17001 / 30620) loss: 1.997592\n",
      "(Iteration 17101 / 30620) loss: 1.922911\n",
      "(Iteration 17201 / 30620) loss: 1.899815\n",
      "(Iteration 17301 / 30620) loss: 1.932455\n",
      "(Iteration 17401 / 30620) loss: 1.865124\n",
      "(Iteration 17501 / 30620) loss: 1.924668\n",
      "(Iteration 17601 / 30620) loss: 1.953763\n",
      "(Iteration 17701 / 30620) loss: 1.847244\n",
      "(Iteration 17801 / 30620) loss: 2.110307\n",
      "(Iteration 17901 / 30620) loss: 1.915324\n",
      "(Iteration 18001 / 30620) loss: 2.068119\n",
      "(Iteration 18101 / 30620) loss: 1.931915\n",
      "(Iteration 18201 / 30620) loss: 1.748851\n",
      "(Iteration 18301 / 30620) loss: 1.901343\n",
      "(Epoch 12 / 20) train acc: 0.453000; val_acc: 0.429000\n",
      "(Iteration 18401 / 30620) loss: 1.898223\n",
      "(Iteration 18501 / 30620) loss: 2.012645\n",
      "(Iteration 18601 / 30620) loss: 1.963427\n",
      "(Iteration 18701 / 30620) loss: 1.947152\n",
      "(Iteration 18801 / 30620) loss: 1.878418\n",
      "(Iteration 18901 / 30620) loss: 1.911668\n",
      "(Iteration 19001 / 30620) loss: 1.902289\n",
      "(Iteration 19101 / 30620) loss: 1.983556\n",
      "(Iteration 19201 / 30620) loss: 1.964538\n",
      "(Iteration 19301 / 30620) loss: 1.975092\n",
      "(Iteration 19401 / 30620) loss: 1.995535\n",
      "(Iteration 19501 / 30620) loss: 1.764445\n",
      "(Iteration 19601 / 30620) loss: 1.992192\n",
      "(Iteration 19701 / 30620) loss: 1.949116\n",
      "(Iteration 19801 / 30620) loss: 1.807423\n",
      "(Iteration 19901 / 30620) loss: 2.151140\n",
      "(Epoch 13 / 20) train acc: 0.426000; val_acc: 0.440000\n",
      "(Iteration 20001 / 30620) loss: 2.024991\n",
      "(Iteration 20101 / 30620) loss: 2.076324\n",
      "(Iteration 20201 / 30620) loss: 2.060479\n",
      "(Iteration 20301 / 30620) loss: 1.874694\n",
      "(Iteration 20401 / 30620) loss: 1.776326\n",
      "(Iteration 20501 / 30620) loss: 2.028667\n",
      "(Iteration 20601 / 30620) loss: 1.838910\n",
      "(Iteration 20701 / 30620) loss: 1.964466\n",
      "(Iteration 20801 / 30620) loss: 1.845196\n",
      "(Iteration 20901 / 30620) loss: 1.956980\n",
      "(Iteration 21001 / 30620) loss: 1.924584\n",
      "(Iteration 21101 / 30620) loss: 1.949062\n",
      "(Iteration 21201 / 30620) loss: 2.052100\n",
      "(Iteration 21301 / 30620) loss: 1.958603\n",
      "(Iteration 21401 / 30620) loss: 1.896813\n",
      "(Epoch 14 / 20) train acc: 0.426000; val_acc: 0.445000\n",
      "(Iteration 21501 / 30620) loss: 2.008876\n",
      "(Iteration 21601 / 30620) loss: 2.022504\n",
      "(Iteration 21701 / 30620) loss: 1.759091\n",
      "(Iteration 21801 / 30620) loss: 2.004349\n",
      "(Iteration 21901 / 30620) loss: 1.788072\n",
      "(Iteration 22001 / 30620) loss: 1.943248\n",
      "(Iteration 22101 / 30620) loss: 2.114505\n",
      "(Iteration 22201 / 30620) loss: 1.868703\n",
      "(Iteration 22301 / 30620) loss: 2.061588\n",
      "(Iteration 22401 / 30620) loss: 2.074568\n",
      "(Iteration 22501 / 30620) loss: 1.932407\n",
      "(Iteration 22601 / 30620) loss: 1.980127\n",
      "(Iteration 22701 / 30620) loss: 1.904260\n",
      "(Iteration 22801 / 30620) loss: 1.872164\n",
      "(Iteration 22901 / 30620) loss: 1.949160\n",
      "(Epoch 15 / 20) train acc: 0.426000; val_acc: 0.425000\n",
      "(Iteration 23001 / 30620) loss: 2.001087\n",
      "(Iteration 23101 / 30620) loss: 1.836675\n",
      "(Iteration 23201 / 30620) loss: 2.070835\n",
      "(Iteration 23301 / 30620) loss: 2.029076\n",
      "(Iteration 23401 / 30620) loss: 2.024203\n",
      "(Iteration 23501 / 30620) loss: 1.880334\n",
      "(Iteration 23601 / 30620) loss: 2.050387\n",
      "(Iteration 23701 / 30620) loss: 1.948246\n",
      "(Iteration 23801 / 30620) loss: 2.142209\n",
      "(Iteration 23901 / 30620) loss: 2.088991\n",
      "(Iteration 24001 / 30620) loss: 2.051231\n",
      "(Iteration 24101 / 30620) loss: 1.850495\n",
      "(Iteration 24201 / 30620) loss: 1.878481\n",
      "(Iteration 24301 / 30620) loss: 1.925123\n",
      "(Iteration 24401 / 30620) loss: 2.000329\n",
      "(Epoch 16 / 20) train acc: 0.467000; val_acc: 0.435000\n",
      "(Iteration 24501 / 30620) loss: 1.927030\n",
      "(Iteration 24601 / 30620) loss: 1.895126\n",
      "(Iteration 24701 / 30620) loss: 2.014496\n",
      "(Iteration 24801 / 30620) loss: 1.996399\n",
      "(Iteration 24901 / 30620) loss: 1.939844\n",
      "(Iteration 25001 / 30620) loss: 2.022134\n",
      "(Iteration 25101 / 30620) loss: 1.989697\n",
      "(Iteration 25201 / 30620) loss: 1.852242\n",
      "(Iteration 25301 / 30620) loss: 1.929010\n",
      "(Iteration 25401 / 30620) loss: 1.862536\n",
      "(Iteration 25501 / 30620) loss: 1.978877\n",
      "(Iteration 25601 / 30620) loss: 2.070911\n",
      "(Iteration 25701 / 30620) loss: 2.047401\n",
      "(Iteration 25801 / 30620) loss: 2.082689\n",
      "(Iteration 25901 / 30620) loss: 1.981656\n",
      "(Iteration 26001 / 30620) loss: 2.131395\n",
      "(Epoch 17 / 20) train acc: 0.465000; val_acc: 0.441000\n",
      "(Iteration 26101 / 30620) loss: 2.009659\n",
      "(Iteration 26201 / 30620) loss: 1.979683\n",
      "(Iteration 26301 / 30620) loss: 2.191094\n",
      "(Iteration 26401 / 30620) loss: 1.810407\n",
      "(Iteration 26501 / 30620) loss: 1.868466\n",
      "(Iteration 26601 / 30620) loss: 2.170550\n",
      "(Iteration 26701 / 30620) loss: 1.881424\n",
      "(Iteration 26801 / 30620) loss: 1.952410\n",
      "(Iteration 26901 / 30620) loss: 1.812582\n",
      "(Iteration 27001 / 30620) loss: 1.870548\n",
      "(Iteration 27101 / 30620) loss: 2.102772\n",
      "(Iteration 27201 / 30620) loss: 1.864036\n",
      "(Iteration 27301 / 30620) loss: 1.943412\n",
      "(Iteration 27401 / 30620) loss: 1.979501\n",
      "(Iteration 27501 / 30620) loss: 2.009908\n",
      "(Epoch 18 / 20) train acc: 0.411000; val_acc: 0.442000\n",
      "(Iteration 27601 / 30620) loss: 1.961182\n",
      "(Iteration 27701 / 30620) loss: 1.932533\n",
      "(Iteration 27801 / 30620) loss: 1.989395\n",
      "(Iteration 27901 / 30620) loss: 1.717518\n",
      "(Iteration 28001 / 30620) loss: 2.090205\n",
      "(Iteration 28101 / 30620) loss: 1.781199\n",
      "(Iteration 28201 / 30620) loss: 1.935369\n",
      "(Iteration 28301 / 30620) loss: 1.950428\n",
      "(Iteration 28401 / 30620) loss: 1.895716\n",
      "(Iteration 28501 / 30620) loss: 2.140395\n",
      "(Iteration 28601 / 30620) loss: 1.945038\n",
      "(Iteration 28701 / 30620) loss: 1.800749\n",
      "(Iteration 28801 / 30620) loss: 1.921555\n",
      "(Iteration 28901 / 30620) loss: 2.041664\n",
      "(Iteration 29001 / 30620) loss: 1.875144\n",
      "(Epoch 19 / 20) train acc: 0.453000; val_acc: 0.439000\n",
      "(Iteration 29101 / 30620) loss: 1.808361\n",
      "(Iteration 29201 / 30620) loss: 1.927710\n",
      "(Iteration 29301 / 30620) loss: 1.949432\n",
      "(Iteration 29401 / 30620) loss: 2.085620\n",
      "(Iteration 29501 / 30620) loss: 1.978057\n",
      "(Iteration 29601 / 30620) loss: 1.878538\n",
      "(Iteration 29701 / 30620) loss: 2.094615\n",
      "(Iteration 29801 / 30620) loss: 1.919790\n",
      "(Iteration 29901 / 30620) loss: 1.894242\n",
      "(Iteration 30001 / 30620) loss: 2.011461\n",
      "(Iteration 30101 / 30620) loss: 2.071885\n",
      "(Iteration 30201 / 30620) loss: 1.921899\n",
      "(Iteration 30301 / 30620) loss: 2.019421\n",
      "(Iteration 30401 / 30620) loss: 1.809703\n",
      "(Iteration 30501 / 30620) loss: 1.988495\n",
      "(Iteration 30601 / 30620) loss: 2.004325\n",
      "(Epoch 20 / 20) train acc: 0.456000; val_acc: 0.437000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.01, 'num_epochs': 20, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 15300) loss: 2.303232\n",
      "(Epoch 0 / 20) train acc: 0.086000; val_acc: 0.078000\n",
      "(Iteration 101 / 15300) loss: 2.302895\n",
      "(Iteration 201 / 15300) loss: 2.302759\n",
      "(Iteration 301 / 15300) loss: 2.302290\n",
      "(Iteration 401 / 15300) loss: 2.300426\n",
      "(Iteration 501 / 15300) loss: 2.298846\n",
      "(Iteration 601 / 15300) loss: 2.299151\n",
      "(Iteration 701 / 15300) loss: 2.288566\n",
      "(Epoch 1 / 20) train acc: 0.231000; val_acc: 0.246000\n",
      "(Iteration 801 / 15300) loss: 2.275560\n",
      "(Iteration 901 / 15300) loss: 2.251442\n",
      "(Iteration 1001 / 15300) loss: 2.233503\n",
      "(Iteration 1101 / 15300) loss: 2.132012\n",
      "(Iteration 1201 / 15300) loss: 2.144402\n",
      "(Iteration 1301 / 15300) loss: 2.054679\n",
      "(Iteration 1401 / 15300) loss: 2.025393\n",
      "(Iteration 1501 / 15300) loss: 1.973083\n",
      "(Epoch 2 / 20) train acc: 0.304000; val_acc: 0.299000\n",
      "(Iteration 1601 / 15300) loss: 2.071073\n",
      "(Iteration 1701 / 15300) loss: 2.067106\n",
      "(Iteration 1801 / 15300) loss: 1.940967\n",
      "(Iteration 1901 / 15300) loss: 1.976366\n",
      "(Iteration 2001 / 15300) loss: 1.831527\n",
      "(Iteration 2101 / 15300) loss: 1.782679\n",
      "(Iteration 2201 / 15300) loss: 1.695115\n",
      "(Epoch 3 / 20) train acc: 0.398000; val_acc: 0.374000\n",
      "(Iteration 2301 / 15300) loss: 1.766424\n",
      "(Iteration 2401 / 15300) loss: 1.878939\n",
      "(Iteration 2501 / 15300) loss: 1.692887\n",
      "(Iteration 2601 / 15300) loss: 1.796443\n",
      "(Iteration 2701 / 15300) loss: 1.826891\n",
      "(Iteration 2801 / 15300) loss: 1.669260\n",
      "(Iteration 2901 / 15300) loss: 1.635843\n",
      "(Iteration 3001 / 15300) loss: 1.730023\n",
      "(Epoch 4 / 20) train acc: 0.416000; val_acc: 0.412000\n",
      "(Iteration 3101 / 15300) loss: 1.583001\n",
      "(Iteration 3201 / 15300) loss: 1.767151\n",
      "(Iteration 3301 / 15300) loss: 1.649328\n",
      "(Iteration 3401 / 15300) loss: 1.613649\n",
      "(Iteration 3501 / 15300) loss: 1.564992\n",
      "(Iteration 3601 / 15300) loss: 1.819522\n",
      "(Iteration 3701 / 15300) loss: 1.654286\n",
      "(Iteration 3801 / 15300) loss: 1.406826\n",
      "(Epoch 5 / 20) train acc: 0.432000; val_acc: 0.442000\n",
      "(Iteration 3901 / 15300) loss: 1.446202\n",
      "(Iteration 4001 / 15300) loss: 1.495953\n",
      "(Iteration 4101 / 15300) loss: 1.822860\n",
      "(Iteration 4201 / 15300) loss: 1.569809\n",
      "(Iteration 4301 / 15300) loss: 1.683565\n",
      "(Iteration 4401 / 15300) loss: 1.412614\n",
      "(Iteration 4501 / 15300) loss: 1.566738\n",
      "(Epoch 6 / 20) train acc: 0.466000; val_acc: 0.454000\n",
      "(Iteration 4601 / 15300) loss: 1.573805\n",
      "(Iteration 4701 / 15300) loss: 1.454721\n",
      "(Iteration 4801 / 15300) loss: 1.525571\n",
      "(Iteration 4901 / 15300) loss: 1.511860\n",
      "(Iteration 5001 / 15300) loss: 1.324432\n",
      "(Iteration 5101 / 15300) loss: 1.537903\n",
      "(Iteration 5201 / 15300) loss: 1.653658\n",
      "(Iteration 5301 / 15300) loss: 1.531148\n",
      "(Epoch 7 / 20) train acc: 0.469000; val_acc: 0.489000\n",
      "(Iteration 5401 / 15300) loss: 1.519377\n",
      "(Iteration 5501 / 15300) loss: 1.521362\n",
      "(Iteration 5601 / 15300) loss: 1.577591\n",
      "(Iteration 5701 / 15300) loss: 1.446031\n",
      "(Iteration 5801 / 15300) loss: 1.445631\n",
      "(Iteration 5901 / 15300) loss: 1.535159\n",
      "(Iteration 6001 / 15300) loss: 1.506868\n",
      "(Iteration 6101 / 15300) loss: 1.437374\n",
      "(Epoch 8 / 20) train acc: 0.489000; val_acc: 0.487000\n",
      "(Iteration 6201 / 15300) loss: 1.576101\n",
      "(Iteration 6301 / 15300) loss: 1.594016\n",
      "(Iteration 6401 / 15300) loss: 1.499344\n",
      "(Iteration 6501 / 15300) loss: 1.461101\n",
      "(Iteration 6601 / 15300) loss: 1.750351\n",
      "(Iteration 6701 / 15300) loss: 1.434698\n",
      "(Iteration 6801 / 15300) loss: 1.498253\n",
      "(Epoch 9 / 20) train acc: 0.525000; val_acc: 0.487000\n",
      "(Iteration 6901 / 15300) loss: 1.566622\n",
      "(Iteration 7001 / 15300) loss: 1.479988\n",
      "(Iteration 7101 / 15300) loss: 1.636138\n",
      "(Iteration 7201 / 15300) loss: 1.532734\n",
      "(Iteration 7301 / 15300) loss: 1.565035\n",
      "(Iteration 7401 / 15300) loss: 1.557314\n",
      "(Iteration 7501 / 15300) loss: 1.557696\n",
      "(Iteration 7601 / 15300) loss: 1.343020\n",
      "(Epoch 10 / 20) train acc: 0.511000; val_acc: 0.498000\n",
      "(Iteration 7701 / 15300) loss: 1.496908\n",
      "(Iteration 7801 / 15300) loss: 1.280641\n",
      "(Iteration 7901 / 15300) loss: 1.703114\n",
      "(Iteration 8001 / 15300) loss: 1.658278\n",
      "(Iteration 8101 / 15300) loss: 1.454730\n",
      "(Iteration 8201 / 15300) loss: 1.359673\n",
      "(Iteration 8301 / 15300) loss: 1.366526\n",
      "(Iteration 8401 / 15300) loss: 1.623800\n",
      "(Epoch 11 / 20) train acc: 0.503000; val_acc: 0.500000\n",
      "(Iteration 8501 / 15300) loss: 1.454946\n",
      "(Iteration 8601 / 15300) loss: 1.568740\n",
      "(Iteration 8701 / 15300) loss: 1.434910\n",
      "(Iteration 8801 / 15300) loss: 1.573802\n",
      "(Iteration 8901 / 15300) loss: 1.378014\n",
      "(Iteration 9001 / 15300) loss: 1.662922\n",
      "(Iteration 9101 / 15300) loss: 1.390137\n",
      "(Epoch 12 / 20) train acc: 0.481000; val_acc: 0.498000\n",
      "(Iteration 9201 / 15300) loss: 1.586427\n",
      "(Iteration 9301 / 15300) loss: 1.356118\n",
      "(Iteration 9401 / 15300) loss: 1.575851\n",
      "(Iteration 9501 / 15300) loss: 1.417382\n",
      "(Iteration 9601 / 15300) loss: 1.468319\n",
      "(Iteration 9701 / 15300) loss: 1.357765\n",
      "(Iteration 9801 / 15300) loss: 1.488540\n",
      "(Iteration 9901 / 15300) loss: 1.434020\n",
      "(Epoch 13 / 20) train acc: 0.519000; val_acc: 0.504000\n",
      "(Iteration 10001 / 15300) loss: 1.418380\n",
      "(Iteration 10101 / 15300) loss: 1.547634\n",
      "(Iteration 10201 / 15300) loss: 1.331960\n",
      "(Iteration 10301 / 15300) loss: 1.517428\n",
      "(Iteration 10401 / 15300) loss: 1.634263\n",
      "(Iteration 10501 / 15300) loss: 1.388132\n",
      "(Iteration 10601 / 15300) loss: 1.594469\n",
      "(Iteration 10701 / 15300) loss: 1.680226\n",
      "(Epoch 14 / 20) train acc: 0.554000; val_acc: 0.503000\n",
      "(Iteration 10801 / 15300) loss: 1.559932\n",
      "(Iteration 10901 / 15300) loss: 1.669466\n",
      "(Iteration 11001 / 15300) loss: 1.623881\n",
      "(Iteration 11101 / 15300) loss: 1.525773\n",
      "(Iteration 11201 / 15300) loss: 1.453207\n",
      "(Iteration 11301 / 15300) loss: 1.425281\n",
      "(Iteration 11401 / 15300) loss: 1.630796\n",
      "(Epoch 15 / 20) train acc: 0.515000; val_acc: 0.507000\n",
      "(Iteration 11501 / 15300) loss: 1.446422\n",
      "(Iteration 11601 / 15300) loss: 1.390319\n",
      "(Iteration 11701 / 15300) loss: 1.621236\n",
      "(Iteration 11801 / 15300) loss: 1.559109\n",
      "(Iteration 11901 / 15300) loss: 1.416208\n",
      "(Iteration 12001 / 15300) loss: 1.231280\n",
      "(Iteration 12101 / 15300) loss: 1.582549\n",
      "(Iteration 12201 / 15300) loss: 1.417317\n",
      "(Epoch 16 / 20) train acc: 0.509000; val_acc: 0.508000\n",
      "(Iteration 12301 / 15300) loss: 1.538879\n",
      "(Iteration 12401 / 15300) loss: 1.578546\n",
      "(Iteration 12501 / 15300) loss: 1.718974\n",
      "(Iteration 12601 / 15300) loss: 1.856068\n",
      "(Iteration 12701 / 15300) loss: 1.474016\n",
      "(Iteration 12801 / 15300) loss: 1.517368\n",
      "(Iteration 12901 / 15300) loss: 1.364527\n",
      "(Iteration 13001 / 15300) loss: 1.330356\n",
      "(Epoch 17 / 20) train acc: 0.527000; val_acc: 0.507000\n",
      "(Iteration 13101 / 15300) loss: 1.660219\n",
      "(Iteration 13201 / 15300) loss: 1.457634\n",
      "(Iteration 13301 / 15300) loss: 1.486510\n",
      "(Iteration 13401 / 15300) loss: 1.545043\n",
      "(Iteration 13501 / 15300) loss: 1.466409\n",
      "(Iteration 13601 / 15300) loss: 1.276324\n",
      "(Iteration 13701 / 15300) loss: 1.392139\n",
      "(Epoch 18 / 20) train acc: 0.536000; val_acc: 0.508000\n",
      "(Iteration 13801 / 15300) loss: 1.345967\n",
      "(Iteration 13901 / 15300) loss: 1.393338\n",
      "(Iteration 14001 / 15300) loss: 1.484261\n",
      "(Iteration 14101 / 15300) loss: 1.354676\n",
      "(Iteration 14201 / 15300) loss: 1.175028\n",
      "(Iteration 14301 / 15300) loss: 1.683857\n",
      "(Iteration 14401 / 15300) loss: 1.369233\n",
      "(Iteration 14501 / 15300) loss: 1.435459\n",
      "(Epoch 19 / 20) train acc: 0.520000; val_acc: 0.511000\n",
      "(Iteration 14601 / 15300) loss: 1.473885\n",
      "(Iteration 14701 / 15300) loss: 1.411052\n",
      "(Iteration 14801 / 15300) loss: 1.353618\n",
      "(Iteration 14901 / 15300) loss: 1.588115\n",
      "(Iteration 15001 / 15300) loss: 1.616651\n",
      "(Iteration 15101 / 15300) loss: 1.464774\n",
      "(Iteration 15201 / 15300) loss: 1.403696\n",
      "(Epoch 20 / 20) train acc: 0.536000; val_acc: 0.511000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.01, 'num_epochs': 20, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 30620) loss: 2.303251\n",
      "(Epoch 0 / 20) train acc: 0.115000; val_acc: 0.117000\n",
      "(Iteration 101 / 30620) loss: 2.302593\n",
      "(Iteration 201 / 30620) loss: 2.301373\n",
      "(Iteration 301 / 30620) loss: 2.302808\n",
      "(Iteration 401 / 30620) loss: 2.301709\n",
      "(Iteration 501 / 30620) loss: 2.299480\n",
      "(Iteration 601 / 30620) loss: 2.296074\n",
      "(Iteration 701 / 30620) loss: 2.288606\n",
      "(Iteration 801 / 30620) loss: 2.276643\n",
      "(Iteration 901 / 30620) loss: 2.264532\n",
      "(Iteration 1001 / 30620) loss: 2.235840\n",
      "(Iteration 1101 / 30620) loss: 2.210684\n",
      "(Iteration 1201 / 30620) loss: 2.106768\n",
      "(Iteration 1301 / 30620) loss: 2.053748\n",
      "(Iteration 1401 / 30620) loss: 2.095025\n",
      "(Iteration 1501 / 30620) loss: 1.966074\n",
      "(Epoch 1 / 20) train acc: 0.308000; val_acc: 0.314000\n",
      "(Iteration 1601 / 30620) loss: 1.675087\n",
      "(Iteration 1701 / 30620) loss: 1.921135\n",
      "(Iteration 1801 / 30620) loss: 1.994326\n",
      "(Iteration 1901 / 30620) loss: 1.993814\n",
      "(Iteration 2001 / 30620) loss: 1.968885\n",
      "(Iteration 2101 / 30620) loss: 1.846097\n",
      "(Iteration 2201 / 30620) loss: 1.820430\n",
      "(Iteration 2301 / 30620) loss: 1.725982\n",
      "(Iteration 2401 / 30620) loss: 1.806987\n",
      "(Iteration 2501 / 30620) loss: 1.776512\n",
      "(Iteration 2601 / 30620) loss: 1.738786\n",
      "(Iteration 2701 / 30620) loss: 1.704158\n",
      "(Iteration 2801 / 30620) loss: 1.482967\n",
      "(Iteration 2901 / 30620) loss: 1.492772\n",
      "(Iteration 3001 / 30620) loss: 1.687332\n",
      "(Epoch 2 / 20) train acc: 0.408000; val_acc: 0.410000\n",
      "(Iteration 3101 / 30620) loss: 1.824704\n",
      "(Iteration 3201 / 30620) loss: 1.359094\n",
      "(Iteration 3301 / 30620) loss: 1.865690\n",
      "(Iteration 3401 / 30620) loss: 1.568616\n",
      "(Iteration 3501 / 30620) loss: 1.332495\n",
      "(Iteration 3601 / 30620) loss: 1.875073\n",
      "(Iteration 3701 / 30620) loss: 1.508430\n",
      "(Iteration 3801 / 30620) loss: 1.584844\n",
      "(Iteration 3901 / 30620) loss: 1.684077\n",
      "(Iteration 4001 / 30620) loss: 1.672376\n",
      "(Iteration 4101 / 30620) loss: 1.357030\n",
      "(Iteration 4201 / 30620) loss: 1.757738\n",
      "(Iteration 4301 / 30620) loss: 1.429199\n",
      "(Iteration 4401 / 30620) loss: 1.346754\n",
      "(Iteration 4501 / 30620) loss: 1.696951\n",
      "(Epoch 3 / 20) train acc: 0.495000; val_acc: 0.486000\n",
      "(Iteration 4601 / 30620) loss: 1.648306\n",
      "(Iteration 4701 / 30620) loss: 1.436068\n",
      "(Iteration 4801 / 30620) loss: 1.559751\n",
      "(Iteration 4901 / 30620) loss: 1.572308\n",
      "(Iteration 5001 / 30620) loss: 1.276827\n",
      "(Iteration 5101 / 30620) loss: 1.411429\n",
      "(Iteration 5201 / 30620) loss: 1.797804\n",
      "(Iteration 5301 / 30620) loss: 1.479195\n",
      "(Iteration 5401 / 30620) loss: 1.543771\n",
      "(Iteration 5501 / 30620) loss: 1.787885\n",
      "(Iteration 5601 / 30620) loss: 1.302985\n",
      "(Iteration 5701 / 30620) loss: 1.463531\n",
      "(Iteration 5801 / 30620) loss: 1.578426\n",
      "(Iteration 5901 / 30620) loss: 1.502585\n",
      "(Iteration 6001 / 30620) loss: 1.381399\n",
      "(Iteration 6101 / 30620) loss: 1.838335\n",
      "(Epoch 4 / 20) train acc: 0.520000; val_acc: 0.507000\n",
      "(Iteration 6201 / 30620) loss: 1.317059\n",
      "(Iteration 6301 / 30620) loss: 1.595497\n",
      "(Iteration 6401 / 30620) loss: 1.485277\n",
      "(Iteration 6501 / 30620) loss: 1.478452\n",
      "(Iteration 6601 / 30620) loss: 1.406319\n",
      "(Iteration 6701 / 30620) loss: 1.403087\n",
      "(Iteration 6801 / 30620) loss: 1.401899\n",
      "(Iteration 6901 / 30620) loss: 1.343017\n",
      "(Iteration 7001 / 30620) loss: 1.641131\n",
      "(Iteration 7101 / 30620) loss: 1.242633\n",
      "(Iteration 7201 / 30620) loss: 1.527250\n",
      "(Iteration 7301 / 30620) loss: 1.573084\n",
      "(Iteration 7401 / 30620) loss: 1.378884\n",
      "(Iteration 7501 / 30620) loss: 1.447254\n",
      "(Iteration 7601 / 30620) loss: 1.782536\n",
      "(Epoch 5 / 20) train acc: 0.523000; val_acc: 0.510000\n",
      "(Iteration 7701 / 30620) loss: 1.585145\n",
      "(Iteration 7801 / 30620) loss: 1.510969\n",
      "(Iteration 7901 / 30620) loss: 1.494272\n",
      "(Iteration 8001 / 30620) loss: 1.606997\n",
      "(Iteration 8101 / 30620) loss: 1.452461\n",
      "(Iteration 8201 / 30620) loss: 1.439028\n",
      "(Iteration 8301 / 30620) loss: 1.630216\n",
      "(Iteration 8401 / 30620) loss: 1.366830\n",
      "(Iteration 8501 / 30620) loss: 1.432512\n",
      "(Iteration 8601 / 30620) loss: 1.856116\n",
      "(Iteration 8701 / 30620) loss: 1.339963\n",
      "(Iteration 8801 / 30620) loss: 1.437296\n",
      "(Iteration 8901 / 30620) loss: 1.152323\n",
      "(Iteration 9001 / 30620) loss: 1.368904\n",
      "(Iteration 9101 / 30620) loss: 1.462939\n",
      "(Epoch 6 / 20) train acc: 0.517000; val_acc: 0.511000\n",
      "(Iteration 9201 / 30620) loss: 1.468201\n",
      "(Iteration 9301 / 30620) loss: 1.590803\n",
      "(Iteration 9401 / 30620) loss: 1.521899\n",
      "(Iteration 9501 / 30620) loss: 1.554121\n",
      "(Iteration 9601 / 30620) loss: 1.387544\n",
      "(Iteration 9701 / 30620) loss: 1.615666\n",
      "(Iteration 9801 / 30620) loss: 1.855999\n",
      "(Iteration 9901 / 30620) loss: 1.422778\n",
      "(Iteration 10001 / 30620) loss: 1.524180\n",
      "(Iteration 10101 / 30620) loss: 1.319057\n",
      "(Iteration 10201 / 30620) loss: 1.499612\n",
      "(Iteration 10301 / 30620) loss: 1.362218\n",
      "(Iteration 10401 / 30620) loss: 1.487975\n",
      "(Iteration 10501 / 30620) loss: 1.607481\n",
      "(Iteration 10601 / 30620) loss: 1.297856\n",
      "(Iteration 10701 / 30620) loss: 1.540953\n",
      "(Epoch 7 / 20) train acc: 0.513000; val_acc: 0.505000\n",
      "(Iteration 10801 / 30620) loss: 1.418599\n",
      "(Iteration 10901 / 30620) loss: 1.174503\n",
      "(Iteration 11001 / 30620) loss: 1.358006\n",
      "(Iteration 11101 / 30620) loss: 1.355339\n",
      "(Iteration 11201 / 30620) loss: 1.487785\n",
      "(Iteration 11301 / 30620) loss: 1.454823\n",
      "(Iteration 11401 / 30620) loss: 1.293276\n",
      "(Iteration 11501 / 30620) loss: 1.354501\n",
      "(Iteration 11601 / 30620) loss: 1.445402\n",
      "(Iteration 11701 / 30620) loss: 1.286656\n",
      "(Iteration 11801 / 30620) loss: 1.419988\n",
      "(Iteration 11901 / 30620) loss: 1.284731\n",
      "(Iteration 12001 / 30620) loss: 1.502510\n",
      "(Iteration 12101 / 30620) loss: 1.232105\n",
      "(Iteration 12201 / 30620) loss: 1.669366\n",
      "(Epoch 8 / 20) train acc: 0.544000; val_acc: 0.506000\n",
      "(Iteration 12301 / 30620) loss: 1.686730\n",
      "(Iteration 12401 / 30620) loss: 1.628568\n",
      "(Iteration 12501 / 30620) loss: 1.324694\n",
      "(Iteration 12601 / 30620) loss: 1.450598\n",
      "(Iteration 12701 / 30620) loss: 1.478239\n",
      "(Iteration 12801 / 30620) loss: 1.628443\n",
      "(Iteration 12901 / 30620) loss: 1.382664\n",
      "(Iteration 13001 / 30620) loss: 1.596880\n",
      "(Iteration 13101 / 30620) loss: 1.562205\n",
      "(Iteration 13201 / 30620) loss: 1.274129\n",
      "(Iteration 13301 / 30620) loss: 1.159005\n",
      "(Iteration 13401 / 30620) loss: 1.498262\n",
      "(Iteration 13501 / 30620) loss: 1.334674\n",
      "(Iteration 13601 / 30620) loss: 1.390402\n",
      "(Iteration 13701 / 30620) loss: 1.077560\n",
      "(Epoch 9 / 20) train acc: 0.536000; val_acc: 0.513000\n",
      "(Iteration 13801 / 30620) loss: 1.224840\n",
      "(Iteration 13901 / 30620) loss: 1.674533\n",
      "(Iteration 14001 / 30620) loss: 1.749363\n",
      "(Iteration 14101 / 30620) loss: 1.530384\n",
      "(Iteration 14201 / 30620) loss: 1.459884\n",
      "(Iteration 14301 / 30620) loss: 1.321312\n",
      "(Iteration 14401 / 30620) loss: 1.670326\n",
      "(Iteration 14501 / 30620) loss: 1.453960\n",
      "(Iteration 14601 / 30620) loss: 1.706304\n",
      "(Iteration 14701 / 30620) loss: 1.439759\n",
      "(Iteration 14801 / 30620) loss: 1.351436\n",
      "(Iteration 14901 / 30620) loss: 1.467367\n",
      "(Iteration 15001 / 30620) loss: 1.541621\n",
      "(Iteration 15101 / 30620) loss: 1.628972\n",
      "(Iteration 15201 / 30620) loss: 1.371025\n",
      "(Iteration 15301 / 30620) loss: 1.480210\n",
      "(Epoch 10 / 20) train acc: 0.534000; val_acc: 0.514000\n",
      "(Iteration 15401 / 30620) loss: 1.685303\n",
      "(Iteration 15501 / 30620) loss: 1.692938\n",
      "(Iteration 15601 / 30620) loss: 1.406000\n",
      "(Iteration 15701 / 30620) loss: 1.535244\n",
      "(Iteration 15801 / 30620) loss: 1.770917\n",
      "(Iteration 15901 / 30620) loss: 1.271606\n",
      "(Iteration 16001 / 30620) loss: 1.568263\n",
      "(Iteration 16101 / 30620) loss: 1.301750\n",
      "(Iteration 16201 / 30620) loss: 1.481172\n",
      "(Iteration 16301 / 30620) loss: 1.476110\n",
      "(Iteration 16401 / 30620) loss: 1.449490\n",
      "(Iteration 16501 / 30620) loss: 1.519491\n",
      "(Iteration 16601 / 30620) loss: 1.563962\n",
      "(Iteration 16701 / 30620) loss: 1.479701\n",
      "(Iteration 16801 / 30620) loss: 1.356174\n",
      "(Epoch 11 / 20) train acc: 0.542000; val_acc: 0.515000\n",
      "(Iteration 16901 / 30620) loss: 1.495849\n",
      "(Iteration 17001 / 30620) loss: 1.740251\n",
      "(Iteration 17101 / 30620) loss: 1.437024\n",
      "(Iteration 17201 / 30620) loss: 1.429318\n",
      "(Iteration 17301 / 30620) loss: 1.777478\n",
      "(Iteration 17401 / 30620) loss: 1.444242\n",
      "(Iteration 17501 / 30620) loss: 1.312217\n",
      "(Iteration 17601 / 30620) loss: 1.635704\n",
      "(Iteration 17701 / 30620) loss: 1.127649\n",
      "(Iteration 17801 / 30620) loss: 1.502722\n",
      "(Iteration 17901 / 30620) loss: 1.793322\n",
      "(Iteration 18001 / 30620) loss: 1.437336\n",
      "(Iteration 18101 / 30620) loss: 1.504119\n",
      "(Iteration 18201 / 30620) loss: 1.662340\n",
      "(Iteration 18301 / 30620) loss: 1.449998\n",
      "(Epoch 12 / 20) train acc: 0.511000; val_acc: 0.517000\n",
      "(Iteration 18401 / 30620) loss: 1.325259\n",
      "(Iteration 18501 / 30620) loss: 1.405234\n",
      "(Iteration 18601 / 30620) loss: 1.408429\n",
      "(Iteration 18701 / 30620) loss: 1.307286\n",
      "(Iteration 18801 / 30620) loss: 1.500858\n",
      "(Iteration 18901 / 30620) loss: 1.460409\n",
      "(Iteration 19001 / 30620) loss: 1.642174\n",
      "(Iteration 19101 / 30620) loss: 1.352219\n",
      "(Iteration 19201 / 30620) loss: 1.456821\n",
      "(Iteration 19301 / 30620) loss: 1.750246\n",
      "(Iteration 19401 / 30620) loss: 1.692667\n",
      "(Iteration 19501 / 30620) loss: 1.711126\n",
      "(Iteration 19601 / 30620) loss: 1.474446\n",
      "(Iteration 19701 / 30620) loss: 1.747359\n",
      "(Iteration 19801 / 30620) loss: 1.715598\n",
      "(Iteration 19901 / 30620) loss: 1.646356\n",
      "(Epoch 13 / 20) train acc: 0.571000; val_acc: 0.522000\n",
      "(Iteration 20001 / 30620) loss: 1.115581\n",
      "(Iteration 20101 / 30620) loss: 1.164558\n",
      "(Iteration 20201 / 30620) loss: 1.606536\n",
      "(Iteration 20301 / 30620) loss: 1.251477\n",
      "(Iteration 20401 / 30620) loss: 1.434409\n",
      "(Iteration 20501 / 30620) loss: 1.549570\n",
      "(Iteration 20601 / 30620) loss: 1.339164\n",
      "(Iteration 20701 / 30620) loss: 1.533408\n",
      "(Iteration 20801 / 30620) loss: 1.558379\n",
      "(Iteration 20901 / 30620) loss: 1.540193\n",
      "(Iteration 21001 / 30620) loss: 1.516695\n",
      "(Iteration 21101 / 30620) loss: 1.337403\n",
      "(Iteration 21201 / 30620) loss: 1.388620\n",
      "(Iteration 21301 / 30620) loss: 1.381234\n",
      "(Iteration 21401 / 30620) loss: 1.463756\n",
      "(Epoch 14 / 20) train acc: 0.526000; val_acc: 0.523000\n",
      "(Iteration 21501 / 30620) loss: 1.339621\n",
      "(Iteration 21601 / 30620) loss: 1.557822\n",
      "(Iteration 21701 / 30620) loss: 1.354698\n",
      "(Iteration 21801 / 30620) loss: 1.418434\n",
      "(Iteration 21901 / 30620) loss: 1.371620\n",
      "(Iteration 22001 / 30620) loss: 1.587993\n",
      "(Iteration 22101 / 30620) loss: 1.410940\n",
      "(Iteration 22201 / 30620) loss: 1.606908\n",
      "(Iteration 22301 / 30620) loss: 1.174602\n",
      "(Iteration 22401 / 30620) loss: 1.352913\n",
      "(Iteration 22501 / 30620) loss: 1.313002\n",
      "(Iteration 22601 / 30620) loss: 1.226925\n",
      "(Iteration 22701 / 30620) loss: 1.279820\n",
      "(Iteration 22801 / 30620) loss: 1.052763\n",
      "(Iteration 22901 / 30620) loss: 1.325985\n",
      "(Epoch 15 / 20) train acc: 0.546000; val_acc: 0.518000\n",
      "(Iteration 23001 / 30620) loss: 1.363488\n",
      "(Iteration 23101 / 30620) loss: 1.303066\n",
      "(Iteration 23201 / 30620) loss: 1.294330\n",
      "(Iteration 23301 / 30620) loss: 1.340394\n",
      "(Iteration 23401 / 30620) loss: 1.470008\n",
      "(Iteration 23501 / 30620) loss: 1.606108\n",
      "(Iteration 23601 / 30620) loss: 1.429149\n",
      "(Iteration 23701 / 30620) loss: 1.554351\n",
      "(Iteration 23801 / 30620) loss: 1.627343\n",
      "(Iteration 23901 / 30620) loss: 1.458591\n",
      "(Iteration 24001 / 30620) loss: 1.314469\n",
      "(Iteration 24101 / 30620) loss: 1.656199\n",
      "(Iteration 24201 / 30620) loss: 1.430589\n",
      "(Iteration 24301 / 30620) loss: 1.479719\n",
      "(Iteration 24401 / 30620) loss: 1.380647\n",
      "(Epoch 16 / 20) train acc: 0.529000; val_acc: 0.520000\n",
      "(Iteration 24501 / 30620) loss: 1.403240\n",
      "(Iteration 24601 / 30620) loss: 1.401145\n",
      "(Iteration 24701 / 30620) loss: 1.604760\n",
      "(Iteration 24801 / 30620) loss: 1.370887\n",
      "(Iteration 24901 / 30620) loss: 1.458810\n",
      "(Iteration 25001 / 30620) loss: 1.388098\n",
      "(Iteration 25101 / 30620) loss: 1.541133\n",
      "(Iteration 25201 / 30620) loss: 1.768150\n",
      "(Iteration 25301 / 30620) loss: 1.155687\n",
      "(Iteration 25401 / 30620) loss: 1.733778\n",
      "(Iteration 25501 / 30620) loss: 1.415641\n",
      "(Iteration 25601 / 30620) loss: 1.532686\n",
      "(Iteration 25701 / 30620) loss: 1.407686\n",
      "(Iteration 25801 / 30620) loss: 1.304152\n",
      "(Iteration 25901 / 30620) loss: 1.516577\n",
      "(Iteration 26001 / 30620) loss: 1.536830\n",
      "(Epoch 17 / 20) train acc: 0.546000; val_acc: 0.516000\n",
      "(Iteration 26101 / 30620) loss: 1.479670\n",
      "(Iteration 26201 / 30620) loss: 1.291883\n",
      "(Iteration 26301 / 30620) loss: 1.381433\n",
      "(Iteration 26401 / 30620) loss: 1.446076\n",
      "(Iteration 26501 / 30620) loss: 1.596019\n",
      "(Iteration 26601 / 30620) loss: 1.585724\n",
      "(Iteration 26701 / 30620) loss: 1.730470\n",
      "(Iteration 26801 / 30620) loss: 1.562179\n",
      "(Iteration 26901 / 30620) loss: 1.460874\n",
      "(Iteration 27001 / 30620) loss: 1.387215\n",
      "(Iteration 27101 / 30620) loss: 1.315978\n",
      "(Iteration 27201 / 30620) loss: 1.145890\n",
      "(Iteration 27301 / 30620) loss: 1.219064\n",
      "(Iteration 27401 / 30620) loss: 1.503181\n",
      "(Iteration 27501 / 30620) loss: 1.424341\n",
      "(Epoch 18 / 20) train acc: 0.559000; val_acc: 0.513000\n",
      "(Iteration 27601 / 30620) loss: 1.382165\n",
      "(Iteration 27701 / 30620) loss: 1.767006\n",
      "(Iteration 27801 / 30620) loss: 1.407618\n",
      "(Iteration 27901 / 30620) loss: 1.375398\n",
      "(Iteration 28001 / 30620) loss: 1.223115\n",
      "(Iteration 28101 / 30620) loss: 1.640121\n",
      "(Iteration 28201 / 30620) loss: 1.549465\n",
      "(Iteration 28301 / 30620) loss: 1.437149\n",
      "(Iteration 28401 / 30620) loss: 1.189985\n",
      "(Iteration 28501 / 30620) loss: 1.382926\n",
      "(Iteration 28601 / 30620) loss: 1.315174\n",
      "(Iteration 28701 / 30620) loss: 1.055859\n",
      "(Iteration 28801 / 30620) loss: 1.341431\n",
      "(Iteration 28901 / 30620) loss: 1.394254\n",
      "(Iteration 29001 / 30620) loss: 1.425195\n",
      "(Epoch 19 / 20) train acc: 0.517000; val_acc: 0.520000\n",
      "(Iteration 29101 / 30620) loss: 1.390760\n",
      "(Iteration 29201 / 30620) loss: 0.985614\n",
      "(Iteration 29301 / 30620) loss: 1.726370\n",
      "(Iteration 29401 / 30620) loss: 1.789996\n",
      "(Iteration 29501 / 30620) loss: 1.608890\n",
      "(Iteration 29601 / 30620) loss: 1.463470\n",
      "(Iteration 29701 / 30620) loss: 1.472526\n",
      "(Iteration 29801 / 30620) loss: 1.549391\n",
      "(Iteration 29901 / 30620) loss: 1.464342\n",
      "(Iteration 30001 / 30620) loss: 1.581649\n",
      "(Iteration 30101 / 30620) loss: 1.525095\n",
      "(Iteration 30201 / 30620) loss: 1.348241\n",
      "(Iteration 30301 / 30620) loss: 1.457890\n",
      "(Iteration 30401 / 30620) loss: 1.645182\n",
      "(Iteration 30501 / 30620) loss: 1.440210\n",
      "(Iteration 30601 / 30620) loss: 1.214821\n",
      "(Epoch 20 / 20) train acc: 0.557000; val_acc: 0.517000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.01, 'num_epochs': 30, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 22950) loss: 2.309200\n",
      "(Epoch 0 / 30) train acc: 0.116000; val_acc: 0.132000\n",
      "(Iteration 101 / 22950) loss: 2.307554\n",
      "(Iteration 201 / 22950) loss: 2.305819\n",
      "(Iteration 301 / 22950) loss: 2.306049\n",
      "(Iteration 401 / 22950) loss: 2.303953\n",
      "(Iteration 501 / 22950) loss: 2.303522\n",
      "(Iteration 601 / 22950) loss: 2.303187\n",
      "(Iteration 701 / 22950) loss: 2.300098\n",
      "(Epoch 1 / 30) train acc: 0.187000; val_acc: 0.169000\n",
      "(Iteration 801 / 22950) loss: 2.299606\n",
      "(Iteration 901 / 22950) loss: 2.295615\n",
      "(Iteration 1001 / 22950) loss: 2.291964\n",
      "(Iteration 1101 / 22950) loss: 2.291280\n",
      "(Iteration 1201 / 22950) loss: 2.272207\n",
      "(Iteration 1301 / 22950) loss: 2.256799\n",
      "(Iteration 1401 / 22950) loss: 2.248918\n",
      "(Iteration 1501 / 22950) loss: 2.248323\n",
      "(Epoch 2 / 30) train acc: 0.223000; val_acc: 0.218000\n",
      "(Iteration 1601 / 22950) loss: 2.254593\n",
      "(Iteration 1701 / 22950) loss: 2.214330\n",
      "(Iteration 1801 / 22950) loss: 2.155043\n",
      "(Iteration 1901 / 22950) loss: 2.148063\n",
      "(Iteration 2001 / 22950) loss: 2.193829\n",
      "(Iteration 2101 / 22950) loss: 2.091498\n",
      "(Iteration 2201 / 22950) loss: 2.093502\n",
      "(Epoch 3 / 30) train acc: 0.284000; val_acc: 0.275000\n",
      "(Iteration 2301 / 22950) loss: 2.083662\n",
      "(Iteration 2401 / 22950) loss: 2.196184\n",
      "(Iteration 2501 / 22950) loss: 2.125911\n",
      "(Iteration 2601 / 22950) loss: 1.990546\n",
      "(Iteration 2701 / 22950) loss: 2.017572\n",
      "(Iteration 2801 / 22950) loss: 2.185354\n",
      "(Iteration 2901 / 22950) loss: 2.037875\n",
      "(Iteration 3001 / 22950) loss: 2.152454\n",
      "(Epoch 4 / 30) train acc: 0.313000; val_acc: 0.310000\n",
      "(Iteration 3101 / 22950) loss: 1.944586\n",
      "(Iteration 3201 / 22950) loss: 2.050376\n",
      "(Iteration 3301 / 22950) loss: 2.089127\n",
      "(Iteration 3401 / 22950) loss: 2.011210\n",
      "(Iteration 3501 / 22950) loss: 2.055495\n",
      "(Iteration 3601 / 22950) loss: 2.090757\n",
      "(Iteration 3701 / 22950) loss: 2.132237\n",
      "(Iteration 3801 / 22950) loss: 1.980267\n",
      "(Epoch 5 / 30) train acc: 0.316000; val_acc: 0.330000\n",
      "(Iteration 3901 / 22950) loss: 2.078720\n",
      "(Iteration 4001 / 22950) loss: 2.101068\n",
      "(Iteration 4101 / 22950) loss: 2.137093\n",
      "(Iteration 4201 / 22950) loss: 1.956849\n",
      "(Iteration 4301 / 22950) loss: 2.143218\n",
      "(Iteration 4401 / 22950) loss: 1.894352\n",
      "(Iteration 4501 / 22950) loss: 2.103841\n",
      "(Epoch 6 / 30) train acc: 0.355000; val_acc: 0.368000\n",
      "(Iteration 4601 / 22950) loss: 1.993802\n",
      "(Iteration 4701 / 22950) loss: 1.985495\n",
      "(Iteration 4801 / 22950) loss: 1.952168\n",
      "(Iteration 4901 / 22950) loss: 2.020923\n",
      "(Iteration 5001 / 22950) loss: 2.089435\n",
      "(Iteration 5101 / 22950) loss: 1.921073\n",
      "(Iteration 5201 / 22950) loss: 2.096635\n",
      "(Iteration 5301 / 22950) loss: 2.080797\n",
      "(Epoch 7 / 30) train acc: 0.354000; val_acc: 0.370000\n",
      "(Iteration 5401 / 22950) loss: 2.096050\n",
      "(Iteration 5501 / 22950) loss: 2.041274\n",
      "(Iteration 5601 / 22950) loss: 1.916553\n",
      "(Iteration 5701 / 22950) loss: 2.011425\n",
      "(Iteration 5801 / 22950) loss: 2.131326\n",
      "(Iteration 5901 / 22950) loss: 2.061777\n",
      "(Iteration 6001 / 22950) loss: 2.050550\n",
      "(Iteration 6101 / 22950) loss: 1.998093\n",
      "(Epoch 8 / 30) train acc: 0.408000; val_acc: 0.387000\n",
      "(Iteration 6201 / 22950) loss: 1.979946\n",
      "(Iteration 6301 / 22950) loss: 1.940337\n",
      "(Iteration 6401 / 22950) loss: 1.968731\n",
      "(Iteration 6501 / 22950) loss: 2.076113\n",
      "(Iteration 6601 / 22950) loss: 1.983411\n",
      "(Iteration 6701 / 22950) loss: 2.020718\n",
      "(Iteration 6801 / 22950) loss: 2.006347\n",
      "(Epoch 9 / 30) train acc: 0.387000; val_acc: 0.403000\n",
      "(Iteration 6901 / 22950) loss: 2.010556\n",
      "(Iteration 7001 / 22950) loss: 2.048188\n",
      "(Iteration 7101 / 22950) loss: 2.003723\n",
      "(Iteration 7201 / 22950) loss: 1.991797\n",
      "(Iteration 7301 / 22950) loss: 2.009084\n",
      "(Iteration 7401 / 22950) loss: 2.023887\n",
      "(Iteration 7501 / 22950) loss: 2.095832\n",
      "(Iteration 7601 / 22950) loss: 2.031146\n",
      "(Epoch 10 / 30) train acc: 0.404000; val_acc: 0.401000\n",
      "(Iteration 7701 / 22950) loss: 1.986309\n",
      "(Iteration 7801 / 22950) loss: 2.097440\n",
      "(Iteration 7901 / 22950) loss: 2.016052\n",
      "(Iteration 8001 / 22950) loss: 1.978608\n",
      "(Iteration 8101 / 22950) loss: 2.018613\n",
      "(Iteration 8201 / 22950) loss: 1.841555\n",
      "(Iteration 8301 / 22950) loss: 2.046759\n",
      "(Iteration 8401 / 22950) loss: 1.972961\n",
      "(Epoch 11 / 30) train acc: 0.408000; val_acc: 0.396000\n",
      "(Iteration 8501 / 22950) loss: 1.998527\n",
      "(Iteration 8601 / 22950) loss: 1.953947\n",
      "(Iteration 8701 / 22950) loss: 2.055311\n",
      "(Iteration 8801 / 22950) loss: 2.079935\n",
      "(Iteration 8901 / 22950) loss: 2.041274\n",
      "(Iteration 9001 / 22950) loss: 1.895494\n",
      "(Iteration 9101 / 22950) loss: 1.966280\n",
      "(Epoch 12 / 30) train acc: 0.434000; val_acc: 0.406000\n",
      "(Iteration 9201 / 22950) loss: 2.030316\n",
      "(Iteration 9301 / 22950) loss: 2.071482\n",
      "(Iteration 9401 / 22950) loss: 1.979457\n",
      "(Iteration 9501 / 22950) loss: 1.973286\n",
      "(Iteration 9601 / 22950) loss: 2.013730\n",
      "(Iteration 9701 / 22950) loss: 2.007596\n",
      "(Iteration 9801 / 22950) loss: 2.001325\n",
      "(Iteration 9901 / 22950) loss: 2.020035\n",
      "(Epoch 13 / 30) train acc: 0.409000; val_acc: 0.413000\n",
      "(Iteration 10001 / 22950) loss: 1.999997\n",
      "(Iteration 10101 / 22950) loss: 2.009345\n",
      "(Iteration 10201 / 22950) loss: 1.900194\n",
      "(Iteration 10301 / 22950) loss: 1.936372\n",
      "(Iteration 10401 / 22950) loss: 1.981344\n",
      "(Iteration 10501 / 22950) loss: 1.902644\n",
      "(Iteration 10601 / 22950) loss: 1.942660\n",
      "(Iteration 10701 / 22950) loss: 2.097229\n",
      "(Epoch 14 / 30) train acc: 0.408000; val_acc: 0.406000\n",
      "(Iteration 10801 / 22950) loss: 2.014025\n",
      "(Iteration 10901 / 22950) loss: 1.928185\n",
      "(Iteration 11001 / 22950) loss: 1.908540\n",
      "(Iteration 11101 / 22950) loss: 2.087969\n",
      "(Iteration 11201 / 22950) loss: 2.001531\n",
      "(Iteration 11301 / 22950) loss: 1.953674\n",
      "(Iteration 11401 / 22950) loss: 1.972306\n",
      "(Epoch 15 / 30) train acc: 0.447000; val_acc: 0.407000\n",
      "(Iteration 11501 / 22950) loss: 1.908168\n",
      "(Iteration 11601 / 22950) loss: 1.996014\n",
      "(Iteration 11701 / 22950) loss: 1.915499\n",
      "(Iteration 11801 / 22950) loss: 2.001203\n",
      "(Iteration 11901 / 22950) loss: 1.971862\n",
      "(Iteration 12001 / 22950) loss: 1.942024\n",
      "(Iteration 12101 / 22950) loss: 1.931271\n",
      "(Iteration 12201 / 22950) loss: 2.023250\n",
      "(Epoch 16 / 30) train acc: 0.423000; val_acc: 0.409000\n",
      "(Iteration 12301 / 22950) loss: 1.990597\n",
      "(Iteration 12401 / 22950) loss: 1.985205\n",
      "(Iteration 12501 / 22950) loss: 1.990655\n",
      "(Iteration 12601 / 22950) loss: 2.007358\n",
      "(Iteration 12701 / 22950) loss: 1.924911\n",
      "(Iteration 12801 / 22950) loss: 1.992230\n",
      "(Iteration 12901 / 22950) loss: 2.000380\n",
      "(Iteration 13001 / 22950) loss: 2.069868\n",
      "(Epoch 17 / 30) train acc: 0.437000; val_acc: 0.413000\n",
      "(Iteration 13101 / 22950) loss: 1.943608\n",
      "(Iteration 13201 / 22950) loss: 1.990427\n",
      "(Iteration 13301 / 22950) loss: 2.028013\n",
      "(Iteration 13401 / 22950) loss: 2.013482\n",
      "(Iteration 13501 / 22950) loss: 2.059281\n",
      "(Iteration 13601 / 22950) loss: 1.965458\n",
      "(Iteration 13701 / 22950) loss: 1.964390\n",
      "(Epoch 18 / 30) train acc: 0.408000; val_acc: 0.408000\n",
      "(Iteration 13801 / 22950) loss: 1.957337\n",
      "(Iteration 13901 / 22950) loss: 1.941593\n",
      "(Iteration 14001 / 22950) loss: 2.023455\n",
      "(Iteration 14101 / 22950) loss: 1.912393\n",
      "(Iteration 14201 / 22950) loss: 2.016668\n",
      "(Iteration 14301 / 22950) loss: 1.981729\n",
      "(Iteration 14401 / 22950) loss: 1.912194\n",
      "(Iteration 14501 / 22950) loss: 1.991429\n",
      "(Epoch 19 / 30) train acc: 0.418000; val_acc: 0.408000\n",
      "(Iteration 14601 / 22950) loss: 1.868754\n",
      "(Iteration 14701 / 22950) loss: 2.015052\n",
      "(Iteration 14801 / 22950) loss: 1.909757\n",
      "(Iteration 14901 / 22950) loss: 2.030974\n",
      "(Iteration 15001 / 22950) loss: 1.930108\n",
      "(Iteration 15101 / 22950) loss: 1.895880\n",
      "(Iteration 15201 / 22950) loss: 1.937099\n",
      "(Epoch 20 / 30) train acc: 0.425000; val_acc: 0.415000\n",
      "(Iteration 15301 / 22950) loss: 1.996003\n",
      "(Iteration 15401 / 22950) loss: 1.918027\n",
      "(Iteration 15501 / 22950) loss: 1.909168\n",
      "(Iteration 15601 / 22950) loss: 1.946717\n",
      "(Iteration 15701 / 22950) loss: 1.984759\n",
      "(Iteration 15801 / 22950) loss: 2.061288\n",
      "(Iteration 15901 / 22950) loss: 1.922478\n",
      "(Iteration 16001 / 22950) loss: 1.911124\n",
      "(Epoch 21 / 30) train acc: 0.437000; val_acc: 0.414000\n",
      "(Iteration 16101 / 22950) loss: 2.075263\n",
      "(Iteration 16201 / 22950) loss: 1.956511\n",
      "(Iteration 16301 / 22950) loss: 1.913910\n",
      "(Iteration 16401 / 22950) loss: 2.062353\n",
      "(Iteration 16501 / 22950) loss: 2.012709\n",
      "(Iteration 16601 / 22950) loss: 1.930065\n",
      "(Iteration 16701 / 22950) loss: 2.044829\n",
      "(Iteration 16801 / 22950) loss: 1.908366\n",
      "(Epoch 22 / 30) train acc: 0.412000; val_acc: 0.419000\n",
      "(Iteration 16901 / 22950) loss: 2.039489\n",
      "(Iteration 17001 / 22950) loss: 2.076501\n",
      "(Iteration 17101 / 22950) loss: 1.993271\n",
      "(Iteration 17201 / 22950) loss: 2.081876\n",
      "(Iteration 17301 / 22950) loss: 2.028451\n",
      "(Iteration 17401 / 22950) loss: 1.950735\n",
      "(Iteration 17501 / 22950) loss: 1.907543\n",
      "(Epoch 23 / 30) train acc: 0.426000; val_acc: 0.417000\n",
      "(Iteration 17601 / 22950) loss: 2.034194\n",
      "(Iteration 17701 / 22950) loss: 1.978756\n",
      "(Iteration 17801 / 22950) loss: 1.953811\n",
      "(Iteration 17901 / 22950) loss: 1.911528\n",
      "(Iteration 18001 / 22950) loss: 1.998136\n",
      "(Iteration 18101 / 22950) loss: 1.869414\n",
      "(Iteration 18201 / 22950) loss: 1.931455\n",
      "(Iteration 18301 / 22950) loss: 1.974279\n",
      "(Epoch 24 / 30) train acc: 0.424000; val_acc: 0.416000\n",
      "(Iteration 18401 / 22950) loss: 2.014574\n",
      "(Iteration 18501 / 22950) loss: 1.899760\n",
      "(Iteration 18601 / 22950) loss: 1.979507\n",
      "(Iteration 18701 / 22950) loss: 1.977417\n",
      "(Iteration 18801 / 22950) loss: 1.922743\n",
      "(Iteration 18901 / 22950) loss: 1.874724\n",
      "(Iteration 19001 / 22950) loss: 1.936192\n",
      "(Iteration 19101 / 22950) loss: 1.948745\n",
      "(Epoch 25 / 30) train acc: 0.442000; val_acc: 0.417000\n",
      "(Iteration 19201 / 22950) loss: 1.889106\n",
      "(Iteration 19301 / 22950) loss: 1.971769\n",
      "(Iteration 19401 / 22950) loss: 1.979329\n",
      "(Iteration 19501 / 22950) loss: 1.969858\n",
      "(Iteration 19601 / 22950) loss: 1.991034\n",
      "(Iteration 19701 / 22950) loss: 1.884826\n",
      "(Iteration 19801 / 22950) loss: 1.903067\n",
      "(Epoch 26 / 30) train acc: 0.412000; val_acc: 0.415000\n",
      "(Iteration 19901 / 22950) loss: 1.857636\n",
      "(Iteration 20001 / 22950) loss: 2.112664\n",
      "(Iteration 20101 / 22950) loss: 2.034710\n",
      "(Iteration 20201 / 22950) loss: 2.026729\n",
      "(Iteration 20301 / 22950) loss: 1.966824\n",
      "(Iteration 20401 / 22950) loss: 1.995561\n",
      "(Iteration 20501 / 22950) loss: 1.987925\n",
      "(Iteration 20601 / 22950) loss: 1.930538\n",
      "(Epoch 27 / 30) train acc: 0.432000; val_acc: 0.415000\n",
      "(Iteration 20701 / 22950) loss: 1.964271\n",
      "(Iteration 20801 / 22950) loss: 1.960736\n",
      "(Iteration 20901 / 22950) loss: 1.848755\n",
      "(Iteration 21001 / 22950) loss: 2.011122\n",
      "(Iteration 21101 / 22950) loss: 1.963488\n",
      "(Iteration 21201 / 22950) loss: 1.820436\n",
      "(Iteration 21301 / 22950) loss: 1.889232\n",
      "(Iteration 21401 / 22950) loss: 1.987136\n",
      "(Epoch 28 / 30) train acc: 0.429000; val_acc: 0.416000\n",
      "(Iteration 21501 / 22950) loss: 1.950290\n",
      "(Iteration 21601 / 22950) loss: 1.906889\n",
      "(Iteration 21701 / 22950) loss: 2.044889\n",
      "(Iteration 21801 / 22950) loss: 1.926234\n",
      "(Iteration 21901 / 22950) loss: 1.957996\n",
      "(Iteration 22001 / 22950) loss: 1.941320\n",
      "(Iteration 22101 / 22950) loss: 1.844034\n",
      "(Epoch 29 / 30) train acc: 0.404000; val_acc: 0.418000\n",
      "(Iteration 22201 / 22950) loss: 2.086936\n",
      "(Iteration 22301 / 22950) loss: 2.074790\n",
      "(Iteration 22401 / 22950) loss: 1.973634\n",
      "(Iteration 22501 / 22950) loss: 1.960850\n",
      "(Iteration 22601 / 22950) loss: 1.981847\n",
      "(Iteration 22701 / 22950) loss: 1.900184\n",
      "(Iteration 22801 / 22950) loss: 1.990693\n",
      "(Iteration 22901 / 22950) loss: 2.061801\n",
      "(Epoch 30 / 30) train acc: 0.422000; val_acc: 0.416000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.01, 'num_epochs': 30, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 45930) loss: 2.309231\n",
      "(Epoch 0 / 30) train acc: 0.101000; val_acc: 0.116000\n",
      "(Iteration 101 / 45930) loss: 2.307499\n",
      "(Iteration 201 / 45930) loss: 2.306638\n",
      "(Iteration 301 / 45930) loss: 2.308836\n",
      "(Iteration 401 / 45930) loss: 2.304012\n",
      "(Iteration 501 / 45930) loss: 2.305211\n",
      "(Iteration 601 / 45930) loss: 2.302651\n",
      "(Iteration 701 / 45930) loss: 2.301936\n",
      "(Iteration 801 / 45930) loss: 2.296810\n",
      "(Iteration 901 / 45930) loss: 2.297862\n",
      "(Iteration 1001 / 45930) loss: 2.285240\n",
      "(Iteration 1101 / 45930) loss: 2.258729\n",
      "(Iteration 1201 / 45930) loss: 2.264218\n",
      "(Iteration 1301 / 45930) loss: 2.274732\n",
      "(Iteration 1401 / 45930) loss: 2.205579\n",
      "(Iteration 1501 / 45930) loss: 2.270664\n",
      "(Epoch 1 / 30) train acc: 0.237000; val_acc: 0.222000\n",
      "(Iteration 1601 / 45930) loss: 2.097421\n",
      "(Iteration 1701 / 45930) loss: 2.255308\n",
      "(Iteration 1801 / 45930) loss: 2.125900\n",
      "(Iteration 1901 / 45930) loss: 2.240945\n",
      "(Iteration 2001 / 45930) loss: 2.179504\n",
      "(Iteration 2101 / 45930) loss: 2.220782\n",
      "(Iteration 2201 / 45930) loss: 2.186773\n",
      "(Iteration 2301 / 45930) loss: 2.048501\n",
      "(Iteration 2401 / 45930) loss: 1.961495\n",
      "(Iteration 2501 / 45930) loss: 1.976985\n",
      "(Iteration 2601 / 45930) loss: 2.205376\n",
      "(Iteration 2701 / 45930) loss: 1.979610\n",
      "(Iteration 2801 / 45930) loss: 1.956603\n",
      "(Iteration 2901 / 45930) loss: 2.094009\n",
      "(Iteration 3001 / 45930) loss: 2.066280\n",
      "(Epoch 2 / 30) train acc: 0.291000; val_acc: 0.318000\n",
      "(Iteration 3101 / 45930) loss: 2.108743\n",
      "(Iteration 3201 / 45930) loss: 2.044535\n",
      "(Iteration 3301 / 45930) loss: 1.922039\n",
      "(Iteration 3401 / 45930) loss: 2.139308\n",
      "(Iteration 3501 / 45930) loss: 2.071367\n",
      "(Iteration 3601 / 45930) loss: 2.130708\n",
      "(Iteration 3701 / 45930) loss: 2.008583\n",
      "(Iteration 3801 / 45930) loss: 2.099747\n",
      "(Iteration 3901 / 45930) loss: 1.902570\n",
      "(Iteration 4001 / 45930) loss: 2.102653\n",
      "(Iteration 4101 / 45930) loss: 2.143653\n",
      "(Iteration 4201 / 45930) loss: 2.134129\n",
      "(Iteration 4301 / 45930) loss: 2.206300\n",
      "(Iteration 4401 / 45930) loss: 2.108454\n",
      "(Iteration 4501 / 45930) loss: 2.022580\n",
      "(Epoch 3 / 30) train acc: 0.404000; val_acc: 0.379000\n",
      "(Iteration 4601 / 45930) loss: 2.060112\n",
      "(Iteration 4701 / 45930) loss: 2.001588\n",
      "(Iteration 4801 / 45930) loss: 1.958851\n",
      "(Iteration 4901 / 45930) loss: 2.079177\n",
      "(Iteration 5001 / 45930) loss: 2.016181\n",
      "(Iteration 5101 / 45930) loss: 1.962396\n",
      "(Iteration 5201 / 45930) loss: 1.965379\n",
      "(Iteration 5301 / 45930) loss: 2.017571\n",
      "(Iteration 5401 / 45930) loss: 1.952489\n",
      "(Iteration 5501 / 45930) loss: 1.838916\n",
      "(Iteration 5601 / 45930) loss: 1.982624\n",
      "(Iteration 5701 / 45930) loss: 1.960598\n",
      "(Iteration 5801 / 45930) loss: 1.794469\n",
      "(Iteration 5901 / 45930) loss: 1.909377\n",
      "(Iteration 6001 / 45930) loss: 2.022146\n",
      "(Iteration 6101 / 45930) loss: 2.049611\n",
      "(Epoch 4 / 30) train acc: 0.375000; val_acc: 0.412000\n",
      "(Iteration 6201 / 45930) loss: 2.000966\n",
      "(Iteration 6301 / 45930) loss: 2.024224\n",
      "(Iteration 6401 / 45930) loss: 2.042985\n",
      "(Iteration 6501 / 45930) loss: 1.856889\n",
      "(Iteration 6601 / 45930) loss: 2.101100\n",
      "(Iteration 6701 / 45930) loss: 1.907842\n",
      "(Iteration 6801 / 45930) loss: 1.984259\n",
      "(Iteration 6901 / 45930) loss: 1.947620\n",
      "(Iteration 7001 / 45930) loss: 2.011666\n",
      "(Iteration 7101 / 45930) loss: 2.070867\n",
      "(Iteration 7201 / 45930) loss: 1.951304\n",
      "(Iteration 7301 / 45930) loss: 2.085590\n",
      "(Iteration 7401 / 45930) loss: 1.924325\n",
      "(Iteration 7501 / 45930) loss: 1.869514\n",
      "(Iteration 7601 / 45930) loss: 2.044478\n",
      "(Epoch 5 / 30) train acc: 0.430000; val_acc: 0.401000\n",
      "(Iteration 7701 / 45930) loss: 1.998629\n",
      "(Iteration 7801 / 45930) loss: 2.020026\n",
      "(Iteration 7901 / 45930) loss: 1.927181\n",
      "(Iteration 8001 / 45930) loss: 2.078736\n",
      "(Iteration 8101 / 45930) loss: 1.904046\n",
      "(Iteration 8201 / 45930) loss: 1.930420\n",
      "(Iteration 8301 / 45930) loss: 1.903942\n",
      "(Iteration 8401 / 45930) loss: 2.183792\n",
      "(Iteration 8501 / 45930) loss: 2.021521\n",
      "(Iteration 8601 / 45930) loss: 1.940393\n",
      "(Iteration 8701 / 45930) loss: 2.057574\n",
      "(Iteration 8801 / 45930) loss: 1.985055\n",
      "(Iteration 8901 / 45930) loss: 2.015957\n",
      "(Iteration 9001 / 45930) loss: 2.007938\n",
      "(Iteration 9101 / 45930) loss: 1.948901\n",
      "(Epoch 6 / 30) train acc: 0.434000; val_acc: 0.430000\n",
      "(Iteration 9201 / 45930) loss: 1.953993\n",
      "(Iteration 9301 / 45930) loss: 2.095784\n",
      "(Iteration 9401 / 45930) loss: 1.905594\n",
      "(Iteration 9501 / 45930) loss: 2.021283\n",
      "(Iteration 9601 / 45930) loss: 1.927336\n",
      "(Iteration 9701 / 45930) loss: 1.874724\n",
      "(Iteration 9801 / 45930) loss: 2.173206\n",
      "(Iteration 9901 / 45930) loss: 1.974764\n",
      "(Iteration 10001 / 45930) loss: 1.948581\n",
      "(Iteration 10101 / 45930) loss: 1.891078\n",
      "(Iteration 10201 / 45930) loss: 1.988206\n",
      "(Iteration 10301 / 45930) loss: 1.773704\n",
      "(Iteration 10401 / 45930) loss: 1.865529\n",
      "(Iteration 10501 / 45930) loss: 1.920959\n",
      "(Iteration 10601 / 45930) loss: 2.012203\n",
      "(Iteration 10701 / 45930) loss: 2.028288\n",
      "(Epoch 7 / 30) train acc: 0.415000; val_acc: 0.431000\n",
      "(Iteration 10801 / 45930) loss: 1.968938\n",
      "(Iteration 10901 / 45930) loss: 1.925738\n",
      "(Iteration 11001 / 45930) loss: 1.933999\n",
      "(Iteration 11101 / 45930) loss: 2.037837\n",
      "(Iteration 11201 / 45930) loss: 2.103809\n",
      "(Iteration 11301 / 45930) loss: 1.892534\n",
      "(Iteration 11401 / 45930) loss: 1.909934\n",
      "(Iteration 11501 / 45930) loss: 1.986133\n",
      "(Iteration 11601 / 45930) loss: 1.935399\n",
      "(Iteration 11701 / 45930) loss: 2.013357\n",
      "(Iteration 11801 / 45930) loss: 1.865482\n",
      "(Iteration 11901 / 45930) loss: 2.188297\n",
      "(Iteration 12001 / 45930) loss: 1.937102\n",
      "(Iteration 12101 / 45930) loss: 2.020338\n",
      "(Iteration 12201 / 45930) loss: 1.826596\n",
      "(Epoch 8 / 30) train acc: 0.450000; val_acc: 0.425000\n",
      "(Iteration 12301 / 45930) loss: 1.913626\n",
      "(Iteration 12401 / 45930) loss: 1.980136\n",
      "(Iteration 12501 / 45930) loss: 1.863129\n",
      "(Iteration 12601 / 45930) loss: 2.139103\n",
      "(Iteration 12701 / 45930) loss: 2.054320\n",
      "(Iteration 12801 / 45930) loss: 1.972788\n",
      "(Iteration 12901 / 45930) loss: 1.776148\n",
      "(Iteration 13001 / 45930) loss: 2.015727\n",
      "(Iteration 13101 / 45930) loss: 2.174278\n",
      "(Iteration 13201 / 45930) loss: 1.968992\n",
      "(Iteration 13301 / 45930) loss: 1.971524\n",
      "(Iteration 13401 / 45930) loss: 2.073541\n",
      "(Iteration 13501 / 45930) loss: 1.886734\n",
      "(Iteration 13601 / 45930) loss: 1.828584\n",
      "(Iteration 13701 / 45930) loss: 2.056370\n",
      "(Epoch 9 / 30) train acc: 0.417000; val_acc: 0.428000\n",
      "(Iteration 13801 / 45930) loss: 2.018524\n",
      "(Iteration 13901 / 45930) loss: 1.918048\n",
      "(Iteration 14001 / 45930) loss: 1.927555\n",
      "(Iteration 14101 / 45930) loss: 2.029838\n",
      "(Iteration 14201 / 45930) loss: 1.969319\n",
      "(Iteration 14301 / 45930) loss: 2.032630\n",
      "(Iteration 14401 / 45930) loss: 1.977248\n",
      "(Iteration 14501 / 45930) loss: 2.006037\n",
      "(Iteration 14601 / 45930) loss: 2.016772\n",
      "(Iteration 14701 / 45930) loss: 1.837335\n",
      "(Iteration 14801 / 45930) loss: 1.882823\n",
      "(Iteration 14901 / 45930) loss: 1.789894\n",
      "(Iteration 15001 / 45930) loss: 1.879542\n",
      "(Iteration 15101 / 45930) loss: 1.808823\n",
      "(Iteration 15201 / 45930) loss: 1.889103\n",
      "(Iteration 15301 / 45930) loss: 1.897379\n",
      "(Epoch 10 / 30) train acc: 0.459000; val_acc: 0.431000\n",
      "(Iteration 15401 / 45930) loss: 1.846712\n",
      "(Iteration 15501 / 45930) loss: 1.900438\n",
      "(Iteration 15601 / 45930) loss: 1.872069\n",
      "(Iteration 15701 / 45930) loss: 2.023693\n",
      "(Iteration 15801 / 45930) loss: 1.832109\n",
      "(Iteration 15901 / 45930) loss: 1.941813\n",
      "(Iteration 16001 / 45930) loss: 1.950078\n",
      "(Iteration 16101 / 45930) loss: 2.017796\n",
      "(Iteration 16201 / 45930) loss: 2.095757\n",
      "(Iteration 16301 / 45930) loss: 2.030082\n",
      "(Iteration 16401 / 45930) loss: 1.940259\n",
      "(Iteration 16501 / 45930) loss: 1.997589\n",
      "(Iteration 16601 / 45930) loss: 1.840219\n",
      "(Iteration 16701 / 45930) loss: 1.959022\n",
      "(Iteration 16801 / 45930) loss: 1.908856\n",
      "(Epoch 11 / 30) train acc: 0.439000; val_acc: 0.436000\n",
      "(Iteration 16901 / 45930) loss: 1.804948\n",
      "(Iteration 17001 / 45930) loss: 2.045046\n",
      "(Iteration 17101 / 45930) loss: 1.753122\n",
      "(Iteration 17201 / 45930) loss: 1.895062\n",
      "(Iteration 17301 / 45930) loss: 1.934900\n",
      "(Iteration 17401 / 45930) loss: 2.013712\n",
      "(Iteration 17501 / 45930) loss: 1.863781\n",
      "(Iteration 17601 / 45930) loss: 1.836772\n",
      "(Iteration 17701 / 45930) loss: 1.875914\n",
      "(Iteration 17801 / 45930) loss: 1.926580\n",
      "(Iteration 17901 / 45930) loss: 2.127904\n",
      "(Iteration 18001 / 45930) loss: 1.971867\n",
      "(Iteration 18101 / 45930) loss: 1.857930\n",
      "(Iteration 18201 / 45930) loss: 2.057010\n",
      "(Iteration 18301 / 45930) loss: 1.964507\n",
      "(Epoch 12 / 30) train acc: 0.453000; val_acc: 0.434000\n",
      "(Iteration 18401 / 45930) loss: 2.044223\n",
      "(Iteration 18501 / 45930) loss: 1.858008\n",
      "(Iteration 18601 / 45930) loss: 1.887922\n",
      "(Iteration 18701 / 45930) loss: 2.107560\n",
      "(Iteration 18801 / 45930) loss: 1.832372\n",
      "(Iteration 18901 / 45930) loss: 1.934377\n",
      "(Iteration 19001 / 45930) loss: 1.960257\n",
      "(Iteration 19101 / 45930) loss: 1.868834\n",
      "(Iteration 19201 / 45930) loss: 1.769379\n",
      "(Iteration 19301 / 45930) loss: 1.827446\n",
      "(Iteration 19401 / 45930) loss: 2.083583\n",
      "(Iteration 19501 / 45930) loss: 1.739986\n",
      "(Iteration 19601 / 45930) loss: 2.124151\n",
      "(Iteration 19701 / 45930) loss: 1.961562\n",
      "(Iteration 19801 / 45930) loss: 1.908637\n",
      "(Iteration 19901 / 45930) loss: 2.055206\n",
      "(Epoch 13 / 30) train acc: 0.415000; val_acc: 0.427000\n",
      "(Iteration 20001 / 45930) loss: 2.095227\n",
      "(Iteration 20101 / 45930) loss: 1.922134\n",
      "(Iteration 20201 / 45930) loss: 1.989320\n",
      "(Iteration 20301 / 45930) loss: 2.030442\n",
      "(Iteration 20401 / 45930) loss: 1.813743\n",
      "(Iteration 20501 / 45930) loss: 1.975591\n",
      "(Iteration 20601 / 45930) loss: 2.075312\n",
      "(Iteration 20701 / 45930) loss: 1.860468\n",
      "(Iteration 20801 / 45930) loss: 1.918907\n",
      "(Iteration 20901 / 45930) loss: 1.956900\n",
      "(Iteration 21001 / 45930) loss: 1.821387\n",
      "(Iteration 21101 / 45930) loss: 1.951724\n",
      "(Iteration 21201 / 45930) loss: 1.888272\n",
      "(Iteration 21301 / 45930) loss: 1.932788\n",
      "(Iteration 21401 / 45930) loss: 1.944318\n",
      "(Epoch 14 / 30) train acc: 0.465000; val_acc: 0.425000\n",
      "(Iteration 21501 / 45930) loss: 2.002054\n",
      "(Iteration 21601 / 45930) loss: 2.222414\n",
      "(Iteration 21701 / 45930) loss: 1.816794\n",
      "(Iteration 21801 / 45930) loss: 2.014001\n",
      "(Iteration 21901 / 45930) loss: 1.826888\n",
      "(Iteration 22001 / 45930) loss: 2.189927\n",
      "(Iteration 22101 / 45930) loss: 2.000133\n",
      "(Iteration 22201 / 45930) loss: 2.003802\n",
      "(Iteration 22301 / 45930) loss: 1.926414\n",
      "(Iteration 22401 / 45930) loss: 1.897586\n",
      "(Iteration 22501 / 45930) loss: 2.036145\n",
      "(Iteration 22601 / 45930) loss: 1.746837\n",
      "(Iteration 22701 / 45930) loss: 1.986968\n",
      "(Iteration 22801 / 45930) loss: 1.911779\n",
      "(Iteration 22901 / 45930) loss: 2.021672\n",
      "(Epoch 15 / 30) train acc: 0.458000; val_acc: 0.427000\n",
      "(Iteration 23001 / 45930) loss: 1.909129\n",
      "(Iteration 23101 / 45930) loss: 1.963360\n",
      "(Iteration 23201 / 45930) loss: 1.768803\n",
      "(Iteration 23301 / 45930) loss: 2.087859\n",
      "(Iteration 23401 / 45930) loss: 1.849005\n",
      "(Iteration 23501 / 45930) loss: 1.826864\n",
      "(Iteration 23601 / 45930) loss: 2.072098\n",
      "(Iteration 23701 / 45930) loss: 1.868241\n",
      "(Iteration 23801 / 45930) loss: 1.961424\n",
      "(Iteration 23901 / 45930) loss: 2.037846\n",
      "(Iteration 24001 / 45930) loss: 2.010608\n",
      "(Iteration 24101 / 45930) loss: 2.123947\n",
      "(Iteration 24201 / 45930) loss: 1.992984\n",
      "(Iteration 24301 / 45930) loss: 1.875967\n",
      "(Iteration 24401 / 45930) loss: 1.749220\n",
      "(Epoch 16 / 30) train acc: 0.433000; val_acc: 0.429000\n",
      "(Iteration 24501 / 45930) loss: 2.041683\n",
      "(Iteration 24601 / 45930) loss: 1.875694\n",
      "(Iteration 24701 / 45930) loss: 1.976201\n",
      "(Iteration 24801 / 45930) loss: 1.950545\n",
      "(Iteration 24901 / 45930) loss: 1.858923\n",
      "(Iteration 25001 / 45930) loss: 1.781567\n",
      "(Iteration 25101 / 45930) loss: 1.786420\n",
      "(Iteration 25201 / 45930) loss: 2.069857\n",
      "(Iteration 25301 / 45930) loss: 1.972525\n",
      "(Iteration 25401 / 45930) loss: 1.884579\n",
      "(Iteration 25501 / 45930) loss: 2.012326\n",
      "(Iteration 25601 / 45930) loss: 1.963266\n",
      "(Iteration 25701 / 45930) loss: 2.037534\n",
      "(Iteration 25801 / 45930) loss: 1.924054\n",
      "(Iteration 25901 / 45930) loss: 1.933866\n",
      "(Iteration 26001 / 45930) loss: 1.794141\n",
      "(Epoch 17 / 30) train acc: 0.437000; val_acc: 0.430000\n",
      "(Iteration 26101 / 45930) loss: 1.934221\n",
      "(Iteration 26201 / 45930) loss: 1.958123\n",
      "(Iteration 26301 / 45930) loss: 2.098281\n",
      "(Iteration 26401 / 45930) loss: 1.974859\n",
      "(Iteration 26501 / 45930) loss: 1.943361\n",
      "(Iteration 26601 / 45930) loss: 1.805331\n",
      "(Iteration 26701 / 45930) loss: 1.815463\n",
      "(Iteration 26801 / 45930) loss: 2.058494\n",
      "(Iteration 26901 / 45930) loss: 1.993697\n",
      "(Iteration 27001 / 45930) loss: 1.867410\n",
      "(Iteration 27101 / 45930) loss: 1.859647\n",
      "(Iteration 27201 / 45930) loss: 1.723341\n",
      "(Iteration 27301 / 45930) loss: 1.898353\n",
      "(Iteration 27401 / 45930) loss: 1.946520\n",
      "(Iteration 27501 / 45930) loss: 1.947832\n",
      "(Epoch 18 / 30) train acc: 0.454000; val_acc: 0.434000\n",
      "(Iteration 27601 / 45930) loss: 1.838261\n",
      "(Iteration 27701 / 45930) loss: 1.783746\n",
      "(Iteration 27801 / 45930) loss: 2.076053\n",
      "(Iteration 27901 / 45930) loss: 2.020234\n",
      "(Iteration 28001 / 45930) loss: 1.830120\n",
      "(Iteration 28101 / 45930) loss: 2.019623\n",
      "(Iteration 28201 / 45930) loss: 2.021771\n",
      "(Iteration 28301 / 45930) loss: 1.857849\n",
      "(Iteration 28401 / 45930) loss: 1.968774\n",
      "(Iteration 28501 / 45930) loss: 1.846574\n",
      "(Iteration 28601 / 45930) loss: 2.132284\n",
      "(Iteration 28701 / 45930) loss: 1.967559\n",
      "(Iteration 28801 / 45930) loss: 1.896538\n",
      "(Iteration 28901 / 45930) loss: 1.977701\n",
      "(Iteration 29001 / 45930) loss: 2.147982\n",
      "(Epoch 19 / 30) train acc: 0.434000; val_acc: 0.436000\n",
      "(Iteration 29101 / 45930) loss: 1.982801\n",
      "(Iteration 29201 / 45930) loss: 2.090789\n",
      "(Iteration 29301 / 45930) loss: 1.936233\n",
      "(Iteration 29401 / 45930) loss: 1.720026\n",
      "(Iteration 29501 / 45930) loss: 2.075983\n",
      "(Iteration 29601 / 45930) loss: 1.853925\n",
      "(Iteration 29701 / 45930) loss: 1.813047\n",
      "(Iteration 29801 / 45930) loss: 2.002901\n",
      "(Iteration 29901 / 45930) loss: 1.968646\n",
      "(Iteration 30001 / 45930) loss: 1.874807\n",
      "(Iteration 30101 / 45930) loss: 2.046073\n",
      "(Iteration 30201 / 45930) loss: 1.877194\n",
      "(Iteration 30301 / 45930) loss: 1.861665\n",
      "(Iteration 30401 / 45930) loss: 1.947922\n",
      "(Iteration 30501 / 45930) loss: 1.911391\n",
      "(Iteration 30601 / 45930) loss: 2.070426\n",
      "(Epoch 20 / 30) train acc: 0.462000; val_acc: 0.436000\n",
      "(Iteration 30701 / 45930) loss: 1.771239\n",
      "(Iteration 30801 / 45930) loss: 2.214627\n",
      "(Iteration 30901 / 45930) loss: 1.813260\n",
      "(Iteration 31001 / 45930) loss: 2.029846\n",
      "(Iteration 31101 / 45930) loss: 1.860797\n",
      "(Iteration 31201 / 45930) loss: 2.077656\n",
      "(Iteration 31301 / 45930) loss: 1.851056\n",
      "(Iteration 31401 / 45930) loss: 2.000102\n",
      "(Iteration 31501 / 45930) loss: 2.023712\n",
      "(Iteration 31601 / 45930) loss: 1.956678\n",
      "(Iteration 31701 / 45930) loss: 1.909213\n",
      "(Iteration 31801 / 45930) loss: 1.898702\n",
      "(Iteration 31901 / 45930) loss: 1.799516\n",
      "(Iteration 32001 / 45930) loss: 1.999478\n",
      "(Iteration 32101 / 45930) loss: 1.808494\n",
      "(Epoch 21 / 30) train acc: 0.442000; val_acc: 0.441000\n",
      "(Iteration 32201 / 45930) loss: 1.889834\n",
      "(Iteration 32301 / 45930) loss: 2.098880\n",
      "(Iteration 32401 / 45930) loss: 1.702433\n",
      "(Iteration 32501 / 45930) loss: 2.011724\n",
      "(Iteration 32601 / 45930) loss: 2.113939\n",
      "(Iteration 32701 / 45930) loss: 1.843556\n",
      "(Iteration 32801 / 45930) loss: 1.993827\n",
      "(Iteration 32901 / 45930) loss: 2.006180\n",
      "(Iteration 33001 / 45930) loss: 1.995357\n",
      "(Iteration 33101 / 45930) loss: 1.802656\n",
      "(Iteration 33201 / 45930) loss: 1.933255\n",
      "(Iteration 33301 / 45930) loss: 1.910532\n",
      "(Iteration 33401 / 45930) loss: 1.848597\n",
      "(Iteration 33501 / 45930) loss: 2.103926\n",
      "(Iteration 33601 / 45930) loss: 2.139902\n",
      "(Epoch 22 / 30) train acc: 0.435000; val_acc: 0.437000\n",
      "(Iteration 33701 / 45930) loss: 2.002804\n",
      "(Iteration 33801 / 45930) loss: 1.992228\n",
      "(Iteration 33901 / 45930) loss: 1.937779\n",
      "(Iteration 34001 / 45930) loss: 1.842619\n",
      "(Iteration 34101 / 45930) loss: 2.074727\n",
      "(Iteration 34201 / 45930) loss: 2.111365\n",
      "(Iteration 34301 / 45930) loss: 1.769650\n",
      "(Iteration 34401 / 45930) loss: 1.975802\n",
      "(Iteration 34501 / 45930) loss: 1.912159\n",
      "(Iteration 34601 / 45930) loss: 1.989274\n",
      "(Iteration 34701 / 45930) loss: 1.755304\n",
      "(Iteration 34801 / 45930) loss: 1.894690\n",
      "(Iteration 34901 / 45930) loss: 1.945006\n",
      "(Iteration 35001 / 45930) loss: 1.884309\n",
      "(Iteration 35101 / 45930) loss: 1.995012\n",
      "(Iteration 35201 / 45930) loss: 1.987726\n",
      "(Epoch 23 / 30) train acc: 0.420000; val_acc: 0.435000\n",
      "(Iteration 35301 / 45930) loss: 1.964781\n",
      "(Iteration 35401 / 45930) loss: 1.893683\n",
      "(Iteration 35501 / 45930) loss: 2.024645\n",
      "(Iteration 35601 / 45930) loss: 2.009204\n",
      "(Iteration 35701 / 45930) loss: 1.909447\n",
      "(Iteration 35801 / 45930) loss: 1.898780\n",
      "(Iteration 35901 / 45930) loss: 1.843667\n",
      "(Iteration 36001 / 45930) loss: 1.858418\n",
      "(Iteration 36101 / 45930) loss: 1.945475\n",
      "(Iteration 36201 / 45930) loss: 1.894056\n",
      "(Iteration 36301 / 45930) loss: 2.068415\n",
      "(Iteration 36401 / 45930) loss: 1.893724\n",
      "(Iteration 36501 / 45930) loss: 1.993945\n",
      "(Iteration 36601 / 45930) loss: 2.101088\n",
      "(Iteration 36701 / 45930) loss: 1.763435\n",
      "(Epoch 24 / 30) train acc: 0.425000; val_acc: 0.440000\n",
      "(Iteration 36801 / 45930) loss: 2.003745\n",
      "(Iteration 36901 / 45930) loss: 2.025872\n",
      "(Iteration 37001 / 45930) loss: 2.003284\n",
      "(Iteration 37101 / 45930) loss: 2.106767\n",
      "(Iteration 37201 / 45930) loss: 1.964475\n",
      "(Iteration 37301 / 45930) loss: 1.980631\n",
      "(Iteration 37401 / 45930) loss: 1.871984\n",
      "(Iteration 37501 / 45930) loss: 1.893575\n",
      "(Iteration 37601 / 45930) loss: 1.800186\n",
      "(Iteration 37701 / 45930) loss: 1.946651\n",
      "(Iteration 37801 / 45930) loss: 1.984791\n",
      "(Iteration 37901 / 45930) loss: 1.927426\n",
      "(Iteration 38001 / 45930) loss: 1.941736\n",
      "(Iteration 38101 / 45930) loss: 2.074615\n",
      "(Iteration 38201 / 45930) loss: 2.013704\n",
      "(Epoch 25 / 30) train acc: 0.457000; val_acc: 0.436000\n",
      "(Iteration 38301 / 45930) loss: 1.745212\n",
      "(Iteration 38401 / 45930) loss: 2.183629\n",
      "(Iteration 38501 / 45930) loss: 1.980709\n",
      "(Iteration 38601 / 45930) loss: 1.893644\n",
      "(Iteration 38701 / 45930) loss: 2.027443\n",
      "(Iteration 38801 / 45930) loss: 1.798563\n",
      "(Iteration 38901 / 45930) loss: 2.169295\n",
      "(Iteration 39001 / 45930) loss: 2.093104\n",
      "(Iteration 39101 / 45930) loss: 1.939663\n",
      "(Iteration 39201 / 45930) loss: 1.729501\n",
      "(Iteration 39301 / 45930) loss: 1.925022\n",
      "(Iteration 39401 / 45930) loss: 1.939050\n",
      "(Iteration 39501 / 45930) loss: 1.931536\n",
      "(Iteration 39601 / 45930) loss: 1.855835\n",
      "(Iteration 39701 / 45930) loss: 1.941121\n",
      "(Iteration 39801 / 45930) loss: 1.906508\n",
      "(Epoch 26 / 30) train acc: 0.429000; val_acc: 0.434000\n",
      "(Iteration 39901 / 45930) loss: 1.928522\n",
      "(Iteration 40001 / 45930) loss: 1.992604\n",
      "(Iteration 40101 / 45930) loss: 2.129627\n",
      "(Iteration 40201 / 45930) loss: 1.891098\n",
      "(Iteration 40301 / 45930) loss: 1.840868\n",
      "(Iteration 40401 / 45930) loss: 1.900162\n",
      "(Iteration 40501 / 45930) loss: 1.911270\n",
      "(Iteration 40601 / 45930) loss: 1.876358\n",
      "(Iteration 40701 / 45930) loss: 1.932305\n",
      "(Iteration 40801 / 45930) loss: 1.873181\n",
      "(Iteration 40901 / 45930) loss: 1.973663\n",
      "(Iteration 41001 / 45930) loss: 1.965931\n",
      "(Iteration 41101 / 45930) loss: 2.016129\n",
      "(Iteration 41201 / 45930) loss: 2.034017\n",
      "(Iteration 41301 / 45930) loss: 2.049253\n",
      "(Epoch 27 / 30) train acc: 0.426000; val_acc: 0.438000\n",
      "(Iteration 41401 / 45930) loss: 2.021942\n",
      "(Iteration 41501 / 45930) loss: 1.817051\n",
      "(Iteration 41601 / 45930) loss: 1.952832\n",
      "(Iteration 41701 / 45930) loss: 2.039715\n",
      "(Iteration 41801 / 45930) loss: 1.952737\n",
      "(Iteration 41901 / 45930) loss: 2.151471\n",
      "(Iteration 42001 / 45930) loss: 1.789110\n",
      "(Iteration 42101 / 45930) loss: 1.914919\n",
      "(Iteration 42201 / 45930) loss: 2.053783\n",
      "(Iteration 42301 / 45930) loss: 1.836327\n",
      "(Iteration 42401 / 45930) loss: 1.972075\n",
      "(Iteration 42501 / 45930) loss: 2.053832\n",
      "(Iteration 42601 / 45930) loss: 1.811591\n",
      "(Iteration 42701 / 45930) loss: 1.952496\n",
      "(Iteration 42801 / 45930) loss: 2.028332\n",
      "(Epoch 28 / 30) train acc: 0.444000; val_acc: 0.440000\n",
      "(Iteration 42901 / 45930) loss: 1.952417\n",
      "(Iteration 43001 / 45930) loss: 1.856176\n",
      "(Iteration 43101 / 45930) loss: 1.780339\n",
      "(Iteration 43201 / 45930) loss: 1.824235\n",
      "(Iteration 43301 / 45930) loss: 2.079809\n",
      "(Iteration 43401 / 45930) loss: 2.091650\n",
      "(Iteration 43501 / 45930) loss: 2.016442\n",
      "(Iteration 43601 / 45930) loss: 2.178174\n",
      "(Iteration 43701 / 45930) loss: 1.891265\n",
      "(Iteration 43801 / 45930) loss: 1.882561\n",
      "(Iteration 43901 / 45930) loss: 2.104208\n",
      "(Iteration 44001 / 45930) loss: 1.952187\n",
      "(Iteration 44101 / 45930) loss: 1.769286\n",
      "(Iteration 44201 / 45930) loss: 1.843324\n",
      "(Iteration 44301 / 45930) loss: 1.909347\n",
      "(Epoch 29 / 30) train acc: 0.436000; val_acc: 0.435000\n",
      "(Iteration 44401 / 45930) loss: 1.934586\n",
      "(Iteration 44501 / 45930) loss: 2.005963\n",
      "(Iteration 44601 / 45930) loss: 1.968685\n",
      "(Iteration 44701 / 45930) loss: 1.808383\n",
      "(Iteration 44801 / 45930) loss: 1.945513\n",
      "(Iteration 44901 / 45930) loss: 2.142679\n",
      "(Iteration 45001 / 45930) loss: 1.919958\n",
      "(Iteration 45101 / 45930) loss: 2.071516\n",
      "(Iteration 45201 / 45930) loss: 2.139878\n",
      "(Iteration 45301 / 45930) loss: 2.044739\n",
      "(Iteration 45401 / 45930) loss: 2.061199\n",
      "(Iteration 45501 / 45930) loss: 2.066182\n",
      "(Iteration 45601 / 45930) loss: 1.926929\n",
      "(Iteration 45701 / 45930) loss: 1.944551\n",
      "(Iteration 45801 / 45930) loss: 1.936265\n",
      "(Iteration 45901 / 45930) loss: 1.832840\n",
      "(Epoch 30 / 30) train acc: 0.428000; val_acc: 0.434000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.01, 'num_epochs': 30, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 22950) loss: 2.303281\n",
      "(Epoch 0 / 30) train acc: 0.101000; val_acc: 0.103000\n",
      "(Iteration 101 / 22950) loss: 2.302828\n",
      "(Iteration 201 / 22950) loss: 2.302558\n",
      "(Iteration 301 / 22950) loss: 2.301500\n",
      "(Iteration 401 / 22950) loss: 2.299769\n",
      "(Iteration 501 / 22950) loss: 2.296912\n",
      "(Iteration 601 / 22950) loss: 2.295933\n",
      "(Iteration 701 / 22950) loss: 2.288948\n",
      "(Epoch 1 / 30) train acc: 0.260000; val_acc: 0.264000\n",
      "(Iteration 801 / 22950) loss: 2.277945\n",
      "(Iteration 901 / 22950) loss: 2.241800\n",
      "(Iteration 1001 / 22950) loss: 2.240590\n",
      "(Iteration 1101 / 22950) loss: 2.214425\n",
      "(Iteration 1201 / 22950) loss: 2.135945\n",
      "(Iteration 1301 / 22950) loss: 2.153299\n",
      "(Iteration 1401 / 22950) loss: 2.099958\n",
      "(Iteration 1501 / 22950) loss: 2.021217\n",
      "(Epoch 2 / 30) train acc: 0.314000; val_acc: 0.305000\n",
      "(Iteration 1601 / 22950) loss: 1.972713\n",
      "(Iteration 1701 / 22950) loss: 1.818770\n",
      "(Iteration 1801 / 22950) loss: 1.948252\n",
      "(Iteration 1901 / 22950) loss: 1.913121\n",
      "(Iteration 2001 / 22950) loss: 1.957587\n",
      "(Iteration 2101 / 22950) loss: 1.769175\n",
      "(Iteration 2201 / 22950) loss: 1.775021\n",
      "(Epoch 3 / 30) train acc: 0.399000; val_acc: 0.370000\n",
      "(Iteration 2301 / 22950) loss: 1.823904\n",
      "(Iteration 2401 / 22950) loss: 1.623359\n",
      "(Iteration 2501 / 22950) loss: 1.834007\n",
      "(Iteration 2601 / 22950) loss: 1.754556\n",
      "(Iteration 2701 / 22950) loss: 1.589487\n",
      "(Iteration 2801 / 22950) loss: 1.757582\n",
      "(Iteration 2901 / 22950) loss: 1.800468\n",
      "(Iteration 3001 / 22950) loss: 1.657032\n",
      "(Epoch 4 / 30) train acc: 0.435000; val_acc: 0.413000\n",
      "(Iteration 3101 / 22950) loss: 1.533295\n",
      "(Iteration 3201 / 22950) loss: 1.527283\n",
      "(Iteration 3301 / 22950) loss: 1.510002\n",
      "(Iteration 3401 / 22950) loss: 1.900911\n",
      "(Iteration 3501 / 22950) loss: 1.551413\n",
      "(Iteration 3601 / 22950) loss: 1.547149\n",
      "(Iteration 3701 / 22950) loss: 1.740153\n",
      "(Iteration 3801 / 22950) loss: 1.324575\n",
      "(Epoch 5 / 30) train acc: 0.437000; val_acc: 0.441000\n",
      "(Iteration 3901 / 22950) loss: 1.695081\n",
      "(Iteration 4001 / 22950) loss: 1.576432\n",
      "(Iteration 4101 / 22950) loss: 1.555229\n",
      "(Iteration 4201 / 22950) loss: 1.416687\n",
      "(Iteration 4301 / 22950) loss: 1.519646\n",
      "(Iteration 4401 / 22950) loss: 1.531196\n",
      "(Iteration 4501 / 22950) loss: 1.534933\n",
      "(Epoch 6 / 30) train acc: 0.489000; val_acc: 0.455000\n",
      "(Iteration 4601 / 22950) loss: 1.479316\n",
      "(Iteration 4701 / 22950) loss: 1.608047\n",
      "(Iteration 4801 / 22950) loss: 1.557817\n",
      "(Iteration 4901 / 22950) loss: 1.567876\n",
      "(Iteration 5001 / 22950) loss: 1.469858\n",
      "(Iteration 5101 / 22950) loss: 1.511589\n",
      "(Iteration 5201 / 22950) loss: 1.619315\n",
      "(Iteration 5301 / 22950) loss: 1.565853\n",
      "(Epoch 7 / 30) train acc: 0.476000; val_acc: 0.476000\n",
      "(Iteration 5401 / 22950) loss: 1.536703\n",
      "(Iteration 5501 / 22950) loss: 1.413051\n",
      "(Iteration 5601 / 22950) loss: 1.504711\n",
      "(Iteration 5701 / 22950) loss: 1.646691\n",
      "(Iteration 5801 / 22950) loss: 1.502123\n",
      "(Iteration 5901 / 22950) loss: 1.541212\n",
      "(Iteration 6001 / 22950) loss: 1.513006\n",
      "(Iteration 6101 / 22950) loss: 1.557834\n",
      "(Epoch 8 / 30) train acc: 0.519000; val_acc: 0.488000\n",
      "(Iteration 6201 / 22950) loss: 1.664907\n",
      "(Iteration 6301 / 22950) loss: 1.502626\n",
      "(Iteration 6401 / 22950) loss: 1.677256\n",
      "(Iteration 6501 / 22950) loss: 1.618106\n",
      "(Iteration 6601 / 22950) loss: 1.785902\n",
      "(Iteration 6701 / 22950) loss: 1.486021\n",
      "(Iteration 6801 / 22950) loss: 1.527503\n",
      "(Epoch 9 / 30) train acc: 0.494000; val_acc: 0.482000\n",
      "(Iteration 6901 / 22950) loss: 1.579842\n",
      "(Iteration 7001 / 22950) loss: 1.549940\n",
      "(Iteration 7101 / 22950) loss: 1.413984\n",
      "(Iteration 7201 / 22950) loss: 1.621177\n",
      "(Iteration 7301 / 22950) loss: 1.760135\n",
      "(Iteration 7401 / 22950) loss: 1.579702\n",
      "(Iteration 7501 / 22950) loss: 1.584773\n",
      "(Iteration 7601 / 22950) loss: 1.391067\n",
      "(Epoch 10 / 30) train acc: 0.489000; val_acc: 0.497000\n",
      "(Iteration 7701 / 22950) loss: 1.487339\n",
      "(Iteration 7801 / 22950) loss: 1.658023\n",
      "(Iteration 7901 / 22950) loss: 1.619930\n",
      "(Iteration 8001 / 22950) loss: 1.434961\n",
      "(Iteration 8101 / 22950) loss: 1.658251\n",
      "(Iteration 8201 / 22950) loss: 1.487129\n",
      "(Iteration 8301 / 22950) loss: 1.340000\n",
      "(Iteration 8401 / 22950) loss: 1.558441\n",
      "(Epoch 11 / 30) train acc: 0.497000; val_acc: 0.505000\n",
      "(Iteration 8501 / 22950) loss: 1.334086\n",
      "(Iteration 8601 / 22950) loss: 1.636891\n",
      "(Iteration 8701 / 22950) loss: 1.473786\n",
      "(Iteration 8801 / 22950) loss: 1.464913\n",
      "(Iteration 8901 / 22950) loss: 1.353038\n",
      "(Iteration 9001 / 22950) loss: 1.639351\n",
      "(Iteration 9101 / 22950) loss: 1.477244\n",
      "(Epoch 12 / 30) train acc: 0.524000; val_acc: 0.502000\n",
      "(Iteration 9201 / 22950) loss: 1.522731\n",
      "(Iteration 9301 / 22950) loss: 1.413605\n",
      "(Iteration 9401 / 22950) loss: 1.330895\n",
      "(Iteration 9501 / 22950) loss: 1.418063\n",
      "(Iteration 9601 / 22950) loss: 1.548701\n",
      "(Iteration 9701 / 22950) loss: 1.507892\n",
      "(Iteration 9801 / 22950) loss: 1.767426\n",
      "(Iteration 9901 / 22950) loss: 1.369999\n",
      "(Epoch 13 / 30) train acc: 0.519000; val_acc: 0.502000\n",
      "(Iteration 10001 / 22950) loss: 1.507874\n",
      "(Iteration 10101 / 22950) loss: 1.379497\n",
      "(Iteration 10201 / 22950) loss: 1.557035\n",
      "(Iteration 10301 / 22950) loss: 1.786662\n",
      "(Iteration 10401 / 22950) loss: 1.446068\n",
      "(Iteration 10501 / 22950) loss: 1.267550\n",
      "(Iteration 10601 / 22950) loss: 1.379352\n",
      "(Iteration 10701 / 22950) loss: 1.569124\n",
      "(Epoch 14 / 30) train acc: 0.543000; val_acc: 0.508000\n",
      "(Iteration 10801 / 22950) loss: 1.447814\n",
      "(Iteration 10901 / 22950) loss: 1.850690\n",
      "(Iteration 11001 / 22950) loss: 1.627360\n",
      "(Iteration 11101 / 22950) loss: 1.452883\n",
      "(Iteration 11201 / 22950) loss: 1.337858\n",
      "(Iteration 11301 / 22950) loss: 1.542511\n",
      "(Iteration 11401 / 22950) loss: 1.544993\n",
      "(Epoch 15 / 30) train acc: 0.499000; val_acc: 0.505000\n",
      "(Iteration 11501 / 22950) loss: 1.502236\n",
      "(Iteration 11601 / 22950) loss: 1.447622\n",
      "(Iteration 11701 / 22950) loss: 1.517091\n",
      "(Iteration 11801 / 22950) loss: 1.467776\n",
      "(Iteration 11901 / 22950) loss: 1.426845\n",
      "(Iteration 12001 / 22950) loss: 1.435303\n",
      "(Iteration 12101 / 22950) loss: 1.508279\n",
      "(Iteration 12201 / 22950) loss: 1.519095\n",
      "(Epoch 16 / 30) train acc: 0.515000; val_acc: 0.514000\n",
      "(Iteration 12301 / 22950) loss: 1.393130\n",
      "(Iteration 12401 / 22950) loss: 1.408445\n",
      "(Iteration 12501 / 22950) loss: 1.624156\n",
      "(Iteration 12601 / 22950) loss: 1.397938\n",
      "(Iteration 12701 / 22950) loss: 1.622478\n",
      "(Iteration 12801 / 22950) loss: 1.591456\n",
      "(Iteration 12901 / 22950) loss: 1.611792\n",
      "(Iteration 13001 / 22950) loss: 1.392620\n",
      "(Epoch 17 / 30) train acc: 0.504000; val_acc: 0.509000\n",
      "(Iteration 13101 / 22950) loss: 1.431263\n",
      "(Iteration 13201 / 22950) loss: 1.504950\n",
      "(Iteration 13301 / 22950) loss: 1.617759\n",
      "(Iteration 13401 / 22950) loss: 1.761527\n",
      "(Iteration 13501 / 22950) loss: 1.345573\n",
      "(Iteration 13601 / 22950) loss: 1.313654\n",
      "(Iteration 13701 / 22950) loss: 1.602420\n",
      "(Epoch 18 / 30) train acc: 0.506000; val_acc: 0.512000\n",
      "(Iteration 13801 / 22950) loss: 1.431905\n",
      "(Iteration 13901 / 22950) loss: 1.468641\n",
      "(Iteration 14001 / 22950) loss: 1.563789\n",
      "(Iteration 14101 / 22950) loss: 1.406596\n",
      "(Iteration 14201 / 22950) loss: 1.508711\n",
      "(Iteration 14301 / 22950) loss: 1.491962\n",
      "(Iteration 14401 / 22950) loss: 1.421813\n",
      "(Iteration 14501 / 22950) loss: 1.496338\n",
      "(Epoch 19 / 30) train acc: 0.512000; val_acc: 0.506000\n",
      "(Iteration 14601 / 22950) loss: 1.494264\n",
      "(Iteration 14701 / 22950) loss: 1.511183\n",
      "(Iteration 14801 / 22950) loss: 1.378319\n",
      "(Iteration 14901 / 22950) loss: 1.470117\n",
      "(Iteration 15001 / 22950) loss: 1.566596\n",
      "(Iteration 15101 / 22950) loss: 1.435386\n",
      "(Iteration 15201 / 22950) loss: 1.422119\n",
      "(Epoch 20 / 30) train acc: 0.536000; val_acc: 0.507000\n",
      "(Iteration 15301 / 22950) loss: 1.409387\n",
      "(Iteration 15401 / 22950) loss: 1.195844\n",
      "(Iteration 15501 / 22950) loss: 1.480849\n",
      "(Iteration 15601 / 22950) loss: 1.535168\n",
      "(Iteration 15701 / 22950) loss: 1.582885\n",
      "(Iteration 15801 / 22950) loss: 1.315123\n",
      "(Iteration 15901 / 22950) loss: 1.528233\n",
      "(Iteration 16001 / 22950) loss: 1.418204\n",
      "(Epoch 21 / 30) train acc: 0.542000; val_acc: 0.505000\n",
      "(Iteration 16101 / 22950) loss: 1.632582\n",
      "(Iteration 16201 / 22950) loss: 1.372990\n",
      "(Iteration 16301 / 22950) loss: 1.364186\n",
      "(Iteration 16401 / 22950) loss: 1.561810\n",
      "(Iteration 16501 / 22950) loss: 1.562755\n",
      "(Iteration 16601 / 22950) loss: 1.495245\n",
      "(Iteration 16701 / 22950) loss: 1.492830\n",
      "(Iteration 16801 / 22950) loss: 1.344062\n",
      "(Epoch 22 / 30) train acc: 0.512000; val_acc: 0.503000\n",
      "(Iteration 16901 / 22950) loss: 1.530103\n",
      "(Iteration 17001 / 22950) loss: 1.594268\n",
      "(Iteration 17101 / 22950) loss: 1.422225\n",
      "(Iteration 17201 / 22950) loss: 1.683247\n",
      "(Iteration 17301 / 22950) loss: 1.601365\n",
      "(Iteration 17401 / 22950) loss: 1.386479\n",
      "(Iteration 17501 / 22950) loss: 1.550598\n",
      "(Epoch 23 / 30) train acc: 0.504000; val_acc: 0.503000\n",
      "(Iteration 17601 / 22950) loss: 1.377430\n",
      "(Iteration 17701 / 22950) loss: 1.428533\n",
      "(Iteration 17801 / 22950) loss: 1.510802\n",
      "(Iteration 17901 / 22950) loss: 1.573562\n",
      "(Iteration 18001 / 22950) loss: 1.439413\n",
      "(Iteration 18101 / 22950) loss: 1.330507\n",
      "(Iteration 18201 / 22950) loss: 1.443297\n",
      "(Iteration 18301 / 22950) loss: 1.510665\n",
      "(Epoch 24 / 30) train acc: 0.525000; val_acc: 0.504000\n",
      "(Iteration 18401 / 22950) loss: 1.552367\n",
      "(Iteration 18501 / 22950) loss: 1.423019\n",
      "(Iteration 18601 / 22950) loss: 1.582273\n",
      "(Iteration 18701 / 22950) loss: 1.409197\n",
      "(Iteration 18801 / 22950) loss: 1.502811\n",
      "(Iteration 18901 / 22950) loss: 1.529317\n",
      "(Iteration 19001 / 22950) loss: 1.581222\n",
      "(Iteration 19101 / 22950) loss: 1.544725\n",
      "(Epoch 25 / 30) train acc: 0.508000; val_acc: 0.501000\n",
      "(Iteration 19201 / 22950) loss: 1.265757\n",
      "(Iteration 19301 / 22950) loss: 1.515956\n",
      "(Iteration 19401 / 22950) loss: 1.113501\n",
      "(Iteration 19501 / 22950) loss: 1.394078\n",
      "(Iteration 19601 / 22950) loss: 1.501915\n",
      "(Iteration 19701 / 22950) loss: 1.501050\n",
      "(Iteration 19801 / 22950) loss: 1.400279\n",
      "(Epoch 26 / 30) train acc: 0.518000; val_acc: 0.504000\n",
      "(Iteration 19901 / 22950) loss: 1.497503\n",
      "(Iteration 20001 / 22950) loss: 1.417090\n",
      "(Iteration 20101 / 22950) loss: 1.489506\n",
      "(Iteration 20201 / 22950) loss: 1.404335\n",
      "(Iteration 20301 / 22950) loss: 1.377757\n",
      "(Iteration 20401 / 22950) loss: 1.573232\n",
      "(Iteration 20501 / 22950) loss: 1.395075\n",
      "(Iteration 20601 / 22950) loss: 1.387422\n",
      "(Epoch 27 / 30) train acc: 0.513000; val_acc: 0.504000\n",
      "(Iteration 20701 / 22950) loss: 1.334411\n",
      "(Iteration 20801 / 22950) loss: 1.735804\n",
      "(Iteration 20901 / 22950) loss: 1.204068\n",
      "(Iteration 21001 / 22950) loss: 1.330987\n",
      "(Iteration 21101 / 22950) loss: 1.435590\n",
      "(Iteration 21201 / 22950) loss: 1.710682\n",
      "(Iteration 21301 / 22950) loss: 1.408169\n",
      "(Iteration 21401 / 22950) loss: 1.365959\n",
      "(Epoch 28 / 30) train acc: 0.526000; val_acc: 0.507000\n",
      "(Iteration 21501 / 22950) loss: 1.585777\n",
      "(Iteration 21601 / 22950) loss: 1.407298\n",
      "(Iteration 21701 / 22950) loss: 1.569217\n",
      "(Iteration 21801 / 22950) loss: 1.352457\n",
      "(Iteration 21901 / 22950) loss: 1.592463\n",
      "(Iteration 22001 / 22950) loss: 1.626562\n",
      "(Iteration 22101 / 22950) loss: 1.383278\n",
      "(Epoch 29 / 30) train acc: 0.540000; val_acc: 0.505000\n",
      "(Iteration 22201 / 22950) loss: 1.474911\n",
      "(Iteration 22301 / 22950) loss: 1.557047\n",
      "(Iteration 22401 / 22950) loss: 1.540180\n",
      "(Iteration 22501 / 22950) loss: 1.381109\n",
      "(Iteration 22601 / 22950) loss: 1.242213\n",
      "(Iteration 22701 / 22950) loss: 1.324653\n",
      "(Iteration 22801 / 22950) loss: 1.528267\n",
      "(Iteration 22901 / 22950) loss: 1.184734\n",
      "(Epoch 30 / 30) train acc: 0.549000; val_acc: 0.503000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.01, 'num_epochs': 30, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 45930) loss: 2.303333\n",
      "(Epoch 0 / 30) train acc: 0.104000; val_acc: 0.098000\n",
      "(Iteration 101 / 45930) loss: 2.301667\n",
      "(Iteration 201 / 45930) loss: 2.303127\n",
      "(Iteration 301 / 45930) loss: 2.300512\n",
      "(Iteration 401 / 45930) loss: 2.302150\n",
      "(Iteration 501 / 45930) loss: 2.303713\n",
      "(Iteration 601 / 45930) loss: 2.297398\n",
      "(Iteration 701 / 45930) loss: 2.284248\n",
      "(Iteration 801 / 45930) loss: 2.289995\n",
      "(Iteration 901 / 45930) loss: 2.279813\n",
      "(Iteration 1001 / 45930) loss: 2.199871\n",
      "(Iteration 1101 / 45930) loss: 2.098927\n",
      "(Iteration 1201 / 45930) loss: 2.071581\n",
      "(Iteration 1301 / 45930) loss: 2.197360\n",
      "(Iteration 1401 / 45930) loss: 2.055740\n",
      "(Iteration 1501 / 45930) loss: 1.953657\n",
      "(Epoch 1 / 30) train acc: 0.301000; val_acc: 0.305000\n",
      "(Iteration 1601 / 45930) loss: 2.012881\n",
      "(Iteration 1701 / 45930) loss: 1.939207\n",
      "(Iteration 1801 / 45930) loss: 1.855590\n",
      "(Iteration 1901 / 45930) loss: 1.764478\n",
      "(Iteration 2001 / 45930) loss: 1.885881\n",
      "(Iteration 2101 / 45930) loss: 1.816440\n",
      "(Iteration 2201 / 45930) loss: 1.629839\n",
      "(Iteration 2301 / 45930) loss: 1.516429\n",
      "(Iteration 2401 / 45930) loss: 1.846028\n",
      "(Iteration 2501 / 45930) loss: 1.822610\n",
      "(Iteration 2601 / 45930) loss: 1.806783\n",
      "(Iteration 2701 / 45930) loss: 1.703858\n",
      "(Iteration 2801 / 45930) loss: 1.811477\n",
      "(Iteration 2901 / 45930) loss: 1.797672\n",
      "(Iteration 3001 / 45930) loss: 1.682470\n",
      "(Epoch 2 / 30) train acc: 0.411000; val_acc: 0.423000\n",
      "(Iteration 3101 / 45930) loss: 1.668628\n",
      "(Iteration 3201 / 45930) loss: 1.659083\n",
      "(Iteration 3301 / 45930) loss: 1.464818\n",
      "(Iteration 3401 / 45930) loss: 1.595390\n",
      "(Iteration 3501 / 45930) loss: 1.774876\n",
      "(Iteration 3601 / 45930) loss: 1.538219\n",
      "(Iteration 3701 / 45930) loss: 1.652625\n",
      "(Iteration 3801 / 45930) loss: 1.615452\n",
      "(Iteration 3901 / 45930) loss: 1.785320\n",
      "(Iteration 4001 / 45930) loss: 1.584109\n",
      "(Iteration 4101 / 45930) loss: 1.562946\n",
      "(Iteration 4201 / 45930) loss: 1.326094\n",
      "(Iteration 4301 / 45930) loss: 1.444346\n",
      "(Iteration 4401 / 45930) loss: 1.348983\n",
      "(Iteration 4501 / 45930) loss: 1.330129\n",
      "(Epoch 3 / 30) train acc: 0.466000; val_acc: 0.494000\n",
      "(Iteration 4601 / 45930) loss: 1.514321\n",
      "(Iteration 4701 / 45930) loss: 1.526305\n",
      "(Iteration 4801 / 45930) loss: 1.494790\n",
      "(Iteration 4901 / 45930) loss: 1.473819\n",
      "(Iteration 5001 / 45930) loss: 1.733879\n",
      "(Iteration 5101 / 45930) loss: 1.780296\n",
      "(Iteration 5201 / 45930) loss: 1.266226\n",
      "(Iteration 5301 / 45930) loss: 1.584756\n",
      "(Iteration 5401 / 45930) loss: 1.599188\n",
      "(Iteration 5501 / 45930) loss: 1.564355\n",
      "(Iteration 5601 / 45930) loss: 1.358135\n",
      "(Iteration 5701 / 45930) loss: 1.691747\n",
      "(Iteration 5801 / 45930) loss: 1.223394\n",
      "(Iteration 5901 / 45930) loss: 1.308230\n",
      "(Iteration 6001 / 45930) loss: 1.189458\n",
      "(Iteration 6101 / 45930) loss: 1.480262\n",
      "(Epoch 4 / 30) train acc: 0.496000; val_acc: 0.499000\n",
      "(Iteration 6201 / 45930) loss: 1.425053\n",
      "(Iteration 6301 / 45930) loss: 1.570840\n",
      "(Iteration 6401 / 45930) loss: 1.524850\n",
      "(Iteration 6501 / 45930) loss: 1.254309\n",
      "(Iteration 6601 / 45930) loss: 1.415506\n",
      "(Iteration 6701 / 45930) loss: 1.346012\n",
      "(Iteration 6801 / 45930) loss: 1.636079\n",
      "(Iteration 6901 / 45930) loss: 1.413720\n",
      "(Iteration 7001 / 45930) loss: 1.393693\n",
      "(Iteration 7101 / 45930) loss: 1.366131\n",
      "(Iteration 7201 / 45930) loss: 1.390632\n",
      "(Iteration 7301 / 45930) loss: 1.700714\n",
      "(Iteration 7401 / 45930) loss: 1.497691\n",
      "(Iteration 7501 / 45930) loss: 1.551316\n",
      "(Iteration 7601 / 45930) loss: 1.564774\n",
      "(Epoch 5 / 30) train acc: 0.522000; val_acc: 0.509000\n",
      "(Iteration 7701 / 45930) loss: 1.616781\n",
      "(Iteration 7801 / 45930) loss: 1.493091\n",
      "(Iteration 7901 / 45930) loss: 1.510600\n",
      "(Iteration 8001 / 45930) loss: 1.396036\n",
      "(Iteration 8101 / 45930) loss: 1.623519\n",
      "(Iteration 8201 / 45930) loss: 1.553633\n",
      "(Iteration 8301 / 45930) loss: 1.369161\n",
      "(Iteration 8401 / 45930) loss: 1.561592\n",
      "(Iteration 8501 / 45930) loss: 1.382537\n",
      "(Iteration 8601 / 45930) loss: 1.254600\n",
      "(Iteration 8701 / 45930) loss: 1.398334\n",
      "(Iteration 8801 / 45930) loss: 1.113243\n",
      "(Iteration 8901 / 45930) loss: 1.399928\n",
      "(Iteration 9001 / 45930) loss: 1.534691\n",
      "(Iteration 9101 / 45930) loss: 1.439820\n",
      "(Epoch 6 / 30) train acc: 0.503000; val_acc: 0.511000\n",
      "(Iteration 9201 / 45930) loss: 1.623469\n",
      "(Iteration 9301 / 45930) loss: 1.308392\n",
      "(Iteration 9401 / 45930) loss: 1.396041\n",
      "(Iteration 9501 / 45930) loss: 1.338155\n",
      "(Iteration 9601 / 45930) loss: 1.863779\n",
      "(Iteration 9701 / 45930) loss: 1.589601\n",
      "(Iteration 9801 / 45930) loss: 1.502802\n",
      "(Iteration 9901 / 45930) loss: 1.706971\n",
      "(Iteration 10001 / 45930) loss: 1.561976\n",
      "(Iteration 10101 / 45930) loss: 1.263082\n",
      "(Iteration 10201 / 45930) loss: 1.652477\n",
      "(Iteration 10301 / 45930) loss: 1.444673\n",
      "(Iteration 10401 / 45930) loss: 1.410337\n",
      "(Iteration 10501 / 45930) loss: 1.691257\n",
      "(Iteration 10601 / 45930) loss: 1.354799\n",
      "(Iteration 10701 / 45930) loss: 1.668610\n",
      "(Epoch 7 / 30) train acc: 0.522000; val_acc: 0.515000\n",
      "(Iteration 10801 / 45930) loss: 1.365158\n",
      "(Iteration 10901 / 45930) loss: 1.513995\n",
      "(Iteration 11001 / 45930) loss: 1.679441\n",
      "(Iteration 11101 / 45930) loss: 1.216778\n",
      "(Iteration 11201 / 45930) loss: 1.227283\n",
      "(Iteration 11301 / 45930) loss: 1.715526\n",
      "(Iteration 11401 / 45930) loss: 1.415013\n",
      "(Iteration 11501 / 45930) loss: 1.510916\n",
      "(Iteration 11601 / 45930) loss: 1.461489\n",
      "(Iteration 11701 / 45930) loss: 1.356296\n",
      "(Iteration 11801 / 45930) loss: 1.676041\n",
      "(Iteration 11901 / 45930) loss: 1.437957\n",
      "(Iteration 12001 / 45930) loss: 1.406154\n",
      "(Iteration 12101 / 45930) loss: 1.468312\n",
      "(Iteration 12201 / 45930) loss: 1.319527\n",
      "(Epoch 8 / 30) train acc: 0.546000; val_acc: 0.517000\n",
      "(Iteration 12301 / 45930) loss: 1.543957\n",
      "(Iteration 12401 / 45930) loss: 1.493044\n",
      "(Iteration 12501 / 45930) loss: 1.508199\n",
      "(Iteration 12601 / 45930) loss: 1.417786\n",
      "(Iteration 12701 / 45930) loss: 1.416874\n",
      "(Iteration 12801 / 45930) loss: 1.385799\n",
      "(Iteration 12901 / 45930) loss: 1.578394\n",
      "(Iteration 13001 / 45930) loss: 1.311889\n",
      "(Iteration 13101 / 45930) loss: 1.538559\n",
      "(Iteration 13201 / 45930) loss: 1.297738\n",
      "(Iteration 13301 / 45930) loss: 1.575572\n",
      "(Iteration 13401 / 45930) loss: 1.205519\n",
      "(Iteration 13501 / 45930) loss: 1.575605\n",
      "(Iteration 13601 / 45930) loss: 1.395094\n",
      "(Iteration 13701 / 45930) loss: 1.404018\n",
      "(Epoch 9 / 30) train acc: 0.514000; val_acc: 0.512000\n",
      "(Iteration 13801 / 45930) loss: 1.326048\n",
      "(Iteration 13901 / 45930) loss: 1.538565\n",
      "(Iteration 14001 / 45930) loss: 1.510005\n",
      "(Iteration 14101 / 45930) loss: 1.569632\n",
      "(Iteration 14201 / 45930) loss: 1.530281\n",
      "(Iteration 14301 / 45930) loss: 1.446508\n",
      "(Iteration 14401 / 45930) loss: 1.676612\n",
      "(Iteration 14501 / 45930) loss: 1.862278\n",
      "(Iteration 14601 / 45930) loss: 1.375262\n",
      "(Iteration 14701 / 45930) loss: 1.544023\n",
      "(Iteration 14801 / 45930) loss: 1.370713\n",
      "(Iteration 14901 / 45930) loss: 1.759664\n",
      "(Iteration 15001 / 45930) loss: 1.078088\n",
      "(Iteration 15101 / 45930) loss: 1.606805\n",
      "(Iteration 15201 / 45930) loss: 1.414806\n",
      "(Iteration 15301 / 45930) loss: 1.199923\n",
      "(Epoch 10 / 30) train acc: 0.539000; val_acc: 0.514000\n",
      "(Iteration 15401 / 45930) loss: 1.406322\n",
      "(Iteration 15501 / 45930) loss: 1.645050\n",
      "(Iteration 15601 / 45930) loss: 1.292960\n",
      "(Iteration 15701 / 45930) loss: 1.351909\n",
      "(Iteration 15801 / 45930) loss: 1.216079\n",
      "(Iteration 15901 / 45930) loss: 1.752952\n",
      "(Iteration 16001 / 45930) loss: 1.668409\n",
      "(Iteration 16101 / 45930) loss: 1.508737\n",
      "(Iteration 16201 / 45930) loss: 1.346526\n",
      "(Iteration 16301 / 45930) loss: 1.228290\n",
      "(Iteration 16401 / 45930) loss: 1.321530\n",
      "(Iteration 16501 / 45930) loss: 1.334495\n",
      "(Iteration 16601 / 45930) loss: 1.404716\n",
      "(Iteration 16701 / 45930) loss: 1.412131\n",
      "(Iteration 16801 / 45930) loss: 1.383274\n",
      "(Epoch 11 / 30) train acc: 0.522000; val_acc: 0.518000\n",
      "(Iteration 16901 / 45930) loss: 1.652771\n",
      "(Iteration 17001 / 45930) loss: 1.380701\n",
      "(Iteration 17101 / 45930) loss: 1.341044\n",
      "(Iteration 17201 / 45930) loss: 1.594666\n",
      "(Iteration 17301 / 45930) loss: 1.683984\n",
      "(Iteration 17401 / 45930) loss: 1.226640\n",
      "(Iteration 17501 / 45930) loss: 1.378116\n",
      "(Iteration 17601 / 45930) loss: 1.338156\n",
      "(Iteration 17701 / 45930) loss: 1.673194\n",
      "(Iteration 17801 / 45930) loss: 1.073775\n",
      "(Iteration 17901 / 45930) loss: 1.327641\n",
      "(Iteration 18001 / 45930) loss: 1.507623\n",
      "(Iteration 18101 / 45930) loss: 1.477868\n",
      "(Iteration 18201 / 45930) loss: 1.421007\n",
      "(Iteration 18301 / 45930) loss: 1.621086\n",
      "(Epoch 12 / 30) train acc: 0.528000; val_acc: 0.518000\n",
      "(Iteration 18401 / 45930) loss: 1.358144\n",
      "(Iteration 18501 / 45930) loss: 1.333337\n",
      "(Iteration 18601 / 45930) loss: 1.713582\n",
      "(Iteration 18701 / 45930) loss: 1.408151\n",
      "(Iteration 18801 / 45930) loss: 1.420832\n",
      "(Iteration 18901 / 45930) loss: 1.239865\n",
      "(Iteration 19001 / 45930) loss: 1.499092\n",
      "(Iteration 19101 / 45930) loss: 1.455568\n",
      "(Iteration 19201 / 45930) loss: 1.745161\n",
      "(Iteration 19301 / 45930) loss: 1.574762\n",
      "(Iteration 19401 / 45930) loss: 1.437408\n",
      "(Iteration 19501 / 45930) loss: 1.725020\n",
      "(Iteration 19601 / 45930) loss: 1.264055\n",
      "(Iteration 19701 / 45930) loss: 1.261494\n",
      "(Iteration 19801 / 45930) loss: 1.576736\n",
      "(Iteration 19901 / 45930) loss: 1.550280\n",
      "(Epoch 13 / 30) train acc: 0.551000; val_acc: 0.515000\n",
      "(Iteration 20001 / 45930) loss: 1.226810\n",
      "(Iteration 20101 / 45930) loss: 1.386853\n",
      "(Iteration 20201 / 45930) loss: 1.478953\n",
      "(Iteration 20301 / 45930) loss: 1.300490\n",
      "(Iteration 20401 / 45930) loss: 1.441983\n",
      "(Iteration 20501 / 45930) loss: 1.609212\n",
      "(Iteration 20601 / 45930) loss: 0.981767\n",
      "(Iteration 20701 / 45930) loss: 1.347487\n",
      "(Iteration 20801 / 45930) loss: 1.366643\n",
      "(Iteration 20901 / 45930) loss: 1.320874\n",
      "(Iteration 21001 / 45930) loss: 1.506246\n",
      "(Iteration 21101 / 45930) loss: 1.691701\n",
      "(Iteration 21201 / 45930) loss: 1.667911\n",
      "(Iteration 21301 / 45930) loss: 1.165946\n",
      "(Iteration 21401 / 45930) loss: 1.580020\n",
      "(Epoch 14 / 30) train acc: 0.539000; val_acc: 0.520000\n",
      "(Iteration 21501 / 45930) loss: 1.616284\n",
      "(Iteration 21601 / 45930) loss: 1.412938\n",
      "(Iteration 21701 / 45930) loss: 1.220182\n",
      "(Iteration 21801 / 45930) loss: 1.527907\n",
      "(Iteration 21901 / 45930) loss: 1.185992\n",
      "(Iteration 22001 / 45930) loss: 1.583531\n",
      "(Iteration 22101 / 45930) loss: 1.613171\n",
      "(Iteration 22201 / 45930) loss: 1.228948\n",
      "(Iteration 22301 / 45930) loss: 1.294967\n",
      "(Iteration 22401 / 45930) loss: 1.190061\n",
      "(Iteration 22501 / 45930) loss: 1.438045\n",
      "(Iteration 22601 / 45930) loss: 1.348035\n",
      "(Iteration 22701 / 45930) loss: 1.492090\n",
      "(Iteration 22801 / 45930) loss: 1.387575\n",
      "(Iteration 22901 / 45930) loss: 1.117773\n",
      "(Epoch 15 / 30) train acc: 0.511000; val_acc: 0.524000\n",
      "(Iteration 23001 / 45930) loss: 1.382495\n",
      "(Iteration 23101 / 45930) loss: 1.737710\n",
      "(Iteration 23201 / 45930) loss: 1.325865\n",
      "(Iteration 23301 / 45930) loss: 1.413895\n",
      "(Iteration 23401 / 45930) loss: 1.384467\n",
      "(Iteration 23501 / 45930) loss: 1.418116\n",
      "(Iteration 23601 / 45930) loss: 1.471850\n",
      "(Iteration 23701 / 45930) loss: 1.741154\n",
      "(Iteration 23801 / 45930) loss: 1.332509\n",
      "(Iteration 23901 / 45930) loss: 1.431583\n",
      "(Iteration 24001 / 45930) loss: 1.303502\n",
      "(Iteration 24101 / 45930) loss: 1.489681\n",
      "(Iteration 24201 / 45930) loss: 1.355433\n",
      "(Iteration 24301 / 45930) loss: 1.341875\n",
      "(Iteration 24401 / 45930) loss: 1.508929\n",
      "(Epoch 16 / 30) train acc: 0.532000; val_acc: 0.523000\n",
      "(Iteration 24501 / 45930) loss: 1.439690\n",
      "(Iteration 24601 / 45930) loss: 1.675539\n",
      "(Iteration 24701 / 45930) loss: 1.454918\n",
      "(Iteration 24801 / 45930) loss: 1.403956\n",
      "(Iteration 24901 / 45930) loss: 1.340486\n",
      "(Iteration 25001 / 45930) loss: 1.290553\n",
      "(Iteration 25101 / 45930) loss: 1.242183\n",
      "(Iteration 25201 / 45930) loss: 1.595865\n",
      "(Iteration 25301 / 45930) loss: 1.450972\n",
      "(Iteration 25401 / 45930) loss: 1.414213\n",
      "(Iteration 25501 / 45930) loss: 1.243545\n",
      "(Iteration 25601 / 45930) loss: 1.651002\n",
      "(Iteration 25701 / 45930) loss: 1.661696\n",
      "(Iteration 25801 / 45930) loss: 1.591126\n",
      "(Iteration 25901 / 45930) loss: 1.173938\n",
      "(Iteration 26001 / 45930) loss: 1.457820\n",
      "(Epoch 17 / 30) train acc: 0.515000; val_acc: 0.517000\n",
      "(Iteration 26101 / 45930) loss: 1.362381\n",
      "(Iteration 26201 / 45930) loss: 1.390803\n",
      "(Iteration 26301 / 45930) loss: 1.582062\n",
      "(Iteration 26401 / 45930) loss: 1.535155\n",
      "(Iteration 26501 / 45930) loss: 1.482159\n",
      "(Iteration 26601 / 45930) loss: 1.755552\n",
      "(Iteration 26701 / 45930) loss: 1.604549\n",
      "(Iteration 26801 / 45930) loss: 1.347925\n",
      "(Iteration 26901 / 45930) loss: 1.543019\n",
      "(Iteration 27001 / 45930) loss: 1.273628\n",
      "(Iteration 27101 / 45930) loss: 1.066158\n",
      "(Iteration 27201 / 45930) loss: 1.684387\n",
      "(Iteration 27301 / 45930) loss: 1.547063\n",
      "(Iteration 27401 / 45930) loss: 1.247518\n",
      "(Iteration 27501 / 45930) loss: 1.285365\n",
      "(Epoch 18 / 30) train acc: 0.531000; val_acc: 0.523000\n",
      "(Iteration 27601 / 45930) loss: 1.274850\n",
      "(Iteration 27701 / 45930) loss: 1.110184\n",
      "(Iteration 27801 / 45930) loss: 1.363213\n",
      "(Iteration 27901 / 45930) loss: 1.282993\n",
      "(Iteration 28001 / 45930) loss: 1.131293\n",
      "(Iteration 28101 / 45930) loss: 1.182129\n",
      "(Iteration 28201 / 45930) loss: 1.488895\n",
      "(Iteration 28301 / 45930) loss: 1.390665\n",
      "(Iteration 28401 / 45930) loss: 1.408846\n",
      "(Iteration 28501 / 45930) loss: 1.524025\n",
      "(Iteration 28601 / 45930) loss: 1.283571\n",
      "(Iteration 28701 / 45930) loss: 1.566417\n",
      "(Iteration 28801 / 45930) loss: 1.272262\n",
      "(Iteration 28901 / 45930) loss: 1.712406\n",
      "(Iteration 29001 / 45930) loss: 1.423267\n",
      "(Epoch 19 / 30) train acc: 0.565000; val_acc: 0.523000\n",
      "(Iteration 29101 / 45930) loss: 1.321869\n",
      "(Iteration 29201 / 45930) loss: 1.388173\n",
      "(Iteration 29301 / 45930) loss: 1.579736\n",
      "(Iteration 29401 / 45930) loss: 1.636661\n",
      "(Iteration 29501 / 45930) loss: 1.534709\n",
      "(Iteration 29601 / 45930) loss: 1.625072\n",
      "(Iteration 29701 / 45930) loss: 1.629192\n",
      "(Iteration 29801 / 45930) loss: 1.562538\n",
      "(Iteration 29901 / 45930) loss: 1.513441\n",
      "(Iteration 30001 / 45930) loss: 1.630410\n",
      "(Iteration 30101 / 45930) loss: 1.108143\n",
      "(Iteration 30201 / 45930) loss: 1.459805\n",
      "(Iteration 30301 / 45930) loss: 1.219524\n",
      "(Iteration 30401 / 45930) loss: 1.328099\n",
      "(Iteration 30501 / 45930) loss: 1.429006\n",
      "(Iteration 30601 / 45930) loss: 1.396753\n",
      "(Epoch 20 / 30) train acc: 0.551000; val_acc: 0.523000\n",
      "(Iteration 30701 / 45930) loss: 1.608668\n",
      "(Iteration 30801 / 45930) loss: 1.340447\n",
      "(Iteration 30901 / 45930) loss: 1.607175\n",
      "(Iteration 31001 / 45930) loss: 1.430822\n",
      "(Iteration 31101 / 45930) loss: 1.408869\n",
      "(Iteration 31201 / 45930) loss: 1.347729\n",
      "(Iteration 31301 / 45930) loss: 1.622906\n",
      "(Iteration 31401 / 45930) loss: 1.602746\n",
      "(Iteration 31501 / 45930) loss: 1.190212\n",
      "(Iteration 31601 / 45930) loss: 1.540235\n",
      "(Iteration 31701 / 45930) loss: 1.738747\n",
      "(Iteration 31801 / 45930) loss: 1.400937\n",
      "(Iteration 31901 / 45930) loss: 1.460212\n",
      "(Iteration 32001 / 45930) loss: 1.486356\n",
      "(Iteration 32101 / 45930) loss: 1.663181\n",
      "(Epoch 21 / 30) train acc: 0.516000; val_acc: 0.523000\n",
      "(Iteration 32201 / 45930) loss: 1.349447\n",
      "(Iteration 32301 / 45930) loss: 1.484164\n",
      "(Iteration 32401 / 45930) loss: 1.286094\n",
      "(Iteration 32501 / 45930) loss: 1.581480\n",
      "(Iteration 32601 / 45930) loss: 1.234243\n",
      "(Iteration 32701 / 45930) loss: 1.384541\n",
      "(Iteration 32801 / 45930) loss: 1.426846\n",
      "(Iteration 32901 / 45930) loss: 1.674772\n",
      "(Iteration 33001 / 45930) loss: 1.204946\n",
      "(Iteration 33101 / 45930) loss: 1.552153\n",
      "(Iteration 33201 / 45930) loss: 1.469307\n",
      "(Iteration 33301 / 45930) loss: 1.566527\n",
      "(Iteration 33401 / 45930) loss: 1.447427\n",
      "(Iteration 33501 / 45930) loss: 1.445476\n",
      "(Iteration 33601 / 45930) loss: 1.346957\n",
      "(Epoch 22 / 30) train acc: 0.518000; val_acc: 0.522000\n",
      "(Iteration 33701 / 45930) loss: 1.502447\n",
      "(Iteration 33801 / 45930) loss: 1.646524\n",
      "(Iteration 33901 / 45930) loss: 1.860036\n",
      "(Iteration 34001 / 45930) loss: 1.318212\n",
      "(Iteration 34101 / 45930) loss: 1.469336\n",
      "(Iteration 34201 / 45930) loss: 1.539294\n",
      "(Iteration 34301 / 45930) loss: 1.769328\n",
      "(Iteration 34401 / 45930) loss: 1.639605\n",
      "(Iteration 34501 / 45930) loss: 1.525556\n",
      "(Iteration 34601 / 45930) loss: 1.427567\n",
      "(Iteration 34701 / 45930) loss: 1.622513\n",
      "(Iteration 34801 / 45930) loss: 1.474066\n",
      "(Iteration 34901 / 45930) loss: 1.605497\n",
      "(Iteration 35001 / 45930) loss: 1.405410\n",
      "(Iteration 35101 / 45930) loss: 1.140872\n",
      "(Iteration 35201 / 45930) loss: 1.384274\n",
      "(Epoch 23 / 30) train acc: 0.523000; val_acc: 0.522000\n",
      "(Iteration 35301 / 45930) loss: 1.767570\n",
      "(Iteration 35401 / 45930) loss: 1.757249\n",
      "(Iteration 35501 / 45930) loss: 1.111862\n",
      "(Iteration 35601 / 45930) loss: 1.510391\n",
      "(Iteration 35701 / 45930) loss: 1.539235\n",
      "(Iteration 35801 / 45930) loss: 1.552424\n",
      "(Iteration 35901 / 45930) loss: 1.505460\n",
      "(Iteration 36001 / 45930) loss: 1.386113\n",
      "(Iteration 36101 / 45930) loss: 1.414050\n",
      "(Iteration 36201 / 45930) loss: 1.482071\n",
      "(Iteration 36301 / 45930) loss: 1.626644\n",
      "(Iteration 36401 / 45930) loss: 1.405151\n",
      "(Iteration 36501 / 45930) loss: 1.789845\n",
      "(Iteration 36601 / 45930) loss: 1.485136\n",
      "(Iteration 36701 / 45930) loss: 1.380799\n",
      "(Epoch 24 / 30) train acc: 0.535000; val_acc: 0.522000\n",
      "(Iteration 36801 / 45930) loss: 1.624057\n",
      "(Iteration 36901 / 45930) loss: 1.415771\n",
      "(Iteration 37001 / 45930) loss: 1.283443\n",
      "(Iteration 37101 / 45930) loss: 1.299690\n",
      "(Iteration 37201 / 45930) loss: 1.505138\n",
      "(Iteration 37301 / 45930) loss: 1.322058\n",
      "(Iteration 37401 / 45930) loss: 1.367643\n",
      "(Iteration 37501 / 45930) loss: 1.387322\n",
      "(Iteration 37601 / 45930) loss: 1.250877\n",
      "(Iteration 37701 / 45930) loss: 1.503773\n",
      "(Iteration 37801 / 45930) loss: 1.309413\n",
      "(Iteration 37901 / 45930) loss: 1.545291\n",
      "(Iteration 38001 / 45930) loss: 1.435218\n",
      "(Iteration 38101 / 45930) loss: 1.667137\n",
      "(Iteration 38201 / 45930) loss: 1.407347\n",
      "(Epoch 25 / 30) train acc: 0.550000; val_acc: 0.523000\n",
      "(Iteration 38301 / 45930) loss: 1.497357\n",
      "(Iteration 38401 / 45930) loss: 1.130537\n",
      "(Iteration 38501 / 45930) loss: 1.690535\n",
      "(Iteration 38601 / 45930) loss: 1.497236\n",
      "(Iteration 38701 / 45930) loss: 1.342812\n",
      "(Iteration 38801 / 45930) loss: 1.481229\n",
      "(Iteration 38901 / 45930) loss: 1.311985\n",
      "(Iteration 39001 / 45930) loss: 1.207477\n",
      "(Iteration 39101 / 45930) loss: 1.361781\n",
      "(Iteration 39201 / 45930) loss: 1.375876\n",
      "(Iteration 39301 / 45930) loss: 1.377588\n",
      "(Iteration 39401 / 45930) loss: 1.668836\n",
      "(Iteration 39501 / 45930) loss: 1.433108\n",
      "(Iteration 39601 / 45930) loss: 1.379051\n",
      "(Iteration 39701 / 45930) loss: 1.454353\n",
      "(Iteration 39801 / 45930) loss: 1.586622\n",
      "(Epoch 26 / 30) train acc: 0.556000; val_acc: 0.522000\n",
      "(Iteration 39901 / 45930) loss: 1.479432\n",
      "(Iteration 40001 / 45930) loss: 1.338245\n",
      "(Iteration 40101 / 45930) loss: 1.404288\n",
      "(Iteration 40201 / 45930) loss: 1.656605\n",
      "(Iteration 40301 / 45930) loss: 1.339519\n",
      "(Iteration 40401 / 45930) loss: 1.519167\n",
      "(Iteration 40501 / 45930) loss: 1.488652\n",
      "(Iteration 40601 / 45930) loss: 1.574140\n",
      "(Iteration 40701 / 45930) loss: 1.468442\n",
      "(Iteration 40801 / 45930) loss: 1.334472\n",
      "(Iteration 40901 / 45930) loss: 1.076214\n",
      "(Iteration 41001 / 45930) loss: 1.336917\n",
      "(Iteration 41101 / 45930) loss: 1.325055\n",
      "(Iteration 41201 / 45930) loss: 1.626241\n",
      "(Iteration 41301 / 45930) loss: 1.457831\n",
      "(Epoch 27 / 30) train acc: 0.536000; val_acc: 0.523000\n",
      "(Iteration 41401 / 45930) loss: 1.583221\n",
      "(Iteration 41501 / 45930) loss: 1.299428\n",
      "(Iteration 41601 / 45930) loss: 1.475274\n",
      "(Iteration 41701 / 45930) loss: 1.454037\n",
      "(Iteration 41801 / 45930) loss: 1.717009\n",
      "(Iteration 41901 / 45930) loss: 1.695793\n",
      "(Iteration 42001 / 45930) loss: 1.167704\n",
      "(Iteration 42101 / 45930) loss: 1.473244\n",
      "(Iteration 42201 / 45930) loss: 1.344830\n",
      "(Iteration 42301 / 45930) loss: 1.468520\n",
      "(Iteration 42401 / 45930) loss: 1.210611\n",
      "(Iteration 42501 / 45930) loss: 1.578997\n",
      "(Iteration 42601 / 45930) loss: 1.483223\n",
      "(Iteration 42701 / 45930) loss: 1.608600\n",
      "(Iteration 42801 / 45930) loss: 1.510505\n",
      "(Epoch 28 / 30) train acc: 0.551000; val_acc: 0.519000\n",
      "(Iteration 42901 / 45930) loss: 1.741211\n",
      "(Iteration 43001 / 45930) loss: 1.671776\n",
      "(Iteration 43101 / 45930) loss: 1.587007\n",
      "(Iteration 43201 / 45930) loss: 1.388876\n",
      "(Iteration 43301 / 45930) loss: 1.521271\n",
      "(Iteration 43401 / 45930) loss: 1.322341\n",
      "(Iteration 43501 / 45930) loss: 1.460555\n",
      "(Iteration 43601 / 45930) loss: 1.600732\n",
      "(Iteration 43701 / 45930) loss: 1.740164\n",
      "(Iteration 43801 / 45930) loss: 1.511566\n",
      "(Iteration 43901 / 45930) loss: 1.517964\n",
      "(Iteration 44001 / 45930) loss: 1.612527\n",
      "(Iteration 44101 / 45930) loss: 1.410331\n",
      "(Iteration 44201 / 45930) loss: 1.320490\n",
      "(Iteration 44301 / 45930) loss: 1.263797\n",
      "(Epoch 29 / 30) train acc: 0.516000; val_acc: 0.521000\n",
      "(Iteration 44401 / 45930) loss: 1.540428\n",
      "(Iteration 44501 / 45930) loss: 1.527972\n",
      "(Iteration 44601 / 45930) loss: 1.289599\n",
      "(Iteration 44701 / 45930) loss: 1.308964\n",
      "(Iteration 44801 / 45930) loss: 1.512618\n",
      "(Iteration 44901 / 45930) loss: 1.444371\n",
      "(Iteration 45001 / 45930) loss: 1.463380\n",
      "(Iteration 45101 / 45930) loss: 1.182013\n",
      "(Iteration 45201 / 45930) loss: 1.473397\n",
      "(Iteration 45301 / 45930) loss: 1.373360\n",
      "(Iteration 45401 / 45930) loss: 1.403035\n",
      "(Iteration 45501 / 45930) loss: 1.440308\n",
      "(Iteration 45601 / 45930) loss: 1.899985\n",
      "(Iteration 45701 / 45930) loss: 1.294901\n",
      "(Iteration 45801 / 45930) loss: 1.430075\n",
      "(Iteration 45901 / 45930) loss: 1.553954\n",
      "(Epoch 30 / 30) train acc: 0.531000; val_acc: 0.519000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.01, 'num_epochs': 40, 'reg': 0.1, 'batch_size': 64}\n",
      "(Iteration 1 / 30600) loss: 2.309072\n",
      "(Epoch 0 / 40) train acc: 0.105000; val_acc: 0.121000\n",
      "(Iteration 101 / 30600) loss: 2.308166\n",
      "(Iteration 201 / 30600) loss: 2.306436\n",
      "(Iteration 301 / 30600) loss: 2.305843\n",
      "(Iteration 401 / 30600) loss: 2.304563\n",
      "(Iteration 501 / 30600) loss: 2.302670\n",
      "(Iteration 601 / 30600) loss: 2.303153\n",
      "(Iteration 701 / 30600) loss: 2.300229\n",
      "(Epoch 1 / 40) train acc: 0.198000; val_acc: 0.163000\n",
      "(Iteration 801 / 30600) loss: 2.299561\n",
      "(Iteration 901 / 30600) loss: 2.297217\n",
      "(Iteration 1001 / 30600) loss: 2.292916\n",
      "(Iteration 1101 / 30600) loss: 2.289924\n",
      "(Iteration 1201 / 30600) loss: 2.273628\n",
      "(Iteration 1301 / 30600) loss: 2.280082\n",
      "(Iteration 1401 / 30600) loss: 2.252248\n",
      "(Iteration 1501 / 30600) loss: 2.186243\n",
      "(Epoch 2 / 40) train acc: 0.237000; val_acc: 0.235000\n",
      "(Iteration 1601 / 30600) loss: 2.269279\n",
      "(Iteration 1701 / 30600) loss: 2.140931\n",
      "(Iteration 1801 / 30600) loss: 2.210055\n",
      "(Iteration 1901 / 30600) loss: 2.155157\n",
      "(Iteration 2001 / 30600) loss: 2.168408\n",
      "(Iteration 2101 / 30600) loss: 2.118457\n",
      "(Iteration 2201 / 30600) loss: 2.151369\n",
      "(Epoch 3 / 40) train acc: 0.304000; val_acc: 0.283000\n",
      "(Iteration 2301 / 30600) loss: 2.187725\n",
      "(Iteration 2401 / 30600) loss: 2.117906\n",
      "(Iteration 2501 / 30600) loss: 2.125434\n",
      "(Iteration 2601 / 30600) loss: 2.112816\n",
      "(Iteration 2701 / 30600) loss: 2.132913\n",
      "(Iteration 2801 / 30600) loss: 2.150493\n",
      "(Iteration 2901 / 30600) loss: 2.116338\n",
      "(Iteration 3001 / 30600) loss: 2.133245\n",
      "(Epoch 4 / 40) train acc: 0.274000; val_acc: 0.288000\n",
      "(Iteration 3101 / 30600) loss: 2.070795\n",
      "(Iteration 3201 / 30600) loss: 2.098512\n",
      "(Iteration 3301 / 30600) loss: 2.056024\n",
      "(Iteration 3401 / 30600) loss: 2.007152\n",
      "(Iteration 3501 / 30600) loss: 2.054765\n",
      "(Iteration 3601 / 30600) loss: 2.047181\n",
      "(Iteration 3701 / 30600) loss: 2.078197\n",
      "(Iteration 3801 / 30600) loss: 2.061441\n",
      "(Epoch 5 / 40) train acc: 0.339000; val_acc: 0.339000\n",
      "(Iteration 3901 / 30600) loss: 2.035620\n",
      "(Iteration 4001 / 30600) loss: 2.010826\n",
      "(Iteration 4101 / 30600) loss: 2.080724\n",
      "(Iteration 4201 / 30600) loss: 2.076359\n",
      "(Iteration 4301 / 30600) loss: 2.168412\n",
      "(Iteration 4401 / 30600) loss: 2.022230\n",
      "(Iteration 4501 / 30600) loss: 1.954409\n",
      "(Epoch 6 / 40) train acc: 0.338000; val_acc: 0.352000\n",
      "(Iteration 4601 / 30600) loss: 2.071094\n",
      "(Iteration 4701 / 30600) loss: 2.006721\n",
      "(Iteration 4801 / 30600) loss: 1.917372\n",
      "(Iteration 4901 / 30600) loss: 1.966132\n",
      "(Iteration 5001 / 30600) loss: 2.008548\n",
      "(Iteration 5101 / 30600) loss: 1.993152\n",
      "(Iteration 5201 / 30600) loss: 1.966180\n",
      "(Iteration 5301 / 30600) loss: 1.979517\n",
      "(Epoch 7 / 40) train acc: 0.364000; val_acc: 0.378000\n",
      "(Iteration 5401 / 30600) loss: 2.015708\n",
      "(Iteration 5501 / 30600) loss: 2.010602\n",
      "(Iteration 5601 / 30600) loss: 2.024671\n",
      "(Iteration 5701 / 30600) loss: 1.964045\n",
      "(Iteration 5801 / 30600) loss: 2.019320\n",
      "(Iteration 5901 / 30600) loss: 2.084529\n",
      "(Iteration 6001 / 30600) loss: 1.923753\n",
      "(Iteration 6101 / 30600) loss: 2.023817\n",
      "(Epoch 8 / 40) train acc: 0.367000; val_acc: 0.392000\n",
      "(Iteration 6201 / 30600) loss: 1.945233\n",
      "(Iteration 6301 / 30600) loss: 1.960409\n",
      "(Iteration 6401 / 30600) loss: 1.909093\n",
      "(Iteration 6501 / 30600) loss: 1.992715\n",
      "(Iteration 6601 / 30600) loss: 1.993184\n",
      "(Iteration 6701 / 30600) loss: 2.031829\n",
      "(Iteration 6801 / 30600) loss: 2.001129\n",
      "(Epoch 9 / 40) train acc: 0.380000; val_acc: 0.391000\n",
      "(Iteration 6901 / 30600) loss: 2.003078\n",
      "(Iteration 7001 / 30600) loss: 2.035364\n",
      "(Iteration 7101 / 30600) loss: 2.015385\n",
      "(Iteration 7201 / 30600) loss: 2.080229\n",
      "(Iteration 7301 / 30600) loss: 1.997688\n",
      "(Iteration 7401 / 30600) loss: 2.024222\n",
      "(Iteration 7501 / 30600) loss: 1.984910\n",
      "(Iteration 7601 / 30600) loss: 2.025850\n",
      "(Epoch 10 / 40) train acc: 0.418000; val_acc: 0.401000\n",
      "(Iteration 7701 / 30600) loss: 1.981178\n",
      "(Iteration 7801 / 30600) loss: 2.054927\n",
      "(Iteration 7901 / 30600) loss: 2.066057\n",
      "(Iteration 8001 / 30600) loss: 1.977472\n",
      "(Iteration 8101 / 30600) loss: 2.051208\n",
      "(Iteration 8201 / 30600) loss: 1.942076\n",
      "(Iteration 8301 / 30600) loss: 2.045645\n",
      "(Iteration 8401 / 30600) loss: 2.128907\n",
      "(Epoch 11 / 40) train acc: 0.390000; val_acc: 0.405000\n",
      "(Iteration 8501 / 30600) loss: 2.017147\n",
      "(Iteration 8601 / 30600) loss: 2.034458\n",
      "(Iteration 8701 / 30600) loss: 2.028879\n",
      "(Iteration 8801 / 30600) loss: 1.977002\n",
      "(Iteration 8901 / 30600) loss: 1.918217\n",
      "(Iteration 9001 / 30600) loss: 1.986315\n",
      "(Iteration 9101 / 30600) loss: 1.986164\n",
      "(Epoch 12 / 40) train acc: 0.421000; val_acc: 0.412000\n",
      "(Iteration 9201 / 30600) loss: 1.947880\n",
      "(Iteration 9301 / 30600) loss: 1.965526\n",
      "(Iteration 9401 / 30600) loss: 1.939767\n",
      "(Iteration 9501 / 30600) loss: 2.072618\n",
      "(Iteration 9601 / 30600) loss: 2.005206\n",
      "(Iteration 9701 / 30600) loss: 1.952558\n",
      "(Iteration 9801 / 30600) loss: 1.881383\n",
      "(Iteration 9901 / 30600) loss: 2.104792\n",
      "(Epoch 13 / 40) train acc: 0.423000; val_acc: 0.405000\n",
      "(Iteration 10001 / 30600) loss: 1.956499\n",
      "(Iteration 10101 / 30600) loss: 2.049959\n",
      "(Iteration 10201 / 30600) loss: 1.917086\n",
      "(Iteration 10301 / 30600) loss: 1.954810\n",
      "(Iteration 10401 / 30600) loss: 2.025025\n",
      "(Iteration 10501 / 30600) loss: 1.883519\n",
      "(Iteration 10601 / 30600) loss: 2.022001\n",
      "(Iteration 10701 / 30600) loss: 2.039827\n",
      "(Epoch 14 / 40) train acc: 0.413000; val_acc: 0.407000\n",
      "(Iteration 10801 / 30600) loss: 1.937511\n",
      "(Iteration 10901 / 30600) loss: 2.003932\n",
      "(Iteration 11001 / 30600) loss: 2.003444\n",
      "(Iteration 11101 / 30600) loss: 2.007731\n",
      "(Iteration 11201 / 30600) loss: 1.901299\n",
      "(Iteration 11301 / 30600) loss: 1.967293\n",
      "(Iteration 11401 / 30600) loss: 1.980219\n",
      "(Epoch 15 / 40) train acc: 0.405000; val_acc: 0.406000\n",
      "(Iteration 11501 / 30600) loss: 2.050418\n",
      "(Iteration 11601 / 30600) loss: 1.930435\n",
      "(Iteration 11701 / 30600) loss: 1.968468\n",
      "(Iteration 11801 / 30600) loss: 1.923529\n",
      "(Iteration 11901 / 30600) loss: 1.942348\n",
      "(Iteration 12001 / 30600) loss: 2.023794\n",
      "(Iteration 12101 / 30600) loss: 1.871956\n",
      "(Iteration 12201 / 30600) loss: 2.045984\n",
      "(Epoch 16 / 40) train acc: 0.408000; val_acc: 0.407000\n",
      "(Iteration 12301 / 30600) loss: 1.887998\n",
      "(Iteration 12401 / 30600) loss: 1.935860\n",
      "(Iteration 12501 / 30600) loss: 1.976789\n",
      "(Iteration 12601 / 30600) loss: 1.997815\n",
      "(Iteration 12701 / 30600) loss: 2.021367\n",
      "(Iteration 12801 / 30600) loss: 1.976826\n",
      "(Iteration 12901 / 30600) loss: 1.942362\n",
      "(Iteration 13001 / 30600) loss: 1.967139\n",
      "(Epoch 17 / 40) train acc: 0.427000; val_acc: 0.413000\n",
      "(Iteration 13101 / 30600) loss: 2.030675\n",
      "(Iteration 13201 / 30600) loss: 1.940592\n",
      "(Iteration 13301 / 30600) loss: 1.922140\n",
      "(Iteration 13401 / 30600) loss: 2.078400\n",
      "(Iteration 13501 / 30600) loss: 1.890994\n",
      "(Iteration 13601 / 30600) loss: 1.903875\n",
      "(Iteration 13701 / 30600) loss: 1.988054\n",
      "(Epoch 18 / 40) train acc: 0.411000; val_acc: 0.411000\n",
      "(Iteration 13801 / 30600) loss: 1.862431\n",
      "(Iteration 13901 / 30600) loss: 1.871915\n",
      "(Iteration 14001 / 30600) loss: 2.005631\n",
      "(Iteration 14101 / 30600) loss: 1.968680\n",
      "(Iteration 14201 / 30600) loss: 1.902129\n",
      "(Iteration 14301 / 30600) loss: 2.000502\n",
      "(Iteration 14401 / 30600) loss: 1.965172\n",
      "(Iteration 14501 / 30600) loss: 2.023942\n",
      "(Epoch 19 / 40) train acc: 0.434000; val_acc: 0.410000\n",
      "(Iteration 14601 / 30600) loss: 1.947747\n",
      "(Iteration 14701 / 30600) loss: 1.938730\n",
      "(Iteration 14801 / 30600) loss: 1.937410\n",
      "(Iteration 14901 / 30600) loss: 1.918930\n",
      "(Iteration 15001 / 30600) loss: 1.908523\n",
      "(Iteration 15101 / 30600) loss: 2.044011\n",
      "(Iteration 15201 / 30600) loss: 1.996734\n",
      "(Epoch 20 / 40) train acc: 0.414000; val_acc: 0.409000\n",
      "(Iteration 15301 / 30600) loss: 1.997307\n",
      "(Iteration 15401 / 30600) loss: 2.027917\n",
      "(Iteration 15501 / 30600) loss: 1.916449\n",
      "(Iteration 15601 / 30600) loss: 2.118127\n",
      "(Iteration 15701 / 30600) loss: 1.885398\n",
      "(Iteration 15801 / 30600) loss: 1.874992\n",
      "(Iteration 15901 / 30600) loss: 1.942312\n",
      "(Iteration 16001 / 30600) loss: 2.005455\n",
      "(Epoch 21 / 40) train acc: 0.428000; val_acc: 0.411000\n",
      "(Iteration 16101 / 30600) loss: 1.865430\n",
      "(Iteration 16201 / 30600) loss: 2.028377\n",
      "(Iteration 16301 / 30600) loss: 2.060305\n",
      "(Iteration 16401 / 30600) loss: 1.988334\n",
      "(Iteration 16501 / 30600) loss: 1.903735\n",
      "(Iteration 16601 / 30600) loss: 1.990626\n",
      "(Iteration 16701 / 30600) loss: 2.065755\n",
      "(Iteration 16801 / 30600) loss: 2.084422\n",
      "(Epoch 22 / 40) train acc: 0.414000; val_acc: 0.411000\n",
      "(Iteration 16901 / 30600) loss: 2.040591\n",
      "(Iteration 17001 / 30600) loss: 1.874671\n",
      "(Iteration 17101 / 30600) loss: 2.000226\n",
      "(Iteration 17201 / 30600) loss: 1.900154\n",
      "(Iteration 17301 / 30600) loss: 2.054139\n",
      "(Iteration 17401 / 30600) loss: 1.954101\n",
      "(Iteration 17501 / 30600) loss: 2.061446\n",
      "(Epoch 23 / 40) train acc: 0.425000; val_acc: 0.418000\n",
      "(Iteration 17601 / 30600) loss: 2.060500\n",
      "(Iteration 17701 / 30600) loss: 1.969815\n",
      "(Iteration 17801 / 30600) loss: 1.973256\n",
      "(Iteration 17901 / 30600) loss: 1.931872\n",
      "(Iteration 18001 / 30600) loss: 1.948250\n",
      "(Iteration 18101 / 30600) loss: 1.936559\n",
      "(Iteration 18201 / 30600) loss: 1.932805\n",
      "(Iteration 18301 / 30600) loss: 1.942878\n",
      "(Epoch 24 / 40) train acc: 0.439000; val_acc: 0.421000\n",
      "(Iteration 18401 / 30600) loss: 1.973600\n",
      "(Iteration 18501 / 30600) loss: 2.005703\n",
      "(Iteration 18601 / 30600) loss: 1.982701\n",
      "(Iteration 18701 / 30600) loss: 2.047709\n",
      "(Iteration 18801 / 30600) loss: 2.023702\n",
      "(Iteration 18901 / 30600) loss: 1.979565\n",
      "(Iteration 19001 / 30600) loss: 1.972076\n",
      "(Iteration 19101 / 30600) loss: 1.915221\n",
      "(Epoch 25 / 40) train acc: 0.435000; val_acc: 0.419000\n",
      "(Iteration 19201 / 30600) loss: 2.020649\n",
      "(Iteration 19301 / 30600) loss: 1.954908\n",
      "(Iteration 19401 / 30600) loss: 1.882872\n",
      "(Iteration 19501 / 30600) loss: 2.021623\n",
      "(Iteration 19601 / 30600) loss: 1.943735\n",
      "(Iteration 19701 / 30600) loss: 1.991053\n",
      "(Iteration 19801 / 30600) loss: 2.072983\n",
      "(Epoch 26 / 40) train acc: 0.436000; val_acc: 0.422000\n",
      "(Iteration 19901 / 30600) loss: 2.051127\n",
      "(Iteration 20001 / 30600) loss: 2.014919\n",
      "(Iteration 20101 / 30600) loss: 2.070776\n",
      "(Iteration 20201 / 30600) loss: 1.958612\n",
      "(Iteration 20301 / 30600) loss: 1.916647\n",
      "(Iteration 20401 / 30600) loss: 1.923310\n",
      "(Iteration 20501 / 30600) loss: 1.934036\n",
      "(Iteration 20601 / 30600) loss: 1.905255\n",
      "(Epoch 27 / 40) train acc: 0.450000; val_acc: 0.425000\n",
      "(Iteration 20701 / 30600) loss: 1.978088\n",
      "(Iteration 20801 / 30600) loss: 1.977087\n",
      "(Iteration 20901 / 30600) loss: 1.904678\n",
      "(Iteration 21001 / 30600) loss: 2.097117\n",
      "(Iteration 21101 / 30600) loss: 1.984862\n",
      "(Iteration 21201 / 30600) loss: 2.008677\n",
      "(Iteration 21301 / 30600) loss: 1.962994\n",
      "(Iteration 21401 / 30600) loss: 1.931839\n",
      "(Epoch 28 / 40) train acc: 0.440000; val_acc: 0.426000\n",
      "(Iteration 21501 / 30600) loss: 1.948928\n",
      "(Iteration 21601 / 30600) loss: 1.924029\n",
      "(Iteration 21701 / 30600) loss: 1.970634\n",
      "(Iteration 21801 / 30600) loss: 1.981533\n",
      "(Iteration 21901 / 30600) loss: 1.889562\n",
      "(Iteration 22001 / 30600) loss: 1.817909\n",
      "(Iteration 22101 / 30600) loss: 2.002880\n",
      "(Epoch 29 / 40) train acc: 0.416000; val_acc: 0.429000\n",
      "(Iteration 22201 / 30600) loss: 2.010649\n",
      "(Iteration 22301 / 30600) loss: 2.203630\n",
      "(Iteration 22401 / 30600) loss: 1.980451\n",
      "(Iteration 22501 / 30600) loss: 1.972165\n",
      "(Iteration 22601 / 30600) loss: 1.925802\n",
      "(Iteration 22701 / 30600) loss: 1.921016\n",
      "(Iteration 22801 / 30600) loss: 1.917556\n",
      "(Iteration 22901 / 30600) loss: 1.986086\n",
      "(Epoch 30 / 40) train acc: 0.415000; val_acc: 0.426000\n",
      "(Iteration 23001 / 30600) loss: 1.879943\n",
      "(Iteration 23101 / 30600) loss: 2.035159\n",
      "(Iteration 23201 / 30600) loss: 1.871354\n",
      "(Iteration 23301 / 30600) loss: 1.907604\n",
      "(Iteration 23401 / 30600) loss: 1.898530\n",
      "(Iteration 23501 / 30600) loss: 2.022000\n",
      "(Iteration 23601 / 30600) loss: 2.020076\n",
      "(Iteration 23701 / 30600) loss: 1.903327\n",
      "(Epoch 31 / 40) train acc: 0.427000; val_acc: 0.427000\n",
      "(Iteration 23801 / 30600) loss: 2.041742\n",
      "(Iteration 23901 / 30600) loss: 2.012080\n",
      "(Iteration 24001 / 30600) loss: 1.921974\n",
      "(Iteration 24101 / 30600) loss: 1.930284\n",
      "(Iteration 24201 / 30600) loss: 1.859446\n",
      "(Iteration 24301 / 30600) loss: 2.049009\n",
      "(Iteration 24401 / 30600) loss: 2.069226\n",
      "(Epoch 32 / 40) train acc: 0.410000; val_acc: 0.427000\n",
      "(Iteration 24501 / 30600) loss: 2.060850\n",
      "(Iteration 24601 / 30600) loss: 2.041865\n",
      "(Iteration 24701 / 30600) loss: 1.995518\n",
      "(Iteration 24801 / 30600) loss: 1.963096\n",
      "(Iteration 24901 / 30600) loss: 1.982963\n",
      "(Iteration 25001 / 30600) loss: 1.959866\n",
      "(Iteration 25101 / 30600) loss: 1.964796\n",
      "(Iteration 25201 / 30600) loss: 2.002364\n",
      "(Epoch 33 / 40) train acc: 0.433000; val_acc: 0.428000\n",
      "(Iteration 25301 / 30600) loss: 1.932892\n",
      "(Iteration 25401 / 30600) loss: 1.977910\n",
      "(Iteration 25501 / 30600) loss: 1.912233\n",
      "(Iteration 25601 / 30600) loss: 1.930629\n",
      "(Iteration 25701 / 30600) loss: 1.900500\n",
      "(Iteration 25801 / 30600) loss: 1.846398\n",
      "(Iteration 25901 / 30600) loss: 2.002336\n",
      "(Iteration 26001 / 30600) loss: 1.909249\n",
      "(Epoch 34 / 40) train acc: 0.428000; val_acc: 0.427000\n",
      "(Iteration 26101 / 30600) loss: 2.073679\n",
      "(Iteration 26201 / 30600) loss: 1.931615\n",
      "(Iteration 26301 / 30600) loss: 2.017952\n",
      "(Iteration 26401 / 30600) loss: 1.906124\n",
      "(Iteration 26501 / 30600) loss: 1.911016\n",
      "(Iteration 26601 / 30600) loss: 2.031082\n",
      "(Iteration 26701 / 30600) loss: 1.905927\n",
      "(Epoch 35 / 40) train acc: 0.448000; val_acc: 0.426000\n",
      "(Iteration 26801 / 30600) loss: 1.892019\n",
      "(Iteration 26901 / 30600) loss: 1.937659\n",
      "(Iteration 27001 / 30600) loss: 1.952803\n",
      "(Iteration 27101 / 30600) loss: 1.994380\n",
      "(Iteration 27201 / 30600) loss: 1.895767\n",
      "(Iteration 27301 / 30600) loss: 1.979852\n",
      "(Iteration 27401 / 30600) loss: 1.964583\n",
      "(Iteration 27501 / 30600) loss: 1.989207\n",
      "(Epoch 36 / 40) train acc: 0.437000; val_acc: 0.425000\n",
      "(Iteration 27601 / 30600) loss: 1.959965\n",
      "(Iteration 27701 / 30600) loss: 2.036024\n",
      "(Iteration 27801 / 30600) loss: 1.996551\n",
      "(Iteration 27901 / 30600) loss: 1.966256\n",
      "(Iteration 28001 / 30600) loss: 2.020333\n",
      "(Iteration 28101 / 30600) loss: 2.035769\n",
      "(Iteration 28201 / 30600) loss: 1.961482\n",
      "(Iteration 28301 / 30600) loss: 2.023011\n",
      "(Epoch 37 / 40) train acc: 0.450000; val_acc: 0.425000\n",
      "(Iteration 28401 / 30600) loss: 1.987997\n",
      "(Iteration 28501 / 30600) loss: 2.025472\n",
      "(Iteration 28601 / 30600) loss: 1.955585\n",
      "(Iteration 28701 / 30600) loss: 1.941786\n",
      "(Iteration 28801 / 30600) loss: 1.945252\n",
      "(Iteration 28901 / 30600) loss: 1.915931\n",
      "(Iteration 29001 / 30600) loss: 2.040495\n",
      "(Epoch 38 / 40) train acc: 0.437000; val_acc: 0.424000\n",
      "(Iteration 29101 / 30600) loss: 1.974557\n",
      "(Iteration 29201 / 30600) loss: 1.823006\n",
      "(Iteration 29301 / 30600) loss: 1.997453\n",
      "(Iteration 29401 / 30600) loss: 2.006264\n",
      "(Iteration 29501 / 30600) loss: 1.879202\n",
      "(Iteration 29601 / 30600) loss: 1.946732\n",
      "(Iteration 29701 / 30600) loss: 1.933785\n",
      "(Iteration 29801 / 30600) loss: 1.929974\n",
      "(Epoch 39 / 40) train acc: 0.453000; val_acc: 0.424000\n",
      "(Iteration 29901 / 30600) loss: 1.828801\n",
      "(Iteration 30001 / 30600) loss: 1.964742\n",
      "(Iteration 30101 / 30600) loss: 1.989336\n",
      "(Iteration 30201 / 30600) loss: 1.839937\n",
      "(Iteration 30301 / 30600) loss: 1.912375\n",
      "(Iteration 30401 / 30600) loss: 1.924499\n",
      "(Iteration 30501 / 30600) loss: 1.904374\n",
      "(Epoch 40 / 40) train acc: 0.450000; val_acc: 0.425000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.01, 'num_epochs': 40, 'reg': 0.1, 'batch_size': 32}\n",
      "(Iteration 1 / 61240) loss: 2.309189\n",
      "(Epoch 0 / 40) train acc: 0.118000; val_acc: 0.111000\n",
      "(Iteration 101 / 61240) loss: 2.307111\n",
      "(Iteration 201 / 61240) loss: 2.306479\n",
      "(Iteration 301 / 61240) loss: 2.304949\n",
      "(Iteration 401 / 61240) loss: 2.303625\n",
      "(Iteration 501 / 61240) loss: 2.303634\n",
      "(Iteration 601 / 61240) loss: 2.303550\n",
      "(Iteration 701 / 61240) loss: 2.300462\n",
      "(Iteration 801 / 61240) loss: 2.299961\n",
      "(Iteration 901 / 61240) loss: 2.290674\n",
      "(Iteration 1001 / 61240) loss: 2.293811\n",
      "(Iteration 1101 / 61240) loss: 2.288198\n",
      "(Iteration 1201 / 61240) loss: 2.293083\n",
      "(Iteration 1301 / 61240) loss: 2.228720\n",
      "(Iteration 1401 / 61240) loss: 2.182038\n",
      "(Iteration 1501 / 61240) loss: 2.150889\n",
      "(Epoch 1 / 40) train acc: 0.206000; val_acc: 0.234000\n",
      "(Iteration 1601 / 61240) loss: 2.194523\n",
      "(Iteration 1701 / 61240) loss: 2.156409\n",
      "(Iteration 1801 / 61240) loss: 2.194864\n",
      "(Iteration 1901 / 61240) loss: 2.236705\n",
      "(Iteration 2001 / 61240) loss: 2.129610\n",
      "(Iteration 2101 / 61240) loss: 2.121238\n",
      "(Iteration 2201 / 61240) loss: 2.127764\n",
      "(Iteration 2301 / 61240) loss: 2.115126\n",
      "(Iteration 2401 / 61240) loss: 2.067138\n",
      "(Iteration 2501 / 61240) loss: 1.980424\n",
      "(Iteration 2601 / 61240) loss: 2.151870\n",
      "(Iteration 2701 / 61240) loss: 1.980280\n",
      "(Iteration 2801 / 61240) loss: 2.020107\n",
      "(Iteration 2901 / 61240) loss: 2.113730\n",
      "(Iteration 3001 / 61240) loss: 2.067958\n",
      "(Epoch 2 / 40) train acc: 0.328000; val_acc: 0.347000\n",
      "(Iteration 3101 / 61240) loss: 2.044435\n",
      "(Iteration 3201 / 61240) loss: 2.135656\n",
      "(Iteration 3301 / 61240) loss: 1.972091\n",
      "(Iteration 3401 / 61240) loss: 2.184246\n",
      "(Iteration 3501 / 61240) loss: 2.046052\n",
      "(Iteration 3601 / 61240) loss: 2.146618\n",
      "(Iteration 3701 / 61240) loss: 1.994641\n",
      "(Iteration 3801 / 61240) loss: 2.008453\n",
      "(Iteration 3901 / 61240) loss: 2.098565\n",
      "(Iteration 4001 / 61240) loss: 2.084598\n",
      "(Iteration 4101 / 61240) loss: 1.925840\n",
      "(Iteration 4201 / 61240) loss: 2.059375\n",
      "(Iteration 4301 / 61240) loss: 1.890445\n",
      "(Iteration 4401 / 61240) loss: 1.965501\n",
      "(Iteration 4501 / 61240) loss: 1.987006\n",
      "(Epoch 3 / 40) train acc: 0.360000; val_acc: 0.389000\n",
      "(Iteration 4601 / 61240) loss: 2.013967\n",
      "(Iteration 4701 / 61240) loss: 2.078712\n",
      "(Iteration 4801 / 61240) loss: 1.987249\n",
      "(Iteration 4901 / 61240) loss: 2.106256\n",
      "(Iteration 5001 / 61240) loss: 1.864379\n",
      "(Iteration 5101 / 61240) loss: 1.847875\n",
      "(Iteration 5201 / 61240) loss: 1.998211\n",
      "(Iteration 5301 / 61240) loss: 1.784521\n",
      "(Iteration 5401 / 61240) loss: 2.072212\n",
      "(Iteration 5501 / 61240) loss: 1.995644\n",
      "(Iteration 5601 / 61240) loss: 2.015392\n",
      "(Iteration 5701 / 61240) loss: 2.005059\n",
      "(Iteration 5801 / 61240) loss: 2.114642\n",
      "(Iteration 5901 / 61240) loss: 2.147600\n",
      "(Iteration 6001 / 61240) loss: 1.938134\n",
      "(Iteration 6101 / 61240) loss: 1.860406\n",
      "(Epoch 4 / 40) train acc: 0.398000; val_acc: 0.419000\n",
      "(Iteration 6201 / 61240) loss: 2.082237\n",
      "(Iteration 6301 / 61240) loss: 1.969129\n",
      "(Iteration 6401 / 61240) loss: 2.036907\n",
      "(Iteration 6501 / 61240) loss: 1.997588\n",
      "(Iteration 6601 / 61240) loss: 2.034711\n",
      "(Iteration 6701 / 61240) loss: 1.982527\n",
      "(Iteration 6801 / 61240) loss: 1.980215\n",
      "(Iteration 6901 / 61240) loss: 1.888276\n",
      "(Iteration 7001 / 61240) loss: 1.984074\n",
      "(Iteration 7101 / 61240) loss: 1.893550\n",
      "(Iteration 7201 / 61240) loss: 2.052142\n",
      "(Iteration 7301 / 61240) loss: 1.910848\n",
      "(Iteration 7401 / 61240) loss: 1.960676\n",
      "(Iteration 7501 / 61240) loss: 1.827343\n",
      "(Iteration 7601 / 61240) loss: 1.847287\n",
      "(Epoch 5 / 40) train acc: 0.423000; val_acc: 0.419000\n",
      "(Iteration 7701 / 61240) loss: 2.023205\n",
      "(Iteration 7801 / 61240) loss: 1.813879\n",
      "(Iteration 7901 / 61240) loss: 2.038897\n",
      "(Iteration 8001 / 61240) loss: 2.021688\n",
      "(Iteration 8101 / 61240) loss: 2.139695\n",
      "(Iteration 8201 / 61240) loss: 1.864117\n",
      "(Iteration 8301 / 61240) loss: 1.751775\n",
      "(Iteration 8401 / 61240) loss: 2.041399\n",
      "(Iteration 8501 / 61240) loss: 1.866324\n",
      "(Iteration 8601 / 61240) loss: 1.987720\n",
      "(Iteration 8701 / 61240) loss: 2.008937\n",
      "(Iteration 8801 / 61240) loss: 2.121149\n",
      "(Iteration 8901 / 61240) loss: 1.946636\n",
      "(Iteration 9001 / 61240) loss: 1.879560\n",
      "(Iteration 9101 / 61240) loss: 1.919324\n",
      "(Epoch 6 / 40) train acc: 0.442000; val_acc: 0.422000\n",
      "(Iteration 9201 / 61240) loss: 2.083917\n",
      "(Iteration 9301 / 61240) loss: 1.918750\n",
      "(Iteration 9401 / 61240) loss: 1.964718\n",
      "(Iteration 9501 / 61240) loss: 2.021640\n",
      "(Iteration 9601 / 61240) loss: 2.021932\n",
      "(Iteration 9701 / 61240) loss: 1.915064\n",
      "(Iteration 9801 / 61240) loss: 1.995890\n",
      "(Iteration 9901 / 61240) loss: 1.939818\n",
      "(Iteration 10001 / 61240) loss: 2.009636\n",
      "(Iteration 10101 / 61240) loss: 2.014499\n",
      "(Iteration 10201 / 61240) loss: 1.942211\n",
      "(Iteration 10301 / 61240) loss: 1.888439\n",
      "(Iteration 10401 / 61240) loss: 2.051865\n",
      "(Iteration 10501 / 61240) loss: 1.823637\n",
      "(Iteration 10601 / 61240) loss: 1.910621\n",
      "(Iteration 10701 / 61240) loss: 1.966576\n",
      "(Epoch 7 / 40) train acc: 0.425000; val_acc: 0.427000\n",
      "(Iteration 10801 / 61240) loss: 1.876969\n",
      "(Iteration 10901 / 61240) loss: 1.906156\n",
      "(Iteration 11001 / 61240) loss: 2.032797\n",
      "(Iteration 11101 / 61240) loss: 1.862384\n",
      "(Iteration 11201 / 61240) loss: 2.066619\n",
      "(Iteration 11301 / 61240) loss: 1.861676\n",
      "(Iteration 11401 / 61240) loss: 1.939064\n",
      "(Iteration 11501 / 61240) loss: 2.074156\n",
      "(Iteration 11601 / 61240) loss: 1.993833\n",
      "(Iteration 11701 / 61240) loss: 1.854536\n",
      "(Iteration 11801 / 61240) loss: 1.985830\n",
      "(Iteration 11901 / 61240) loss: 2.074749\n",
      "(Iteration 12001 / 61240) loss: 2.252456\n",
      "(Iteration 12101 / 61240) loss: 1.962774\n",
      "(Iteration 12201 / 61240) loss: 1.915233\n",
      "(Epoch 8 / 40) train acc: 0.445000; val_acc: 0.424000\n",
      "(Iteration 12301 / 61240) loss: 2.039747\n",
      "(Iteration 12401 / 61240) loss: 1.771562\n",
      "(Iteration 12501 / 61240) loss: 1.955581\n",
      "(Iteration 12601 / 61240) loss: 2.107684\n",
      "(Iteration 12701 / 61240) loss: 2.056569\n",
      "(Iteration 12801 / 61240) loss: 1.956298\n",
      "(Iteration 12901 / 61240) loss: 2.042399\n",
      "(Iteration 13001 / 61240) loss: 2.022194\n",
      "(Iteration 13101 / 61240) loss: 1.952363\n",
      "(Iteration 13201 / 61240) loss: 2.138333\n",
      "(Iteration 13301 / 61240) loss: 1.677765\n",
      "(Iteration 13401 / 61240) loss: 1.894281\n",
      "(Iteration 13501 / 61240) loss: 1.954629\n",
      "(Iteration 13601 / 61240) loss: 1.893641\n",
      "(Iteration 13701 / 61240) loss: 1.846547\n",
      "(Epoch 9 / 40) train acc: 0.447000; val_acc: 0.424000\n",
      "(Iteration 13801 / 61240) loss: 1.764699\n",
      "(Iteration 13901 / 61240) loss: 1.997389\n",
      "(Iteration 14001 / 61240) loss: 2.024059\n",
      "(Iteration 14101 / 61240) loss: 1.917622\n",
      "(Iteration 14201 / 61240) loss: 1.939511\n",
      "(Iteration 14301 / 61240) loss: 1.833343\n",
      "(Iteration 14401 / 61240) loss: 2.006353\n",
      "(Iteration 14501 / 61240) loss: 1.934208\n",
      "(Iteration 14601 / 61240) loss: 2.087317\n",
      "(Iteration 14701 / 61240) loss: 2.050792\n",
      "(Iteration 14801 / 61240) loss: 2.151450\n",
      "(Iteration 14901 / 61240) loss: 2.112292\n",
      "(Iteration 15001 / 61240) loss: 1.818503\n",
      "(Iteration 15101 / 61240) loss: 1.853489\n",
      "(Iteration 15201 / 61240) loss: 2.030095\n",
      "(Iteration 15301 / 61240) loss: 1.952855\n",
      "(Epoch 10 / 40) train acc: 0.440000; val_acc: 0.435000\n",
      "(Iteration 15401 / 61240) loss: 2.049142\n",
      "(Iteration 15501 / 61240) loss: 1.940936\n",
      "(Iteration 15601 / 61240) loss: 1.915474\n",
      "(Iteration 15701 / 61240) loss: 1.826682\n",
      "(Iteration 15801 / 61240) loss: 2.061849\n",
      "(Iteration 15901 / 61240) loss: 1.801515\n",
      "(Iteration 16001 / 61240) loss: 1.911715\n",
      "(Iteration 16101 / 61240) loss: 2.045751\n",
      "(Iteration 16201 / 61240) loss: 2.015837\n",
      "(Iteration 16301 / 61240) loss: 2.087038\n",
      "(Iteration 16401 / 61240) loss: 1.810627\n",
      "(Iteration 16501 / 61240) loss: 1.869833\n",
      "(Iteration 16601 / 61240) loss: 1.915037\n",
      "(Iteration 16701 / 61240) loss: 1.922872\n",
      "(Iteration 16801 / 61240) loss: 1.832507\n",
      "(Epoch 11 / 40) train acc: 0.426000; val_acc: 0.423000\n",
      "(Iteration 16901 / 61240) loss: 1.996092\n",
      "(Iteration 17001 / 61240) loss: 1.911419\n",
      "(Iteration 17101 / 61240) loss: 2.066565\n",
      "(Iteration 17201 / 61240) loss: 1.960751\n",
      "(Iteration 17301 / 61240) loss: 1.904896\n",
      "(Iteration 17401 / 61240) loss: 2.027554\n",
      "(Iteration 17501 / 61240) loss: 2.072856\n",
      "(Iteration 17601 / 61240) loss: 2.030286\n",
      "(Iteration 17701 / 61240) loss: 1.972646\n",
      "(Iteration 17801 / 61240) loss: 1.961601\n",
      "(Iteration 17901 / 61240) loss: 2.182803\n",
      "(Iteration 18001 / 61240) loss: 1.848018\n",
      "(Iteration 18101 / 61240) loss: 1.861049\n",
      "(Iteration 18201 / 61240) loss: 1.876150\n",
      "(Iteration 18301 / 61240) loss: 1.825787\n",
      "(Epoch 12 / 40) train acc: 0.445000; val_acc: 0.429000\n",
      "(Iteration 18401 / 61240) loss: 2.304425\n",
      "(Iteration 18501 / 61240) loss: 1.983513\n",
      "(Iteration 18601 / 61240) loss: 1.959564\n",
      "(Iteration 18701 / 61240) loss: 2.099226\n",
      "(Iteration 18801 / 61240) loss: 2.121222\n",
      "(Iteration 18901 / 61240) loss: 1.934937\n",
      "(Iteration 19001 / 61240) loss: 1.884596\n",
      "(Iteration 19101 / 61240) loss: 2.055852\n",
      "(Iteration 19201 / 61240) loss: 1.958035\n",
      "(Iteration 19301 / 61240) loss: 1.813628\n",
      "(Iteration 19401 / 61240) loss: 1.977161\n",
      "(Iteration 19501 / 61240) loss: 1.958979\n",
      "(Iteration 19601 / 61240) loss: 2.029382\n",
      "(Iteration 19701 / 61240) loss: 1.836301\n",
      "(Iteration 19801 / 61240) loss: 1.947087\n",
      "(Iteration 19901 / 61240) loss: 1.813546\n",
      "(Epoch 13 / 40) train acc: 0.439000; val_acc: 0.434000\n",
      "(Iteration 20001 / 61240) loss: 2.012824\n",
      "(Iteration 20101 / 61240) loss: 1.900993\n",
      "(Iteration 20201 / 61240) loss: 1.882168\n",
      "(Iteration 20301 / 61240) loss: 1.979504\n",
      "(Iteration 20401 / 61240) loss: 1.922625\n",
      "(Iteration 20501 / 61240) loss: 2.172712\n",
      "(Iteration 20601 / 61240) loss: 2.007937\n",
      "(Iteration 20701 / 61240) loss: 1.868302\n",
      "(Iteration 20801 / 61240) loss: 2.037436\n",
      "(Iteration 20901 / 61240) loss: 2.019717\n",
      "(Iteration 21001 / 61240) loss: 1.875563\n",
      "(Iteration 21101 / 61240) loss: 1.830687\n",
      "(Iteration 21201 / 61240) loss: 2.013394\n",
      "(Iteration 21301 / 61240) loss: 1.893672\n",
      "(Iteration 21401 / 61240) loss: 2.004472\n",
      "(Epoch 14 / 40) train acc: 0.459000; val_acc: 0.441000\n",
      "(Iteration 21501 / 61240) loss: 1.915039\n",
      "(Iteration 21601 / 61240) loss: 1.853382\n",
      "(Iteration 21701 / 61240) loss: 1.835596\n",
      "(Iteration 21801 / 61240) loss: 1.857533\n",
      "(Iteration 21901 / 61240) loss: 1.994038\n",
      "(Iteration 22001 / 61240) loss: 1.961498\n",
      "(Iteration 22101 / 61240) loss: 2.087782\n",
      "(Iteration 22201 / 61240) loss: 2.052818\n",
      "(Iteration 22301 / 61240) loss: 1.764959\n",
      "(Iteration 22401 / 61240) loss: 1.989176\n",
      "(Iteration 22501 / 61240) loss: 2.060458\n",
      "(Iteration 22601 / 61240) loss: 2.052469\n",
      "(Iteration 22701 / 61240) loss: 1.952915\n",
      "(Iteration 22801 / 61240) loss: 1.945358\n",
      "(Iteration 22901 / 61240) loss: 1.805494\n",
      "(Epoch 15 / 40) train acc: 0.433000; val_acc: 0.431000\n",
      "(Iteration 23001 / 61240) loss: 1.901148\n",
      "(Iteration 23101 / 61240) loss: 1.980835\n",
      "(Iteration 23201 / 61240) loss: 1.741572\n",
      "(Iteration 23301 / 61240) loss: 1.875936\n",
      "(Iteration 23401 / 61240) loss: 1.954011\n",
      "(Iteration 23501 / 61240) loss: 1.856461\n",
      "(Iteration 23601 / 61240) loss: 1.825100\n",
      "(Iteration 23701 / 61240) loss: 1.888279\n",
      "(Iteration 23801 / 61240) loss: 2.202695\n",
      "(Iteration 23901 / 61240) loss: 1.768530\n",
      "(Iteration 24001 / 61240) loss: 1.974047\n",
      "(Iteration 24101 / 61240) loss: 2.075994\n",
      "(Iteration 24201 / 61240) loss: 1.779381\n",
      "(Iteration 24301 / 61240) loss: 2.019378\n",
      "(Iteration 24401 / 61240) loss: 1.969514\n",
      "(Epoch 16 / 40) train acc: 0.429000; val_acc: 0.436000\n",
      "(Iteration 24501 / 61240) loss: 2.000722\n",
      "(Iteration 24601 / 61240) loss: 2.169853\n",
      "(Iteration 24701 / 61240) loss: 2.127942\n",
      "(Iteration 24801 / 61240) loss: 1.893904\n",
      "(Iteration 24901 / 61240) loss: 1.986065\n",
      "(Iteration 25001 / 61240) loss: 1.875128\n",
      "(Iteration 25101 / 61240) loss: 1.788394\n",
      "(Iteration 25201 / 61240) loss: 1.848196\n",
      "(Iteration 25301 / 61240) loss: 1.863510\n",
      "(Iteration 25401 / 61240) loss: 2.040977\n",
      "(Iteration 25501 / 61240) loss: 2.082348\n",
      "(Iteration 25601 / 61240) loss: 1.928027\n",
      "(Iteration 25701 / 61240) loss: 1.981340\n",
      "(Iteration 25801 / 61240) loss: 1.817448\n",
      "(Iteration 25901 / 61240) loss: 1.965871\n",
      "(Iteration 26001 / 61240) loss: 1.939719\n",
      "(Epoch 17 / 40) train acc: 0.411000; val_acc: 0.434000\n",
      "(Iteration 26101 / 61240) loss: 1.868837\n",
      "(Iteration 26201 / 61240) loss: 1.851758\n",
      "(Iteration 26301 / 61240) loss: 1.914011\n",
      "(Iteration 26401 / 61240) loss: 1.974957\n",
      "(Iteration 26501 / 61240) loss: 2.022284\n",
      "(Iteration 26601 / 61240) loss: 1.947752\n",
      "(Iteration 26701 / 61240) loss: 2.070204\n",
      "(Iteration 26801 / 61240) loss: 1.919815\n",
      "(Iteration 26901 / 61240) loss: 1.892477\n",
      "(Iteration 27001 / 61240) loss: 2.023744\n",
      "(Iteration 27101 / 61240) loss: 1.953627\n",
      "(Iteration 27201 / 61240) loss: 2.087694\n",
      "(Iteration 27301 / 61240) loss: 1.858340\n",
      "(Iteration 27401 / 61240) loss: 1.896185\n",
      "(Iteration 27501 / 61240) loss: 1.882949\n",
      "(Epoch 18 / 40) train acc: 0.430000; val_acc: 0.434000\n",
      "(Iteration 27601 / 61240) loss: 1.852571\n",
      "(Iteration 27701 / 61240) loss: 2.016722\n",
      "(Iteration 27801 / 61240) loss: 1.864396\n",
      "(Iteration 27901 / 61240) loss: 1.865473\n",
      "(Iteration 28001 / 61240) loss: 1.945911\n",
      "(Iteration 28101 / 61240) loss: 1.925361\n",
      "(Iteration 28201 / 61240) loss: 2.036189\n",
      "(Iteration 28301 / 61240) loss: 1.914419\n",
      "(Iteration 28401 / 61240) loss: 1.657153\n",
      "(Iteration 28501 / 61240) loss: 2.084704\n",
      "(Iteration 28601 / 61240) loss: 2.012404\n",
      "(Iteration 28701 / 61240) loss: 2.055716\n",
      "(Iteration 28801 / 61240) loss: 2.021185\n",
      "(Iteration 28901 / 61240) loss: 1.933177\n",
      "(Iteration 29001 / 61240) loss: 2.001262\n",
      "(Epoch 19 / 40) train acc: 0.475000; val_acc: 0.429000\n",
      "(Iteration 29101 / 61240) loss: 2.002501\n",
      "(Iteration 29201 / 61240) loss: 1.834341\n",
      "(Iteration 29301 / 61240) loss: 2.048591\n",
      "(Iteration 29401 / 61240) loss: 2.184658\n",
      "(Iteration 29501 / 61240) loss: 1.866217\n",
      "(Iteration 29601 / 61240) loss: 1.839584\n",
      "(Iteration 29701 / 61240) loss: 1.905110\n",
      "(Iteration 29801 / 61240) loss: 1.806314\n",
      "(Iteration 29901 / 61240) loss: 1.902549\n",
      "(Iteration 30001 / 61240) loss: 1.955192\n",
      "(Iteration 30101 / 61240) loss: 1.909108\n",
      "(Iteration 30201 / 61240) loss: 1.920289\n",
      "(Iteration 30301 / 61240) loss: 1.805363\n",
      "(Iteration 30401 / 61240) loss: 1.950439\n",
      "(Iteration 30501 / 61240) loss: 1.897818\n",
      "(Iteration 30601 / 61240) loss: 1.991593\n",
      "(Epoch 20 / 40) train acc: 0.420000; val_acc: 0.435000\n",
      "(Iteration 30701 / 61240) loss: 1.862446\n",
      "(Iteration 30801 / 61240) loss: 1.837953\n",
      "(Iteration 30901 / 61240) loss: 2.019574\n",
      "(Iteration 31001 / 61240) loss: 1.910133\n",
      "(Iteration 31101 / 61240) loss: 2.019208\n",
      "(Iteration 31201 / 61240) loss: 1.867324\n",
      "(Iteration 31301 / 61240) loss: 1.878366\n",
      "(Iteration 31401 / 61240) loss: 2.040309\n",
      "(Iteration 31501 / 61240) loss: 1.946810\n",
      "(Iteration 31601 / 61240) loss: 2.043724\n",
      "(Iteration 31701 / 61240) loss: 1.760143\n",
      "(Iteration 31801 / 61240) loss: 1.958989\n",
      "(Iteration 31901 / 61240) loss: 1.774714\n",
      "(Iteration 32001 / 61240) loss: 2.053159\n",
      "(Iteration 32101 / 61240) loss: 1.934825\n",
      "(Epoch 21 / 40) train acc: 0.434000; val_acc: 0.432000\n",
      "(Iteration 32201 / 61240) loss: 2.070467\n",
      "(Iteration 32301 / 61240) loss: 1.853605\n",
      "(Iteration 32401 / 61240) loss: 2.133193\n",
      "(Iteration 32501 / 61240) loss: 2.179002\n",
      "(Iteration 32601 / 61240) loss: 2.070282\n",
      "(Iteration 32701 / 61240) loss: 2.035815\n",
      "(Iteration 32801 / 61240) loss: 2.155699\n",
      "(Iteration 32901 / 61240) loss: 1.939519\n",
      "(Iteration 33001 / 61240) loss: 1.858173\n",
      "(Iteration 33101 / 61240) loss: 1.848641\n",
      "(Iteration 33201 / 61240) loss: 2.015112\n",
      "(Iteration 33301 / 61240) loss: 1.778347\n",
      "(Iteration 33401 / 61240) loss: 1.884122\n",
      "(Iteration 33501 / 61240) loss: 2.013935\n",
      "(Iteration 33601 / 61240) loss: 1.969318\n",
      "(Epoch 22 / 40) train acc: 0.443000; val_acc: 0.435000\n",
      "(Iteration 33701 / 61240) loss: 1.826694\n",
      "(Iteration 33801 / 61240) loss: 1.975402\n",
      "(Iteration 33901 / 61240) loss: 1.913694\n",
      "(Iteration 34001 / 61240) loss: 2.012499\n",
      "(Iteration 34101 / 61240) loss: 1.900301\n",
      "(Iteration 34201 / 61240) loss: 1.784277\n",
      "(Iteration 34301 / 61240) loss: 1.971744\n",
      "(Iteration 34401 / 61240) loss: 1.997567\n",
      "(Iteration 34501 / 61240) loss: 1.909643\n",
      "(Iteration 34601 / 61240) loss: 2.028019\n",
      "(Iteration 34701 / 61240) loss: 1.934142\n",
      "(Iteration 34801 / 61240) loss: 1.977822\n",
      "(Iteration 34901 / 61240) loss: 2.147333\n",
      "(Iteration 35001 / 61240) loss: 1.927347\n",
      "(Iteration 35101 / 61240) loss: 1.914145\n",
      "(Iteration 35201 / 61240) loss: 1.875822\n",
      "(Epoch 23 / 40) train acc: 0.475000; val_acc: 0.437000\n",
      "(Iteration 35301 / 61240) loss: 1.980975\n",
      "(Iteration 35401 / 61240) loss: 2.058908\n",
      "(Iteration 35501 / 61240) loss: 1.786948\n",
      "(Iteration 35601 / 61240) loss: 1.925250\n",
      "(Iteration 35701 / 61240) loss: 1.844350\n",
      "(Iteration 35801 / 61240) loss: 1.834943\n",
      "(Iteration 35901 / 61240) loss: 1.915524\n",
      "(Iteration 36001 / 61240) loss: 1.878056\n",
      "(Iteration 36101 / 61240) loss: 1.907588\n",
      "(Iteration 36201 / 61240) loss: 1.855552\n",
      "(Iteration 36301 / 61240) loss: 1.876945\n",
      "(Iteration 36401 / 61240) loss: 2.085126\n",
      "(Iteration 36501 / 61240) loss: 1.996026\n",
      "(Iteration 36601 / 61240) loss: 2.138237\n",
      "(Iteration 36701 / 61240) loss: 1.930086\n",
      "(Epoch 24 / 40) train acc: 0.450000; val_acc: 0.435000\n",
      "(Iteration 36801 / 61240) loss: 1.971783\n",
      "(Iteration 36901 / 61240) loss: 2.068966\n",
      "(Iteration 37001 / 61240) loss: 1.894700\n",
      "(Iteration 37101 / 61240) loss: 1.993606\n",
      "(Iteration 37201 / 61240) loss: 1.960137\n",
      "(Iteration 37301 / 61240) loss: 2.014875\n",
      "(Iteration 37401 / 61240) loss: 1.847495\n",
      "(Iteration 37501 / 61240) loss: 2.105245\n",
      "(Iteration 37601 / 61240) loss: 2.152162\n",
      "(Iteration 37701 / 61240) loss: 1.940355\n",
      "(Iteration 37801 / 61240) loss: 1.956185\n",
      "(Iteration 37901 / 61240) loss: 1.868586\n",
      "(Iteration 38001 / 61240) loss: 2.001170\n",
      "(Iteration 38101 / 61240) loss: 1.960962\n",
      "(Iteration 38201 / 61240) loss: 1.917806\n",
      "(Epoch 25 / 40) train acc: 0.421000; val_acc: 0.434000\n",
      "(Iteration 38301 / 61240) loss: 1.896860\n",
      "(Iteration 38401 / 61240) loss: 2.040032\n",
      "(Iteration 38501 / 61240) loss: 1.864626\n",
      "(Iteration 38601 / 61240) loss: 1.885074\n",
      "(Iteration 38701 / 61240) loss: 1.848181\n",
      "(Iteration 38801 / 61240) loss: 1.911244\n",
      "(Iteration 38901 / 61240) loss: 1.957861\n",
      "(Iteration 39001 / 61240) loss: 1.887424\n",
      "(Iteration 39101 / 61240) loss: 1.851069\n",
      "(Iteration 39201 / 61240) loss: 1.918177\n",
      "(Iteration 39301 / 61240) loss: 1.843332\n",
      "(Iteration 39401 / 61240) loss: 2.096346\n",
      "(Iteration 39501 / 61240) loss: 1.901357\n",
      "(Iteration 39601 / 61240) loss: 1.852884\n",
      "(Iteration 39701 / 61240) loss: 2.120277\n",
      "(Iteration 39801 / 61240) loss: 2.077988\n",
      "(Epoch 26 / 40) train acc: 0.465000; val_acc: 0.440000\n",
      "(Iteration 39901 / 61240) loss: 1.878641\n",
      "(Iteration 40001 / 61240) loss: 1.903591\n",
      "(Iteration 40101 / 61240) loss: 1.797781\n",
      "(Iteration 40201 / 61240) loss: 1.889630\n",
      "(Iteration 40301 / 61240) loss: 1.976203\n",
      "(Iteration 40401 / 61240) loss: 1.959887\n",
      "(Iteration 40501 / 61240) loss: 1.839784\n",
      "(Iteration 40601 / 61240) loss: 1.893594\n",
      "(Iteration 40701 / 61240) loss: 1.904639\n",
      "(Iteration 40801 / 61240) loss: 1.874907\n",
      "(Iteration 40901 / 61240) loss: 1.992073\n",
      "(Iteration 41001 / 61240) loss: 2.031527\n",
      "(Iteration 41101 / 61240) loss: 2.031818\n",
      "(Iteration 41201 / 61240) loss: 1.889516\n",
      "(Iteration 41301 / 61240) loss: 1.901366\n",
      "(Epoch 27 / 40) train acc: 0.457000; val_acc: 0.442000\n",
      "(Iteration 41401 / 61240) loss: 1.881931\n",
      "(Iteration 41501 / 61240) loss: 1.977649\n",
      "(Iteration 41601 / 61240) loss: 2.048558\n",
      "(Iteration 41701 / 61240) loss: 1.802610\n",
      "(Iteration 41801 / 61240) loss: 2.068400\n",
      "(Iteration 41901 / 61240) loss: 1.980385\n",
      "(Iteration 42001 / 61240) loss: 1.938450\n",
      "(Iteration 42101 / 61240) loss: 1.975958\n",
      "(Iteration 42201 / 61240) loss: 1.805151\n",
      "(Iteration 42301 / 61240) loss: 1.946539\n",
      "(Iteration 42401 / 61240) loss: 2.105304\n",
      "(Iteration 42501 / 61240) loss: 1.991354\n",
      "(Iteration 42601 / 61240) loss: 1.937289\n",
      "(Iteration 42701 / 61240) loss: 1.867237\n",
      "(Iteration 42801 / 61240) loss: 1.736942\n",
      "(Epoch 28 / 40) train acc: 0.447000; val_acc: 0.439000\n",
      "(Iteration 42901 / 61240) loss: 2.212412\n",
      "(Iteration 43001 / 61240) loss: 1.947355\n",
      "(Iteration 43101 / 61240) loss: 1.809302\n",
      "(Iteration 43201 / 61240) loss: 1.853037\n",
      "(Iteration 43301 / 61240) loss: 1.986007\n",
      "(Iteration 43401 / 61240) loss: 2.104593\n",
      "(Iteration 43501 / 61240) loss: 1.975887\n",
      "(Iteration 43601 / 61240) loss: 1.965322\n",
      "(Iteration 43701 / 61240) loss: 1.915678\n",
      "(Iteration 43801 / 61240) loss: 1.768428\n",
      "(Iteration 43901 / 61240) loss: 2.015634\n",
      "(Iteration 44001 / 61240) loss: 1.786013\n",
      "(Iteration 44101 / 61240) loss: 1.978085\n",
      "(Iteration 44201 / 61240) loss: 2.080508\n",
      "(Iteration 44301 / 61240) loss: 1.954463\n",
      "(Epoch 29 / 40) train acc: 0.455000; val_acc: 0.433000\n",
      "(Iteration 44401 / 61240) loss: 1.932687\n",
      "(Iteration 44501 / 61240) loss: 2.018492\n",
      "(Iteration 44601 / 61240) loss: 2.032793\n",
      "(Iteration 44701 / 61240) loss: 1.885888\n",
      "(Iteration 44801 / 61240) loss: 1.996699\n",
      "(Iteration 44901 / 61240) loss: 1.984357\n",
      "(Iteration 45001 / 61240) loss: 1.823534\n",
      "(Iteration 45101 / 61240) loss: 1.965947\n",
      "(Iteration 45201 / 61240) loss: 2.145982\n",
      "(Iteration 45301 / 61240) loss: 2.013377\n",
      "(Iteration 45401 / 61240) loss: 1.901225\n",
      "(Iteration 45501 / 61240) loss: 1.931248\n",
      "(Iteration 45601 / 61240) loss: 1.876379\n",
      "(Iteration 45701 / 61240) loss: 1.809583\n",
      "(Iteration 45801 / 61240) loss: 2.020128\n",
      "(Iteration 45901 / 61240) loss: 1.771559\n",
      "(Epoch 30 / 40) train acc: 0.437000; val_acc: 0.439000\n",
      "(Iteration 46001 / 61240) loss: 1.909233\n",
      "(Iteration 46101 / 61240) loss: 2.006209\n",
      "(Iteration 46201 / 61240) loss: 1.879615\n",
      "(Iteration 46301 / 61240) loss: 1.872841\n",
      "(Iteration 46401 / 61240) loss: 1.996013\n",
      "(Iteration 46501 / 61240) loss: 1.741861\n",
      "(Iteration 46601 / 61240) loss: 2.062005\n",
      "(Iteration 46701 / 61240) loss: 1.934203\n",
      "(Iteration 46801 / 61240) loss: 1.956309\n",
      "(Iteration 46901 / 61240) loss: 1.903007\n",
      "(Iteration 47001 / 61240) loss: 1.989279\n",
      "(Iteration 47101 / 61240) loss: 1.880770\n",
      "(Iteration 47201 / 61240) loss: 2.013531\n",
      "(Iteration 47301 / 61240) loss: 2.019312\n",
      "(Iteration 47401 / 61240) loss: 1.964180\n",
      "(Epoch 31 / 40) train acc: 0.454000; val_acc: 0.433000\n",
      "(Iteration 47501 / 61240) loss: 1.819334\n",
      "(Iteration 47601 / 61240) loss: 1.830127\n",
      "(Iteration 47701 / 61240) loss: 1.823247\n",
      "(Iteration 47801 / 61240) loss: 2.077778\n",
      "(Iteration 47901 / 61240) loss: 2.168804\n",
      "(Iteration 48001 / 61240) loss: 2.013209\n",
      "(Iteration 48101 / 61240) loss: 1.922924\n",
      "(Iteration 48201 / 61240) loss: 1.985236\n",
      "(Iteration 48301 / 61240) loss: 1.803490\n",
      "(Iteration 48401 / 61240) loss: 1.960054\n",
      "(Iteration 48501 / 61240) loss: 2.047974\n",
      "(Iteration 48601 / 61240) loss: 2.076060\n",
      "(Iteration 48701 / 61240) loss: 1.921432\n",
      "(Iteration 48801 / 61240) loss: 1.877470\n",
      "(Iteration 48901 / 61240) loss: 1.866573\n",
      "(Epoch 32 / 40) train acc: 0.445000; val_acc: 0.434000\n",
      "(Iteration 49001 / 61240) loss: 2.120313\n",
      "(Iteration 49101 / 61240) loss: 2.148703\n",
      "(Iteration 49201 / 61240) loss: 1.965886\n",
      "(Iteration 49301 / 61240) loss: 1.902243\n",
      "(Iteration 49401 / 61240) loss: 1.914408\n",
      "(Iteration 49501 / 61240) loss: 2.097604\n",
      "(Iteration 49601 / 61240) loss: 1.994894\n",
      "(Iteration 49701 / 61240) loss: 2.048726\n",
      "(Iteration 49801 / 61240) loss: 2.030535\n",
      "(Iteration 49901 / 61240) loss: 1.901965\n",
      "(Iteration 50001 / 61240) loss: 1.950442\n",
      "(Iteration 50101 / 61240) loss: 2.109401\n",
      "(Iteration 50201 / 61240) loss: 1.752637\n",
      "(Iteration 50301 / 61240) loss: 2.180711\n",
      "(Iteration 50401 / 61240) loss: 1.957647\n",
      "(Iteration 50501 / 61240) loss: 2.006585\n",
      "(Epoch 33 / 40) train acc: 0.445000; val_acc: 0.432000\n",
      "(Iteration 50601 / 61240) loss: 1.865940\n",
      "(Iteration 50701 / 61240) loss: 2.155852\n",
      "(Iteration 50801 / 61240) loss: 1.940162\n",
      "(Iteration 50901 / 61240) loss: 2.057944\n",
      "(Iteration 51001 / 61240) loss: 2.007900\n",
      "(Iteration 51101 / 61240) loss: 2.115140\n",
      "(Iteration 51201 / 61240) loss: 1.740979\n",
      "(Iteration 51301 / 61240) loss: 1.981112\n",
      "(Iteration 51401 / 61240) loss: 1.962583\n",
      "(Iteration 51501 / 61240) loss: 1.963250\n",
      "(Iteration 51601 / 61240) loss: 1.871784\n",
      "(Iteration 51701 / 61240) loss: 1.989020\n",
      "(Iteration 51801 / 61240) loss: 2.043312\n",
      "(Iteration 51901 / 61240) loss: 1.835273\n",
      "(Iteration 52001 / 61240) loss: 2.025078\n",
      "(Epoch 34 / 40) train acc: 0.456000; val_acc: 0.433000\n",
      "(Iteration 52101 / 61240) loss: 1.847357\n",
      "(Iteration 52201 / 61240) loss: 1.788228\n",
      "(Iteration 52301 / 61240) loss: 1.841473\n",
      "(Iteration 52401 / 61240) loss: 1.888865\n",
      "(Iteration 52501 / 61240) loss: 1.924319\n",
      "(Iteration 52601 / 61240) loss: 1.933138\n",
      "(Iteration 52701 / 61240) loss: 1.978022\n",
      "(Iteration 52801 / 61240) loss: 1.875433\n",
      "(Iteration 52901 / 61240) loss: 1.944896\n",
      "(Iteration 53001 / 61240) loss: 1.915471\n",
      "(Iteration 53101 / 61240) loss: 1.805304\n",
      "(Iteration 53201 / 61240) loss: 1.949203\n",
      "(Iteration 53301 / 61240) loss: 1.998314\n",
      "(Iteration 53401 / 61240) loss: 1.834428\n",
      "(Iteration 53501 / 61240) loss: 1.898176\n",
      "(Epoch 35 / 40) train acc: 0.454000; val_acc: 0.433000\n",
      "(Iteration 53601 / 61240) loss: 1.936076\n",
      "(Iteration 53701 / 61240) loss: 2.037625\n",
      "(Iteration 53801 / 61240) loss: 1.745324\n",
      "(Iteration 53901 / 61240) loss: 2.011579\n",
      "(Iteration 54001 / 61240) loss: 2.102409\n",
      "(Iteration 54101 / 61240) loss: 1.850257\n",
      "(Iteration 54201 / 61240) loss: 1.889094\n",
      "(Iteration 54301 / 61240) loss: 1.986203\n",
      "(Iteration 54401 / 61240) loss: 2.058173\n",
      "(Iteration 54501 / 61240) loss: 1.954070\n",
      "(Iteration 54601 / 61240) loss: 1.979653\n",
      "(Iteration 54701 / 61240) loss: 1.959258\n",
      "(Iteration 54801 / 61240) loss: 1.928684\n",
      "(Iteration 54901 / 61240) loss: 1.829997\n",
      "(Iteration 55001 / 61240) loss: 1.981106\n",
      "(Iteration 55101 / 61240) loss: 2.013414\n",
      "(Epoch 36 / 40) train acc: 0.438000; val_acc: 0.441000\n",
      "(Iteration 55201 / 61240) loss: 1.849196\n",
      "(Iteration 55301 / 61240) loss: 2.297029\n",
      "(Iteration 55401 / 61240) loss: 1.986086\n",
      "(Iteration 55501 / 61240) loss: 2.021855\n",
      "(Iteration 55601 / 61240) loss: 1.953556\n",
      "(Iteration 55701 / 61240) loss: 1.934780\n",
      "(Iteration 55801 / 61240) loss: 1.922698\n",
      "(Iteration 55901 / 61240) loss: 1.882830\n",
      "(Iteration 56001 / 61240) loss: 1.931677\n",
      "(Iteration 56101 / 61240) loss: 1.903892\n",
      "(Iteration 56201 / 61240) loss: 1.979436\n",
      "(Iteration 56301 / 61240) loss: 1.986661\n",
      "(Iteration 56401 / 61240) loss: 1.929598\n",
      "(Iteration 56501 / 61240) loss: 1.981980\n",
      "(Iteration 56601 / 61240) loss: 1.999708\n",
      "(Epoch 37 / 40) train acc: 0.445000; val_acc: 0.434000\n",
      "(Iteration 56701 / 61240) loss: 2.017845\n",
      "(Iteration 56801 / 61240) loss: 1.966705\n",
      "(Iteration 56901 / 61240) loss: 2.099437\n",
      "(Iteration 57001 / 61240) loss: 2.018659\n",
      "(Iteration 57101 / 61240) loss: 1.945619\n",
      "(Iteration 57201 / 61240) loss: 1.927965\n",
      "(Iteration 57301 / 61240) loss: 1.955101\n",
      "(Iteration 57401 / 61240) loss: 1.939821\n",
      "(Iteration 57501 / 61240) loss: 1.923476\n",
      "(Iteration 57601 / 61240) loss: 2.046571\n",
      "(Iteration 57701 / 61240) loss: 2.012108\n",
      "(Iteration 57801 / 61240) loss: 1.925378\n",
      "(Iteration 57901 / 61240) loss: 1.819221\n",
      "(Iteration 58001 / 61240) loss: 2.039275\n",
      "(Iteration 58101 / 61240) loss: 1.840685\n",
      "(Epoch 38 / 40) train acc: 0.442000; val_acc: 0.437000\n",
      "(Iteration 58201 / 61240) loss: 2.000813\n",
      "(Iteration 58301 / 61240) loss: 2.012977\n",
      "(Iteration 58401 / 61240) loss: 2.243022\n",
      "(Iteration 58501 / 61240) loss: 1.960305\n",
      "(Iteration 58601 / 61240) loss: 1.996074\n",
      "(Iteration 58701 / 61240) loss: 2.014908\n",
      "(Iteration 58801 / 61240) loss: 2.166686\n",
      "(Iteration 58901 / 61240) loss: 1.898680\n",
      "(Iteration 59001 / 61240) loss: 1.808368\n",
      "(Iteration 59101 / 61240) loss: 1.921447\n",
      "(Iteration 59201 / 61240) loss: 1.876110\n",
      "(Iteration 59301 / 61240) loss: 2.094886\n",
      "(Iteration 59401 / 61240) loss: 1.979015\n",
      "(Iteration 59501 / 61240) loss: 1.925480\n",
      "(Iteration 59601 / 61240) loss: 2.003989\n",
      "(Iteration 59701 / 61240) loss: 1.934778\n",
      "(Epoch 39 / 40) train acc: 0.418000; val_acc: 0.442000\n",
      "(Iteration 59801 / 61240) loss: 1.826737\n",
      "(Iteration 59901 / 61240) loss: 1.992585\n",
      "(Iteration 60001 / 61240) loss: 1.825392\n",
      "(Iteration 60101 / 61240) loss: 1.847963\n",
      "(Iteration 60201 / 61240) loss: 2.060167\n",
      "(Iteration 60301 / 61240) loss: 1.962790\n",
      "(Iteration 60401 / 61240) loss: 2.047826\n",
      "(Iteration 60501 / 61240) loss: 2.053894\n",
      "(Iteration 60601 / 61240) loss: 1.819039\n",
      "(Iteration 60701 / 61240) loss: 1.965225\n",
      "(Iteration 60801 / 61240) loss: 1.728918\n",
      "(Iteration 60901 / 61240) loss: 2.015503\n",
      "(Iteration 61001 / 61240) loss: 2.026415\n",
      "(Iteration 61101 / 61240) loss: 1.959188\n",
      "(Iteration 61201 / 61240) loss: 1.877501\n",
      "(Epoch 40 / 40) train acc: 0.472000; val_acc: 0.436000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.01, 'num_epochs': 40, 'reg': 0.01, 'batch_size': 64}\n",
      "(Iteration 1 / 30600) loss: 2.303331\n",
      "(Epoch 0 / 40) train acc: 0.099000; val_acc: 0.117000\n",
      "(Iteration 101 / 30600) loss: 2.302608\n",
      "(Iteration 201 / 30600) loss: 2.302792\n",
      "(Iteration 301 / 30600) loss: 2.301452\n",
      "(Iteration 401 / 30600) loss: 2.300194\n",
      "(Iteration 501 / 30600) loss: 2.300322\n",
      "(Iteration 601 / 30600) loss: 2.296845\n",
      "(Iteration 701 / 30600) loss: 2.289139\n",
      "(Epoch 1 / 40) train acc: 0.270000; val_acc: 0.274000\n",
      "(Iteration 801 / 30600) loss: 2.273721\n",
      "(Iteration 901 / 30600) loss: 2.270116\n",
      "(Iteration 1001 / 30600) loss: 2.226466\n",
      "(Iteration 1101 / 30600) loss: 2.166411\n",
      "(Iteration 1201 / 30600) loss: 2.143572\n",
      "(Iteration 1301 / 30600) loss: 2.114491\n",
      "(Iteration 1401 / 30600) loss: 2.045554\n",
      "(Iteration 1501 / 30600) loss: 2.097901\n",
      "(Epoch 2 / 40) train acc: 0.281000; val_acc: 0.290000\n",
      "(Iteration 1601 / 30600) loss: 2.011898\n",
      "(Iteration 1701 / 30600) loss: 2.014251\n",
      "(Iteration 1801 / 30600) loss: 1.872634\n",
      "(Iteration 1901 / 30600) loss: 1.843910\n",
      "(Iteration 2001 / 30600) loss: 2.003859\n",
      "(Iteration 2101 / 30600) loss: 1.712211\n",
      "(Iteration 2201 / 30600) loss: 1.829933\n",
      "(Epoch 3 / 40) train acc: 0.390000; val_acc: 0.368000\n",
      "(Iteration 2301 / 30600) loss: 1.899040\n",
      "(Iteration 2401 / 30600) loss: 1.828137\n",
      "(Iteration 2501 / 30600) loss: 1.873990\n",
      "(Iteration 2601 / 30600) loss: 1.681542\n",
      "(Iteration 2701 / 30600) loss: 1.690051\n",
      "(Iteration 2801 / 30600) loss: 1.616579\n",
      "(Iteration 2901 / 30600) loss: 1.675454\n",
      "(Iteration 3001 / 30600) loss: 1.564862\n",
      "(Epoch 4 / 40) train acc: 0.405000; val_acc: 0.411000\n",
      "(Iteration 3101 / 30600) loss: 1.592589\n",
      "(Iteration 3201 / 30600) loss: 1.680658\n",
      "(Iteration 3301 / 30600) loss: 1.696664\n",
      "(Iteration 3401 / 30600) loss: 1.542095\n",
      "(Iteration 3501 / 30600) loss: 1.682037\n",
      "(Iteration 3601 / 30600) loss: 1.795036\n",
      "(Iteration 3701 / 30600) loss: 1.566061\n",
      "(Iteration 3801 / 30600) loss: 1.659728\n",
      "(Epoch 5 / 40) train acc: 0.422000; val_acc: 0.433000\n",
      "(Iteration 3901 / 30600) loss: 1.486655\n",
      "(Iteration 4001 / 30600) loss: 1.516336\n",
      "(Iteration 4101 / 30600) loss: 1.756861\n",
      "(Iteration 4201 / 30600) loss: 1.328615\n",
      "(Iteration 4301 / 30600) loss: 1.485256\n",
      "(Iteration 4401 / 30600) loss: 1.635374\n",
      "(Iteration 4501 / 30600) loss: 1.473199\n",
      "(Epoch 6 / 40) train acc: 0.454000; val_acc: 0.462000\n",
      "(Iteration 4601 / 30600) loss: 1.617933\n",
      "(Iteration 4701 / 30600) loss: 1.665310\n",
      "(Iteration 4801 / 30600) loss: 1.612176\n",
      "(Iteration 4901 / 30600) loss: 1.383399\n",
      "(Iteration 5001 / 30600) loss: 1.552582\n",
      "(Iteration 5101 / 30600) loss: 1.555442\n",
      "(Iteration 5201 / 30600) loss: 1.454330\n",
      "(Iteration 5301 / 30600) loss: 1.468633\n",
      "(Epoch 7 / 40) train acc: 0.481000; val_acc: 0.476000\n",
      "(Iteration 5401 / 30600) loss: 1.460094\n",
      "(Iteration 5501 / 30600) loss: 1.501057\n",
      "(Iteration 5601 / 30600) loss: 1.473229\n",
      "(Iteration 5701 / 30600) loss: 1.478025\n",
      "(Iteration 5801 / 30600) loss: 1.367739\n",
      "(Iteration 5901 / 30600) loss: 1.497809\n",
      "(Iteration 6001 / 30600) loss: 1.611633\n",
      "(Iteration 6101 / 30600) loss: 1.284643\n",
      "(Epoch 8 / 40) train acc: 0.491000; val_acc: 0.484000\n",
      "(Iteration 6201 / 30600) loss: 1.678181\n",
      "(Iteration 6301 / 30600) loss: 1.321099\n",
      "(Iteration 6401 / 30600) loss: 1.485377\n",
      "(Iteration 6501 / 30600) loss: 1.699200\n",
      "(Iteration 6601 / 30600) loss: 1.655924\n",
      "(Iteration 6701 / 30600) loss: 1.306772\n",
      "(Iteration 6801 / 30600) loss: 1.509319\n",
      "(Epoch 9 / 40) train acc: 0.498000; val_acc: 0.488000\n",
      "(Iteration 6901 / 30600) loss: 1.413057\n",
      "(Iteration 7001 / 30600) loss: 1.387913\n",
      "(Iteration 7101 / 30600) loss: 1.489633\n",
      "(Iteration 7201 / 30600) loss: 1.379840\n",
      "(Iteration 7301 / 30600) loss: 1.699148\n",
      "(Iteration 7401 / 30600) loss: 1.505713\n",
      "(Iteration 7501 / 30600) loss: 1.574780\n",
      "(Iteration 7601 / 30600) loss: 1.474047\n",
      "(Epoch 10 / 40) train acc: 0.493000; val_acc: 0.496000\n",
      "(Iteration 7701 / 30600) loss: 1.476030\n",
      "(Iteration 7801 / 30600) loss: 1.467937\n",
      "(Iteration 7901 / 30600) loss: 1.424383\n",
      "(Iteration 8001 / 30600) loss: 1.512890\n",
      "(Iteration 8101 / 30600) loss: 1.587995\n",
      "(Iteration 8201 / 30600) loss: 1.416790\n",
      "(Iteration 8301 / 30600) loss: 1.528189\n",
      "(Iteration 8401 / 30600) loss: 1.447572\n",
      "(Epoch 11 / 40) train acc: 0.525000; val_acc: 0.499000\n",
      "(Iteration 8501 / 30600) loss: 1.381312\n",
      "(Iteration 8601 / 30600) loss: 1.155798\n",
      "(Iteration 8701 / 30600) loss: 1.712540\n",
      "(Iteration 8801 / 30600) loss: 1.475066\n",
      "(Iteration 8901 / 30600) loss: 1.288011\n",
      "(Iteration 9001 / 30600) loss: 1.500181\n",
      "(Iteration 9101 / 30600) loss: 1.471051\n",
      "(Epoch 12 / 40) train acc: 0.493000; val_acc: 0.508000\n",
      "(Iteration 9201 / 30600) loss: 1.593471\n",
      "(Iteration 9301 / 30600) loss: 1.378118\n",
      "(Iteration 9401 / 30600) loss: 1.508675\n",
      "(Iteration 9501 / 30600) loss: 1.611871\n",
      "(Iteration 9601 / 30600) loss: 1.565422\n",
      "(Iteration 9701 / 30600) loss: 1.442353\n",
      "(Iteration 9801 / 30600) loss: 1.508533\n",
      "(Iteration 9901 / 30600) loss: 1.364817\n",
      "(Epoch 13 / 40) train acc: 0.542000; val_acc: 0.505000\n",
      "(Iteration 10001 / 30600) loss: 1.581604\n",
      "(Iteration 10101 / 30600) loss: 1.645250\n",
      "(Iteration 10201 / 30600) loss: 1.305062\n",
      "(Iteration 10301 / 30600) loss: 1.630433\n",
      "(Iteration 10401 / 30600) loss: 1.477569\n",
      "(Iteration 10501 / 30600) loss: 1.397218\n",
      "(Iteration 10601 / 30600) loss: 1.242220\n",
      "(Iteration 10701 / 30600) loss: 1.603963\n",
      "(Epoch 14 / 40) train acc: 0.481000; val_acc: 0.515000\n",
      "(Iteration 10801 / 30600) loss: 1.403852\n",
      "(Iteration 10901 / 30600) loss: 1.569941\n",
      "(Iteration 11001 / 30600) loss: 1.410027\n",
      "(Iteration 11101 / 30600) loss: 1.486763\n",
      "(Iteration 11201 / 30600) loss: 1.765164\n",
      "(Iteration 11301 / 30600) loss: 1.584090\n",
      "(Iteration 11401 / 30600) loss: 1.479552\n",
      "(Epoch 15 / 40) train acc: 0.510000; val_acc: 0.512000\n",
      "(Iteration 11501 / 30600) loss: 1.465959\n",
      "(Iteration 11601 / 30600) loss: 1.570059\n",
      "(Iteration 11701 / 30600) loss: 1.555425\n",
      "(Iteration 11801 / 30600) loss: 1.301864\n",
      "(Iteration 11901 / 30600) loss: 1.506413\n",
      "(Iteration 12001 / 30600) loss: 1.765065\n",
      "(Iteration 12101 / 30600) loss: 1.407642\n",
      "(Iteration 12201 / 30600) loss: 1.380123\n",
      "(Epoch 16 / 40) train acc: 0.540000; val_acc: 0.512000\n",
      "(Iteration 12301 / 30600) loss: 1.576670\n",
      "(Iteration 12401 / 30600) loss: 1.422951\n",
      "(Iteration 12501 / 30600) loss: 1.419308\n",
      "(Iteration 12601 / 30600) loss: 1.418631\n",
      "(Iteration 12701 / 30600) loss: 1.585088\n",
      "(Iteration 12801 / 30600) loss: 1.393947\n",
      "(Iteration 12901 / 30600) loss: 1.480011\n",
      "(Iteration 13001 / 30600) loss: 1.339621\n",
      "(Epoch 17 / 40) train acc: 0.496000; val_acc: 0.513000\n",
      "(Iteration 13101 / 30600) loss: 1.435533\n",
      "(Iteration 13201 / 30600) loss: 1.487019\n",
      "(Iteration 13301 / 30600) loss: 1.576819\n",
      "(Iteration 13401 / 30600) loss: 1.368604\n",
      "(Iteration 13501 / 30600) loss: 1.264788\n",
      "(Iteration 13601 / 30600) loss: 1.448675\n",
      "(Iteration 13701 / 30600) loss: 1.387440\n",
      "(Epoch 18 / 40) train acc: 0.497000; val_acc: 0.517000\n",
      "(Iteration 13801 / 30600) loss: 1.465401\n",
      "(Iteration 13901 / 30600) loss: 1.426654\n",
      "(Iteration 14001 / 30600) loss: 1.579305\n",
      "(Iteration 14101 / 30600) loss: 1.383735\n",
      "(Iteration 14201 / 30600) loss: 1.353514\n",
      "(Iteration 14301 / 30600) loss: 1.594629\n",
      "(Iteration 14401 / 30600) loss: 1.719129\n",
      "(Iteration 14501 / 30600) loss: 1.666167\n",
      "(Epoch 19 / 40) train acc: 0.536000; val_acc: 0.512000\n",
      "(Iteration 14601 / 30600) loss: 1.547166\n",
      "(Iteration 14701 / 30600) loss: 1.407294\n",
      "(Iteration 14801 / 30600) loss: 1.391074\n",
      "(Iteration 14901 / 30600) loss: 1.513289\n",
      "(Iteration 15001 / 30600) loss: 1.610690\n",
      "(Iteration 15101 / 30600) loss: 1.393135\n",
      "(Iteration 15201 / 30600) loss: 1.514962\n",
      "(Epoch 20 / 40) train acc: 0.507000; val_acc: 0.510000\n",
      "(Iteration 15301 / 30600) loss: 1.324267\n",
      "(Iteration 15401 / 30600) loss: 1.627827\n",
      "(Iteration 15501 / 30600) loss: 1.307992\n",
      "(Iteration 15601 / 30600) loss: 1.482075\n",
      "(Iteration 15701 / 30600) loss: 1.505252\n",
      "(Iteration 15801 / 30600) loss: 1.367666\n",
      "(Iteration 15901 / 30600) loss: 1.468802\n",
      "(Iteration 16001 / 30600) loss: 1.495192\n",
      "(Epoch 21 / 40) train acc: 0.492000; val_acc: 0.508000\n",
      "(Iteration 16101 / 30600) loss: 1.442149\n",
      "(Iteration 16201 / 30600) loss: 1.612618\n",
      "(Iteration 16301 / 30600) loss: 1.510511\n",
      "(Iteration 16401 / 30600) loss: 1.436164\n",
      "(Iteration 16501 / 30600) loss: 1.478086\n",
      "(Iteration 16601 / 30600) loss: 1.420465\n",
      "(Iteration 16701 / 30600) loss: 1.267418\n",
      "(Iteration 16801 / 30600) loss: 1.502564\n",
      "(Epoch 22 / 40) train acc: 0.527000; val_acc: 0.512000\n",
      "(Iteration 16901 / 30600) loss: 1.294326\n",
      "(Iteration 17001 / 30600) loss: 1.322949\n",
      "(Iteration 17101 / 30600) loss: 1.488297\n",
      "(Iteration 17201 / 30600) loss: 1.454594\n",
      "(Iteration 17301 / 30600) loss: 1.558214\n",
      "(Iteration 17401 / 30600) loss: 1.521048\n",
      "(Iteration 17501 / 30600) loss: 1.568423\n",
      "(Epoch 23 / 40) train acc: 0.546000; val_acc: 0.510000\n",
      "(Iteration 17601 / 30600) loss: 1.596963\n",
      "(Iteration 17701 / 30600) loss: 1.618927\n",
      "(Iteration 17801 / 30600) loss: 1.514810\n",
      "(Iteration 17901 / 30600) loss: 1.480548\n",
      "(Iteration 18001 / 30600) loss: 1.415665\n",
      "(Iteration 18101 / 30600) loss: 1.463537\n",
      "(Iteration 18201 / 30600) loss: 1.244214\n",
      "(Iteration 18301 / 30600) loss: 1.436452\n",
      "(Epoch 24 / 40) train acc: 0.511000; val_acc: 0.511000\n",
      "(Iteration 18401 / 30600) loss: 1.316046\n",
      "(Iteration 18501 / 30600) loss: 1.462367\n",
      "(Iteration 18601 / 30600) loss: 1.546345\n",
      "(Iteration 18701 / 30600) loss: 1.383233\n",
      "(Iteration 18801 / 30600) loss: 1.419184\n",
      "(Iteration 18901 / 30600) loss: 1.456697\n",
      "(Iteration 19001 / 30600) loss: 1.414877\n",
      "(Iteration 19101 / 30600) loss: 1.448410\n",
      "(Epoch 25 / 40) train acc: 0.531000; val_acc: 0.512000\n",
      "(Iteration 19201 / 30600) loss: 1.459086\n",
      "(Iteration 19301 / 30600) loss: 1.495380\n",
      "(Iteration 19401 / 30600) loss: 1.602416\n",
      "(Iteration 19501 / 30600) loss: 1.473707\n",
      "(Iteration 19601 / 30600) loss: 1.497995\n",
      "(Iteration 19701 / 30600) loss: 1.443776\n",
      "(Iteration 19801 / 30600) loss: 1.671381\n",
      "(Epoch 26 / 40) train acc: 0.504000; val_acc: 0.510000\n",
      "(Iteration 19901 / 30600) loss: 1.587553\n",
      "(Iteration 20001 / 30600) loss: 1.281893\n",
      "(Iteration 20101 / 30600) loss: 1.796814\n",
      "(Iteration 20201 / 30600) loss: 1.296542\n",
      "(Iteration 20301 / 30600) loss: 1.476187\n",
      "(Iteration 20401 / 30600) loss: 1.514524\n",
      "(Iteration 20501 / 30600) loss: 1.539572\n",
      "(Iteration 20601 / 30600) loss: 1.503688\n",
      "(Epoch 27 / 40) train acc: 0.514000; val_acc: 0.512000\n",
      "(Iteration 20701 / 30600) loss: 1.514721\n",
      "(Iteration 20801 / 30600) loss: 1.388455\n",
      "(Iteration 20901 / 30600) loss: 1.398360\n",
      "(Iteration 21001 / 30600) loss: 1.918553\n",
      "(Iteration 21101 / 30600) loss: 1.467894\n",
      "(Iteration 21201 / 30600) loss: 1.357417\n",
      "(Iteration 21301 / 30600) loss: 1.437485\n",
      "(Iteration 21401 / 30600) loss: 1.431208\n",
      "(Epoch 28 / 40) train acc: 0.522000; val_acc: 0.511000\n",
      "(Iteration 21501 / 30600) loss: 1.426000\n",
      "(Iteration 21601 / 30600) loss: 1.574697\n",
      "(Iteration 21701 / 30600) loss: 1.549878\n",
      "(Iteration 21801 / 30600) loss: 1.478458\n",
      "(Iteration 21901 / 30600) loss: 1.358432\n",
      "(Iteration 22001 / 30600) loss: 1.426781\n",
      "(Iteration 22101 / 30600) loss: 1.524474\n",
      "(Epoch 29 / 40) train acc: 0.529000; val_acc: 0.512000\n",
      "(Iteration 22201 / 30600) loss: 1.418287\n",
      "(Iteration 22301 / 30600) loss: 1.298256\n",
      "(Iteration 22401 / 30600) loss: 1.390675\n",
      "(Iteration 22501 / 30600) loss: 1.447337\n",
      "(Iteration 22601 / 30600) loss: 1.480208\n",
      "(Iteration 22701 / 30600) loss: 1.460920\n",
      "(Iteration 22801 / 30600) loss: 1.509438\n",
      "(Iteration 22901 / 30600) loss: 1.450030\n",
      "(Epoch 30 / 40) train acc: 0.511000; val_acc: 0.512000\n",
      "(Iteration 23001 / 30600) loss: 1.659345\n",
      "(Iteration 23101 / 30600) loss: 1.543650\n",
      "(Iteration 23201 / 30600) loss: 1.544661\n",
      "(Iteration 23301 / 30600) loss: 1.419984\n",
      "(Iteration 23401 / 30600) loss: 1.329500\n",
      "(Iteration 23501 / 30600) loss: 1.471997\n",
      "(Iteration 23601 / 30600) loss: 1.649073\n",
      "(Iteration 23701 / 30600) loss: 1.417106\n",
      "(Epoch 31 / 40) train acc: 0.526000; val_acc: 0.509000\n",
      "(Iteration 23801 / 30600) loss: 1.447729\n",
      "(Iteration 23901 / 30600) loss: 1.557293\n",
      "(Iteration 24001 / 30600) loss: 1.556919\n",
      "(Iteration 24101 / 30600) loss: 1.461854\n",
      "(Iteration 24201 / 30600) loss: 1.585458\n",
      "(Iteration 24301 / 30600) loss: 1.302985\n",
      "(Iteration 24401 / 30600) loss: 1.454913\n",
      "(Epoch 32 / 40) train acc: 0.517000; val_acc: 0.509000\n",
      "(Iteration 24501 / 30600) loss: 1.328211\n",
      "(Iteration 24601 / 30600) loss: 1.563794\n",
      "(Iteration 24701 / 30600) loss: 1.487672\n",
      "(Iteration 24801 / 30600) loss: 1.437265\n",
      "(Iteration 24901 / 30600) loss: 1.640094\n",
      "(Iteration 25001 / 30600) loss: 1.503631\n",
      "(Iteration 25101 / 30600) loss: 1.609120\n",
      "(Iteration 25201 / 30600) loss: 1.657460\n",
      "(Epoch 33 / 40) train acc: 0.526000; val_acc: 0.508000\n",
      "(Iteration 25301 / 30600) loss: 1.215732\n",
      "(Iteration 25401 / 30600) loss: 1.399996\n",
      "(Iteration 25501 / 30600) loss: 1.420679\n",
      "(Iteration 25601 / 30600) loss: 1.655461\n",
      "(Iteration 25701 / 30600) loss: 1.409468\n",
      "(Iteration 25801 / 30600) loss: 1.389256\n",
      "(Iteration 25901 / 30600) loss: 1.446952\n",
      "(Iteration 26001 / 30600) loss: 1.536531\n",
      "(Epoch 34 / 40) train acc: 0.499000; val_acc: 0.508000\n",
      "(Iteration 26101 / 30600) loss: 1.379240\n",
      "(Iteration 26201 / 30600) loss: 1.396260\n",
      "(Iteration 26301 / 30600) loss: 1.528890\n",
      "(Iteration 26401 / 30600) loss: 1.412626\n",
      "(Iteration 26501 / 30600) loss: 1.454685\n",
      "(Iteration 26601 / 30600) loss: 1.430641\n",
      "(Iteration 26701 / 30600) loss: 1.455351\n",
      "(Epoch 35 / 40) train acc: 0.506000; val_acc: 0.508000\n",
      "(Iteration 26801 / 30600) loss: 1.708688\n",
      "(Iteration 26901 / 30600) loss: 1.742585\n",
      "(Iteration 27001 / 30600) loss: 1.282668\n",
      "(Iteration 27101 / 30600) loss: 1.311860\n",
      "(Iteration 27201 / 30600) loss: 1.544510\n",
      "(Iteration 27301 / 30600) loss: 1.495581\n",
      "(Iteration 27401 / 30600) loss: 1.367559\n",
      "(Iteration 27501 / 30600) loss: 1.648604\n",
      "(Epoch 36 / 40) train acc: 0.523000; val_acc: 0.509000\n",
      "(Iteration 27601 / 30600) loss: 1.443477\n",
      "(Iteration 27701 / 30600) loss: 1.456639\n",
      "(Iteration 27801 / 30600) loss: 1.327751\n",
      "(Iteration 27901 / 30600) loss: 1.538943\n",
      "(Iteration 28001 / 30600) loss: 1.637355\n",
      "(Iteration 28101 / 30600) loss: 1.402604\n",
      "(Iteration 28201 / 30600) loss: 1.401043\n",
      "(Iteration 28301 / 30600) loss: 1.667970\n",
      "(Epoch 37 / 40) train acc: 0.530000; val_acc: 0.509000\n",
      "(Iteration 28401 / 30600) loss: 1.442008\n",
      "(Iteration 28501 / 30600) loss: 1.569692\n",
      "(Iteration 28601 / 30600) loss: 1.572343\n",
      "(Iteration 28701 / 30600) loss: 1.392122\n",
      "(Iteration 28801 / 30600) loss: 1.315180\n",
      "(Iteration 28901 / 30600) loss: 1.333834\n",
      "(Iteration 29001 / 30600) loss: 1.389369\n",
      "(Epoch 38 / 40) train acc: 0.535000; val_acc: 0.510000\n",
      "(Iteration 29101 / 30600) loss: 1.544426\n",
      "(Iteration 29201 / 30600) loss: 1.586055\n",
      "(Iteration 29301 / 30600) loss: 1.376678\n",
      "(Iteration 29401 / 30600) loss: 1.462857\n",
      "(Iteration 29501 / 30600) loss: 1.500975\n",
      "(Iteration 29601 / 30600) loss: 1.422806\n",
      "(Iteration 29701 / 30600) loss: 1.330077\n",
      "(Iteration 29801 / 30600) loss: 1.464058\n",
      "(Epoch 39 / 40) train acc: 0.510000; val_acc: 0.510000\n",
      "(Iteration 29901 / 30600) loss: 1.412862\n",
      "(Iteration 30001 / 30600) loss: 1.690809\n",
      "(Iteration 30101 / 30600) loss: 1.427382\n",
      "(Iteration 30201 / 30600) loss: 1.588516\n",
      "(Iteration 30301 / 30600) loss: 1.286557\n",
      "(Iteration 30401 / 30600) loss: 1.810262\n",
      "(Iteration 30501 / 30600) loss: 1.407911\n",
      "(Epoch 40 / 40) train acc: 0.511000; val_acc: 0.510000\n",
      "Training with parameters: {'hidden_size': 800, 'learning_rate': 0.01, 'num_epochs': 40, 'reg': 0.01, 'batch_size': 32}\n",
      "(Iteration 1 / 61240) loss: 2.303264\n",
      "(Epoch 0 / 40) train acc: 0.089000; val_acc: 0.093000\n",
      "(Iteration 101 / 61240) loss: 2.303185\n",
      "(Iteration 201 / 61240) loss: 2.304993\n",
      "(Iteration 301 / 61240) loss: 2.302387\n",
      "(Iteration 401 / 61240) loss: 2.300772\n",
      "(Iteration 501 / 61240) loss: 2.299997\n",
      "(Iteration 601 / 61240) loss: 2.293261\n",
      "(Iteration 701 / 61240) loss: 2.288818\n",
      "(Iteration 801 / 61240) loss: 2.286858\n",
      "(Iteration 901 / 61240) loss: 2.255654\n",
      "(Iteration 1001 / 61240) loss: 2.227887\n",
      "(Iteration 1101 / 61240) loss: 2.217201\n",
      "(Iteration 1201 / 61240) loss: 2.186273\n",
      "(Iteration 1301 / 61240) loss: 2.149748\n",
      "(Iteration 1401 / 61240) loss: 2.149569\n",
      "(Iteration 1501 / 61240) loss: 1.967591\n",
      "(Epoch 1 / 40) train acc: 0.313000; val_acc: 0.313000\n",
      "(Iteration 1601 / 61240) loss: 2.038830\n",
      "(Iteration 1701 / 61240) loss: 1.968348\n",
      "(Iteration 1801 / 61240) loss: 1.693591\n",
      "(Iteration 1901 / 61240) loss: 1.690347\n",
      "(Iteration 2001 / 61240) loss: 1.993039\n",
      "(Iteration 2101 / 61240) loss: 1.703539\n",
      "(Iteration 2201 / 61240) loss: 1.679598\n",
      "(Iteration 2301 / 61240) loss: 1.807300\n",
      "(Iteration 2401 / 61240) loss: 1.697571\n",
      "(Iteration 2501 / 61240) loss: 1.760308\n",
      "(Iteration 2601 / 61240) loss: 1.652151\n",
      "(Iteration 2701 / 61240) loss: 1.654971\n",
      "(Iteration 2801 / 61240) loss: 1.622704\n",
      "(Iteration 2901 / 61240) loss: 1.625138\n",
      "(Iteration 3001 / 61240) loss: 1.623916\n",
      "(Epoch 2 / 40) train acc: 0.451000; val_acc: 0.432000\n",
      "(Iteration 3101 / 61240) loss: 1.488792\n",
      "(Iteration 3201 / 61240) loss: 1.711210\n",
      "(Iteration 3301 / 61240) loss: 1.960561\n",
      "(Iteration 3401 / 61240) loss: 1.629082\n",
      "(Iteration 3501 / 61240) loss: 1.545928\n",
      "(Iteration 3601 / 61240) loss: 1.464552\n",
      "(Iteration 3701 / 61240) loss: 1.375737\n",
      "(Iteration 3801 / 61240) loss: 1.533937\n",
      "(Iteration 3901 / 61240) loss: 1.508357\n",
      "(Iteration 4001 / 61240) loss: 1.678400\n",
      "(Iteration 4101 / 61240) loss: 1.540747\n",
      "(Iteration 4201 / 61240) loss: 1.463401\n",
      "(Iteration 4301 / 61240) loss: 1.199113\n",
      "(Iteration 4401 / 61240) loss: 1.554529\n",
      "(Iteration 4501 / 61240) loss: 1.464329\n",
      "(Epoch 3 / 40) train acc: 0.469000; val_acc: 0.472000\n",
      "(Iteration 4601 / 61240) loss: 1.811460\n",
      "(Iteration 4701 / 61240) loss: 1.558507\n",
      "(Iteration 4801 / 61240) loss: 1.629609\n",
      "(Iteration 4901 / 61240) loss: 1.750882\n",
      "(Iteration 5001 / 61240) loss: 1.430864\n",
      "(Iteration 5101 / 61240) loss: 1.394263\n",
      "(Iteration 5201 / 61240) loss: 1.684451\n",
      "(Iteration 5301 / 61240) loss: 1.317029\n",
      "(Iteration 5401 / 61240) loss: 1.380282\n",
      "(Iteration 5501 / 61240) loss: 1.768009\n",
      "(Iteration 5601 / 61240) loss: 1.917151\n",
      "(Iteration 5701 / 61240) loss: 1.290333\n",
      "(Iteration 5801 / 61240) loss: 1.454142\n",
      "(Iteration 5901 / 61240) loss: 1.644462\n",
      "(Iteration 6001 / 61240) loss: 1.426314\n",
      "(Iteration 6101 / 61240) loss: 1.606682\n",
      "(Epoch 4 / 40) train acc: 0.533000; val_acc: 0.495000\n",
      "(Iteration 6201 / 61240) loss: 1.910181\n",
      "(Iteration 6301 / 61240) loss: 1.149867\n",
      "(Iteration 6401 / 61240) loss: 1.184233\n",
      "(Iteration 6501 / 61240) loss: 1.452280\n",
      "(Iteration 6601 / 61240) loss: 1.276572\n",
      "(Iteration 6701 / 61240) loss: 1.538302\n",
      "(Iteration 6801 / 61240) loss: 1.214163\n",
      "(Iteration 6901 / 61240) loss: 1.367752\n",
      "(Iteration 7001 / 61240) loss: 1.235615\n",
      "(Iteration 7101 / 61240) loss: 1.033393\n",
      "(Iteration 7201 / 61240) loss: 1.672420\n",
      "(Iteration 7301 / 61240) loss: 1.510128\n",
      "(Iteration 7401 / 61240) loss: 1.532647\n",
      "(Iteration 7501 / 61240) loss: 1.590927\n",
      "(Iteration 7601 / 61240) loss: 1.354621\n",
      "(Epoch 5 / 40) train acc: 0.538000; val_acc: 0.499000\n",
      "(Iteration 7701 / 61240) loss: 1.417700\n",
      "(Iteration 7801 / 61240) loss: 1.200701\n",
      "(Iteration 7901 / 61240) loss: 1.475921\n",
      "(Iteration 8001 / 61240) loss: 1.439493\n",
      "(Iteration 8101 / 61240) loss: 1.593208\n",
      "(Iteration 8201 / 61240) loss: 1.607591\n",
      "(Iteration 8301 / 61240) loss: 1.399934\n",
      "(Iteration 8401 / 61240) loss: 1.186448\n",
      "(Iteration 8501 / 61240) loss: 1.136458\n",
      "(Iteration 8601 / 61240) loss: 1.686083\n",
      "(Iteration 8701 / 61240) loss: 1.381801\n",
      "(Iteration 8801 / 61240) loss: 1.491810\n",
      "(Iteration 8901 / 61240) loss: 1.797843\n",
      "(Iteration 9001 / 61240) loss: 1.458076\n",
      "(Iteration 9101 / 61240) loss: 1.261123\n",
      "(Epoch 6 / 40) train acc: 0.518000; val_acc: 0.503000\n",
      "(Iteration 9201 / 61240) loss: 1.290776\n",
      "(Iteration 9301 / 61240) loss: 1.608065\n",
      "(Iteration 9401 / 61240) loss: 1.519015\n",
      "(Iteration 9501 / 61240) loss: 1.537753\n",
      "(Iteration 9601 / 61240) loss: 1.562703\n",
      "(Iteration 9701 / 61240) loss: 1.492798\n",
      "(Iteration 9801 / 61240) loss: 1.355562\n",
      "(Iteration 9901 / 61240) loss: 1.602004\n",
      "(Iteration 10001 / 61240) loss: 1.602936\n",
      "(Iteration 10101 / 61240) loss: 1.392383\n",
      "(Iteration 10201 / 61240) loss: 1.135321\n",
      "(Iteration 10301 / 61240) loss: 1.289118\n",
      "(Iteration 10401 / 61240) loss: 1.419932\n",
      "(Iteration 10501 / 61240) loss: 1.619339\n",
      "(Iteration 10601 / 61240) loss: 1.598722\n",
      "(Iteration 10701 / 61240) loss: 1.340636\n",
      "(Epoch 7 / 40) train acc: 0.556000; val_acc: 0.500000\n",
      "(Iteration 10801 / 61240) loss: 1.601642\n",
      "(Iteration 10901 / 61240) loss: 1.510501\n",
      "(Iteration 11001 / 61240) loss: 1.649731\n",
      "(Iteration 11101 / 61240) loss: 1.312771\n",
      "(Iteration 11201 / 61240) loss: 1.474926\n",
      "(Iteration 11301 / 61240) loss: 1.303143\n",
      "(Iteration 11401 / 61240) loss: 1.427269\n",
      "(Iteration 11501 / 61240) loss: 1.832798\n",
      "(Iteration 11601 / 61240) loss: 1.383366\n",
      "(Iteration 11701 / 61240) loss: 1.453098\n",
      "(Iteration 11801 / 61240) loss: 1.389880\n",
      "(Iteration 11901 / 61240) loss: 1.530001\n",
      "(Iteration 12001 / 61240) loss: 1.572358\n",
      "(Iteration 12101 / 61240) loss: 1.335202\n",
      "(Iteration 12201 / 61240) loss: 1.402452\n",
      "(Epoch 8 / 40) train acc: 0.541000; val_acc: 0.505000\n",
      "(Iteration 12301 / 61240) loss: 1.461735\n",
      "(Iteration 12401 / 61240) loss: 1.167439\n",
      "(Iteration 12501 / 61240) loss: 1.507577\n",
      "(Iteration 12601 / 61240) loss: 1.405938\n",
      "(Iteration 12701 / 61240) loss: 1.363981\n",
      "(Iteration 12801 / 61240) loss: 1.595128\n",
      "(Iteration 12901 / 61240) loss: 1.469251\n",
      "(Iteration 13001 / 61240) loss: 1.424816\n",
      "(Iteration 13101 / 61240) loss: 1.252852\n",
      "(Iteration 13201 / 61240) loss: 1.464636\n",
      "(Iteration 13301 / 61240) loss: 1.557343\n",
      "(Iteration 13401 / 61240) loss: 1.515184\n",
      "(Iteration 13501 / 61240) loss: 1.704934\n",
      "(Iteration 13601 / 61240) loss: 1.565260\n",
      "(Iteration 13701 / 61240) loss: 1.261050\n",
      "(Epoch 9 / 40) train acc: 0.517000; val_acc: 0.510000\n",
      "(Iteration 13801 / 61240) loss: 1.877345\n",
      "(Iteration 13901 / 61240) loss: 1.352171\n",
      "(Iteration 14001 / 61240) loss: 1.516385\n",
      "(Iteration 14101 / 61240) loss: 1.554813\n",
      "(Iteration 14201 / 61240) loss: 1.654492\n",
      "(Iteration 14301 / 61240) loss: 1.381030\n",
      "(Iteration 14401 / 61240) loss: 1.328985\n",
      "(Iteration 14501 / 61240) loss: 1.530556\n",
      "(Iteration 14601 / 61240) loss: 1.556827\n",
      "(Iteration 14701 / 61240) loss: 1.542414\n",
      "(Iteration 14801 / 61240) loss: 1.317233\n",
      "(Iteration 14901 / 61240) loss: 1.283025\n",
      "(Iteration 15001 / 61240) loss: 1.696351\n",
      "(Iteration 15101 / 61240) loss: 1.523244\n",
      "(Iteration 15201 / 61240) loss: 1.521894\n",
      "(Iteration 15301 / 61240) loss: 1.632879\n",
      "(Epoch 10 / 40) train acc: 0.519000; val_acc: 0.518000\n",
      "(Iteration 15401 / 61240) loss: 1.285912\n",
      "(Iteration 15501 / 61240) loss: 1.558122\n",
      "(Iteration 15601 / 61240) loss: 1.626270\n",
      "(Iteration 15701 / 61240) loss: 1.680460\n",
      "(Iteration 15801 / 61240) loss: 1.424530\n",
      "(Iteration 15901 / 61240) loss: 1.498204\n",
      "(Iteration 16001 / 61240) loss: 1.325998\n",
      "(Iteration 16101 / 61240) loss: 1.506730\n",
      "(Iteration 16201 / 61240) loss: 1.551461\n",
      "(Iteration 16301 / 61240) loss: 1.630526\n",
      "(Iteration 16401 / 61240) loss: 1.534030\n",
      "(Iteration 16501 / 61240) loss: 1.415926\n",
      "(Iteration 16601 / 61240) loss: 1.391808\n",
      "(Iteration 16701 / 61240) loss: 1.425569\n",
      "(Iteration 16801 / 61240) loss: 1.485578\n",
      "(Epoch 11 / 40) train acc: 0.550000; val_acc: 0.506000\n",
      "(Iteration 16901 / 61240) loss: 1.444232\n",
      "(Iteration 17001 / 61240) loss: 1.296074\n",
      "(Iteration 17101 / 61240) loss: 1.371809\n",
      "(Iteration 17201 / 61240) loss: 1.548177\n",
      "(Iteration 17301 / 61240) loss: 1.304096\n",
      "(Iteration 17401 / 61240) loss: 1.198632\n",
      "(Iteration 17501 / 61240) loss: 1.354122\n",
      "(Iteration 17601 / 61240) loss: 1.359960\n",
      "(Iteration 17701 / 61240) loss: 1.408353\n",
      "(Iteration 17801 / 61240) loss: 1.538180\n",
      "(Iteration 17901 / 61240) loss: 1.631967\n",
      "(Iteration 18001 / 61240) loss: 1.581339\n",
      "(Iteration 18101 / 61240) loss: 1.686879\n",
      "(Iteration 18201 / 61240) loss: 1.626833\n",
      "(Iteration 18301 / 61240) loss: 1.294530\n",
      "(Epoch 12 / 40) train acc: 0.539000; val_acc: 0.520000\n",
      "(Iteration 18401 / 61240) loss: 1.521202\n",
      "(Iteration 18501 / 61240) loss: 1.376343\n",
      "(Iteration 18601 / 61240) loss: 1.424299\n",
      "(Iteration 18701 / 61240) loss: 1.552230\n",
      "(Iteration 18801 / 61240) loss: 1.529176\n",
      "(Iteration 18901 / 61240) loss: 1.401494\n",
      "(Iteration 19001 / 61240) loss: 1.322687\n",
      "(Iteration 19101 / 61240) loss: 1.619201\n",
      "(Iteration 19201 / 61240) loss: 1.576846\n",
      "(Iteration 19301 / 61240) loss: 1.210644\n",
      "(Iteration 19401 / 61240) loss: 1.339642\n",
      "(Iteration 19501 / 61240) loss: 1.503309\n",
      "(Iteration 19601 / 61240) loss: 1.514517\n",
      "(Iteration 19701 / 61240) loss: 1.225280\n",
      "(Iteration 19801 / 61240) loss: 1.378555\n",
      "(Iteration 19901 / 61240) loss: 1.575714\n",
      "(Epoch 13 / 40) train acc: 0.529000; val_acc: 0.519000\n",
      "(Iteration 20001 / 61240) loss: 1.376916\n",
      "(Iteration 20101 / 61240) loss: 1.334952\n",
      "(Iteration 20201 / 61240) loss: 1.346996\n",
      "(Iteration 20301 / 61240) loss: 1.580491\n",
      "(Iteration 20401 / 61240) loss: 1.525501\n",
      "(Iteration 20501 / 61240) loss: 1.488727\n",
      "(Iteration 20601 / 61240) loss: 1.317321\n",
      "(Iteration 20701 / 61240) loss: 1.523556\n",
      "(Iteration 20801 / 61240) loss: 1.410048\n",
      "(Iteration 20901 / 61240) loss: 1.543774\n",
      "(Iteration 21001 / 61240) loss: 1.462817\n",
      "(Iteration 21101 / 61240) loss: 1.146649\n",
      "(Iteration 21201 / 61240) loss: 1.712403\n",
      "(Iteration 21301 / 61240) loss: 1.360614\n",
      "(Iteration 21401 / 61240) loss: 1.386650\n",
      "(Epoch 14 / 40) train acc: 0.519000; val_acc: 0.509000\n",
      "(Iteration 21501 / 61240) loss: 1.277044\n",
      "(Iteration 21601 / 61240) loss: 1.350207\n",
      "(Iteration 21701 / 61240) loss: 1.450090\n",
      "(Iteration 21801 / 61240) loss: 1.424616\n",
      "(Iteration 21901 / 61240) loss: 1.360384\n",
      "(Iteration 22001 / 61240) loss: 1.580764\n",
      "(Iteration 22101 / 61240) loss: 1.433121\n",
      "(Iteration 22201 / 61240) loss: 1.615439\n",
      "(Iteration 22301 / 61240) loss: 1.589216\n",
      "(Iteration 22401 / 61240) loss: 1.704006\n",
      "(Iteration 22501 / 61240) loss: 1.448967\n",
      "(Iteration 22601 / 61240) loss: 1.626928\n",
      "(Iteration 22701 / 61240) loss: 1.385153\n",
      "(Iteration 22801 / 61240) loss: 1.468010\n",
      "(Iteration 22901 / 61240) loss: 1.508395\n",
      "(Epoch 15 / 40) train acc: 0.536000; val_acc: 0.511000\n",
      "(Iteration 23001 / 61240) loss: 1.365365\n",
      "(Iteration 23101 / 61240) loss: 1.132957\n",
      "(Iteration 23201 / 61240) loss: 1.740347\n",
      "(Iteration 23301 / 61240) loss: 1.620114\n",
      "(Iteration 23401 / 61240) loss: 1.662084\n",
      "(Iteration 23501 / 61240) loss: 1.419841\n",
      "(Iteration 23601 / 61240) loss: 1.247043\n",
      "(Iteration 23701 / 61240) loss: 1.492613\n",
      "(Iteration 23801 / 61240) loss: 1.467403\n",
      "(Iteration 23901 / 61240) loss: 1.487389\n",
      "(Iteration 24001 / 61240) loss: 1.854440\n",
      "(Iteration 24101 / 61240) loss: 1.421731\n",
      "(Iteration 24201 / 61240) loss: 1.504761\n",
      "(Iteration 24301 / 61240) loss: 1.700155\n",
      "(Iteration 24401 / 61240) loss: 1.334793\n",
      "(Epoch 16 / 40) train acc: 0.529000; val_acc: 0.516000\n",
      "(Iteration 24501 / 61240) loss: 1.302604\n",
      "(Iteration 24601 / 61240) loss: 1.653955\n",
      "(Iteration 24701 / 61240) loss: 1.534560\n",
      "(Iteration 24801 / 61240) loss: 1.177141\n",
      "(Iteration 24901 / 61240) loss: 1.567636\n",
      "(Iteration 25001 / 61240) loss: 1.582654\n",
      "(Iteration 25101 / 61240) loss: 1.606960\n",
      "(Iteration 25201 / 61240) loss: 1.432891\n",
      "(Iteration 25301 / 61240) loss: 1.100227\n",
      "(Iteration 25401 / 61240) loss: 1.420012\n",
      "(Iteration 25501 / 61240) loss: 1.242969\n",
      "(Iteration 25601 / 61240) loss: 1.536766\n",
      "(Iteration 25701 / 61240) loss: 1.540764\n",
      "(Iteration 25801 / 61240) loss: 1.301172\n",
      "(Iteration 25901 / 61240) loss: 1.311644\n",
      "(Iteration 26001 / 61240) loss: 1.483374\n",
      "(Epoch 17 / 40) train acc: 0.553000; val_acc: 0.518000\n",
      "(Iteration 26101 / 61240) loss: 1.260602\n",
      "(Iteration 26201 / 61240) loss: 1.370793\n",
      "(Iteration 26301 / 61240) loss: 1.246118\n",
      "(Iteration 26401 / 61240) loss: 1.736664\n",
      "(Iteration 26501 / 61240) loss: 1.348781\n",
      "(Iteration 26601 / 61240) loss: 1.760193\n",
      "(Iteration 26701 / 61240) loss: 1.423832\n",
      "(Iteration 26801 / 61240) loss: 1.268135\n",
      "(Iteration 26901 / 61240) loss: 1.531428\n",
      "(Iteration 27001 / 61240) loss: 1.224966\n",
      "(Iteration 27101 / 61240) loss: 1.448057\n",
      "(Iteration 27201 / 61240) loss: 1.735672\n",
      "(Iteration 27301 / 61240) loss: 1.501271\n",
      "(Iteration 27401 / 61240) loss: 1.539555\n",
      "(Iteration 27501 / 61240) loss: 1.308582\n",
      "(Epoch 18 / 40) train acc: 0.552000; val_acc: 0.517000\n",
      "(Iteration 27601 / 61240) loss: 1.503543\n",
      "(Iteration 27701 / 61240) loss: 1.441402\n",
      "(Iteration 27801 / 61240) loss: 1.122720\n",
      "(Iteration 27901 / 61240) loss: 1.475836\n",
      "(Iteration 28001 / 61240) loss: 1.446676\n",
      "(Iteration 28101 / 61240) loss: 1.483593\n",
      "(Iteration 28201 / 61240) loss: 1.313372\n",
      "(Iteration 28301 / 61240) loss: 1.503431\n",
      "(Iteration 28401 / 61240) loss: 1.545067\n",
      "(Iteration 28501 / 61240) loss: 1.433061\n",
      "(Iteration 28601 / 61240) loss: 1.263605\n",
      "(Iteration 28701 / 61240) loss: 1.501354\n",
      "(Iteration 28801 / 61240) loss: 1.532609\n",
      "(Iteration 28901 / 61240) loss: 1.382583\n",
      "(Iteration 29001 / 61240) loss: 1.735887\n",
      "(Epoch 19 / 40) train acc: 0.527000; val_acc: 0.518000\n",
      "(Iteration 29101 / 61240) loss: 1.655103\n",
      "(Iteration 29201 / 61240) loss: 1.211099\n",
      "(Iteration 29301 / 61240) loss: 1.868894\n",
      "(Iteration 29401 / 61240) loss: 1.472374\n",
      "(Iteration 29501 / 61240) loss: 1.344484\n",
      "(Iteration 29601 / 61240) loss: 1.252635\n",
      "(Iteration 29701 / 61240) loss: 1.369350\n",
      "(Iteration 29801 / 61240) loss: 1.402925\n",
      "(Iteration 29901 / 61240) loss: 1.514302\n",
      "(Iteration 30001 / 61240) loss: 1.550275\n",
      "(Iteration 30101 / 61240) loss: 1.604034\n",
      "(Iteration 30201 / 61240) loss: 1.345700\n",
      "(Iteration 30301 / 61240) loss: 1.527014\n",
      "(Iteration 30401 / 61240) loss: 1.576175\n",
      "(Iteration 30501 / 61240) loss: 1.368616\n",
      "(Iteration 30601 / 61240) loss: 1.057026\n",
      "(Epoch 20 / 40) train acc: 0.519000; val_acc: 0.518000\n",
      "(Iteration 30701 / 61240) loss: 1.777786\n",
      "(Iteration 30801 / 61240) loss: 1.199966\n",
      "(Iteration 30901 / 61240) loss: 1.325153\n",
      "(Iteration 31001 / 61240) loss: 1.741315\n",
      "(Iteration 31101 / 61240) loss: 1.557180\n",
      "(Iteration 31201 / 61240) loss: 1.329244\n",
      "(Iteration 31301 / 61240) loss: 1.316125\n",
      "(Iteration 31401 / 61240) loss: 1.386904\n",
      "(Iteration 31501 / 61240) loss: 1.418159\n",
      "(Iteration 31601 / 61240) loss: 1.755615\n",
      "(Iteration 31701 / 61240) loss: 1.562578\n",
      "(Iteration 31801 / 61240) loss: 1.567129\n",
      "(Iteration 31901 / 61240) loss: 1.693376\n",
      "(Iteration 32001 / 61240) loss: 1.312957\n",
      "(Iteration 32101 / 61240) loss: 1.654976\n",
      "(Epoch 21 / 40) train acc: 0.536000; val_acc: 0.518000\n",
      "(Iteration 32201 / 61240) loss: 1.684552\n",
      "(Iteration 32301 / 61240) loss: 1.596618\n",
      "(Iteration 32401 / 61240) loss: 1.324666\n",
      "(Iteration 32501 / 61240) loss: 1.341263\n",
      "(Iteration 32601 / 61240) loss: 1.421274\n",
      "(Iteration 32701 / 61240) loss: 1.229485\n",
      "(Iteration 32801 / 61240) loss: 1.409207\n",
      "(Iteration 32901 / 61240) loss: 1.607957\n",
      "(Iteration 33001 / 61240) loss: 1.406827\n",
      "(Iteration 33101 / 61240) loss: 1.114576\n",
      "(Iteration 33201 / 61240) loss: 1.408178\n",
      "(Iteration 33301 / 61240) loss: 1.361990\n",
      "(Iteration 33401 / 61240) loss: 1.204199\n",
      "(Iteration 33501 / 61240) loss: 1.268648\n",
      "(Iteration 33601 / 61240) loss: 0.994564\n",
      "(Epoch 22 / 40) train acc: 0.523000; val_acc: 0.517000\n",
      "(Iteration 33701 / 61240) loss: 1.195728\n",
      "(Iteration 33801 / 61240) loss: 1.283260\n",
      "(Iteration 33901 / 61240) loss: 1.926217\n",
      "(Iteration 34001 / 61240) loss: 1.372345\n",
      "(Iteration 34101 / 61240) loss: 1.558567\n",
      "(Iteration 34201 / 61240) loss: 1.461133\n",
      "(Iteration 34301 / 61240) loss: 1.656424\n",
      "(Iteration 34401 / 61240) loss: 1.400488\n",
      "(Iteration 34501 / 61240) loss: 1.636084\n",
      "(Iteration 34601 / 61240) loss: 1.397350\n",
      "(Iteration 34701 / 61240) loss: 1.745396\n",
      "(Iteration 34801 / 61240) loss: 1.679402\n",
      "(Iteration 34901 / 61240) loss: 1.388653\n",
      "(Iteration 35001 / 61240) loss: 1.557903\n",
      "(Iteration 35101 / 61240) loss: 1.512519\n",
      "(Iteration 35201 / 61240) loss: 1.261232\n",
      "(Epoch 23 / 40) train acc: 0.555000; val_acc: 0.518000\n",
      "(Iteration 35301 / 61240) loss: 1.368081\n",
      "(Iteration 35401 / 61240) loss: 1.310571\n",
      "(Iteration 35501 / 61240) loss: 1.241001\n",
      "(Iteration 35601 / 61240) loss: 1.655293\n",
      "(Iteration 35701 / 61240) loss: 1.320459\n",
      "(Iteration 35801 / 61240) loss: 1.325481\n",
      "(Iteration 35901 / 61240) loss: 1.451801\n",
      "(Iteration 36001 / 61240) loss: 1.464506\n",
      "(Iteration 36101 / 61240) loss: 1.377883\n",
      "(Iteration 36201 / 61240) loss: 1.388992\n",
      "(Iteration 36301 / 61240) loss: 1.410373\n",
      "(Iteration 36401 / 61240) loss: 1.175336\n",
      "(Iteration 36501 / 61240) loss: 1.838657\n",
      "(Iteration 36601 / 61240) loss: 1.167683\n",
      "(Iteration 36701 / 61240) loss: 1.408129\n",
      "(Epoch 24 / 40) train acc: 0.567000; val_acc: 0.516000\n",
      "(Iteration 36801 / 61240) loss: 1.273455\n",
      "(Iteration 36901 / 61240) loss: 1.419954\n",
      "(Iteration 37001 / 61240) loss: 1.416293\n",
      "(Iteration 37101 / 61240) loss: 1.287988\n",
      "(Iteration 37201 / 61240) loss: 1.468888\n",
      "(Iteration 37301 / 61240) loss: 1.277571\n",
      "(Iteration 37401 / 61240) loss: 1.367516\n",
      "(Iteration 37501 / 61240) loss: 1.490132\n",
      "(Iteration 37601 / 61240) loss: 1.179792\n",
      "(Iteration 37701 / 61240) loss: 1.461030\n",
      "(Iteration 37801 / 61240) loss: 1.873444\n",
      "(Iteration 37901 / 61240) loss: 1.846605\n",
      "(Iteration 38001 / 61240) loss: 1.215440\n",
      "(Iteration 38101 / 61240) loss: 1.166036\n",
      "(Iteration 38201 / 61240) loss: 1.454043\n",
      "(Epoch 25 / 40) train acc: 0.536000; val_acc: 0.518000\n",
      "(Iteration 38301 / 61240) loss: 1.626826\n",
      "(Iteration 38401 / 61240) loss: 1.273957\n",
      "(Iteration 38501 / 61240) loss: 1.592378\n",
      "(Iteration 38601 / 61240) loss: 1.464642\n",
      "(Iteration 38701 / 61240) loss: 1.753380\n",
      "(Iteration 38801 / 61240) loss: 1.601655\n",
      "(Iteration 38901 / 61240) loss: 1.827575\n",
      "(Iteration 39001 / 61240) loss: 1.249509\n",
      "(Iteration 39101 / 61240) loss: 1.526905\n",
      "(Iteration 39201 / 61240) loss: 1.558625\n",
      "(Iteration 39301 / 61240) loss: 1.384484\n",
      "(Iteration 39401 / 61240) loss: 1.649172\n",
      "(Iteration 39501 / 61240) loss: 1.413473\n",
      "(Iteration 39601 / 61240) loss: 1.707183\n",
      "(Iteration 39701 / 61240) loss: 1.635652\n",
      "(Iteration 39801 / 61240) loss: 1.155004\n",
      "(Epoch 26 / 40) train acc: 0.539000; val_acc: 0.520000\n",
      "(Iteration 39901 / 61240) loss: 1.453108\n",
      "(Iteration 40001 / 61240) loss: 1.496174\n",
      "(Iteration 40101 / 61240) loss: 1.412050\n",
      "(Iteration 40201 / 61240) loss: 1.376699\n",
      "(Iteration 40301 / 61240) loss: 1.485804\n",
      "(Iteration 40401 / 61240) loss: 1.344475\n",
      "(Iteration 40501 / 61240) loss: 1.307160\n",
      "(Iteration 40601 / 61240) loss: 1.543320\n",
      "(Iteration 40701 / 61240) loss: 1.246836\n",
      "(Iteration 40801 / 61240) loss: 1.613839\n",
      "(Iteration 40901 / 61240) loss: 1.307308\n",
      "(Iteration 41001 / 61240) loss: 1.142133\n",
      "(Iteration 41101 / 61240) loss: 1.346162\n",
      "(Iteration 41201 / 61240) loss: 1.332024\n",
      "(Iteration 41301 / 61240) loss: 1.381609\n",
      "(Epoch 27 / 40) train acc: 0.520000; val_acc: 0.520000\n",
      "(Iteration 41401 / 61240) loss: 1.222159\n",
      "(Iteration 41501 / 61240) loss: 1.537434\n",
      "(Iteration 41601 / 61240) loss: 1.323740\n",
      "(Iteration 41701 / 61240) loss: 1.776553\n",
      "(Iteration 41801 / 61240) loss: 1.449115\n",
      "(Iteration 41901 / 61240) loss: 1.373365\n",
      "(Iteration 42001 / 61240) loss: 1.433052\n",
      "(Iteration 42101 / 61240) loss: 1.647806\n",
      "(Iteration 42201 / 61240) loss: 1.661605\n",
      "(Iteration 42301 / 61240) loss: 1.654684\n",
      "(Iteration 42401 / 61240) loss: 1.485552\n",
      "(Iteration 42501 / 61240) loss: 1.461741\n",
      "(Iteration 42601 / 61240) loss: 1.713943\n",
      "(Iteration 42701 / 61240) loss: 1.533518\n",
      "(Iteration 42801 / 61240) loss: 1.250304\n",
      "(Epoch 28 / 40) train acc: 0.528000; val_acc: 0.516000\n",
      "(Iteration 42901 / 61240) loss: 1.385332\n",
      "(Iteration 43001 / 61240) loss: 1.216824\n",
      "(Iteration 43101 / 61240) loss: 1.349159\n",
      "(Iteration 43201 / 61240) loss: 1.388704\n",
      "(Iteration 43301 / 61240) loss: 1.469986\n",
      "(Iteration 43401 / 61240) loss: 1.557740\n",
      "(Iteration 43501 / 61240) loss: 1.398152\n",
      "(Iteration 43601 / 61240) loss: 0.889498\n",
      "(Iteration 43701 / 61240) loss: 1.711671\n",
      "(Iteration 43801 / 61240) loss: 1.218263\n",
      "(Iteration 43901 / 61240) loss: 1.453070\n",
      "(Iteration 44001 / 61240) loss: 1.451867\n",
      "(Iteration 44101 / 61240) loss: 1.799062\n",
      "(Iteration 44201 / 61240) loss: 1.428916\n",
      "(Iteration 44301 / 61240) loss: 1.389642\n",
      "(Epoch 29 / 40) train acc: 0.557000; val_acc: 0.519000\n",
      "(Iteration 44401 / 61240) loss: 1.621189\n",
      "(Iteration 44501 / 61240) loss: 1.435455\n",
      "(Iteration 44601 / 61240) loss: 1.319968\n",
      "(Iteration 44701 / 61240) loss: 1.118753\n",
      "(Iteration 44801 / 61240) loss: 1.494703\n",
      "(Iteration 44901 / 61240) loss: 1.403896\n",
      "(Iteration 45001 / 61240) loss: 1.253860\n",
      "(Iteration 45101 / 61240) loss: 1.307833\n",
      "(Iteration 45201 / 61240) loss: 1.635986\n",
      "(Iteration 45301 / 61240) loss: 1.515282\n",
      "(Iteration 45401 / 61240) loss: 1.263858\n",
      "(Iteration 45501 / 61240) loss: 1.689740\n",
      "(Iteration 45601 / 61240) loss: 1.516323\n",
      "(Iteration 45701 / 61240) loss: 1.148174\n",
      "(Iteration 45801 / 61240) loss: 1.572686\n",
      "(Iteration 45901 / 61240) loss: 1.384159\n",
      "(Epoch 30 / 40) train acc: 0.515000; val_acc: 0.517000\n",
      "(Iteration 46001 / 61240) loss: 1.530244\n",
      "(Iteration 46101 / 61240) loss: 1.314423\n",
      "(Iteration 46201 / 61240) loss: 1.463509\n",
      "(Iteration 46301 / 61240) loss: 1.508589\n",
      "(Iteration 46401 / 61240) loss: 1.541390\n",
      "(Iteration 46501 / 61240) loss: 1.749888\n",
      "(Iteration 46601 / 61240) loss: 1.283778\n",
      "(Iteration 46701 / 61240) loss: 1.520419\n",
      "(Iteration 46801 / 61240) loss: 1.192321\n",
      "(Iteration 46901 / 61240) loss: 1.502889\n",
      "(Iteration 47001 / 61240) loss: 1.594394\n",
      "(Iteration 47101 / 61240) loss: 1.485063\n",
      "(Iteration 47201 / 61240) loss: 1.538874\n",
      "(Iteration 47301 / 61240) loss: 1.518884\n",
      "(Iteration 47401 / 61240) loss: 1.655817\n",
      "(Epoch 31 / 40) train acc: 0.572000; val_acc: 0.519000\n",
      "(Iteration 47501 / 61240) loss: 1.425973\n",
      "(Iteration 47601 / 61240) loss: 1.577874\n",
      "(Iteration 47701 / 61240) loss: 1.261815\n",
      "(Iteration 47801 / 61240) loss: 1.495264\n",
      "(Iteration 47901 / 61240) loss: 1.409949\n",
      "(Iteration 48001 / 61240) loss: 1.525122\n",
      "(Iteration 48101 / 61240) loss: 1.382864\n",
      "(Iteration 48201 / 61240) loss: 1.337136\n",
      "(Iteration 48301 / 61240) loss: 1.718728\n",
      "(Iteration 48401 / 61240) loss: 1.704890\n",
      "(Iteration 48501 / 61240) loss: 1.552799\n",
      "(Iteration 48601 / 61240) loss: 1.508148\n",
      "(Iteration 48701 / 61240) loss: 1.704593\n",
      "(Iteration 48801 / 61240) loss: 1.471488\n",
      "(Iteration 48901 / 61240) loss: 1.352551\n",
      "(Epoch 32 / 40) train acc: 0.545000; val_acc: 0.518000\n",
      "(Iteration 49001 / 61240) loss: 1.201580\n",
      "(Iteration 49101 / 61240) loss: 1.579607\n",
      "(Iteration 49201 / 61240) loss: 1.398515\n",
      "(Iteration 49301 / 61240) loss: 1.572051\n",
      "(Iteration 49401 / 61240) loss: 1.209294\n",
      "(Iteration 49501 / 61240) loss: 1.222382\n",
      "(Iteration 49601 / 61240) loss: 1.284867\n",
      "(Iteration 49701 / 61240) loss: 1.361294\n",
      "(Iteration 49801 / 61240) loss: 1.151782\n",
      "(Iteration 49901 / 61240) loss: 1.442776\n",
      "(Iteration 50001 / 61240) loss: 1.598383\n",
      "(Iteration 50101 / 61240) loss: 1.231049\n",
      "(Iteration 50201 / 61240) loss: 1.415230\n",
      "(Iteration 50301 / 61240) loss: 1.249103\n",
      "(Iteration 50401 / 61240) loss: 1.569183\n",
      "(Iteration 50501 / 61240) loss: 1.648693\n",
      "(Epoch 33 / 40) train acc: 0.546000; val_acc: 0.520000\n",
      "(Iteration 50601 / 61240) loss: 1.484243\n",
      "(Iteration 50701 / 61240) loss: 1.330014\n",
      "(Iteration 50801 / 61240) loss: 1.421847\n",
      "(Iteration 50901 / 61240) loss: 1.262299\n",
      "(Iteration 51001 / 61240) loss: 1.206835\n",
      "(Iteration 51101 / 61240) loss: 1.516212\n",
      "(Iteration 51201 / 61240) loss: 1.375988\n",
      "(Iteration 51301 / 61240) loss: 1.569751\n",
      "(Iteration 51401 / 61240) loss: 1.288020\n",
      "(Iteration 51501 / 61240) loss: 1.425072\n",
      "(Iteration 51601 / 61240) loss: 1.428619\n",
      "(Iteration 51701 / 61240) loss: 1.446535\n",
      "(Iteration 51801 / 61240) loss: 1.613988\n",
      "(Iteration 51901 / 61240) loss: 1.695613\n",
      "(Iteration 52001 / 61240) loss: 1.567359\n",
      "(Epoch 34 / 40) train acc: 0.532000; val_acc: 0.519000\n",
      "(Iteration 52101 / 61240) loss: 1.043935\n",
      "(Iteration 52201 / 61240) loss: 1.153538\n",
      "(Iteration 52301 / 61240) loss: 1.508307\n",
      "(Iteration 52401 / 61240) loss: 1.378662\n",
      "(Iteration 52501 / 61240) loss: 1.329870\n",
      "(Iteration 52601 / 61240) loss: 1.408428\n",
      "(Iteration 52701 / 61240) loss: 1.211353\n",
      "(Iteration 52801 / 61240) loss: 1.329112\n",
      "(Iteration 52901 / 61240) loss: 1.315429\n",
      "(Iteration 53001 / 61240) loss: 1.671481\n",
      "(Iteration 53101 / 61240) loss: 1.494152\n",
      "(Iteration 53201 / 61240) loss: 1.065365\n",
      "(Iteration 53301 / 61240) loss: 1.347349\n",
      "(Iteration 53401 / 61240) loss: 1.343995\n",
      "(Iteration 53501 / 61240) loss: 1.559631\n",
      "(Epoch 35 / 40) train acc: 0.528000; val_acc: 0.518000\n",
      "(Iteration 53601 / 61240) loss: 1.497623\n",
      "(Iteration 53701 / 61240) loss: 1.465914\n",
      "(Iteration 53801 / 61240) loss: 1.679150\n",
      "(Iteration 53901 / 61240) loss: 1.372835\n",
      "(Iteration 54001 / 61240) loss: 1.527470\n",
      "(Iteration 54101 / 61240) loss: 1.562790\n",
      "(Iteration 54201 / 61240) loss: 1.240776\n",
      "(Iteration 54301 / 61240) loss: 1.403661\n",
      "(Iteration 54401 / 61240) loss: 1.199426\n",
      "(Iteration 54501 / 61240) loss: 1.229780\n",
      "(Iteration 54601 / 61240) loss: 1.572491\n",
      "(Iteration 54701 / 61240) loss: 1.841101\n",
      "(Iteration 54801 / 61240) loss: 1.680743\n",
      "(Iteration 54901 / 61240) loss: 1.423385\n",
      "(Iteration 55001 / 61240) loss: 1.097440\n",
      "(Iteration 55101 / 61240) loss: 1.336726\n",
      "(Epoch 36 / 40) train acc: 0.557000; val_acc: 0.519000\n",
      "(Iteration 55201 / 61240) loss: 1.454871\n",
      "(Iteration 55301 / 61240) loss: 1.054132\n",
      "(Iteration 55401 / 61240) loss: 1.469239\n",
      "(Iteration 55501 / 61240) loss: 1.518784\n",
      "(Iteration 55601 / 61240) loss: 1.344987\n",
      "(Iteration 55701 / 61240) loss: 1.312581\n",
      "(Iteration 55801 / 61240) loss: 1.564587\n",
      "(Iteration 55901 / 61240) loss: 1.247282\n",
      "(Iteration 56001 / 61240) loss: 1.375031\n",
      "(Iteration 56101 / 61240) loss: 1.340651\n",
      "(Iteration 56201 / 61240) loss: 1.500555\n",
      "(Iteration 56301 / 61240) loss: 1.499394\n",
      "(Iteration 56401 / 61240) loss: 1.438606\n",
      "(Iteration 56501 / 61240) loss: 1.408672\n",
      "(Iteration 56601 / 61240) loss: 1.274508\n",
      "(Epoch 37 / 40) train acc: 0.502000; val_acc: 0.521000\n",
      "(Iteration 56701 / 61240) loss: 1.498725\n",
      "(Iteration 56801 / 61240) loss: 1.244466\n",
      "(Iteration 56901 / 61240) loss: 1.293696\n",
      "(Iteration 57001 / 61240) loss: 1.549813\n",
      "(Iteration 57101 / 61240) loss: 1.676880\n",
      "(Iteration 57201 / 61240) loss: 1.274027\n",
      "(Iteration 57301 / 61240) loss: 1.507098\n",
      "(Iteration 57401 / 61240) loss: 1.316303\n",
      "(Iteration 57501 / 61240) loss: 1.394199\n",
      "(Iteration 57601 / 61240) loss: 1.213064\n",
      "(Iteration 57701 / 61240) loss: 1.573719\n",
      "(Iteration 57801 / 61240) loss: 1.439199\n",
      "(Iteration 57901 / 61240) loss: 1.573339\n",
      "(Iteration 58001 / 61240) loss: 1.566600\n",
      "(Iteration 58101 / 61240) loss: 1.617162\n",
      "(Epoch 38 / 40) train acc: 0.527000; val_acc: 0.520000\n",
      "(Iteration 58201 / 61240) loss: 1.346129\n",
      "(Iteration 58301 / 61240) loss: 1.249409\n",
      "(Iteration 58401 / 61240) loss: 1.487851\n",
      "(Iteration 58501 / 61240) loss: 1.661972\n",
      "(Iteration 58601 / 61240) loss: 1.382506\n",
      "(Iteration 58701 / 61240) loss: 1.652661\n",
      "(Iteration 58801 / 61240) loss: 1.178671\n",
      "(Iteration 58901 / 61240) loss: 1.231741\n",
      "(Iteration 59001 / 61240) loss: 1.383070\n",
      "(Iteration 59101 / 61240) loss: 1.331910\n",
      "(Iteration 59201 / 61240) loss: 1.445655\n",
      "(Iteration 59301 / 61240) loss: 1.356824\n",
      "(Iteration 59401 / 61240) loss: 1.421844\n",
      "(Iteration 59501 / 61240) loss: 1.318169\n",
      "(Iteration 59601 / 61240) loss: 1.522275\n",
      "(Iteration 59701 / 61240) loss: 1.048513\n",
      "(Epoch 39 / 40) train acc: 0.532000; val_acc: 0.524000\n",
      "(Iteration 59801 / 61240) loss: 1.567199\n",
      "(Iteration 59901 / 61240) loss: 1.439506\n",
      "(Iteration 60001 / 61240) loss: 1.609951\n",
      "(Iteration 60101 / 61240) loss: 1.292772\n",
      "(Iteration 60201 / 61240) loss: 1.248025\n",
      "(Iteration 60301 / 61240) loss: 1.517692\n",
      "(Iteration 60401 / 61240) loss: 1.247598\n",
      "(Iteration 60501 / 61240) loss: 1.661644\n",
      "(Iteration 60601 / 61240) loss: 1.584925\n",
      "(Iteration 60701 / 61240) loss: 1.474546\n",
      "(Iteration 60801 / 61240) loss: 1.454450\n",
      "(Iteration 60901 / 61240) loss: 1.595943\n",
      "(Iteration 61001 / 61240) loss: 1.397750\n",
      "(Iteration 61101 / 61240) loss: 1.185621\n",
      "(Iteration 61201 / 61240) loss: 1.381995\n",
      "(Epoch 40 / 40) train acc: 0.545000; val_acc: 0.522000\n",
      "Best validation accuracy: 0.4988\n",
      "Best hyperparameters: {'hidden_size': 700, 'learning_rate': 0.01, 'num_epochs': 40, 'reg': 0.01, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "from cs231n.classifiers.fc_net import TwoLayerNet\n",
    "from cs231n.solver import Solver\n",
    "import itertools\n",
    "\n",
    "data = {\n",
    "    'X_train': X_train_feats, \n",
    "    'y_train': y_train, \n",
    "    'X_val': X_val_feats, \n",
    "    'y_val': y_val, \n",
    "    'X_test': X_test_feats, \n",
    "    'y_test': y_test, \n",
    "}\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TODO: Train a two-layer neural network on image features. You may want to    #\n",
    "# cross-validate various parameters as in previous sections. Store your best   #\n",
    "# model in the best_net variable.                                              #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "#################################################################################\n",
    "### FUNCTION THAT PERFORMS THE TRAINING BASED ON PARAMETERS\n",
    "\n",
    "def train_ann(data, hidden_size, learning_rate, num_epochs, reg, batch_size):\n",
    "\n",
    "    input_size = data['X_train'].shape[1]\n",
    "    num_classes = 10\n",
    "    \n",
    "    model = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "    model.reg = reg\n",
    "\n",
    "    solver = Solver(model, \n",
    "                    data,\n",
    "                    update_rule='sgd',\n",
    "                    optim_config={\n",
    "                        'learning_rate': learning_rate,\n",
    "                    },\n",
    "                    lr_decay=0.9,\n",
    "                    num_epochs=num_epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    print_every=100)\n",
    "\n",
    "    solver.train()   \n",
    "\n",
    "    return solver, model\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "best_val_accuracy = -1\n",
    "results = {}\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_size': [ 400, 700, 800], \n",
    "    'learning_rate': [1e-3, 1e-2],  \n",
    "    'num_epochs': [20, 30, 40],  \n",
    "    'reg': [0.1, 0.01],  \n",
    "    #'lr_decay': [0.9, 0.95],  \n",
    "    'batch_size': [64, 32]  \n",
    "}\n",
    "\n",
    "# Grid search with itertools.product\n",
    "keys, values = zip(*param_grid.items())\n",
    "\n",
    "for v in itertools.product(*values):\n",
    "    params = dict(zip(keys, v))\n",
    "\n",
    "    print(f\"Training with parameters: {params}\")\n",
    "\n",
    "    solver, model = train_ann(data, **params)\n",
    "\n",
    "    # Update dictionary (train, val)\n",
    "    results[tuple(params.items())] = (np.mean(solver.train_acc_history), np.mean(solver.val_acc_history))\n",
    "\n",
    "    # Track the best model\n",
    "    if np.mean(solver.val_acc_history) > best_val_accuracy:\n",
    "        best_val_accuracy = np.mean(solver.val_acc_history)\n",
    "        best_model = model\n",
    "        best_solver = solver\n",
    "        best_params = params  # Save the best hyperparameters\n",
    "        print(f\"New best model found with validation accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "# Summary of best results\n",
    "print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "441fd5c2",
   "metadata": {
    "test": "nn_test_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.521\n"
     ]
    }
   ],
   "source": [
    "# Run your best neural net classifier on the test set. You should be able\n",
    "# to get more than 55% accuracy.\n",
    "\n",
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "test_acc = (y_test_pred == data['y_test']).mean()\n",
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
