{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060c71d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Google Colab\n",
      "/Users/mateo/Library/CloudStorage/OneDrive-MusitelliFilm&Digital/04 - FING/DLVIS/E1/assignment1/cs231n/datasets\n",
      "/Users/mateo/Library/CloudStorage/OneDrive-MusitelliFilm&Digital/04 - FING/DLVIS/E1/assignment1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Check if running on Google Colab\n",
    "# If so, mount Google Drive and download CIFAR-10 dataset\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # This mounts your Google Drive to the Colab VM.\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "    # assignment folder, e.g. 'cs231n/assignments/assignment1/'\n",
    "    FOLDERNAME = None\n",
    "    assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "    # Now that we've mounted your Drive, this ensures that\n",
    "    # the Python interpreter of the Colab VM can load\n",
    "    # python files from within it.\n",
    "    sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "    # This downloads the CIFAR-10 dataset to your Drive\n",
    "    # if it doesn't already exist.\n",
    "    %cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "    !bash get_datasets.sh\n",
    "    %cd /content/drive/My\\ Drive/$FOLDERNAME\n",
    "# END OF COLAB SETUP\n",
    "\n",
    "# If not running on Google Colab, download CIFAR-10 dataset locally\n",
    "else:\n",
    "    print(\"Not running on Google Colab\")\n",
    "    %cd ./cs231n/datasets/\n",
    "    !bash get_datasets.sh\n",
    "    %cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56528d9",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Image features exercise\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](https://eva.fing.edu.uy/mod/assign/view.php?id=194303) on the course website.*\n",
    "\n",
    "We have seen that we can achieve reasonable performance on an image classification task by training a linear classifier on the pixels of the input image. In this exercise we will show that we can improve our classification performance by training linear classifiers not on raw pixels but on features that are computed from the raw pixels.\n",
    "\n",
    "All of your work for this exercise will be done in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ed3d6f7",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9de2e",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Load data\n",
    "Similar to previous exercises, we will load CIFAR-10 data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "498a936a",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "from cs231n.features import color_histogram_hsv, hog_feature\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "\n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b699165d",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Extract Features\n",
    "For each image we will compute a Histogram of Oriented\n",
    "Gradients (HOG) as well as a color histogram using the hue channel in HSV\n",
    "color space. We form our final feature vector for each image by concatenating\n",
    "the HOG and color histogram feature vectors.\n",
    "\n",
    "Roughly speaking, HOG should capture the texture of the image while ignoring\n",
    "color information, and the color histogram represents the color of the input\n",
    "image while ignoring texture. As a result, we expect that using both together\n",
    "ought to work better than using either alone. Verifying this assumption would\n",
    "be a good thing to try for your own interest.\n",
    "\n",
    "The `hog_feature` and `color_histogram_hsv` functions both operate on a single\n",
    "image and return a feature vector for that image. The `extract_features`\n",
    "function takes a set of images and a list of feature functions and evaluates\n",
    "each feature function on each image, storing the results in a matrix where\n",
    "each column is the concatenation of all feature vectors for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1877e14",
   "metadata": {
    "scrolled": true,
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting features for 1000 / 49000 images\n",
      "Done extracting features for 2000 / 49000 images\n",
      "Done extracting features for 3000 / 49000 images\n",
      "Done extracting features for 4000 / 49000 images\n",
      "Done extracting features for 5000 / 49000 images\n",
      "Done extracting features for 6000 / 49000 images\n",
      "Done extracting features for 7000 / 49000 images\n",
      "Done extracting features for 8000 / 49000 images\n",
      "Done extracting features for 9000 / 49000 images\n",
      "Done extracting features for 10000 / 49000 images\n",
      "Done extracting features for 11000 / 49000 images\n",
      "Done extracting features for 12000 / 49000 images\n",
      "Done extracting features for 13000 / 49000 images\n",
      "Done extracting features for 14000 / 49000 images\n",
      "Done extracting features for 15000 / 49000 images\n",
      "Done extracting features for 16000 / 49000 images\n",
      "Done extracting features for 17000 / 49000 images\n",
      "Done extracting features for 18000 / 49000 images\n",
      "Done extracting features for 19000 / 49000 images\n",
      "Done extracting features for 20000 / 49000 images\n",
      "Done extracting features for 21000 / 49000 images\n",
      "Done extracting features for 22000 / 49000 images\n",
      "Done extracting features for 23000 / 49000 images\n",
      "Done extracting features for 24000 / 49000 images\n",
      "Done extracting features for 25000 / 49000 images\n",
      "Done extracting features for 26000 / 49000 images\n",
      "Done extracting features for 27000 / 49000 images\n",
      "Done extracting features for 28000 / 49000 images\n",
      "Done extracting features for 29000 / 49000 images\n",
      "Done extracting features for 30000 / 49000 images\n",
      "Done extracting features for 31000 / 49000 images\n",
      "Done extracting features for 32000 / 49000 images\n",
      "Done extracting features for 33000 / 49000 images\n",
      "Done extracting features for 34000 / 49000 images\n",
      "Done extracting features for 35000 / 49000 images\n",
      "Done extracting features for 36000 / 49000 images\n",
      "Done extracting features for 37000 / 49000 images\n",
      "Done extracting features for 38000 / 49000 images\n",
      "Done extracting features for 39000 / 49000 images\n",
      "Done extracting features for 40000 / 49000 images\n",
      "Done extracting features for 41000 / 49000 images\n",
      "Done extracting features for 42000 / 49000 images\n",
      "Done extracting features for 43000 / 49000 images\n",
      "Done extracting features for 44000 / 49000 images\n",
      "Done extracting features for 45000 / 49000 images\n",
      "Done extracting features for 46000 / 49000 images\n",
      "Done extracting features for 47000 / 49000 images\n",
      "Done extracting features for 48000 / 49000 images\n",
      "Done extracting features for 49000 / 49000 images\n"
     ]
    }
   ],
   "source": [
    "from cs231n.features import *\n",
    "\n",
    "num_color_bins = 10 # Number of bins in the color histogram\n",
    "feature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n",
    "X_train_feats = extract_features(X_train, feature_fns, verbose=True)\n",
    "X_val_feats = extract_features(X_val, feature_fns)\n",
    "X_test_feats = extract_features(X_test, feature_fns)\n",
    "\n",
    "# Preprocessing: Subtract the mean feature\n",
    "mean_feat = np.mean(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats -= mean_feat\n",
    "X_val_feats -= mean_feat\n",
    "X_test_feats -= mean_feat\n",
    "\n",
    "# Preprocessing: Divide by standard deviation. This ensures that each feature\n",
    "# has roughly the same scale.\n",
    "std_feat = np.std(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats /= std_feat\n",
    "X_val_feats /= std_feat\n",
    "X_test_feats /= std_feat\n",
    "\n",
    "# Preprocessing: Add a bias dimension\n",
    "X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\n",
    "X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])\n",
    "X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a07aac",
   "metadata": {},
   "source": [
    "## Train Softmax on features\n",
    "Using the multiclass Softmax code developed earlier in the assignment, train Softmax on top of the features extracted above; this should achieve better results than training Softmax directly on top of raw pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96f29a59",
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateo/Library/CloudStorage/OneDrive-MusitelliFilm&Digital/04 - FING/DLVIS/E1/assignment1/cs231n/classifiers/softmax.py:94: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.mean(np.log(np.e**s_y / np.sum(np.e**s))) + reg * np.sum(W**2)\n",
      "/Users/mateo/Library/CloudStorage/OneDrive-MusitelliFilm&Digital/04 - FING/DLVIS/E1/assignment1/cs231n/classifiers/softmax.py:99: RuntimeWarning: invalid value encountered in divide\n",
      "  P = np.e ** s / np.sum(np.e ** s, axis = 1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.000000e-12 reg 1.000000e+01 train accuracy: 0.084020 val accuracy: 0.091000\n",
      "lr 1.000000e-12 reg 5.000000e+01 train accuracy: 0.086082 val accuracy: 0.095000\n",
      "lr 1.000000e-12 reg 1.000000e+02 train accuracy: 0.097959 val accuracy: 0.106000\n",
      "lr 1.000000e-12 reg 2.000000e+02 train accuracy: 0.095367 val accuracy: 0.113000\n",
      "lr 1.000000e-12 reg 1.000000e+03 train accuracy: 0.105469 val accuracy: 0.111000\n",
      "lr 1.000000e-12 reg 5.000000e+03 train accuracy: 0.097388 val accuracy: 0.087000\n",
      "lr 1.000000e-12 reg 2.500000e+04 train accuracy: 0.100918 val accuracy: 0.097000\n",
      "lr 1.000000e-12 reg 5.000000e+04 train accuracy: 0.115306 val accuracy: 0.108000\n",
      "lr 1.000000e-12 reg 1.000000e+06 train accuracy: 0.100429 val accuracy: 0.100000\n",
      "lr 1.000000e-12 reg 2.000000e+06 train accuracy: 0.090755 val accuracy: 0.073000\n",
      "lr 5.000000e-12 reg 1.000000e+01 train accuracy: 0.103388 val accuracy: 0.118000\n",
      "lr 5.000000e-12 reg 5.000000e+01 train accuracy: 0.130633 val accuracy: 0.124000\n",
      "lr 5.000000e-12 reg 1.000000e+02 train accuracy: 0.095347 val accuracy: 0.087000\n",
      "lr 5.000000e-12 reg 2.000000e+02 train accuracy: 0.109163 val accuracy: 0.111000\n",
      "lr 5.000000e-12 reg 1.000000e+03 train accuracy: 0.106265 val accuracy: 0.120000\n",
      "lr 5.000000e-12 reg 5.000000e+03 train accuracy: 0.080265 val accuracy: 0.080000\n",
      "lr 5.000000e-12 reg 2.500000e+04 train accuracy: 0.098286 val accuracy: 0.096000\n",
      "lr 5.000000e-12 reg 5.000000e+04 train accuracy: 0.096571 val accuracy: 0.096000\n",
      "lr 5.000000e-12 reg 1.000000e+06 train accuracy: 0.089306 val accuracy: 0.093000\n",
      "lr 5.000000e-12 reg 2.000000e+06 train accuracy: 0.120020 val accuracy: 0.128000\n",
      "lr 1.000000e-10 reg 1.000000e+01 train accuracy: 0.092061 val accuracy: 0.094000\n",
      "lr 1.000000e-10 reg 5.000000e+01 train accuracy: 0.117939 val accuracy: 0.125000\n",
      "lr 1.000000e-10 reg 1.000000e+02 train accuracy: 0.088959 val accuracy: 0.091000\n",
      "lr 1.000000e-10 reg 2.000000e+02 train accuracy: 0.099163 val accuracy: 0.106000\n",
      "lr 1.000000e-10 reg 1.000000e+03 train accuracy: 0.102735 val accuracy: 0.106000\n",
      "lr 1.000000e-10 reg 5.000000e+03 train accuracy: 0.090571 val accuracy: 0.099000\n",
      "lr 1.000000e-10 reg 2.500000e+04 train accuracy: 0.096347 val accuracy: 0.103000\n",
      "lr 1.000000e-10 reg 5.000000e+04 train accuracy: 0.102612 val accuracy: 0.096000\n",
      "lr 1.000000e-10 reg 1.000000e+06 train accuracy: 0.075061 val accuracy: 0.070000\n",
      "lr 1.000000e-10 reg 2.000000e+06 train accuracy: 0.095306 val accuracy: 0.086000\n",
      "lr 5.000000e-10 reg 1.000000e+01 train accuracy: 0.096878 val accuracy: 0.096000\n",
      "lr 5.000000e-10 reg 5.000000e+01 train accuracy: 0.097653 val accuracy: 0.087000\n",
      "lr 5.000000e-10 reg 1.000000e+02 train accuracy: 0.091939 val accuracy: 0.095000\n",
      "lr 5.000000e-10 reg 2.000000e+02 train accuracy: 0.086633 val accuracy: 0.076000\n",
      "lr 5.000000e-10 reg 1.000000e+03 train accuracy: 0.080204 val accuracy: 0.083000\n",
      "lr 5.000000e-10 reg 5.000000e+03 train accuracy: 0.078449 val accuracy: 0.082000\n",
      "lr 5.000000e-10 reg 2.500000e+04 train accuracy: 0.098041 val accuracy: 0.081000\n",
      "lr 5.000000e-10 reg 5.000000e+04 train accuracy: 0.115673 val accuracy: 0.109000\n",
      "lr 5.000000e-10 reg 1.000000e+06 train accuracy: 0.096796 val accuracy: 0.102000\n",
      "lr 5.000000e-10 reg 2.000000e+06 train accuracy: 0.105327 val accuracy: 0.104000\n",
      "lr 1.000000e-09 reg 1.000000e+01 train accuracy: 0.106837 val accuracy: 0.092000\n",
      "lr 1.000000e-09 reg 5.000000e+01 train accuracy: 0.093694 val accuracy: 0.088000\n",
      "lr 1.000000e-09 reg 1.000000e+02 train accuracy: 0.093694 val accuracy: 0.112000\n",
      "lr 1.000000e-09 reg 2.000000e+02 train accuracy: 0.080265 val accuracy: 0.079000\n",
      "lr 1.000000e-09 reg 1.000000e+03 train accuracy: 0.114490 val accuracy: 0.112000\n",
      "lr 1.000000e-09 reg 5.000000e+03 train accuracy: 0.109776 val accuracy: 0.106000\n",
      "lr 1.000000e-09 reg 2.500000e+04 train accuracy: 0.131837 val accuracy: 0.138000\n",
      "lr 1.000000e-09 reg 5.000000e+04 train accuracy: 0.103837 val accuracy: 0.098000\n",
      "lr 1.000000e-09 reg 1.000000e+06 train accuracy: 0.101000 val accuracy: 0.095000\n",
      "lr 1.000000e-09 reg 2.000000e+06 train accuracy: 0.118469 val accuracy: 0.107000\n",
      "lr 5.000000e-09 reg 1.000000e+01 train accuracy: 0.102347 val accuracy: 0.105000\n",
      "lr 5.000000e-09 reg 5.000000e+01 train accuracy: 0.119020 val accuracy: 0.118000\n",
      "lr 5.000000e-09 reg 1.000000e+02 train accuracy: 0.082408 val accuracy: 0.072000\n",
      "lr 5.000000e-09 reg 2.000000e+02 train accuracy: 0.105612 val accuracy: 0.100000\n",
      "lr 5.000000e-09 reg 1.000000e+03 train accuracy: 0.111939 val accuracy: 0.117000\n",
      "lr 5.000000e-09 reg 5.000000e+03 train accuracy: 0.091020 val accuracy: 0.093000\n",
      "lr 5.000000e-09 reg 2.500000e+04 train accuracy: 0.111163 val accuracy: 0.106000\n",
      "lr 5.000000e-09 reg 5.000000e+04 train accuracy: 0.076633 val accuracy: 0.076000\n",
      "lr 5.000000e-09 reg 1.000000e+06 train accuracy: 0.417082 val accuracy: 0.419000\n",
      "lr 5.000000e-09 reg 2.000000e+06 train accuracy: 0.412122 val accuracy: 0.413000\n",
      "lr 1.000000e-07 reg 1.000000e+01 train accuracy: 0.111469 val accuracy: 0.116000\n",
      "lr 1.000000e-07 reg 5.000000e+01 train accuracy: 0.084490 val accuracy: 0.087000\n",
      "lr 1.000000e-07 reg 1.000000e+02 train accuracy: 0.107327 val accuracy: 0.105000\n",
      "lr 1.000000e-07 reg 2.000000e+02 train accuracy: 0.099367 val accuracy: 0.092000\n",
      "lr 1.000000e-07 reg 1.000000e+03 train accuracy: 0.067776 val accuracy: 0.086000\n",
      "lr 1.000000e-07 reg 5.000000e+03 train accuracy: 0.095000 val accuracy: 0.092000\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.348918 val accuracy: 0.343000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.417633 val accuracy: 0.416000\n",
      "lr 1.000000e-07 reg 1.000000e+06 train accuracy: 0.398327 val accuracy: 0.399000\n",
      "lr 1.000000e-07 reg 2.000000e+06 train accuracy: 0.385122 val accuracy: 0.375000\n",
      "lr 5.000000e-07 reg 1.000000e+01 train accuracy: 0.146755 val accuracy: 0.146000\n",
      "lr 5.000000e-07 reg 5.000000e+01 train accuracy: 0.112653 val accuracy: 0.114000\n",
      "lr 5.000000e-07 reg 1.000000e+02 train accuracy: 0.107000 val accuracy: 0.108000\n",
      "lr 5.000000e-07 reg 2.000000e+02 train accuracy: 0.114694 val accuracy: 0.117000\n",
      "lr 5.000000e-07 reg 1.000000e+03 train accuracy: 0.119122 val accuracy: 0.132000\n",
      "lr 5.000000e-07 reg 5.000000e+03 train accuracy: 0.413653 val accuracy: 0.412000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.415633 val accuracy: 0.422000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.406143 val accuracy: 0.407000\n",
      "lr 5.000000e-07 reg 1.000000e+06 train accuracy: 0.314020 val accuracy: 0.318000\n",
      "lr 5.000000e-07 reg 2.000000e+06 train accuracy: 0.096837 val accuracy: 0.110000\n",
      "lr 1.000000e-05 reg 1.000000e+01 train accuracy: 0.262878 val accuracy: 0.277000\n",
      "lr 1.000000e-05 reg 5.000000e+01 train accuracy: 0.345163 val accuracy: 0.330000\n",
      "lr 1.000000e-05 reg 1.000000e+02 train accuracy: 0.398224 val accuracy: 0.379000\n",
      "lr 1.000000e-05 reg 2.000000e+02 train accuracy: 0.414980 val accuracy: 0.417000\n",
      "lr 1.000000e-05 reg 1.000000e+03 train accuracy: 0.410000 val accuracy: 0.411000\n",
      "lr 1.000000e-05 reg 5.000000e+03 train accuracy: 0.407959 val accuracy: 0.396000\n",
      "lr 1.000000e-05 reg 2.500000e+04 train accuracy: 0.358245 val accuracy: 0.367000\n",
      "lr 1.000000e-05 reg 5.000000e+04 train accuracy: 0.323429 val accuracy: 0.334000\n",
      "lr 1.000000e-05 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 2.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 1.000000e+01 train accuracy: 0.411122 val accuracy: 0.413000\n",
      "lr 5.000000e-05 reg 5.000000e+01 train accuracy: 0.415755 val accuracy: 0.422000\n",
      "lr 5.000000e-05 reg 1.000000e+02 train accuracy: 0.415469 val accuracy: 0.412000\n",
      "lr 5.000000e-05 reg 2.000000e+02 train accuracy: 0.418204 val accuracy: 0.418000\n",
      "lr 5.000000e-05 reg 1.000000e+03 train accuracy: 0.409755 val accuracy: 0.410000\n",
      "lr 5.000000e-05 reg 5.000000e+03 train accuracy: 0.373306 val accuracy: 0.392000\n",
      "lr 5.000000e-05 reg 2.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 2.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 1.000000e+01 train accuracy: 0.413918 val accuracy: 0.423000\n",
      "lr 1.000000e-03 reg 5.000000e+01 train accuracy: 0.407163 val accuracy: 0.400000\n",
      "lr 1.000000e-03 reg 1.000000e+02 train accuracy: 0.398592 val accuracy: 0.402000\n",
      "lr 1.000000e-03 reg 2.000000e+02 train accuracy: 0.399265 val accuracy: 0.390000\n",
      "lr 1.000000e-03 reg 1.000000e+03 train accuracy: 0.130612 val accuracy: 0.120000\n",
      "lr 1.000000e-03 reg 5.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 2.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 2.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 1.000000e+01 train accuracy: 0.407408 val accuracy: 0.391000\n",
      "lr 5.000000e-03 reg 5.000000e+01 train accuracy: 0.380388 val accuracy: 0.375000\n",
      "lr 5.000000e-03 reg 1.000000e+02 train accuracy: 0.329286 val accuracy: 0.335000\n",
      "lr 5.000000e-03 reg 2.000000e+02 train accuracy: 0.052694 val accuracy: 0.055000\n",
      "lr 5.000000e-03 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 5.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 2.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 2.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved: 0.423000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune the learning rate and regularization strength\n",
    "\n",
    "from cs231n.classifiers.linear_classifier import Softmax\n",
    "\n",
    "learning_rates = [1e-3, 5e-3, 1e-5, 5e-5, 1e-7, 5e-7, 1e-9, 5e-9, 1e-10, 5e-10, 1e-12, 5e-12]\n",
    "regularization_strengths = [10, 50, 1e2, 2e2, 1e3, 5e3, 2.5e4, 5e4, 1e6, 2e6]\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the Softmax;     #\n",
    "# save the best trained classifer in best_softmax. You might also want to play #\n",
    "# with different numbers of bins in the color histogram. If you are careful    #\n",
    "# you should be able to get accuracy of near 0.42 on the validation set.       #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# All-for-all GridSearch\n",
    "for lr in learning_rates:\n",
    "    for rs in regularization_strengths:\n",
    "\n",
    "        # Initialize classifier\n",
    "        softmax = Softmax()\n",
    "        # Train\n",
    "        loss_hist = softmax.train(X_train_feats, y_train, learning_rate=lr, reg=rs, num_iters=1500, verbose=False)\n",
    "        # Evaluate on train\n",
    "        y_train_pred = softmax.predict(X_train_feats)\n",
    "        # Evaluate on validation\n",
    "        y_val_pred = softmax.predict(X_val_feats)\n",
    "        \n",
    "        # Accuracies\n",
    "        train_acc = np.mean(y_train == y_train_pred)\n",
    "        val_acc = np.mean(y_val == y_val_pred)\n",
    "\n",
    "        # Update dictionary (train, val)\n",
    "        results[(lr,rs)] = (train_acc, val_acc)\n",
    "\n",
    "        # Compare with previous parameters\n",
    "        if val_acc > best_val:\n",
    "            # If accuracy increases, update the best result and the classifier\n",
    "            best_val = val_acc\n",
    "            best_softmax = softmax\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfb068d2",
   "metadata": {
    "test": "svm_test_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.422\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your trained Softmax on the test set: you should be able to get at least 0.40\n",
    "y_test_pred = best_softmax.predict(X_test_feats)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66ab4f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 80 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# An important way to gain intuition about how an algorithm works is to\n",
    "# visualize the mistakes that it makes. In this visualization, we show examples\n",
    "# of images that are misclassified by our current system. The first column\n",
    "# shows images that our system labeled as \"plane\" but whose true label is\n",
    "# something other than \"plane\".\n",
    "\n",
    "examples_per_class = 8\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for cls, cls_name in enumerate(classes):\n",
    "    idxs = np.where((y_test != cls) & (y_test_pred == cls))[0]\n",
    "    idxs = np.random.choice(idxs, examples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)\n",
    "        plt.imshow(X_test[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cde5e7",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "### Inline question 1:\n",
    "Describe the misclassification results that you see. Do they make sense?\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "Errors make a lot of sense!\n",
    "\n",
    "The model pays special attention to pixel values and images showing a vast majority of pixel values that match, will be classified as belonging to the same class.\n",
    "\n",
    "For instance, fifth image on the ship column (starting from 0), displays a plane surrounding by blue... Blue is the color of the sea, which makes this image to be classified as a ship given its blue background."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59b590",
   "metadata": {},
   "source": [
    "## Neural Network on image features\n",
    "Earlier in this assigment we saw that training a two-layer neural network on raw pixels achieved better classification performance than linear classifiers on raw pixels. In this notebook we have seen that linear classifiers on image features outperform linear classifiers on raw pixels. \n",
    "\n",
    "For completeness, we should also try training a neural network on image features. This approach should outperform all previous approaches: you should easily be able to achieve over 55% classification accuracy on the test set; our best model achieves about 60% classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b1ebd7d",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 155)\n",
      "(49000, 154)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: Remove the bias dimension\n",
    "# Make sure to run this cell only ONCE\n",
    "print(X_train_feats.shape)\n",
    "X_train_feats = X_train_feats[:, :-1]\n",
    "X_val_feats = X_val_feats[:, :-1]\n",
    "X_test_feats = X_test_feats[:, :-1]\n",
    "\n",
    "print(X_train_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fbe3367",
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.304597\n",
      "(Epoch 0 / 80) train acc: 0.109000; val_acc: 0.129000\n",
      "(Iteration 101 / 61200) loss: 2.304595\n",
      "(Iteration 201 / 61200) loss: 2.304599\n",
      "(Iteration 301 / 61200) loss: 2.304589\n",
      "(Iteration 401 / 61200) loss: 2.304601\n",
      "(Iteration 501 / 61200) loss: 2.304589\n",
      "(Iteration 601 / 61200) loss: 2.304590\n",
      "(Iteration 701 / 61200) loss: 2.304583\n",
      "(Epoch 1 / 80) train acc: 0.120000; val_acc: 0.129000\n",
      "(Iteration 801 / 61200) loss: 2.304597\n",
      "(Iteration 901 / 61200) loss: 2.304587\n",
      "(Iteration 1001 / 61200) loss: 2.304598\n",
      "(Iteration 1101 / 61200) loss: 2.304601\n",
      "(Iteration 1201 / 61200) loss: 2.304592\n",
      "(Iteration 1301 / 61200) loss: 2.304596\n",
      "(Iteration 1401 / 61200) loss: 2.304597\n",
      "(Iteration 1501 / 61200) loss: 2.304599\n",
      "(Epoch 2 / 80) train acc: 0.113000; val_acc: 0.129000\n",
      "(Iteration 1601 / 61200) loss: 2.304594\n",
      "(Iteration 1701 / 61200) loss: 2.304605\n",
      "(Iteration 1801 / 61200) loss: 2.304587\n",
      "(Iteration 1901 / 61200) loss: 2.304608\n",
      "(Iteration 2001 / 61200) loss: 2.304594\n",
      "(Iteration 2101 / 61200) loss: 2.304603\n",
      "(Iteration 2201 / 61200) loss: 2.304596\n",
      "(Epoch 3 / 80) train acc: 0.114000; val_acc: 0.129000\n",
      "(Iteration 2301 / 61200) loss: 2.304588\n",
      "(Iteration 2401 / 61200) loss: 2.304592\n",
      "(Iteration 2501 / 61200) loss: 2.304611\n",
      "(Iteration 2601 / 61200) loss: 2.304599\n",
      "(Iteration 2701 / 61200) loss: 2.304597\n",
      "(Iteration 2801 / 61200) loss: 2.304593\n",
      "(Iteration 2901 / 61200) loss: 2.304599\n",
      "(Iteration 3001 / 61200) loss: 2.304596\n",
      "(Epoch 4 / 80) train acc: 0.092000; val_acc: 0.129000\n",
      "(Iteration 3101 / 61200) loss: 2.304593\n",
      "(Iteration 3201 / 61200) loss: 2.304600\n",
      "(Iteration 3301 / 61200) loss: 2.304595\n",
      "(Iteration 3401 / 61200) loss: 2.304594\n",
      "(Iteration 3501 / 61200) loss: 2.304608\n",
      "(Iteration 3601 / 61200) loss: 2.304614\n",
      "(Iteration 3701 / 61200) loss: 2.304589\n",
      "(Iteration 3801 / 61200) loss: 2.304575\n",
      "(Epoch 5 / 80) train acc: 0.126000; val_acc: 0.129000\n",
      "(Iteration 3901 / 61200) loss: 2.304583\n",
      "(Iteration 4001 / 61200) loss: 2.304596\n",
      "(Iteration 4101 / 61200) loss: 2.304604\n",
      "(Iteration 4201 / 61200) loss: 2.304595\n",
      "(Iteration 4301 / 61200) loss: 2.304564\n",
      "(Iteration 4401 / 61200) loss: 2.304587\n",
      "(Iteration 4501 / 61200) loss: 2.304592\n",
      "(Epoch 6 / 80) train acc: 0.123000; val_acc: 0.129000\n",
      "(Iteration 4601 / 61200) loss: 2.304596\n",
      "(Iteration 4701 / 61200) loss: 2.304603\n",
      "(Iteration 4801 / 61200) loss: 2.304588\n",
      "(Iteration 4901 / 61200) loss: 2.304594\n",
      "(Iteration 5001 / 61200) loss: 2.304587\n",
      "(Iteration 5101 / 61200) loss: 2.304574\n",
      "(Iteration 5201 / 61200) loss: 2.304580\n",
      "(Iteration 5301 / 61200) loss: 2.304599\n",
      "(Epoch 7 / 80) train acc: 0.100000; val_acc: 0.129000\n",
      "(Iteration 5401 / 61200) loss: 2.304597\n",
      "(Iteration 5501 / 61200) loss: 2.304595\n",
      "(Iteration 5601 / 61200) loss: 2.304584\n",
      "(Iteration 5701 / 61200) loss: 2.304595\n",
      "(Iteration 5801 / 61200) loss: 2.304593\n",
      "(Iteration 5901 / 61200) loss: 2.304585\n",
      "(Iteration 6001 / 61200) loss: 2.304594\n",
      "(Iteration 6101 / 61200) loss: 2.304601\n",
      "(Epoch 8 / 80) train acc: 0.121000; val_acc: 0.129000\n",
      "(Iteration 6201 / 61200) loss: 2.304599\n",
      "(Iteration 6301 / 61200) loss: 2.304591\n",
      "(Iteration 6401 / 61200) loss: 2.304598\n",
      "(Iteration 6501 / 61200) loss: 2.304589\n",
      "(Iteration 6601 / 61200) loss: 2.304594\n",
      "(Iteration 6701 / 61200) loss: 2.304600\n",
      "(Iteration 6801 / 61200) loss: 2.304605\n",
      "(Epoch 9 / 80) train acc: 0.104000; val_acc: 0.129000\n",
      "(Iteration 6901 / 61200) loss: 2.304582\n",
      "(Iteration 7001 / 61200) loss: 2.304602\n",
      "(Iteration 7101 / 61200) loss: 2.304613\n",
      "(Iteration 7201 / 61200) loss: 2.304609\n",
      "(Iteration 7301 / 61200) loss: 2.304585\n",
      "(Iteration 7401 / 61200) loss: 2.304587\n",
      "(Iteration 7501 / 61200) loss: 2.304582\n",
      "(Iteration 7601 / 61200) loss: 2.304590\n",
      "(Epoch 10 / 80) train acc: 0.112000; val_acc: 0.129000\n",
      "(Iteration 7701 / 61200) loss: 2.304601\n",
      "(Iteration 7801 / 61200) loss: 2.304604\n",
      "(Iteration 7901 / 61200) loss: 2.304599\n",
      "(Iteration 8001 / 61200) loss: 2.304596\n",
      "(Iteration 8101 / 61200) loss: 2.304592\n",
      "(Iteration 8201 / 61200) loss: 2.304604\n",
      "(Iteration 8301 / 61200) loss: 2.304589\n",
      "(Iteration 8401 / 61200) loss: 2.304597\n",
      "(Epoch 11 / 80) train acc: 0.123000; val_acc: 0.129000\n",
      "(Iteration 8501 / 61200) loss: 2.304600\n",
      "(Iteration 8601 / 61200) loss: 2.304601\n",
      "(Iteration 8701 / 61200) loss: 2.304601\n",
      "(Iteration 8801 / 61200) loss: 2.304589\n",
      "(Iteration 8901 / 61200) loss: 2.304598\n",
      "(Iteration 9001 / 61200) loss: 2.304588\n",
      "(Iteration 9101 / 61200) loss: 2.304594\n",
      "(Epoch 12 / 80) train acc: 0.116000; val_acc: 0.129000\n",
      "(Iteration 9201 / 61200) loss: 2.304600\n",
      "(Iteration 9301 / 61200) loss: 2.304594\n",
      "(Iteration 9401 / 61200) loss: 2.304582\n",
      "(Iteration 9501 / 61200) loss: 2.304591\n",
      "(Iteration 9601 / 61200) loss: 2.304594\n",
      "(Iteration 9701 / 61200) loss: 2.304586\n",
      "(Iteration 9801 / 61200) loss: 2.304595\n",
      "(Iteration 9901 / 61200) loss: 2.304594\n",
      "(Epoch 13 / 80) train acc: 0.102000; val_acc: 0.129000\n",
      "(Iteration 10001 / 61200) loss: 2.304610\n",
      "(Iteration 10101 / 61200) loss: 2.304582\n",
      "(Iteration 10201 / 61200) loss: 2.304586\n",
      "(Iteration 10301 / 61200) loss: 2.304586\n",
      "(Iteration 10401 / 61200) loss: 2.304609\n",
      "(Iteration 10501 / 61200) loss: 2.304590\n",
      "(Iteration 10601 / 61200) loss: 2.304583\n",
      "(Iteration 10701 / 61200) loss: 2.304597\n",
      "(Epoch 14 / 80) train acc: 0.117000; val_acc: 0.129000\n",
      "(Iteration 10801 / 61200) loss: 2.304612\n",
      "(Iteration 10901 / 61200) loss: 2.304613\n",
      "(Iteration 11001 / 61200) loss: 2.304602\n",
      "(Iteration 11101 / 61200) loss: 2.304596\n",
      "(Iteration 11201 / 61200) loss: 2.304597\n",
      "(Iteration 11301 / 61200) loss: 2.304596\n",
      "(Iteration 11401 / 61200) loss: 2.304592\n",
      "(Epoch 15 / 80) train acc: 0.110000; val_acc: 0.129000\n",
      "(Iteration 11501 / 61200) loss: 2.304605\n",
      "(Iteration 11601 / 61200) loss: 2.304603\n",
      "(Iteration 11701 / 61200) loss: 2.304595\n",
      "(Iteration 11801 / 61200) loss: 2.304599\n",
      "(Iteration 11901 / 61200) loss: 2.304577\n",
      "(Iteration 12001 / 61200) loss: 2.304601\n",
      "(Iteration 12101 / 61200) loss: 2.304596\n",
      "(Iteration 12201 / 61200) loss: 2.304607\n",
      "(Epoch 16 / 80) train acc: 0.114000; val_acc: 0.129000\n",
      "(Iteration 12301 / 61200) loss: 2.304601\n",
      "(Iteration 12401 / 61200) loss: 2.304601\n",
      "(Iteration 12501 / 61200) loss: 2.304614\n",
      "(Iteration 12601 / 61200) loss: 2.304601\n",
      "(Iteration 12701 / 61200) loss: 2.304601\n",
      "(Iteration 12801 / 61200) loss: 2.304588\n",
      "(Iteration 12901 / 61200) loss: 2.304598\n",
      "(Iteration 13001 / 61200) loss: 2.304578\n",
      "(Epoch 17 / 80) train acc: 0.111000; val_acc: 0.129000\n",
      "(Iteration 13101 / 61200) loss: 2.304592\n",
      "(Iteration 13201 / 61200) loss: 2.304591\n",
      "(Iteration 13301 / 61200) loss: 2.304601\n",
      "(Iteration 13401 / 61200) loss: 2.304601\n",
      "(Iteration 13501 / 61200) loss: 2.304602\n",
      "(Iteration 13601 / 61200) loss: 2.304602\n",
      "(Iteration 13701 / 61200) loss: 2.304595\n",
      "(Epoch 18 / 80) train acc: 0.115000; val_acc: 0.129000\n",
      "(Iteration 13801 / 61200) loss: 2.304592\n",
      "(Iteration 13901 / 61200) loss: 2.304593\n",
      "(Iteration 14001 / 61200) loss: 2.304604\n",
      "(Iteration 14101 / 61200) loss: 2.304586\n",
      "(Iteration 14201 / 61200) loss: 2.304572\n",
      "(Iteration 14301 / 61200) loss: 2.304581\n",
      "(Iteration 14401 / 61200) loss: 2.304584\n",
      "(Iteration 14501 / 61200) loss: 2.304581\n",
      "(Epoch 19 / 80) train acc: 0.112000; val_acc: 0.129000\n",
      "(Iteration 14601 / 61200) loss: 2.304604\n",
      "(Iteration 14701 / 61200) loss: 2.304589\n",
      "(Iteration 14801 / 61200) loss: 2.304597\n",
      "(Iteration 14901 / 61200) loss: 2.304606\n",
      "(Iteration 15001 / 61200) loss: 2.304594\n",
      "(Iteration 15101 / 61200) loss: 2.304601\n",
      "(Iteration 15201 / 61200) loss: 2.304596\n",
      "(Epoch 20 / 80) train acc: 0.106000; val_acc: 0.129000\n",
      "(Iteration 15301 / 61200) loss: 2.304586\n",
      "(Iteration 15401 / 61200) loss: 2.304589\n",
      "(Iteration 15501 / 61200) loss: 2.304595\n",
      "(Iteration 15601 / 61200) loss: 2.304595\n",
      "(Iteration 15701 / 61200) loss: 2.304604\n",
      "(Iteration 15801 / 61200) loss: 2.304589\n",
      "(Iteration 15901 / 61200) loss: 2.304583\n",
      "(Iteration 16001 / 61200) loss: 2.304581\n",
      "(Epoch 21 / 80) train acc: 0.114000; val_acc: 0.129000\n",
      "(Iteration 16101 / 61200) loss: 2.304594\n",
      "(Iteration 16201 / 61200) loss: 2.304598\n",
      "(Iteration 16301 / 61200) loss: 2.304596\n",
      "(Iteration 16401 / 61200) loss: 2.304598\n",
      "(Iteration 16501 / 61200) loss: 2.304605\n",
      "(Iteration 16601 / 61200) loss: 2.304597\n",
      "(Iteration 16701 / 61200) loss: 2.304601\n",
      "(Iteration 16801 / 61200) loss: 2.304576\n",
      "(Epoch 22 / 80) train acc: 0.110000; val_acc: 0.129000\n",
      "(Iteration 16901 / 61200) loss: 2.304583\n",
      "(Iteration 17001 / 61200) loss: 2.304600\n",
      "(Iteration 17101 / 61200) loss: 2.304600\n",
      "(Iteration 17201 / 61200) loss: 2.304598\n",
      "(Iteration 17301 / 61200) loss: 2.304615\n",
      "(Iteration 17401 / 61200) loss: 2.304592\n",
      "(Iteration 17501 / 61200) loss: 2.304587\n",
      "(Epoch 23 / 80) train acc: 0.100000; val_acc: 0.129000\n",
      "(Iteration 17601 / 61200) loss: 2.304592\n",
      "(Iteration 17701 / 61200) loss: 2.304566\n",
      "(Iteration 17801 / 61200) loss: 2.304601\n",
      "(Iteration 17901 / 61200) loss: 2.304588\n",
      "(Iteration 18001 / 61200) loss: 2.304594\n",
      "(Iteration 18101 / 61200) loss: 2.304610\n",
      "(Iteration 18201 / 61200) loss: 2.304586\n",
      "(Iteration 18301 / 61200) loss: 2.304600\n",
      "(Epoch 24 / 80) train acc: 0.092000; val_acc: 0.129000\n",
      "(Iteration 18401 / 61200) loss: 2.304589\n",
      "(Iteration 18501 / 61200) loss: 2.304602\n",
      "(Iteration 18601 / 61200) loss: 2.304596\n",
      "(Iteration 18701 / 61200) loss: 2.304591\n",
      "(Iteration 18801 / 61200) loss: 2.304585\n",
      "(Iteration 18901 / 61200) loss: 2.304594\n",
      "(Iteration 19001 / 61200) loss: 2.304595\n",
      "(Iteration 19101 / 61200) loss: 2.304606\n",
      "(Epoch 25 / 80) train acc: 0.094000; val_acc: 0.129000\n",
      "(Iteration 19201 / 61200) loss: 2.304585\n",
      "(Iteration 19301 / 61200) loss: 2.304601\n",
      "(Iteration 19401 / 61200) loss: 2.304588\n",
      "(Iteration 19501 / 61200) loss: 2.304601\n",
      "(Iteration 19601 / 61200) loss: 2.304606\n",
      "(Iteration 19701 / 61200) loss: 2.304600\n",
      "(Iteration 19801 / 61200) loss: 2.304585\n",
      "(Epoch 26 / 80) train acc: 0.135000; val_acc: 0.129000\n",
      "(Iteration 19901 / 61200) loss: 2.304586\n",
      "(Iteration 20001 / 61200) loss: 2.304589\n",
      "(Iteration 20101 / 61200) loss: 2.304603\n",
      "(Iteration 20201 / 61200) loss: 2.304589\n",
      "(Iteration 20301 / 61200) loss: 2.304590\n",
      "(Iteration 20401 / 61200) loss: 2.304597\n",
      "(Iteration 20501 / 61200) loss: 2.304595\n",
      "(Iteration 20601 / 61200) loss: 2.304586\n",
      "(Epoch 27 / 80) train acc: 0.117000; val_acc: 0.129000\n",
      "(Iteration 20701 / 61200) loss: 2.304598\n",
      "(Iteration 20801 / 61200) loss: 2.304601\n",
      "(Iteration 20901 / 61200) loss: 2.304580\n",
      "(Iteration 21001 / 61200) loss: 2.304599\n",
      "(Iteration 21101 / 61200) loss: 2.304623\n",
      "(Iteration 21201 / 61200) loss: 2.304592\n",
      "(Iteration 21301 / 61200) loss: 2.304592\n",
      "(Iteration 21401 / 61200) loss: 2.304581\n",
      "(Epoch 28 / 80) train acc: 0.112000; val_acc: 0.129000\n",
      "(Iteration 21501 / 61200) loss: 2.304585\n",
      "(Iteration 21601 / 61200) loss: 2.304607\n",
      "(Iteration 21701 / 61200) loss: 2.304592\n",
      "(Iteration 21801 / 61200) loss: 2.304591\n",
      "(Iteration 21901 / 61200) loss: 2.304603\n",
      "(Iteration 22001 / 61200) loss: 2.304580\n",
      "(Iteration 22101 / 61200) loss: 2.304588\n",
      "(Epoch 29 / 80) train acc: 0.103000; val_acc: 0.129000\n",
      "(Iteration 22201 / 61200) loss: 2.304587\n",
      "(Iteration 22301 / 61200) loss: 2.304595\n",
      "(Iteration 22401 / 61200) loss: 2.304587\n",
      "(Iteration 22501 / 61200) loss: 2.304613\n",
      "(Iteration 22601 / 61200) loss: 2.304603\n",
      "(Iteration 22701 / 61200) loss: 2.304577\n",
      "(Iteration 22801 / 61200) loss: 2.304602\n",
      "(Iteration 22901 / 61200) loss: 2.304599\n",
      "(Epoch 30 / 80) train acc: 0.102000; val_acc: 0.129000\n",
      "(Iteration 23001 / 61200) loss: 2.304601\n",
      "(Iteration 23101 / 61200) loss: 2.304586\n",
      "(Iteration 23201 / 61200) loss: 2.304596\n",
      "(Iteration 23301 / 61200) loss: 2.304602\n",
      "(Iteration 23401 / 61200) loss: 2.304606\n",
      "(Iteration 23501 / 61200) loss: 2.304595\n",
      "(Iteration 23601 / 61200) loss: 2.304586\n",
      "(Iteration 23701 / 61200) loss: 2.304591\n",
      "(Epoch 31 / 80) train acc: 0.118000; val_acc: 0.129000\n",
      "(Iteration 23801 / 61200) loss: 2.304590\n",
      "(Iteration 23901 / 61200) loss: 2.304588\n",
      "(Iteration 24001 / 61200) loss: 2.304602\n",
      "(Iteration 24101 / 61200) loss: 2.304599\n",
      "(Iteration 24201 / 61200) loss: 2.304581\n",
      "(Iteration 24301 / 61200) loss: 2.304594\n",
      "(Iteration 24401 / 61200) loss: 2.304594\n",
      "(Epoch 32 / 80) train acc: 0.102000; val_acc: 0.129000\n",
      "(Iteration 24501 / 61200) loss: 2.304593\n",
      "(Iteration 24601 / 61200) loss: 2.304593\n",
      "(Iteration 24701 / 61200) loss: 2.304574\n",
      "(Iteration 24801 / 61200) loss: 2.304592\n",
      "(Iteration 24901 / 61200) loss: 2.304599\n",
      "(Iteration 25001 / 61200) loss: 2.304592\n",
      "(Iteration 25101 / 61200) loss: 2.304591\n",
      "(Iteration 25201 / 61200) loss: 2.304584\n",
      "(Epoch 33 / 80) train acc: 0.105000; val_acc: 0.129000\n",
      "(Iteration 25301 / 61200) loss: 2.304598\n",
      "(Iteration 25401 / 61200) loss: 2.304594\n",
      "(Iteration 25501 / 61200) loss: 2.304596\n",
      "(Iteration 25601 / 61200) loss: 2.304585\n",
      "(Iteration 25701 / 61200) loss: 2.304589\n",
      "(Iteration 25801 / 61200) loss: 2.304589\n",
      "(Iteration 25901 / 61200) loss: 2.304586\n",
      "(Iteration 26001 / 61200) loss: 2.304590\n",
      "(Epoch 34 / 80) train acc: 0.116000; val_acc: 0.129000\n",
      "(Iteration 26101 / 61200) loss: 2.304584\n",
      "(Iteration 26201 / 61200) loss: 2.304604\n",
      "(Iteration 26301 / 61200) loss: 2.304574\n",
      "(Iteration 26401 / 61200) loss: 2.304599\n",
      "(Iteration 26501 / 61200) loss: 2.304600\n",
      "(Iteration 26601 / 61200) loss: 2.304596\n",
      "(Iteration 26701 / 61200) loss: 2.304611\n",
      "(Epoch 35 / 80) train acc: 0.120000; val_acc: 0.129000\n",
      "(Iteration 26801 / 61200) loss: 2.304604\n",
      "(Iteration 26901 / 61200) loss: 2.304579\n",
      "(Iteration 27001 / 61200) loss: 2.304608\n",
      "(Iteration 27101 / 61200) loss: 2.304605\n",
      "(Iteration 27201 / 61200) loss: 2.304592\n",
      "(Iteration 27301 / 61200) loss: 2.304580\n",
      "(Iteration 27401 / 61200) loss: 2.304593\n",
      "(Iteration 27501 / 61200) loss: 2.304596\n",
      "(Epoch 36 / 80) train acc: 0.097000; val_acc: 0.129000\n",
      "(Iteration 27601 / 61200) loss: 2.304591\n",
      "(Iteration 27701 / 61200) loss: 2.304608\n",
      "(Iteration 27801 / 61200) loss: 2.304596\n",
      "(Iteration 27901 / 61200) loss: 2.304591\n",
      "(Iteration 28001 / 61200) loss: 2.304595\n",
      "(Iteration 28101 / 61200) loss: 2.304590\n",
      "(Iteration 28201 / 61200) loss: 2.304592\n",
      "(Iteration 28301 / 61200) loss: 2.304595\n",
      "(Epoch 37 / 80) train acc: 0.129000; val_acc: 0.129000\n",
      "(Iteration 28401 / 61200) loss: 2.304592\n",
      "(Iteration 28501 / 61200) loss: 2.304605\n",
      "(Iteration 28601 / 61200) loss: 2.304584\n",
      "(Iteration 28701 / 61200) loss: 2.304584\n",
      "(Iteration 28801 / 61200) loss: 2.304587\n",
      "(Iteration 28901 / 61200) loss: 2.304600\n",
      "(Iteration 29001 / 61200) loss: 2.304592\n",
      "(Epoch 38 / 80) train acc: 0.129000; val_acc: 0.129000\n",
      "(Iteration 29101 / 61200) loss: 2.304598\n",
      "(Iteration 29201 / 61200) loss: 2.304581\n",
      "(Iteration 29301 / 61200) loss: 2.304587\n",
      "(Iteration 29401 / 61200) loss: 2.304584\n",
      "(Iteration 29501 / 61200) loss: 2.304612\n",
      "(Iteration 29601 / 61200) loss: 2.304583\n",
      "(Iteration 29701 / 61200) loss: 2.304600\n",
      "(Iteration 29801 / 61200) loss: 2.304611\n",
      "(Epoch 39 / 80) train acc: 0.115000; val_acc: 0.129000\n",
      "(Iteration 29901 / 61200) loss: 2.304602\n",
      "(Iteration 30001 / 61200) loss: 2.304612\n",
      "(Iteration 30101 / 61200) loss: 2.304594\n",
      "(Iteration 30201 / 61200) loss: 2.304602\n",
      "(Iteration 30301 / 61200) loss: 2.304609\n",
      "(Iteration 30401 / 61200) loss: 2.304588\n",
      "(Iteration 30501 / 61200) loss: 2.304590\n",
      "(Epoch 40 / 80) train acc: 0.122000; val_acc: 0.129000\n",
      "(Iteration 30601 / 61200) loss: 2.304596\n",
      "(Iteration 30701 / 61200) loss: 2.304594\n",
      "(Iteration 30801 / 61200) loss: 2.304594\n",
      "(Iteration 30901 / 61200) loss: 2.304600\n",
      "(Iteration 31001 / 61200) loss: 2.304594\n",
      "(Iteration 31101 / 61200) loss: 2.304591\n",
      "(Iteration 31201 / 61200) loss: 2.304569\n",
      "(Iteration 31301 / 61200) loss: 2.304591\n",
      "(Epoch 41 / 80) train acc: 0.109000; val_acc: 0.129000\n",
      "(Iteration 31401 / 61200) loss: 2.304591\n",
      "(Iteration 31501 / 61200) loss: 2.304589\n",
      "(Iteration 31601 / 61200) loss: 2.304566\n",
      "(Iteration 31701 / 61200) loss: 2.304565\n",
      "(Iteration 31801 / 61200) loss: 2.304594\n",
      "(Iteration 31901 / 61200) loss: 2.304588\n",
      "(Iteration 32001 / 61200) loss: 2.304599\n",
      "(Iteration 32101 / 61200) loss: 2.304607\n",
      "(Epoch 42 / 80) train acc: 0.116000; val_acc: 0.129000\n",
      "(Iteration 32201 / 61200) loss: 2.304598\n",
      "(Iteration 32301 / 61200) loss: 2.304594\n",
      "(Iteration 32401 / 61200) loss: 2.304580\n",
      "(Iteration 32501 / 61200) loss: 2.304594\n",
      "(Iteration 32601 / 61200) loss: 2.304596\n",
      "(Iteration 32701 / 61200) loss: 2.304589\n",
      "(Iteration 32801 / 61200) loss: 2.304595\n",
      "(Epoch 43 / 80) train acc: 0.127000; val_acc: 0.129000\n",
      "(Iteration 32901 / 61200) loss: 2.304585\n",
      "(Iteration 33001 / 61200) loss: 2.304579\n",
      "(Iteration 33101 / 61200) loss: 2.304600\n",
      "(Iteration 33201 / 61200) loss: 2.304590\n",
      "(Iteration 33301 / 61200) loss: 2.304606\n",
      "(Iteration 33401 / 61200) loss: 2.304592\n",
      "(Iteration 33501 / 61200) loss: 2.304585\n",
      "(Iteration 33601 / 61200) loss: 2.304586\n",
      "(Epoch 44 / 80) train acc: 0.106000; val_acc: 0.129000\n",
      "(Iteration 33701 / 61200) loss: 2.304587\n",
      "(Iteration 33801 / 61200) loss: 2.304588\n",
      "(Iteration 33901 / 61200) loss: 2.304592\n",
      "(Iteration 34001 / 61200) loss: 2.304596\n",
      "(Iteration 34101 / 61200) loss: 2.304585\n",
      "(Iteration 34201 / 61200) loss: 2.304608\n",
      "(Iteration 34301 / 61200) loss: 2.304587\n",
      "(Iteration 34401 / 61200) loss: 2.304587\n",
      "(Epoch 45 / 80) train acc: 0.110000; val_acc: 0.129000\n",
      "(Iteration 34501 / 61200) loss: 2.304589\n",
      "(Iteration 34601 / 61200) loss: 2.304602\n",
      "(Iteration 34701 / 61200) loss: 2.304585\n",
      "(Iteration 34801 / 61200) loss: 2.304589\n",
      "(Iteration 34901 / 61200) loss: 2.304585\n",
      "(Iteration 35001 / 61200) loss: 2.304586\n",
      "(Iteration 35101 / 61200) loss: 2.304599\n",
      "(Epoch 46 / 80) train acc: 0.100000; val_acc: 0.129000\n",
      "(Iteration 35201 / 61200) loss: 2.304587\n",
      "(Iteration 35301 / 61200) loss: 2.304595\n",
      "(Iteration 35401 / 61200) loss: 2.304585\n",
      "(Iteration 35501 / 61200) loss: 2.304595\n",
      "(Iteration 35601 / 61200) loss: 2.304599\n",
      "(Iteration 35701 / 61200) loss: 2.304589\n",
      "(Iteration 35801 / 61200) loss: 2.304595\n",
      "(Iteration 35901 / 61200) loss: 2.304592\n",
      "(Epoch 47 / 80) train acc: 0.109000; val_acc: 0.129000\n",
      "(Iteration 36001 / 61200) loss: 2.304591\n",
      "(Iteration 36101 / 61200) loss: 2.304603\n",
      "(Iteration 36201 / 61200) loss: 2.304593\n",
      "(Iteration 36301 / 61200) loss: 2.304586\n",
      "(Iteration 36401 / 61200) loss: 2.304584\n",
      "(Iteration 36501 / 61200) loss: 2.304578\n",
      "(Iteration 36601 / 61200) loss: 2.304601\n",
      "(Iteration 36701 / 61200) loss: 2.304593\n",
      "(Epoch 48 / 80) train acc: 0.111000; val_acc: 0.129000\n",
      "(Iteration 36801 / 61200) loss: 2.304610\n",
      "(Iteration 36901 / 61200) loss: 2.304586\n",
      "(Iteration 37001 / 61200) loss: 2.304599\n",
      "(Iteration 37101 / 61200) loss: 2.304581\n",
      "(Iteration 37201 / 61200) loss: 2.304607\n",
      "(Iteration 37301 / 61200) loss: 2.304589\n",
      "(Iteration 37401 / 61200) loss: 2.304595\n",
      "(Epoch 49 / 80) train acc: 0.101000; val_acc: 0.129000\n",
      "(Iteration 37501 / 61200) loss: 2.304601\n",
      "(Iteration 37601 / 61200) loss: 2.304602\n",
      "(Iteration 37701 / 61200) loss: 2.304600\n",
      "(Iteration 37801 / 61200) loss: 2.304588\n",
      "(Iteration 37901 / 61200) loss: 2.304585\n",
      "(Iteration 38001 / 61200) loss: 2.304601\n",
      "(Iteration 38101 / 61200) loss: 2.304600\n",
      "(Iteration 38201 / 61200) loss: 2.304578\n",
      "(Epoch 50 / 80) train acc: 0.115000; val_acc: 0.129000\n",
      "(Iteration 38301 / 61200) loss: 2.304594\n",
      "(Iteration 38401 / 61200) loss: 2.304605\n",
      "(Iteration 38501 / 61200) loss: 2.304588\n",
      "(Iteration 38601 / 61200) loss: 2.304604\n",
      "(Iteration 38701 / 61200) loss: 2.304583\n",
      "(Iteration 38801 / 61200) loss: 2.304593\n",
      "(Iteration 38901 / 61200) loss: 2.304583\n",
      "(Iteration 39001 / 61200) loss: 2.304600\n",
      "(Epoch 51 / 80) train acc: 0.133000; val_acc: 0.129000\n",
      "(Iteration 39101 / 61200) loss: 2.304605\n",
      "(Iteration 39201 / 61200) loss: 2.304585\n",
      "(Iteration 39301 / 61200) loss: 2.304590\n",
      "(Iteration 39401 / 61200) loss: 2.304584\n",
      "(Iteration 39501 / 61200) loss: 2.304587\n",
      "(Iteration 39601 / 61200) loss: 2.304590\n",
      "(Iteration 39701 / 61200) loss: 2.304583\n",
      "(Epoch 52 / 80) train acc: 0.116000; val_acc: 0.129000\n",
      "(Iteration 39801 / 61200) loss: 2.304598\n",
      "(Iteration 39901 / 61200) loss: 2.304601\n",
      "(Iteration 40001 / 61200) loss: 2.304571\n",
      "(Iteration 40101 / 61200) loss: 2.304578\n",
      "(Iteration 40201 / 61200) loss: 2.304593\n",
      "(Iteration 40301 / 61200) loss: 2.304590\n",
      "(Iteration 40401 / 61200) loss: 2.304598\n",
      "(Iteration 40501 / 61200) loss: 2.304584\n",
      "(Epoch 53 / 80) train acc: 0.108000; val_acc: 0.129000\n",
      "(Iteration 40601 / 61200) loss: 2.304580\n",
      "(Iteration 40701 / 61200) loss: 2.304606\n",
      "(Iteration 40801 / 61200) loss: 2.304589\n",
      "(Iteration 40901 / 61200) loss: 2.304592\n",
      "(Iteration 41001 / 61200) loss: 2.304593\n",
      "(Iteration 41101 / 61200) loss: 2.304582\n",
      "(Iteration 41201 / 61200) loss: 2.304583\n",
      "(Iteration 41301 / 61200) loss: 2.304596\n",
      "(Epoch 54 / 80) train acc: 0.109000; val_acc: 0.129000\n",
      "(Iteration 41401 / 61200) loss: 2.304590\n",
      "(Iteration 41501 / 61200) loss: 2.304607\n",
      "(Iteration 41601 / 61200) loss: 2.304588\n",
      "(Iteration 41701 / 61200) loss: 2.304572\n",
      "(Iteration 41801 / 61200) loss: 2.304613\n",
      "(Iteration 41901 / 61200) loss: 2.304601\n",
      "(Iteration 42001 / 61200) loss: 2.304590\n",
      "(Epoch 55 / 80) train acc: 0.113000; val_acc: 0.129000\n",
      "(Iteration 42101 / 61200) loss: 2.304604\n",
      "(Iteration 42201 / 61200) loss: 2.304604\n",
      "(Iteration 42301 / 61200) loss: 2.304602\n",
      "(Iteration 42401 / 61200) loss: 2.304608\n",
      "(Iteration 42501 / 61200) loss: 2.304581\n",
      "(Iteration 42601 / 61200) loss: 2.304587\n",
      "(Iteration 42701 / 61200) loss: 2.304579\n",
      "(Iteration 42801 / 61200) loss: 2.304601\n",
      "(Epoch 56 / 80) train acc: 0.124000; val_acc: 0.129000\n",
      "(Iteration 42901 / 61200) loss: 2.304591\n",
      "(Iteration 43001 / 61200) loss: 2.304599\n",
      "(Iteration 43101 / 61200) loss: 2.304607\n",
      "(Iteration 43201 / 61200) loss: 2.304598\n",
      "(Iteration 43301 / 61200) loss: 2.304585\n",
      "(Iteration 43401 / 61200) loss: 2.304598\n",
      "(Iteration 43501 / 61200) loss: 2.304605\n",
      "(Iteration 43601 / 61200) loss: 2.304601\n",
      "(Epoch 57 / 80) train acc: 0.094000; val_acc: 0.129000\n",
      "(Iteration 43701 / 61200) loss: 2.304578\n",
      "(Iteration 43801 / 61200) loss: 2.304603\n",
      "(Iteration 43901 / 61200) loss: 2.304586\n",
      "(Iteration 44001 / 61200) loss: 2.304615\n",
      "(Iteration 44101 / 61200) loss: 2.304603\n",
      "(Iteration 44201 / 61200) loss: 2.304591\n",
      "(Iteration 44301 / 61200) loss: 2.304586\n",
      "(Epoch 58 / 80) train acc: 0.115000; val_acc: 0.129000\n",
      "(Iteration 44401 / 61200) loss: 2.304589\n",
      "(Iteration 44501 / 61200) loss: 2.304591\n",
      "(Iteration 44601 / 61200) loss: 2.304586\n",
      "(Iteration 44701 / 61200) loss: 2.304586\n",
      "(Iteration 44801 / 61200) loss: 2.304597\n",
      "(Iteration 44901 / 61200) loss: 2.304603\n",
      "(Iteration 45001 / 61200) loss: 2.304588\n",
      "(Iteration 45101 / 61200) loss: 2.304588\n",
      "(Epoch 59 / 80) train acc: 0.113000; val_acc: 0.129000\n",
      "(Iteration 45201 / 61200) loss: 2.304585\n",
      "(Iteration 45301 / 61200) loss: 2.304594\n",
      "(Iteration 45401 / 61200) loss: 2.304588\n",
      "(Iteration 45501 / 61200) loss: 2.304583\n",
      "(Iteration 45601 / 61200) loss: 2.304586\n",
      "(Iteration 45701 / 61200) loss: 2.304617\n",
      "(Iteration 45801 / 61200) loss: 2.304599\n",
      "(Epoch 60 / 80) train acc: 0.110000; val_acc: 0.129000\n",
      "(Iteration 45901 / 61200) loss: 2.304589\n",
      "(Iteration 46001 / 61200) loss: 2.304586\n",
      "(Iteration 46101 / 61200) loss: 2.304593\n",
      "(Iteration 46201 / 61200) loss: 2.304590\n",
      "(Iteration 46301 / 61200) loss: 2.304594\n",
      "(Iteration 46401 / 61200) loss: 2.304583\n",
      "(Iteration 46501 / 61200) loss: 2.304603\n",
      "(Iteration 46601 / 61200) loss: 2.304590\n",
      "(Epoch 61 / 80) train acc: 0.103000; val_acc: 0.129000\n",
      "(Iteration 46701 / 61200) loss: 2.304599\n",
      "(Iteration 46801 / 61200) loss: 2.304604\n",
      "(Iteration 46901 / 61200) loss: 2.304599\n",
      "(Iteration 47001 / 61200) loss: 2.304579\n",
      "(Iteration 47101 / 61200) loss: 2.304605\n",
      "(Iteration 47201 / 61200) loss: 2.304597\n",
      "(Iteration 47301 / 61200) loss: 2.304598\n",
      "(Iteration 47401 / 61200) loss: 2.304597\n",
      "(Epoch 62 / 80) train acc: 0.118000; val_acc: 0.129000\n",
      "(Iteration 47501 / 61200) loss: 2.304590\n",
      "(Iteration 47601 / 61200) loss: 2.304599\n",
      "(Iteration 47701 / 61200) loss: 2.304602\n",
      "(Iteration 47801 / 61200) loss: 2.304604\n",
      "(Iteration 47901 / 61200) loss: 2.304593\n",
      "(Iteration 48001 / 61200) loss: 2.304587\n",
      "(Iteration 48101 / 61200) loss: 2.304587\n",
      "(Epoch 63 / 80) train acc: 0.128000; val_acc: 0.129000\n",
      "(Iteration 48201 / 61200) loss: 2.304581\n",
      "(Iteration 48301 / 61200) loss: 2.304607\n",
      "(Iteration 48401 / 61200) loss: 2.304589\n",
      "(Iteration 48501 / 61200) loss: 2.304588\n",
      "(Iteration 48601 / 61200) loss: 2.304580\n",
      "(Iteration 48701 / 61200) loss: 2.304591\n",
      "(Iteration 48801 / 61200) loss: 2.304590\n",
      "(Iteration 48901 / 61200) loss: 2.304595\n",
      "(Epoch 64 / 80) train acc: 0.111000; val_acc: 0.129000\n",
      "(Iteration 49001 / 61200) loss: 2.304601\n",
      "(Iteration 49101 / 61200) loss: 2.304604\n",
      "(Iteration 49201 / 61200) loss: 2.304597\n",
      "(Iteration 49301 / 61200) loss: 2.304597\n",
      "(Iteration 49401 / 61200) loss: 2.304590\n",
      "(Iteration 49501 / 61200) loss: 2.304608\n",
      "(Iteration 49601 / 61200) loss: 2.304599\n",
      "(Iteration 49701 / 61200) loss: 2.304597\n",
      "(Epoch 65 / 80) train acc: 0.104000; val_acc: 0.129000\n",
      "(Iteration 49801 / 61200) loss: 2.304586\n",
      "(Iteration 49901 / 61200) loss: 2.304612\n",
      "(Iteration 50001 / 61200) loss: 2.304588\n",
      "(Iteration 50101 / 61200) loss: 2.304595\n",
      "(Iteration 50201 / 61200) loss: 2.304587\n",
      "(Iteration 50301 / 61200) loss: 2.304591\n",
      "(Iteration 50401 / 61200) loss: 2.304599\n",
      "(Epoch 66 / 80) train acc: 0.112000; val_acc: 0.129000\n",
      "(Iteration 50501 / 61200) loss: 2.304593\n",
      "(Iteration 50601 / 61200) loss: 2.304597\n",
      "(Iteration 50701 / 61200) loss: 2.304591\n",
      "(Iteration 50801 / 61200) loss: 2.304601\n",
      "(Iteration 50901 / 61200) loss: 2.304587\n",
      "(Iteration 51001 / 61200) loss: 2.304609\n",
      "(Iteration 51101 / 61200) loss: 2.304595\n",
      "(Iteration 51201 / 61200) loss: 2.304587\n",
      "(Epoch 67 / 80) train acc: 0.111000; val_acc: 0.129000\n",
      "(Iteration 51301 / 61200) loss: 2.304586\n",
      "(Iteration 51401 / 61200) loss: 2.304599\n",
      "(Iteration 51501 / 61200) loss: 2.304596\n",
      "(Iteration 51601 / 61200) loss: 2.304594\n",
      "(Iteration 51701 / 61200) loss: 2.304589\n",
      "(Iteration 51801 / 61200) loss: 2.304606\n",
      "(Iteration 51901 / 61200) loss: 2.304588\n",
      "(Iteration 52001 / 61200) loss: 2.304577\n",
      "(Epoch 68 / 80) train acc: 0.117000; val_acc: 0.129000\n",
      "(Iteration 52101 / 61200) loss: 2.304589\n",
      "(Iteration 52201 / 61200) loss: 2.304578\n",
      "(Iteration 52301 / 61200) loss: 2.304589\n",
      "(Iteration 52401 / 61200) loss: 2.304592\n",
      "(Iteration 52501 / 61200) loss: 2.304580\n",
      "(Iteration 52601 / 61200) loss: 2.304599\n",
      "(Iteration 52701 / 61200) loss: 2.304589\n",
      "(Epoch 69 / 80) train acc: 0.119000; val_acc: 0.129000\n",
      "(Iteration 52801 / 61200) loss: 2.304591\n",
      "(Iteration 52901 / 61200) loss: 2.304596\n",
      "(Iteration 53001 / 61200) loss: 2.304573\n",
      "(Iteration 53101 / 61200) loss: 2.304608\n",
      "(Iteration 53201 / 61200) loss: 2.304605\n",
      "(Iteration 53301 / 61200) loss: 2.304581\n",
      "(Iteration 53401 / 61200) loss: 2.304598\n",
      "(Iteration 53501 / 61200) loss: 2.304602\n",
      "(Epoch 70 / 80) train acc: 0.118000; val_acc: 0.129000\n",
      "(Iteration 53601 / 61200) loss: 2.304576\n",
      "(Iteration 53701 / 61200) loss: 2.304593\n",
      "(Iteration 53801 / 61200) loss: 2.304605\n",
      "(Iteration 53901 / 61200) loss: 2.304591\n",
      "(Iteration 54001 / 61200) loss: 2.304582\n",
      "(Iteration 54101 / 61200) loss: 2.304597\n",
      "(Iteration 54201 / 61200) loss: 2.304591\n",
      "(Iteration 54301 / 61200) loss: 2.304584\n",
      "(Epoch 71 / 80) train acc: 0.124000; val_acc: 0.129000\n",
      "(Iteration 54401 / 61200) loss: 2.304572\n",
      "(Iteration 54501 / 61200) loss: 2.304591\n",
      "(Iteration 54601 / 61200) loss: 2.304585\n",
      "(Iteration 54701 / 61200) loss: 2.304588\n",
      "(Iteration 54801 / 61200) loss: 2.304610\n",
      "(Iteration 54901 / 61200) loss: 2.304609\n",
      "(Iteration 55001 / 61200) loss: 2.304580\n",
      "(Epoch 72 / 80) train acc: 0.100000; val_acc: 0.129000\n",
      "(Iteration 55101 / 61200) loss: 2.304592\n",
      "(Iteration 55201 / 61200) loss: 2.304574\n",
      "(Iteration 55301 / 61200) loss: 2.304591\n",
      "(Iteration 55401 / 61200) loss: 2.304589\n",
      "(Iteration 55501 / 61200) loss: 2.304598\n",
      "(Iteration 55601 / 61200) loss: 2.304590\n",
      "(Iteration 55701 / 61200) loss: 2.304581\n",
      "(Iteration 55801 / 61200) loss: 2.304596\n",
      "(Epoch 73 / 80) train acc: 0.100000; val_acc: 0.129000\n",
      "(Iteration 55901 / 61200) loss: 2.304586\n",
      "(Iteration 56001 / 61200) loss: 2.304605\n",
      "(Iteration 56101 / 61200) loss: 2.304598\n",
      "(Iteration 56201 / 61200) loss: 2.304590\n",
      "(Iteration 56301 / 61200) loss: 2.304588\n",
      "(Iteration 56401 / 61200) loss: 2.304587\n",
      "(Iteration 56501 / 61200) loss: 2.304601\n",
      "(Iteration 56601 / 61200) loss: 2.304586\n",
      "(Epoch 74 / 80) train acc: 0.102000; val_acc: 0.129000\n",
      "(Iteration 56701 / 61200) loss: 2.304602\n",
      "(Iteration 56801 / 61200) loss: 2.304606\n",
      "(Iteration 56901 / 61200) loss: 2.304592\n",
      "(Iteration 57001 / 61200) loss: 2.304590\n",
      "(Iteration 57101 / 61200) loss: 2.304599\n",
      "(Iteration 57201 / 61200) loss: 2.304587\n",
      "(Iteration 57301 / 61200) loss: 2.304600\n",
      "(Epoch 75 / 80) train acc: 0.107000; val_acc: 0.129000\n",
      "(Iteration 57401 / 61200) loss: 2.304580\n",
      "(Iteration 57501 / 61200) loss: 2.304586\n",
      "(Iteration 57601 / 61200) loss: 2.304583\n",
      "(Iteration 57701 / 61200) loss: 2.304585\n",
      "(Iteration 57801 / 61200) loss: 2.304597\n",
      "(Iteration 57901 / 61200) loss: 2.304581\n",
      "(Iteration 58001 / 61200) loss: 2.304596\n",
      "(Iteration 58101 / 61200) loss: 2.304598\n",
      "(Epoch 76 / 80) train acc: 0.118000; val_acc: 0.129000\n",
      "(Iteration 58201 / 61200) loss: 2.304603\n",
      "(Iteration 58301 / 61200) loss: 2.304593\n",
      "(Iteration 58401 / 61200) loss: 2.304597\n",
      "(Iteration 58501 / 61200) loss: 2.304588\n",
      "(Iteration 58601 / 61200) loss: 2.304601\n",
      "(Iteration 58701 / 61200) loss: 2.304597\n",
      "(Iteration 58801 / 61200) loss: 2.304579\n",
      "(Iteration 58901 / 61200) loss: 2.304591\n",
      "(Epoch 77 / 80) train acc: 0.118000; val_acc: 0.129000\n",
      "(Iteration 59001 / 61200) loss: 2.304602\n",
      "(Iteration 59101 / 61200) loss: 2.304592\n",
      "(Iteration 59201 / 61200) loss: 2.304592\n",
      "(Iteration 59301 / 61200) loss: 2.304597\n",
      "(Iteration 59401 / 61200) loss: 2.304594\n",
      "(Iteration 59501 / 61200) loss: 2.304593\n",
      "(Iteration 59601 / 61200) loss: 2.304598\n",
      "(Epoch 78 / 80) train acc: 0.113000; val_acc: 0.129000\n",
      "(Iteration 59701 / 61200) loss: 2.304588\n",
      "(Iteration 59801 / 61200) loss: 2.304600\n",
      "(Iteration 59901 / 61200) loss: 2.304599\n",
      "(Iteration 60001 / 61200) loss: 2.304580\n",
      "(Iteration 60101 / 61200) loss: 2.304595\n",
      "(Iteration 60201 / 61200) loss: 2.304587\n",
      "(Iteration 60301 / 61200) loss: 2.304586\n",
      "(Iteration 60401 / 61200) loss: 2.304604\n",
      "(Epoch 79 / 80) train acc: 0.107000; val_acc: 0.129000\n",
      "(Iteration 60501 / 61200) loss: 2.304582\n",
      "(Iteration 60601 / 61200) loss: 2.304590\n",
      "(Iteration 60701 / 61200) loss: 2.304577\n",
      "(Iteration 60801 / 61200) loss: 2.304599\n",
      "(Iteration 60901 / 61200) loss: 2.304591\n",
      "(Iteration 61001 / 61200) loss: 2.304589\n",
      "(Iteration 61101 / 61200) loss: 2.304595\n",
      "(Epoch 80 / 80) train acc: 0.113000; val_acc: 0.129000\n",
      "New best model found with validation accuracy: 0.1290\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.304692\n",
      "(Epoch 0 / 80) train acc: 0.073000; val_acc: 0.084000\n",
      "(Iteration 101 / 30560) loss: 2.304689\n",
      "(Iteration 201 / 30560) loss: 2.304687\n",
      "(Iteration 301 / 30560) loss: 2.304689\n",
      "(Epoch 1 / 80) train acc: 0.092000; val_acc: 0.084000\n",
      "(Iteration 401 / 30560) loss: 2.304688\n",
      "(Iteration 501 / 30560) loss: 2.304682\n",
      "(Iteration 601 / 30560) loss: 2.304684\n",
      "(Iteration 701 / 30560) loss: 2.304692\n",
      "(Epoch 2 / 80) train acc: 0.086000; val_acc: 0.084000\n",
      "(Iteration 801 / 30560) loss: 2.304684\n",
      "(Iteration 901 / 30560) loss: 2.304675\n",
      "(Iteration 1001 / 30560) loss: 2.304678\n",
      "(Iteration 1101 / 30560) loss: 2.304689\n",
      "(Epoch 3 / 80) train acc: 0.083000; val_acc: 0.085000\n",
      "(Iteration 1201 / 30560) loss: 2.304681\n",
      "(Iteration 1301 / 30560) loss: 2.304689\n",
      "(Iteration 1401 / 30560) loss: 2.304677\n",
      "(Iteration 1501 / 30560) loss: 2.304685\n",
      "(Epoch 4 / 80) train acc: 0.091000; val_acc: 0.085000\n",
      "(Iteration 1601 / 30560) loss: 2.304677\n",
      "(Iteration 1701 / 30560) loss: 2.304685\n",
      "(Iteration 1801 / 30560) loss: 2.304676\n",
      "(Iteration 1901 / 30560) loss: 2.304692\n",
      "(Epoch 5 / 80) train acc: 0.085000; val_acc: 0.084000\n",
      "(Iteration 2001 / 30560) loss: 2.304686\n",
      "(Iteration 2101 / 30560) loss: 2.304687\n",
      "(Iteration 2201 / 30560) loss: 2.304690\n",
      "(Epoch 6 / 80) train acc: 0.092000; val_acc: 0.085000\n",
      "(Iteration 2301 / 30560) loss: 2.304683\n",
      "(Iteration 2401 / 30560) loss: 2.304680\n",
      "(Iteration 2501 / 30560) loss: 2.304685\n",
      "(Iteration 2601 / 30560) loss: 2.304688\n",
      "(Epoch 7 / 80) train acc: 0.085000; val_acc: 0.085000\n",
      "(Iteration 2701 / 30560) loss: 2.304692\n",
      "(Iteration 2801 / 30560) loss: 2.304688\n",
      "(Iteration 2901 / 30560) loss: 2.304692\n",
      "(Iteration 3001 / 30560) loss: 2.304695\n",
      "(Epoch 8 / 80) train acc: 0.103000; val_acc: 0.084000\n",
      "(Iteration 3101 / 30560) loss: 2.304685\n",
      "(Iteration 3201 / 30560) loss: 2.304685\n",
      "(Iteration 3301 / 30560) loss: 2.304684\n",
      "(Iteration 3401 / 30560) loss: 2.304686\n",
      "(Epoch 9 / 80) train acc: 0.085000; val_acc: 0.084000\n",
      "(Iteration 3501 / 30560) loss: 2.304685\n",
      "(Iteration 3601 / 30560) loss: 2.304687\n",
      "(Iteration 3701 / 30560) loss: 2.304685\n",
      "(Iteration 3801 / 30560) loss: 2.304683\n",
      "(Epoch 10 / 80) train acc: 0.085000; val_acc: 0.084000\n",
      "(Iteration 3901 / 30560) loss: 2.304683\n",
      "(Iteration 4001 / 30560) loss: 2.304687\n",
      "(Iteration 4101 / 30560) loss: 2.304682\n",
      "(Iteration 4201 / 30560) loss: 2.304674\n",
      "(Epoch 11 / 80) train acc: 0.078000; val_acc: 0.084000\n",
      "(Iteration 4301 / 30560) loss: 2.304691\n",
      "(Iteration 4401 / 30560) loss: 2.304683\n",
      "(Iteration 4501 / 30560) loss: 2.304683\n",
      "(Epoch 12 / 80) train acc: 0.089000; val_acc: 0.084000\n",
      "(Iteration 4601 / 30560) loss: 2.304672\n",
      "(Iteration 4701 / 30560) loss: 2.304690\n",
      "(Iteration 4801 / 30560) loss: 2.304680\n",
      "(Iteration 4901 / 30560) loss: 2.304677\n",
      "(Epoch 13 / 80) train acc: 0.075000; val_acc: 0.084000\n",
      "(Iteration 5001 / 30560) loss: 2.304685\n",
      "(Iteration 5101 / 30560) loss: 2.304688\n",
      "(Iteration 5201 / 30560) loss: 2.304687\n",
      "(Iteration 5301 / 30560) loss: 2.304684\n",
      "(Epoch 14 / 80) train acc: 0.093000; val_acc: 0.084000\n",
      "(Iteration 5401 / 30560) loss: 2.304684\n",
      "(Iteration 5501 / 30560) loss: 2.304685\n",
      "(Iteration 5601 / 30560) loss: 2.304694\n",
      "(Iteration 5701 / 30560) loss: 2.304691\n",
      "(Epoch 15 / 80) train acc: 0.078000; val_acc: 0.084000\n",
      "(Iteration 5801 / 30560) loss: 2.304680\n",
      "(Iteration 5901 / 30560) loss: 2.304683\n",
      "(Iteration 6001 / 30560) loss: 2.304680\n",
      "(Iteration 6101 / 30560) loss: 2.304678\n",
      "(Epoch 16 / 80) train acc: 0.072000; val_acc: 0.084000\n",
      "(Iteration 6201 / 30560) loss: 2.304688\n",
      "(Iteration 6301 / 30560) loss: 2.304678\n",
      "(Iteration 6401 / 30560) loss: 2.304686\n",
      "(Epoch 17 / 80) train acc: 0.087000; val_acc: 0.084000\n",
      "(Iteration 6501 / 30560) loss: 2.304686\n",
      "(Iteration 6601 / 30560) loss: 2.304684\n",
      "(Iteration 6701 / 30560) loss: 2.304681\n",
      "(Iteration 6801 / 30560) loss: 2.304692\n",
      "(Epoch 18 / 80) train acc: 0.083000; val_acc: 0.084000\n",
      "(Iteration 6901 / 30560) loss: 2.304687\n",
      "(Iteration 7001 / 30560) loss: 2.304682\n",
      "(Iteration 7101 / 30560) loss: 2.304683\n",
      "(Iteration 7201 / 30560) loss: 2.304684\n",
      "(Epoch 19 / 80) train acc: 0.076000; val_acc: 0.084000\n",
      "(Iteration 7301 / 30560) loss: 2.304686\n",
      "(Iteration 7401 / 30560) loss: 2.304687\n",
      "(Iteration 7501 / 30560) loss: 2.304684\n",
      "(Iteration 7601 / 30560) loss: 2.304688\n",
      "(Epoch 20 / 80) train acc: 0.112000; val_acc: 0.084000\n",
      "(Iteration 7701 / 30560) loss: 2.304686\n",
      "(Iteration 7801 / 30560) loss: 2.304696\n",
      "(Iteration 7901 / 30560) loss: 2.304683\n",
      "(Iteration 8001 / 30560) loss: 2.304690\n",
      "(Epoch 21 / 80) train acc: 0.084000; val_acc: 0.084000\n",
      "(Iteration 8101 / 30560) loss: 2.304684\n",
      "(Iteration 8201 / 30560) loss: 2.304684\n",
      "(Iteration 8301 / 30560) loss: 2.304693\n",
      "(Iteration 8401 / 30560) loss: 2.304676\n",
      "(Epoch 22 / 80) train acc: 0.072000; val_acc: 0.084000\n",
      "(Iteration 8501 / 30560) loss: 2.304684\n",
      "(Iteration 8601 / 30560) loss: 2.304680\n",
      "(Iteration 8701 / 30560) loss: 2.304684\n",
      "(Epoch 23 / 80) train acc: 0.081000; val_acc: 0.084000\n",
      "(Iteration 8801 / 30560) loss: 2.304684\n",
      "(Iteration 8901 / 30560) loss: 2.304682\n",
      "(Iteration 9001 / 30560) loss: 2.304676\n",
      "(Iteration 9101 / 30560) loss: 2.304693\n",
      "(Epoch 24 / 80) train acc: 0.072000; val_acc: 0.084000\n",
      "(Iteration 9201 / 30560) loss: 2.304677\n",
      "(Iteration 9301 / 30560) loss: 2.304682\n",
      "(Iteration 9401 / 30560) loss: 2.304677\n",
      "(Iteration 9501 / 30560) loss: 2.304690\n",
      "(Epoch 25 / 80) train acc: 0.095000; val_acc: 0.084000\n",
      "(Iteration 9601 / 30560) loss: 2.304682\n",
      "(Iteration 9701 / 30560) loss: 2.304686\n",
      "(Iteration 9801 / 30560) loss: 2.304685\n",
      "(Iteration 9901 / 30560) loss: 2.304688\n",
      "(Epoch 26 / 80) train acc: 0.099000; val_acc: 0.084000\n",
      "(Iteration 10001 / 30560) loss: 2.304681\n",
      "(Iteration 10101 / 30560) loss: 2.304684\n",
      "(Iteration 10201 / 30560) loss: 2.304678\n",
      "(Iteration 10301 / 30560) loss: 2.304686\n",
      "(Epoch 27 / 80) train acc: 0.090000; val_acc: 0.084000\n",
      "(Iteration 10401 / 30560) loss: 2.304687\n",
      "(Iteration 10501 / 30560) loss: 2.304683\n",
      "(Iteration 10601 / 30560) loss: 2.304680\n",
      "(Epoch 28 / 80) train acc: 0.068000; val_acc: 0.084000\n",
      "(Iteration 10701 / 30560) loss: 2.304677\n",
      "(Iteration 10801 / 30560) loss: 2.304682\n",
      "(Iteration 10901 / 30560) loss: 2.304686\n",
      "(Iteration 11001 / 30560) loss: 2.304683\n",
      "(Epoch 29 / 80) train acc: 0.064000; val_acc: 0.084000\n",
      "(Iteration 11101 / 30560) loss: 2.304683\n",
      "(Iteration 11201 / 30560) loss: 2.304685\n",
      "(Iteration 11301 / 30560) loss: 2.304682\n",
      "(Iteration 11401 / 30560) loss: 2.304689\n",
      "(Epoch 30 / 80) train acc: 0.089000; val_acc: 0.084000\n",
      "(Iteration 11501 / 30560) loss: 2.304685\n",
      "(Iteration 11601 / 30560) loss: 2.304683\n",
      "(Iteration 11701 / 30560) loss: 2.304688\n",
      "(Iteration 11801 / 30560) loss: 2.304686\n",
      "(Epoch 31 / 80) train acc: 0.085000; val_acc: 0.084000\n",
      "(Iteration 11901 / 30560) loss: 2.304685\n",
      "(Iteration 12001 / 30560) loss: 2.304684\n",
      "(Iteration 12101 / 30560) loss: 2.304685\n",
      "(Iteration 12201 / 30560) loss: 2.304678\n",
      "(Epoch 32 / 80) train acc: 0.080000; val_acc: 0.084000\n",
      "(Iteration 12301 / 30560) loss: 2.304688\n",
      "(Iteration 12401 / 30560) loss: 2.304686\n",
      "(Iteration 12501 / 30560) loss: 2.304686\n",
      "(Iteration 12601 / 30560) loss: 2.304684\n",
      "(Epoch 33 / 80) train acc: 0.085000; val_acc: 0.084000\n",
      "(Iteration 12701 / 30560) loss: 2.304681\n",
      "(Iteration 12801 / 30560) loss: 2.304683\n",
      "(Iteration 12901 / 30560) loss: 2.304687\n",
      "(Epoch 34 / 80) train acc: 0.084000; val_acc: 0.084000\n",
      "(Iteration 13001 / 30560) loss: 2.304685\n",
      "(Iteration 13101 / 30560) loss: 2.304675\n",
      "(Iteration 13201 / 30560) loss: 2.304688\n",
      "(Iteration 13301 / 30560) loss: 2.304679\n",
      "(Epoch 35 / 80) train acc: 0.088000; val_acc: 0.084000\n",
      "(Iteration 13401 / 30560) loss: 2.304684\n",
      "(Iteration 13501 / 30560) loss: 2.304685\n",
      "(Iteration 13601 / 30560) loss: 2.304688\n",
      "(Iteration 13701 / 30560) loss: 2.304681\n",
      "(Epoch 36 / 80) train acc: 0.076000; val_acc: 0.084000\n",
      "(Iteration 13801 / 30560) loss: 2.304686\n",
      "(Iteration 13901 / 30560) loss: 2.304688\n",
      "(Iteration 14001 / 30560) loss: 2.304692\n",
      "(Iteration 14101 / 30560) loss: 2.304682\n",
      "(Epoch 37 / 80) train acc: 0.088000; val_acc: 0.084000\n",
      "(Iteration 14201 / 30560) loss: 2.304683\n",
      "(Iteration 14301 / 30560) loss: 2.304674\n",
      "(Iteration 14401 / 30560) loss: 2.304685\n",
      "(Iteration 14501 / 30560) loss: 2.304687\n",
      "(Epoch 38 / 80) train acc: 0.081000; val_acc: 0.084000\n",
      "(Iteration 14601 / 30560) loss: 2.304680\n",
      "(Iteration 14701 / 30560) loss: 2.304678\n",
      "(Iteration 14801 / 30560) loss: 2.304683\n",
      "(Epoch 39 / 80) train acc: 0.085000; val_acc: 0.084000\n",
      "(Iteration 14901 / 30560) loss: 2.304683\n",
      "(Iteration 15001 / 30560) loss: 2.304684\n",
      "(Iteration 15101 / 30560) loss: 2.304689\n",
      "(Iteration 15201 / 30560) loss: 2.304688\n",
      "(Epoch 40 / 80) train acc: 0.086000; val_acc: 0.084000\n",
      "(Iteration 15301 / 30560) loss: 2.304688\n",
      "(Iteration 15401 / 30560) loss: 2.304686\n",
      "(Iteration 15501 / 30560) loss: 2.304685\n",
      "(Iteration 15601 / 30560) loss: 2.304683\n",
      "(Epoch 41 / 80) train acc: 0.097000; val_acc: 0.084000\n",
      "(Iteration 15701 / 30560) loss: 2.304673\n",
      "(Iteration 15801 / 30560) loss: 2.304682\n",
      "(Iteration 15901 / 30560) loss: 2.304694\n",
      "(Iteration 16001 / 30560) loss: 2.304688\n",
      "(Epoch 42 / 80) train acc: 0.080000; val_acc: 0.084000\n",
      "(Iteration 16101 / 30560) loss: 2.304695\n",
      "(Iteration 16201 / 30560) loss: 2.304676\n",
      "(Iteration 16301 / 30560) loss: 2.304678\n",
      "(Iteration 16401 / 30560) loss: 2.304695\n",
      "(Epoch 43 / 80) train acc: 0.062000; val_acc: 0.084000\n",
      "(Iteration 16501 / 30560) loss: 2.304683\n",
      "(Iteration 16601 / 30560) loss: 2.304671\n",
      "(Iteration 16701 / 30560) loss: 2.304682\n",
      "(Iteration 16801 / 30560) loss: 2.304678\n",
      "(Epoch 44 / 80) train acc: 0.077000; val_acc: 0.084000\n",
      "(Iteration 16901 / 30560) loss: 2.304691\n",
      "(Iteration 17001 / 30560) loss: 2.304689\n",
      "(Iteration 17101 / 30560) loss: 2.304680\n",
      "(Epoch 45 / 80) train acc: 0.101000; val_acc: 0.084000\n",
      "(Iteration 17201 / 30560) loss: 2.304693\n",
      "(Iteration 17301 / 30560) loss: 2.304686\n",
      "(Iteration 17401 / 30560) loss: 2.304683\n",
      "(Iteration 17501 / 30560) loss: 2.304693\n",
      "(Epoch 46 / 80) train acc: 0.102000; val_acc: 0.084000\n",
      "(Iteration 17601 / 30560) loss: 2.304685\n",
      "(Iteration 17701 / 30560) loss: 2.304683\n",
      "(Iteration 17801 / 30560) loss: 2.304691\n",
      "(Iteration 17901 / 30560) loss: 2.304681\n",
      "(Epoch 47 / 80) train acc: 0.084000; val_acc: 0.084000\n",
      "(Iteration 18001 / 30560) loss: 2.304685\n",
      "(Iteration 18101 / 30560) loss: 2.304678\n",
      "(Iteration 18201 / 30560) loss: 2.304690\n",
      "(Iteration 18301 / 30560) loss: 2.304685\n",
      "(Epoch 48 / 80) train acc: 0.093000; val_acc: 0.084000\n",
      "(Iteration 18401 / 30560) loss: 2.304689\n",
      "(Iteration 18501 / 30560) loss: 2.304696\n",
      "(Iteration 18601 / 30560) loss: 2.304686\n",
      "(Iteration 18701 / 30560) loss: 2.304685\n",
      "(Epoch 49 / 80) train acc: 0.083000; val_acc: 0.084000\n",
      "(Iteration 18801 / 30560) loss: 2.304685\n",
      "(Iteration 18901 / 30560) loss: 2.304691\n",
      "(Iteration 19001 / 30560) loss: 2.304686\n",
      "(Epoch 50 / 80) train acc: 0.075000; val_acc: 0.084000\n",
      "(Iteration 19101 / 30560) loss: 2.304671\n",
      "(Iteration 19201 / 30560) loss: 2.304681\n",
      "(Iteration 19301 / 30560) loss: 2.304690\n",
      "(Iteration 19401 / 30560) loss: 2.304679\n",
      "(Epoch 51 / 80) train acc: 0.095000; val_acc: 0.084000\n",
      "(Iteration 19501 / 30560) loss: 2.304684\n",
      "(Iteration 19601 / 30560) loss: 2.304683\n",
      "(Iteration 19701 / 30560) loss: 2.304682\n",
      "(Iteration 19801 / 30560) loss: 2.304682\n",
      "(Epoch 52 / 80) train acc: 0.086000; val_acc: 0.084000\n",
      "(Iteration 19901 / 30560) loss: 2.304686\n",
      "(Iteration 20001 / 30560) loss: 2.304688\n",
      "(Iteration 20101 / 30560) loss: 2.304680\n",
      "(Iteration 20201 / 30560) loss: 2.304676\n",
      "(Epoch 53 / 80) train acc: 0.086000; val_acc: 0.084000\n",
      "(Iteration 20301 / 30560) loss: 2.304684\n",
      "(Iteration 20401 / 30560) loss: 2.304682\n",
      "(Iteration 20501 / 30560) loss: 2.304685\n",
      "(Iteration 20601 / 30560) loss: 2.304689\n",
      "(Epoch 54 / 80) train acc: 0.095000; val_acc: 0.084000\n",
      "(Iteration 20701 / 30560) loss: 2.304683\n",
      "(Iteration 20801 / 30560) loss: 2.304681\n",
      "(Iteration 20901 / 30560) loss: 2.304679\n",
      "(Iteration 21001 / 30560) loss: 2.304680\n",
      "(Epoch 55 / 80) train acc: 0.085000; val_acc: 0.084000\n",
      "(Iteration 21101 / 30560) loss: 2.304678\n",
      "(Iteration 21201 / 30560) loss: 2.304692\n",
      "(Iteration 21301 / 30560) loss: 2.304691\n",
      "(Epoch 56 / 80) train acc: 0.090000; val_acc: 0.084000\n",
      "(Iteration 21401 / 30560) loss: 2.304685\n",
      "(Iteration 21501 / 30560) loss: 2.304684\n",
      "(Iteration 21601 / 30560) loss: 2.304682\n",
      "(Iteration 21701 / 30560) loss: 2.304693\n",
      "(Epoch 57 / 80) train acc: 0.091000; val_acc: 0.084000\n",
      "(Iteration 21801 / 30560) loss: 2.304690\n",
      "(Iteration 21901 / 30560) loss: 2.304673\n",
      "(Iteration 22001 / 30560) loss: 2.304686\n",
      "(Iteration 22101 / 30560) loss: 2.304684\n",
      "(Epoch 58 / 80) train acc: 0.094000; val_acc: 0.084000\n",
      "(Iteration 22201 / 30560) loss: 2.304682\n",
      "(Iteration 22301 / 30560) loss: 2.304687\n",
      "(Iteration 22401 / 30560) loss: 2.304682\n",
      "(Iteration 22501 / 30560) loss: 2.304689\n",
      "(Epoch 59 / 80) train acc: 0.075000; val_acc: 0.084000\n",
      "(Iteration 22601 / 30560) loss: 2.304678\n",
      "(Iteration 22701 / 30560) loss: 2.304679\n",
      "(Iteration 22801 / 30560) loss: 2.304679\n",
      "(Iteration 22901 / 30560) loss: 2.304686\n",
      "(Epoch 60 / 80) train acc: 0.083000; val_acc: 0.084000\n",
      "(Iteration 23001 / 30560) loss: 2.304682\n",
      "(Iteration 23101 / 30560) loss: 2.304684\n",
      "(Iteration 23201 / 30560) loss: 2.304675\n",
      "(Iteration 23301 / 30560) loss: 2.304683\n",
      "(Epoch 61 / 80) train acc: 0.094000; val_acc: 0.084000\n",
      "(Iteration 23401 / 30560) loss: 2.304673\n",
      "(Iteration 23501 / 30560) loss: 2.304694\n",
      "(Iteration 23601 / 30560) loss: 2.304687\n",
      "(Epoch 62 / 80) train acc: 0.090000; val_acc: 0.084000\n",
      "(Iteration 23701 / 30560) loss: 2.304677\n",
      "(Iteration 23801 / 30560) loss: 2.304686\n",
      "(Iteration 23901 / 30560) loss: 2.304685\n",
      "(Iteration 24001 / 30560) loss: 2.304679\n",
      "(Epoch 63 / 80) train acc: 0.077000; val_acc: 0.084000\n",
      "(Iteration 24101 / 30560) loss: 2.304678\n",
      "(Iteration 24201 / 30560) loss: 2.304683\n",
      "(Iteration 24301 / 30560) loss: 2.304689\n",
      "(Iteration 24401 / 30560) loss: 2.304684\n",
      "(Epoch 64 / 80) train acc: 0.067000; val_acc: 0.084000\n",
      "(Iteration 24501 / 30560) loss: 2.304680\n",
      "(Iteration 24601 / 30560) loss: 2.304696\n",
      "(Iteration 24701 / 30560) loss: 2.304692\n",
      "(Iteration 24801 / 30560) loss: 2.304682\n",
      "(Epoch 65 / 80) train acc: 0.088000; val_acc: 0.084000\n",
      "(Iteration 24901 / 30560) loss: 2.304673\n",
      "(Iteration 25001 / 30560) loss: 2.304683\n",
      "(Iteration 25101 / 30560) loss: 2.304686\n",
      "(Iteration 25201 / 30560) loss: 2.304682\n",
      "(Epoch 66 / 80) train acc: 0.102000; val_acc: 0.084000\n",
      "(Iteration 25301 / 30560) loss: 2.304682\n",
      "(Iteration 25401 / 30560) loss: 2.304682\n",
      "(Iteration 25501 / 30560) loss: 2.304684\n",
      "(Epoch 67 / 80) train acc: 0.101000; val_acc: 0.084000\n",
      "(Iteration 25601 / 30560) loss: 2.304692\n",
      "(Iteration 25701 / 30560) loss: 2.304684\n",
      "(Iteration 25801 / 30560) loss: 2.304689\n",
      "(Iteration 25901 / 30560) loss: 2.304679\n",
      "(Epoch 68 / 80) train acc: 0.091000; val_acc: 0.084000\n",
      "(Iteration 26001 / 30560) loss: 2.304680\n",
      "(Iteration 26101 / 30560) loss: 2.304687\n",
      "(Iteration 26201 / 30560) loss: 2.304684\n",
      "(Iteration 26301 / 30560) loss: 2.304680\n",
      "(Epoch 69 / 80) train acc: 0.093000; val_acc: 0.084000\n",
      "(Iteration 26401 / 30560) loss: 2.304682\n",
      "(Iteration 26501 / 30560) loss: 2.304686\n",
      "(Iteration 26601 / 30560) loss: 2.304674\n",
      "(Iteration 26701 / 30560) loss: 2.304688\n",
      "(Epoch 70 / 80) train acc: 0.103000; val_acc: 0.084000\n",
      "(Iteration 26801 / 30560) loss: 2.304689\n",
      "(Iteration 26901 / 30560) loss: 2.304701\n",
      "(Iteration 27001 / 30560) loss: 2.304684\n",
      "(Iteration 27101 / 30560) loss: 2.304684\n",
      "(Epoch 71 / 80) train acc: 0.097000; val_acc: 0.084000\n",
      "(Iteration 27201 / 30560) loss: 2.304682\n",
      "(Iteration 27301 / 30560) loss: 2.304688\n",
      "(Iteration 27401 / 30560) loss: 2.304689\n",
      "(Iteration 27501 / 30560) loss: 2.304684\n",
      "(Epoch 72 / 80) train acc: 0.090000; val_acc: 0.084000\n",
      "(Iteration 27601 / 30560) loss: 2.304681\n",
      "(Iteration 27701 / 30560) loss: 2.304685\n",
      "(Iteration 27801 / 30560) loss: 2.304694\n",
      "(Epoch 73 / 80) train acc: 0.074000; val_acc: 0.084000\n",
      "(Iteration 27901 / 30560) loss: 2.304686\n",
      "(Iteration 28001 / 30560) loss: 2.304692\n",
      "(Iteration 28101 / 30560) loss: 2.304688\n",
      "(Iteration 28201 / 30560) loss: 2.304679\n",
      "(Epoch 74 / 80) train acc: 0.070000; val_acc: 0.084000\n",
      "(Iteration 28301 / 30560) loss: 2.304684\n",
      "(Iteration 28401 / 30560) loss: 2.304683\n",
      "(Iteration 28501 / 30560) loss: 2.304692\n",
      "(Iteration 28601 / 30560) loss: 2.304676\n",
      "(Epoch 75 / 80) train acc: 0.070000; val_acc: 0.084000\n",
      "(Iteration 28701 / 30560) loss: 2.304683\n",
      "(Iteration 28801 / 30560) loss: 2.304683\n",
      "(Iteration 28901 / 30560) loss: 2.304674\n",
      "(Iteration 29001 / 30560) loss: 2.304682\n",
      "(Epoch 76 / 80) train acc: 0.088000; val_acc: 0.084000\n",
      "(Iteration 29101 / 30560) loss: 2.304681\n",
      "(Iteration 29201 / 30560) loss: 2.304685\n",
      "(Iteration 29301 / 30560) loss: 2.304684\n",
      "(Iteration 29401 / 30560) loss: 2.304684\n",
      "(Epoch 77 / 80) train acc: 0.085000; val_acc: 0.084000\n",
      "(Iteration 29501 / 30560) loss: 2.304688\n",
      "(Iteration 29601 / 30560) loss: 2.304687\n",
      "(Iteration 29701 / 30560) loss: 2.304679\n",
      "(Epoch 78 / 80) train acc: 0.084000; val_acc: 0.084000\n",
      "(Iteration 29801 / 30560) loss: 2.304679\n",
      "(Iteration 29901 / 30560) loss: 2.304690\n",
      "(Iteration 30001 / 30560) loss: 2.304681\n",
      "(Iteration 30101 / 30560) loss: 2.304692\n",
      "(Epoch 79 / 80) train acc: 0.101000; val_acc: 0.084000\n",
      "(Iteration 30201 / 30560) loss: 2.304679\n",
      "(Iteration 30301 / 30560) loss: 2.304687\n",
      "(Iteration 30401 / 30560) loss: 2.304684\n",
      "(Iteration 30501 / 30560) loss: 2.304690\n",
      "(Epoch 80 / 80) train acc: 0.089000; val_acc: 0.084000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.304643\n",
      "(Epoch 0 / 80) train acc: 0.119000; val_acc: 0.150000\n",
      "(Iteration 101 / 61200) loss: 2.304640\n",
      "(Iteration 201 / 61200) loss: 2.304649\n",
      "(Iteration 301 / 61200) loss: 2.304649\n",
      "(Iteration 401 / 61200) loss: 2.304663\n",
      "(Iteration 501 / 61200) loss: 2.304634\n",
      "(Iteration 601 / 61200) loss: 2.304653\n",
      "(Iteration 701 / 61200) loss: 2.304637\n",
      "(Epoch 1 / 80) train acc: 0.121000; val_acc: 0.150000\n",
      "(Iteration 801 / 61200) loss: 2.304642\n",
      "(Iteration 901 / 61200) loss: 2.304640\n",
      "(Iteration 1001 / 61200) loss: 2.304645\n",
      "(Iteration 1101 / 61200) loss: 2.304647\n",
      "(Iteration 1201 / 61200) loss: 2.304637\n",
      "(Iteration 1301 / 61200) loss: 2.304651\n",
      "(Iteration 1401 / 61200) loss: 2.304642\n",
      "(Iteration 1501 / 61200) loss: 2.304661\n",
      "(Epoch 2 / 80) train acc: 0.120000; val_acc: 0.151000\n",
      "(Iteration 1601 / 61200) loss: 2.304659\n",
      "(Iteration 1701 / 61200) loss: 2.304652\n",
      "(Iteration 1801 / 61200) loss: 2.304654\n",
      "(Iteration 1901 / 61200) loss: 2.304642\n",
      "(Iteration 2001 / 61200) loss: 2.304637\n",
      "(Iteration 2101 / 61200) loss: 2.304658\n",
      "(Iteration 2201 / 61200) loss: 2.304643\n",
      "(Epoch 3 / 80) train acc: 0.146000; val_acc: 0.151000\n",
      "(Iteration 2301 / 61200) loss: 2.304654\n",
      "(Iteration 2401 / 61200) loss: 2.304637\n",
      "(Iteration 2501 / 61200) loss: 2.304649\n",
      "(Iteration 2601 / 61200) loss: 2.304649\n",
      "(Iteration 2701 / 61200) loss: 2.304658\n",
      "(Iteration 2801 / 61200) loss: 2.304653\n",
      "(Iteration 2901 / 61200) loss: 2.304645\n",
      "(Iteration 3001 / 61200) loss: 2.304659\n",
      "(Epoch 4 / 80) train acc: 0.128000; val_acc: 0.150000\n",
      "(Iteration 3101 / 61200) loss: 2.304646\n",
      "(Iteration 3201 / 61200) loss: 2.304651\n",
      "(Iteration 3301 / 61200) loss: 2.304641\n",
      "(Iteration 3401 / 61200) loss: 2.304661\n",
      "(Iteration 3501 / 61200) loss: 2.304662\n",
      "(Iteration 3601 / 61200) loss: 2.304647\n",
      "(Iteration 3701 / 61200) loss: 2.304652\n",
      "(Iteration 3801 / 61200) loss: 2.304653\n",
      "(Epoch 5 / 80) train acc: 0.104000; val_acc: 0.150000\n",
      "(Iteration 3901 / 61200) loss: 2.304644\n",
      "(Iteration 4001 / 61200) loss: 2.304640\n",
      "(Iteration 4101 / 61200) loss: 2.304640\n",
      "(Iteration 4201 / 61200) loss: 2.304655\n",
      "(Iteration 4301 / 61200) loss: 2.304638\n",
      "(Iteration 4401 / 61200) loss: 2.304660\n",
      "(Iteration 4501 / 61200) loss: 2.304646\n",
      "(Epoch 6 / 80) train acc: 0.124000; val_acc: 0.150000\n",
      "(Iteration 4601 / 61200) loss: 2.304637\n",
      "(Iteration 4701 / 61200) loss: 2.304659\n",
      "(Iteration 4801 / 61200) loss: 2.304641\n",
      "(Iteration 4901 / 61200) loss: 2.304644\n",
      "(Iteration 5001 / 61200) loss: 2.304649\n",
      "(Iteration 5101 / 61200) loss: 2.304664\n",
      "(Iteration 5201 / 61200) loss: 2.304643\n",
      "(Iteration 5301 / 61200) loss: 2.304646\n",
      "(Epoch 7 / 80) train acc: 0.112000; val_acc: 0.152000\n",
      "(Iteration 5401 / 61200) loss: 2.304642\n",
      "(Iteration 5501 / 61200) loss: 2.304645\n",
      "(Iteration 5601 / 61200) loss: 2.304641\n",
      "(Iteration 5701 / 61200) loss: 2.304644\n",
      "(Iteration 5801 / 61200) loss: 2.304648\n",
      "(Iteration 5901 / 61200) loss: 2.304640\n",
      "(Iteration 6001 / 61200) loss: 2.304650\n",
      "(Iteration 6101 / 61200) loss: 2.304654\n",
      "(Epoch 8 / 80) train acc: 0.127000; val_acc: 0.152000\n",
      "(Iteration 6201 / 61200) loss: 2.304658\n",
      "(Iteration 6301 / 61200) loss: 2.304634\n",
      "(Iteration 6401 / 61200) loss: 2.304664\n",
      "(Iteration 6501 / 61200) loss: 2.304639\n",
      "(Iteration 6601 / 61200) loss: 2.304649\n",
      "(Iteration 6701 / 61200) loss: 2.304632\n",
      "(Iteration 6801 / 61200) loss: 2.304645\n",
      "(Epoch 9 / 80) train acc: 0.133000; val_acc: 0.152000\n",
      "(Iteration 6901 / 61200) loss: 2.304636\n",
      "(Iteration 7001 / 61200) loss: 2.304643\n",
      "(Iteration 7101 / 61200) loss: 2.304648\n",
      "(Iteration 7201 / 61200) loss: 2.304651\n",
      "(Iteration 7301 / 61200) loss: 2.304660\n",
      "(Iteration 7401 / 61200) loss: 2.304657\n",
      "(Iteration 7501 / 61200) loss: 2.304649\n",
      "(Iteration 7601 / 61200) loss: 2.304634\n",
      "(Epoch 10 / 80) train acc: 0.115000; val_acc: 0.151000\n",
      "(Iteration 7701 / 61200) loss: 2.304650\n",
      "(Iteration 7801 / 61200) loss: 2.304641\n",
      "(Iteration 7901 / 61200) loss: 2.304669\n",
      "(Iteration 8001 / 61200) loss: 2.304654\n",
      "(Iteration 8101 / 61200) loss: 2.304643\n",
      "(Iteration 8201 / 61200) loss: 2.304640\n",
      "(Iteration 8301 / 61200) loss: 2.304635\n",
      "(Iteration 8401 / 61200) loss: 2.304658\n",
      "(Epoch 11 / 80) train acc: 0.115000; val_acc: 0.151000\n",
      "(Iteration 8501 / 61200) loss: 2.304647\n",
      "(Iteration 8601 / 61200) loss: 2.304644\n",
      "(Iteration 8701 / 61200) loss: 2.304654\n",
      "(Iteration 8801 / 61200) loss: 2.304640\n",
      "(Iteration 8901 / 61200) loss: 2.304643\n",
      "(Iteration 9001 / 61200) loss: 2.304644\n",
      "(Iteration 9101 / 61200) loss: 2.304644\n",
      "(Epoch 12 / 80) train acc: 0.109000; val_acc: 0.151000\n",
      "(Iteration 9201 / 61200) loss: 2.304651\n",
      "(Iteration 9301 / 61200) loss: 2.304644\n",
      "(Iteration 9401 / 61200) loss: 2.304653\n",
      "(Iteration 9501 / 61200) loss: 2.304665\n",
      "(Iteration 9601 / 61200) loss: 2.304646\n",
      "(Iteration 9701 / 61200) loss: 2.304653\n",
      "(Iteration 9801 / 61200) loss: 2.304636\n",
      "(Iteration 9901 / 61200) loss: 2.304656\n",
      "(Epoch 13 / 80) train acc: 0.108000; val_acc: 0.150000\n",
      "(Iteration 10001 / 61200) loss: 2.304649\n",
      "(Iteration 10101 / 61200) loss: 2.304625\n",
      "(Iteration 10201 / 61200) loss: 2.304658\n",
      "(Iteration 10301 / 61200) loss: 2.304641\n",
      "(Iteration 10401 / 61200) loss: 2.304636\n",
      "(Iteration 10501 / 61200) loss: 2.304648\n",
      "(Iteration 10601 / 61200) loss: 2.304645\n",
      "(Iteration 10701 / 61200) loss: 2.304653\n",
      "(Epoch 14 / 80) train acc: 0.135000; val_acc: 0.150000\n",
      "(Iteration 10801 / 61200) loss: 2.304655\n",
      "(Iteration 10901 / 61200) loss: 2.304640\n",
      "(Iteration 11001 / 61200) loss: 2.304647\n",
      "(Iteration 11101 / 61200) loss: 2.304660\n",
      "(Iteration 11201 / 61200) loss: 2.304644\n",
      "(Iteration 11301 / 61200) loss: 2.304644\n",
      "(Iteration 11401 / 61200) loss: 2.304643\n",
      "(Epoch 15 / 80) train acc: 0.120000; val_acc: 0.150000\n",
      "(Iteration 11501 / 61200) loss: 2.304654\n",
      "(Iteration 11601 / 61200) loss: 2.304648\n",
      "(Iteration 11701 / 61200) loss: 2.304651\n",
      "(Iteration 11801 / 61200) loss: 2.304639\n",
      "(Iteration 11901 / 61200) loss: 2.304654\n",
      "(Iteration 12001 / 61200) loss: 2.304645\n",
      "(Iteration 12101 / 61200) loss: 2.304656\n",
      "(Iteration 12201 / 61200) loss: 2.304646\n",
      "(Epoch 16 / 80) train acc: 0.105000; val_acc: 0.151000\n",
      "(Iteration 12301 / 61200) loss: 2.304646\n",
      "(Iteration 12401 / 61200) loss: 2.304645\n",
      "(Iteration 12501 / 61200) loss: 2.304651\n",
      "(Iteration 12601 / 61200) loss: 2.304644\n",
      "(Iteration 12701 / 61200) loss: 2.304644\n",
      "(Iteration 12801 / 61200) loss: 2.304642\n",
      "(Iteration 12901 / 61200) loss: 2.304659\n",
      "(Iteration 13001 / 61200) loss: 2.304639\n",
      "(Epoch 17 / 80) train acc: 0.123000; val_acc: 0.151000\n",
      "(Iteration 13101 / 61200) loss: 2.304644\n",
      "(Iteration 13201 / 61200) loss: 2.304649\n",
      "(Iteration 13301 / 61200) loss: 2.304650\n",
      "(Iteration 13401 / 61200) loss: 2.304652\n",
      "(Iteration 13501 / 61200) loss: 2.304633\n",
      "(Iteration 13601 / 61200) loss: 2.304652\n",
      "(Iteration 13701 / 61200) loss: 2.304652\n",
      "(Epoch 18 / 80) train acc: 0.138000; val_acc: 0.150000\n",
      "(Iteration 13801 / 61200) loss: 2.304646\n",
      "(Iteration 13901 / 61200) loss: 2.304657\n",
      "(Iteration 14001 / 61200) loss: 2.304645\n",
      "(Iteration 14101 / 61200) loss: 2.304649\n",
      "(Iteration 14201 / 61200) loss: 2.304660\n",
      "(Iteration 14301 / 61200) loss: 2.304639\n",
      "(Iteration 14401 / 61200) loss: 2.304639\n",
      "(Iteration 14501 / 61200) loss: 2.304643\n",
      "(Epoch 19 / 80) train acc: 0.131000; val_acc: 0.151000\n",
      "(Iteration 14601 / 61200) loss: 2.304650\n",
      "(Iteration 14701 / 61200) loss: 2.304646\n",
      "(Iteration 14801 / 61200) loss: 2.304653\n",
      "(Iteration 14901 / 61200) loss: 2.304651\n",
      "(Iteration 15001 / 61200) loss: 2.304642\n",
      "(Iteration 15101 / 61200) loss: 2.304650\n",
      "(Iteration 15201 / 61200) loss: 2.304645\n",
      "(Epoch 20 / 80) train acc: 0.134000; val_acc: 0.151000\n",
      "(Iteration 15301 / 61200) loss: 2.304651\n",
      "(Iteration 15401 / 61200) loss: 2.304650\n",
      "(Iteration 15501 / 61200) loss: 2.304640\n",
      "(Iteration 15601 / 61200) loss: 2.304654\n",
      "(Iteration 15701 / 61200) loss: 2.304654\n",
      "(Iteration 15801 / 61200) loss: 2.304630\n",
      "(Iteration 15901 / 61200) loss: 2.304631\n",
      "(Iteration 16001 / 61200) loss: 2.304660\n",
      "(Epoch 21 / 80) train acc: 0.130000; val_acc: 0.150000\n",
      "(Iteration 16101 / 61200) loss: 2.304637\n",
      "(Iteration 16201 / 61200) loss: 2.304658\n",
      "(Iteration 16301 / 61200) loss: 2.304652\n",
      "(Iteration 16401 / 61200) loss: 2.304647\n",
      "(Iteration 16501 / 61200) loss: 2.304647\n",
      "(Iteration 16601 / 61200) loss: 2.304630\n",
      "(Iteration 16701 / 61200) loss: 2.304642\n",
      "(Iteration 16801 / 61200) loss: 2.304669\n",
      "(Epoch 22 / 80) train acc: 0.114000; val_acc: 0.151000\n",
      "(Iteration 16901 / 61200) loss: 2.304644\n",
      "(Iteration 17001 / 61200) loss: 2.304650\n",
      "(Iteration 17101 / 61200) loss: 2.304639\n",
      "(Iteration 17201 / 61200) loss: 2.304656\n",
      "(Iteration 17301 / 61200) loss: 2.304648\n",
      "(Iteration 17401 / 61200) loss: 2.304654\n",
      "(Iteration 17501 / 61200) loss: 2.304650\n",
      "(Epoch 23 / 80) train acc: 0.130000; val_acc: 0.152000\n",
      "(Iteration 17601 / 61200) loss: 2.304635\n",
      "(Iteration 17701 / 61200) loss: 2.304649\n",
      "(Iteration 17801 / 61200) loss: 2.304643\n",
      "(Iteration 17901 / 61200) loss: 2.304633\n",
      "(Iteration 18001 / 61200) loss: 2.304662\n",
      "(Iteration 18101 / 61200) loss: 2.304638\n",
      "(Iteration 18201 / 61200) loss: 2.304650\n",
      "(Iteration 18301 / 61200) loss: 2.304661\n",
      "(Epoch 24 / 80) train acc: 0.128000; val_acc: 0.152000\n",
      "(Iteration 18401 / 61200) loss: 2.304642\n",
      "(Iteration 18501 / 61200) loss: 2.304630\n",
      "(Iteration 18601 / 61200) loss: 2.304643\n",
      "(Iteration 18701 / 61200) loss: 2.304653\n",
      "(Iteration 18801 / 61200) loss: 2.304652\n",
      "(Iteration 18901 / 61200) loss: 2.304645\n",
      "(Iteration 19001 / 61200) loss: 2.304645\n",
      "(Iteration 19101 / 61200) loss: 2.304640\n",
      "(Epoch 25 / 80) train acc: 0.126000; val_acc: 0.152000\n",
      "(Iteration 19201 / 61200) loss: 2.304662\n",
      "(Iteration 19301 / 61200) loss: 2.304626\n",
      "(Iteration 19401 / 61200) loss: 2.304648\n",
      "(Iteration 19501 / 61200) loss: 2.304643\n",
      "(Iteration 19601 / 61200) loss: 2.304643\n",
      "(Iteration 19701 / 61200) loss: 2.304653\n",
      "(Iteration 19801 / 61200) loss: 2.304638\n",
      "(Epoch 26 / 80) train acc: 0.137000; val_acc: 0.151000\n",
      "(Iteration 19901 / 61200) loss: 2.304653\n",
      "(Iteration 20001 / 61200) loss: 2.304650\n",
      "(Iteration 20101 / 61200) loss: 2.304642\n",
      "(Iteration 20201 / 61200) loss: 2.304642\n",
      "(Iteration 20301 / 61200) loss: 2.304637\n",
      "(Iteration 20401 / 61200) loss: 2.304642\n",
      "(Iteration 20501 / 61200) loss: 2.304647\n",
      "(Iteration 20601 / 61200) loss: 2.304639\n",
      "(Epoch 27 / 80) train acc: 0.108000; val_acc: 0.151000\n",
      "(Iteration 20701 / 61200) loss: 2.304641\n",
      "(Iteration 20801 / 61200) loss: 2.304657\n",
      "(Iteration 20901 / 61200) loss: 2.304644\n",
      "(Iteration 21001 / 61200) loss: 2.304640\n",
      "(Iteration 21101 / 61200) loss: 2.304649\n",
      "(Iteration 21201 / 61200) loss: 2.304648\n",
      "(Iteration 21301 / 61200) loss: 2.304658\n",
      "(Iteration 21401 / 61200) loss: 2.304644\n",
      "(Epoch 28 / 80) train acc: 0.122000; val_acc: 0.151000\n",
      "(Iteration 21501 / 61200) loss: 2.304635\n",
      "(Iteration 21601 / 61200) loss: 2.304651\n",
      "(Iteration 21701 / 61200) loss: 2.304636\n",
      "(Iteration 21801 / 61200) loss: 2.304653\n",
      "(Iteration 21901 / 61200) loss: 2.304649\n",
      "(Iteration 22001 / 61200) loss: 2.304646\n",
      "(Iteration 22101 / 61200) loss: 2.304658\n",
      "(Epoch 29 / 80) train acc: 0.121000; val_acc: 0.152000\n",
      "(Iteration 22201 / 61200) loss: 2.304643\n",
      "(Iteration 22301 / 61200) loss: 2.304640\n",
      "(Iteration 22401 / 61200) loss: 2.304636\n",
      "(Iteration 22501 / 61200) loss: 2.304663\n",
      "(Iteration 22601 / 61200) loss: 2.304649\n",
      "(Iteration 22701 / 61200) loss: 2.304645\n",
      "(Iteration 22801 / 61200) loss: 2.304653\n",
      "(Iteration 22901 / 61200) loss: 2.304646\n",
      "(Epoch 30 / 80) train acc: 0.130000; val_acc: 0.151000\n",
      "(Iteration 23001 / 61200) loss: 2.304647\n",
      "(Iteration 23101 / 61200) loss: 2.304646\n",
      "(Iteration 23201 / 61200) loss: 2.304662\n",
      "(Iteration 23301 / 61200) loss: 2.304647\n",
      "(Iteration 23401 / 61200) loss: 2.304648\n",
      "(Iteration 23501 / 61200) loss: 2.304642\n",
      "(Iteration 23601 / 61200) loss: 2.304663\n",
      "(Iteration 23701 / 61200) loss: 2.304649\n",
      "(Epoch 31 / 80) train acc: 0.123000; val_acc: 0.152000\n",
      "(Iteration 23801 / 61200) loss: 2.304643\n",
      "(Iteration 23901 / 61200) loss: 2.304644\n",
      "(Iteration 24001 / 61200) loss: 2.304639\n",
      "(Iteration 24101 / 61200) loss: 2.304637\n",
      "(Iteration 24201 / 61200) loss: 2.304660\n",
      "(Iteration 24301 / 61200) loss: 2.304639\n",
      "(Iteration 24401 / 61200) loss: 2.304641\n",
      "(Epoch 32 / 80) train acc: 0.114000; val_acc: 0.152000\n",
      "(Iteration 24501 / 61200) loss: 2.304659\n",
      "(Iteration 24601 / 61200) loss: 2.304660\n",
      "(Iteration 24701 / 61200) loss: 2.304656\n",
      "(Iteration 24801 / 61200) loss: 2.304639\n",
      "(Iteration 24901 / 61200) loss: 2.304635\n",
      "(Iteration 25001 / 61200) loss: 2.304660\n",
      "(Iteration 25101 / 61200) loss: 2.304656\n",
      "(Iteration 25201 / 61200) loss: 2.304635\n",
      "(Epoch 33 / 80) train acc: 0.119000; val_acc: 0.152000\n",
      "(Iteration 25301 / 61200) loss: 2.304641\n",
      "(Iteration 25401 / 61200) loss: 2.304644\n",
      "(Iteration 25501 / 61200) loss: 2.304642\n",
      "(Iteration 25601 / 61200) loss: 2.304636\n",
      "(Iteration 25701 / 61200) loss: 2.304647\n",
      "(Iteration 25801 / 61200) loss: 2.304636\n",
      "(Iteration 25901 / 61200) loss: 2.304644\n",
      "(Iteration 26001 / 61200) loss: 2.304641\n",
      "(Epoch 34 / 80) train acc: 0.136000; val_acc: 0.152000\n",
      "(Iteration 26101 / 61200) loss: 2.304654\n",
      "(Iteration 26201 / 61200) loss: 2.304645\n",
      "(Iteration 26301 / 61200) loss: 2.304647\n",
      "(Iteration 26401 / 61200) loss: 2.304662\n",
      "(Iteration 26501 / 61200) loss: 2.304623\n",
      "(Iteration 26601 / 61200) loss: 2.304654\n",
      "(Iteration 26701 / 61200) loss: 2.304632\n",
      "(Epoch 35 / 80) train acc: 0.106000; val_acc: 0.152000\n",
      "(Iteration 26801 / 61200) loss: 2.304646\n",
      "(Iteration 26901 / 61200) loss: 2.304647\n",
      "(Iteration 27001 / 61200) loss: 2.304654\n",
      "(Iteration 27101 / 61200) loss: 2.304650\n",
      "(Iteration 27201 / 61200) loss: 2.304632\n",
      "(Iteration 27301 / 61200) loss: 2.304657\n",
      "(Iteration 27401 / 61200) loss: 2.304636\n",
      "(Iteration 27501 / 61200) loss: 2.304652\n",
      "(Epoch 36 / 80) train acc: 0.129000; val_acc: 0.152000\n",
      "(Iteration 27601 / 61200) loss: 2.304645\n",
      "(Iteration 27701 / 61200) loss: 2.304644\n",
      "(Iteration 27801 / 61200) loss: 2.304643\n",
      "(Iteration 27901 / 61200) loss: 2.304637\n",
      "(Iteration 28001 / 61200) loss: 2.304645\n",
      "(Iteration 28101 / 61200) loss: 2.304648\n",
      "(Iteration 28201 / 61200) loss: 2.304640\n",
      "(Iteration 28301 / 61200) loss: 2.304652\n",
      "(Epoch 37 / 80) train acc: 0.092000; val_acc: 0.152000\n",
      "(Iteration 28401 / 61200) loss: 2.304654\n",
      "(Iteration 28501 / 61200) loss: 2.304634\n",
      "(Iteration 28601 / 61200) loss: 2.304641\n",
      "(Iteration 28701 / 61200) loss: 2.304651\n",
      "(Iteration 28801 / 61200) loss: 2.304651\n",
      "(Iteration 28901 / 61200) loss: 2.304634\n",
      "(Iteration 29001 / 61200) loss: 2.304657\n",
      "(Epoch 38 / 80) train acc: 0.131000; val_acc: 0.152000\n",
      "(Iteration 29101 / 61200) loss: 2.304639\n",
      "(Iteration 29201 / 61200) loss: 2.304663\n",
      "(Iteration 29301 / 61200) loss: 2.304654\n",
      "(Iteration 29401 / 61200) loss: 2.304644\n",
      "(Iteration 29501 / 61200) loss: 2.304659\n",
      "(Iteration 29601 / 61200) loss: 2.304647\n",
      "(Iteration 29701 / 61200) loss: 2.304651\n",
      "(Iteration 29801 / 61200) loss: 2.304638\n",
      "(Epoch 39 / 80) train acc: 0.118000; val_acc: 0.151000\n",
      "(Iteration 29901 / 61200) loss: 2.304635\n",
      "(Iteration 30001 / 61200) loss: 2.304655\n",
      "(Iteration 30101 / 61200) loss: 2.304647\n",
      "(Iteration 30201 / 61200) loss: 2.304647\n",
      "(Iteration 30301 / 61200) loss: 2.304641\n",
      "(Iteration 30401 / 61200) loss: 2.304651\n",
      "(Iteration 30501 / 61200) loss: 2.304655\n",
      "(Epoch 40 / 80) train acc: 0.123000; val_acc: 0.151000\n",
      "(Iteration 30601 / 61200) loss: 2.304640\n",
      "(Iteration 30701 / 61200) loss: 2.304654\n",
      "(Iteration 30801 / 61200) loss: 2.304638\n",
      "(Iteration 30901 / 61200) loss: 2.304634\n",
      "(Iteration 31001 / 61200) loss: 2.304648\n",
      "(Iteration 31101 / 61200) loss: 2.304638\n",
      "(Iteration 31201 / 61200) loss: 2.304642\n",
      "(Iteration 31301 / 61200) loss: 2.304635\n",
      "(Epoch 41 / 80) train acc: 0.128000; val_acc: 0.151000\n",
      "(Iteration 31401 / 61200) loss: 2.304653\n",
      "(Iteration 31501 / 61200) loss: 2.304650\n",
      "(Iteration 31601 / 61200) loss: 2.304638\n",
      "(Iteration 31701 / 61200) loss: 2.304651\n",
      "(Iteration 31801 / 61200) loss: 2.304655\n",
      "(Iteration 31901 / 61200) loss: 2.304649\n",
      "(Iteration 32001 / 61200) loss: 2.304633\n",
      "(Iteration 32101 / 61200) loss: 2.304644\n",
      "(Epoch 42 / 80) train acc: 0.118000; val_acc: 0.151000\n",
      "(Iteration 32201 / 61200) loss: 2.304638\n",
      "(Iteration 32301 / 61200) loss: 2.304636\n",
      "(Iteration 32401 / 61200) loss: 2.304652\n",
      "(Iteration 32501 / 61200) loss: 2.304643\n",
      "(Iteration 32601 / 61200) loss: 2.304665\n",
      "(Iteration 32701 / 61200) loss: 2.304639\n",
      "(Iteration 32801 / 61200) loss: 2.304645\n",
      "(Epoch 43 / 80) train acc: 0.129000; val_acc: 0.151000\n",
      "(Iteration 32901 / 61200) loss: 2.304645\n",
      "(Iteration 33001 / 61200) loss: 2.304645\n",
      "(Iteration 33101 / 61200) loss: 2.304659\n",
      "(Iteration 33201 / 61200) loss: 2.304637\n",
      "(Iteration 33301 / 61200) loss: 2.304651\n",
      "(Iteration 33401 / 61200) loss: 2.304646\n",
      "(Iteration 33501 / 61200) loss: 2.304653\n",
      "(Iteration 33601 / 61200) loss: 2.304656\n",
      "(Epoch 44 / 80) train acc: 0.107000; val_acc: 0.151000\n",
      "(Iteration 33701 / 61200) loss: 2.304659\n",
      "(Iteration 33801 / 61200) loss: 2.304649\n",
      "(Iteration 33901 / 61200) loss: 2.304647\n",
      "(Iteration 34001 / 61200) loss: 2.304632\n",
      "(Iteration 34101 / 61200) loss: 2.304652\n",
      "(Iteration 34201 / 61200) loss: 2.304632\n",
      "(Iteration 34301 / 61200) loss: 2.304644\n",
      "(Iteration 34401 / 61200) loss: 2.304640\n",
      "(Epoch 45 / 80) train acc: 0.111000; val_acc: 0.150000\n",
      "(Iteration 34501 / 61200) loss: 2.304647\n",
      "(Iteration 34601 / 61200) loss: 2.304648\n",
      "(Iteration 34701 / 61200) loss: 2.304655\n",
      "(Iteration 34801 / 61200) loss: 2.304646\n",
      "(Iteration 34901 / 61200) loss: 2.304643\n",
      "(Iteration 35001 / 61200) loss: 2.304637\n",
      "(Iteration 35101 / 61200) loss: 2.304649\n",
      "(Epoch 46 / 80) train acc: 0.122000; val_acc: 0.150000\n",
      "(Iteration 35201 / 61200) loss: 2.304652\n",
      "(Iteration 35301 / 61200) loss: 2.304638\n",
      "(Iteration 35401 / 61200) loss: 2.304639\n",
      "(Iteration 35501 / 61200) loss: 2.304643\n",
      "(Iteration 35601 / 61200) loss: 2.304651\n",
      "(Iteration 35701 / 61200) loss: 2.304644\n",
      "(Iteration 35801 / 61200) loss: 2.304649\n",
      "(Iteration 35901 / 61200) loss: 2.304641\n",
      "(Epoch 47 / 80) train acc: 0.117000; val_acc: 0.150000\n",
      "(Iteration 36001 / 61200) loss: 2.304648\n",
      "(Iteration 36101 / 61200) loss: 2.304657\n",
      "(Iteration 36201 / 61200) loss: 2.304642\n",
      "(Iteration 36301 / 61200) loss: 2.304640\n",
      "(Iteration 36401 / 61200) loss: 2.304654\n",
      "(Iteration 36501 / 61200) loss: 2.304647\n",
      "(Iteration 36601 / 61200) loss: 2.304645\n",
      "(Iteration 36701 / 61200) loss: 2.304653\n",
      "(Epoch 48 / 80) train acc: 0.115000; val_acc: 0.150000\n",
      "(Iteration 36801 / 61200) loss: 2.304634\n",
      "(Iteration 36901 / 61200) loss: 2.304647\n",
      "(Iteration 37001 / 61200) loss: 2.304640\n",
      "(Iteration 37101 / 61200) loss: 2.304643\n",
      "(Iteration 37201 / 61200) loss: 2.304652\n",
      "(Iteration 37301 / 61200) loss: 2.304648\n",
      "(Iteration 37401 / 61200) loss: 2.304627\n",
      "(Epoch 49 / 80) train acc: 0.129000; val_acc: 0.150000\n",
      "(Iteration 37501 / 61200) loss: 2.304643\n",
      "(Iteration 37601 / 61200) loss: 2.304655\n",
      "(Iteration 37701 / 61200) loss: 2.304636\n",
      "(Iteration 37801 / 61200) loss: 2.304647\n",
      "(Iteration 37901 / 61200) loss: 2.304652\n",
      "(Iteration 38001 / 61200) loss: 2.304646\n",
      "(Iteration 38101 / 61200) loss: 2.304649\n",
      "(Iteration 38201 / 61200) loss: 2.304647\n",
      "(Epoch 50 / 80) train acc: 0.129000; val_acc: 0.150000\n",
      "(Iteration 38301 / 61200) loss: 2.304643\n",
      "(Iteration 38401 / 61200) loss: 2.304642\n",
      "(Iteration 38501 / 61200) loss: 2.304633\n",
      "(Iteration 38601 / 61200) loss: 2.304642\n",
      "(Iteration 38701 / 61200) loss: 2.304638\n",
      "(Iteration 38801 / 61200) loss: 2.304655\n",
      "(Iteration 38901 / 61200) loss: 2.304649\n",
      "(Iteration 39001 / 61200) loss: 2.304645\n",
      "(Epoch 51 / 80) train acc: 0.126000; val_acc: 0.150000\n",
      "(Iteration 39101 / 61200) loss: 2.304655\n",
      "(Iteration 39201 / 61200) loss: 2.304657\n",
      "(Iteration 39301 / 61200) loss: 2.304634\n",
      "(Iteration 39401 / 61200) loss: 2.304644\n",
      "(Iteration 39501 / 61200) loss: 2.304645\n",
      "(Iteration 39601 / 61200) loss: 2.304654\n",
      "(Iteration 39701 / 61200) loss: 2.304633\n",
      "(Epoch 52 / 80) train acc: 0.115000; val_acc: 0.150000\n",
      "(Iteration 39801 / 61200) loss: 2.304644\n",
      "(Iteration 39901 / 61200) loss: 2.304651\n",
      "(Iteration 40001 / 61200) loss: 2.304660\n",
      "(Iteration 40101 / 61200) loss: 2.304649\n",
      "(Iteration 40201 / 61200) loss: 2.304638\n",
      "(Iteration 40301 / 61200) loss: 2.304637\n",
      "(Iteration 40401 / 61200) loss: 2.304643\n",
      "(Iteration 40501 / 61200) loss: 2.304656\n",
      "(Epoch 53 / 80) train acc: 0.128000; val_acc: 0.150000\n",
      "(Iteration 40601 / 61200) loss: 2.304637\n",
      "(Iteration 40701 / 61200) loss: 2.304635\n",
      "(Iteration 40801 / 61200) loss: 2.304643\n",
      "(Iteration 40901 / 61200) loss: 2.304656\n",
      "(Iteration 41001 / 61200) loss: 2.304650\n",
      "(Iteration 41101 / 61200) loss: 2.304641\n",
      "(Iteration 41201 / 61200) loss: 2.304647\n",
      "(Iteration 41301 / 61200) loss: 2.304635\n",
      "(Epoch 54 / 80) train acc: 0.105000; val_acc: 0.150000\n",
      "(Iteration 41401 / 61200) loss: 2.304646\n",
      "(Iteration 41501 / 61200) loss: 2.304646\n",
      "(Iteration 41601 / 61200) loss: 2.304633\n",
      "(Iteration 41701 / 61200) loss: 2.304660\n",
      "(Iteration 41801 / 61200) loss: 2.304641\n",
      "(Iteration 41901 / 61200) loss: 2.304652\n",
      "(Iteration 42001 / 61200) loss: 2.304650\n",
      "(Epoch 55 / 80) train acc: 0.117000; val_acc: 0.150000\n",
      "(Iteration 42101 / 61200) loss: 2.304651\n",
      "(Iteration 42201 / 61200) loss: 2.304652\n",
      "(Iteration 42301 / 61200) loss: 2.304651\n",
      "(Iteration 42401 / 61200) loss: 2.304650\n",
      "(Iteration 42501 / 61200) loss: 2.304638\n",
      "(Iteration 42601 / 61200) loss: 2.304652\n",
      "(Iteration 42701 / 61200) loss: 2.304629\n",
      "(Iteration 42801 / 61200) loss: 2.304629\n",
      "(Epoch 56 / 80) train acc: 0.129000; val_acc: 0.151000\n",
      "(Iteration 42901 / 61200) loss: 2.304645\n",
      "(Iteration 43001 / 61200) loss: 2.304633\n",
      "(Iteration 43101 / 61200) loss: 2.304635\n",
      "(Iteration 43201 / 61200) loss: 2.304673\n",
      "(Iteration 43301 / 61200) loss: 2.304651\n",
      "(Iteration 43401 / 61200) loss: 2.304651\n",
      "(Iteration 43501 / 61200) loss: 2.304636\n",
      "(Iteration 43601 / 61200) loss: 2.304653\n",
      "(Epoch 57 / 80) train acc: 0.119000; val_acc: 0.151000\n",
      "(Iteration 43701 / 61200) loss: 2.304634\n",
      "(Iteration 43801 / 61200) loss: 2.304628\n",
      "(Iteration 43901 / 61200) loss: 2.304647\n",
      "(Iteration 44001 / 61200) loss: 2.304649\n",
      "(Iteration 44101 / 61200) loss: 2.304652\n",
      "(Iteration 44201 / 61200) loss: 2.304631\n",
      "(Iteration 44301 / 61200) loss: 2.304642\n",
      "(Epoch 58 / 80) train acc: 0.119000; val_acc: 0.151000\n",
      "(Iteration 44401 / 61200) loss: 2.304635\n",
      "(Iteration 44501 / 61200) loss: 2.304640\n",
      "(Iteration 44601 / 61200) loss: 2.304635\n",
      "(Iteration 44701 / 61200) loss: 2.304651\n",
      "(Iteration 44801 / 61200) loss: 2.304631\n",
      "(Iteration 44901 / 61200) loss: 2.304639\n",
      "(Iteration 45001 / 61200) loss: 2.304636\n",
      "(Iteration 45101 / 61200) loss: 2.304643\n",
      "(Epoch 59 / 80) train acc: 0.123000; val_acc: 0.151000\n",
      "(Iteration 45201 / 61200) loss: 2.304656\n",
      "(Iteration 45301 / 61200) loss: 2.304664\n",
      "(Iteration 45401 / 61200) loss: 2.304649\n",
      "(Iteration 45501 / 61200) loss: 2.304652\n",
      "(Iteration 45601 / 61200) loss: 2.304649\n",
      "(Iteration 45701 / 61200) loss: 2.304645\n",
      "(Iteration 45801 / 61200) loss: 2.304650\n",
      "(Epoch 60 / 80) train acc: 0.130000; val_acc: 0.151000\n",
      "(Iteration 45901 / 61200) loss: 2.304635\n",
      "(Iteration 46001 / 61200) loss: 2.304652\n",
      "(Iteration 46101 / 61200) loss: 2.304643\n",
      "(Iteration 46201 / 61200) loss: 2.304655\n",
      "(Iteration 46301 / 61200) loss: 2.304643\n",
      "(Iteration 46401 / 61200) loss: 2.304644\n",
      "(Iteration 46501 / 61200) loss: 2.304650\n",
      "(Iteration 46601 / 61200) loss: 2.304641\n",
      "(Epoch 61 / 80) train acc: 0.120000; val_acc: 0.151000\n",
      "(Iteration 46701 / 61200) loss: 2.304651\n",
      "(Iteration 46801 / 61200) loss: 2.304654\n",
      "(Iteration 46901 / 61200) loss: 2.304637\n",
      "(Iteration 47001 / 61200) loss: 2.304640\n",
      "(Iteration 47101 / 61200) loss: 2.304635\n",
      "(Iteration 47201 / 61200) loss: 2.304642\n",
      "(Iteration 47301 / 61200) loss: 2.304636\n",
      "(Iteration 47401 / 61200) loss: 2.304637\n",
      "(Epoch 62 / 80) train acc: 0.117000; val_acc: 0.151000\n",
      "(Iteration 47501 / 61200) loss: 2.304645\n",
      "(Iteration 47601 / 61200) loss: 2.304644\n",
      "(Iteration 47701 / 61200) loss: 2.304644\n",
      "(Iteration 47801 / 61200) loss: 2.304656\n",
      "(Iteration 47901 / 61200) loss: 2.304631\n",
      "(Iteration 48001 / 61200) loss: 2.304658\n",
      "(Iteration 48101 / 61200) loss: 2.304637\n",
      "(Epoch 63 / 80) train acc: 0.106000; val_acc: 0.151000\n",
      "(Iteration 48201 / 61200) loss: 2.304643\n",
      "(Iteration 48301 / 61200) loss: 2.304651\n",
      "(Iteration 48401 / 61200) loss: 2.304653\n",
      "(Iteration 48501 / 61200) loss: 2.304650\n",
      "(Iteration 48601 / 61200) loss: 2.304646\n",
      "(Iteration 48701 / 61200) loss: 2.304637\n",
      "(Iteration 48801 / 61200) loss: 2.304647\n",
      "(Iteration 48901 / 61200) loss: 2.304645\n",
      "(Epoch 64 / 80) train acc: 0.111000; val_acc: 0.151000\n",
      "(Iteration 49001 / 61200) loss: 2.304661\n",
      "(Iteration 49101 / 61200) loss: 2.304648\n",
      "(Iteration 49201 / 61200) loss: 2.304644\n",
      "(Iteration 49301 / 61200) loss: 2.304653\n",
      "(Iteration 49401 / 61200) loss: 2.304648\n",
      "(Iteration 49501 / 61200) loss: 2.304655\n",
      "(Iteration 49601 / 61200) loss: 2.304656\n",
      "(Iteration 49701 / 61200) loss: 2.304636\n",
      "(Epoch 65 / 80) train acc: 0.109000; val_acc: 0.151000\n",
      "(Iteration 49801 / 61200) loss: 2.304640\n",
      "(Iteration 49901 / 61200) loss: 2.304643\n",
      "(Iteration 50001 / 61200) loss: 2.304643\n",
      "(Iteration 50101 / 61200) loss: 2.304637\n",
      "(Iteration 50201 / 61200) loss: 2.304637\n",
      "(Iteration 50301 / 61200) loss: 2.304652\n",
      "(Iteration 50401 / 61200) loss: 2.304638\n",
      "(Epoch 66 / 80) train acc: 0.142000; val_acc: 0.151000\n",
      "(Iteration 50501 / 61200) loss: 2.304650\n",
      "(Iteration 50601 / 61200) loss: 2.304639\n",
      "(Iteration 50701 / 61200) loss: 2.304639\n",
      "(Iteration 50801 / 61200) loss: 2.304655\n",
      "(Iteration 50901 / 61200) loss: 2.304641\n",
      "(Iteration 51001 / 61200) loss: 2.304642\n",
      "(Iteration 51101 / 61200) loss: 2.304648\n",
      "(Iteration 51201 / 61200) loss: 2.304641\n",
      "(Epoch 67 / 80) train acc: 0.113000; val_acc: 0.151000\n",
      "(Iteration 51301 / 61200) loss: 2.304631\n",
      "(Iteration 51401 / 61200) loss: 2.304640\n",
      "(Iteration 51501 / 61200) loss: 2.304650\n",
      "(Iteration 51601 / 61200) loss: 2.304653\n",
      "(Iteration 51701 / 61200) loss: 2.304652\n",
      "(Iteration 51801 / 61200) loss: 2.304650\n",
      "(Iteration 51901 / 61200) loss: 2.304648\n",
      "(Iteration 52001 / 61200) loss: 2.304656\n",
      "(Epoch 68 / 80) train acc: 0.119000; val_acc: 0.151000\n",
      "(Iteration 52101 / 61200) loss: 2.304646\n",
      "(Iteration 52201 / 61200) loss: 2.304647\n",
      "(Iteration 52301 / 61200) loss: 2.304655\n",
      "(Iteration 52401 / 61200) loss: 2.304658\n",
      "(Iteration 52501 / 61200) loss: 2.304640\n",
      "(Iteration 52601 / 61200) loss: 2.304640\n",
      "(Iteration 52701 / 61200) loss: 2.304655\n",
      "(Epoch 69 / 80) train acc: 0.137000; val_acc: 0.151000\n",
      "(Iteration 52801 / 61200) loss: 2.304630\n",
      "(Iteration 52901 / 61200) loss: 2.304647\n",
      "(Iteration 53001 / 61200) loss: 2.304635\n",
      "(Iteration 53101 / 61200) loss: 2.304638\n",
      "(Iteration 53201 / 61200) loss: 2.304654\n",
      "(Iteration 53301 / 61200) loss: 2.304653\n",
      "(Iteration 53401 / 61200) loss: 2.304658\n",
      "(Iteration 53501 / 61200) loss: 2.304664\n",
      "(Epoch 70 / 80) train acc: 0.111000; val_acc: 0.151000\n",
      "(Iteration 53601 / 61200) loss: 2.304648\n",
      "(Iteration 53701 / 61200) loss: 2.304654\n",
      "(Iteration 53801 / 61200) loss: 2.304646\n",
      "(Iteration 53901 / 61200) loss: 2.304648\n",
      "(Iteration 54001 / 61200) loss: 2.304660\n",
      "(Iteration 54101 / 61200) loss: 2.304639\n",
      "(Iteration 54201 / 61200) loss: 2.304651\n",
      "(Iteration 54301 / 61200) loss: 2.304655\n",
      "(Epoch 71 / 80) train acc: 0.150000; val_acc: 0.151000\n",
      "(Iteration 54401 / 61200) loss: 2.304639\n",
      "(Iteration 54501 / 61200) loss: 2.304652\n",
      "(Iteration 54601 / 61200) loss: 2.304640\n",
      "(Iteration 54701 / 61200) loss: 2.304642\n",
      "(Iteration 54801 / 61200) loss: 2.304643\n",
      "(Iteration 54901 / 61200) loss: 2.304651\n",
      "(Iteration 55001 / 61200) loss: 2.304655\n",
      "(Epoch 72 / 80) train acc: 0.100000; val_acc: 0.151000\n",
      "(Iteration 55101 / 61200) loss: 2.304643\n",
      "(Iteration 55201 / 61200) loss: 2.304650\n",
      "(Iteration 55301 / 61200) loss: 2.304645\n",
      "(Iteration 55401 / 61200) loss: 2.304653\n",
      "(Iteration 55501 / 61200) loss: 2.304657\n",
      "(Iteration 55601 / 61200) loss: 2.304641\n",
      "(Iteration 55701 / 61200) loss: 2.304642\n",
      "(Iteration 55801 / 61200) loss: 2.304633\n",
      "(Epoch 73 / 80) train acc: 0.108000; val_acc: 0.151000\n",
      "(Iteration 55901 / 61200) loss: 2.304662\n",
      "(Iteration 56001 / 61200) loss: 2.304639\n",
      "(Iteration 56101 / 61200) loss: 2.304638\n",
      "(Iteration 56201 / 61200) loss: 2.304642\n",
      "(Iteration 56301 / 61200) loss: 2.304651\n",
      "(Iteration 56401 / 61200) loss: 2.304643\n",
      "(Iteration 56501 / 61200) loss: 2.304647\n",
      "(Iteration 56601 / 61200) loss: 2.304645\n",
      "(Epoch 74 / 80) train acc: 0.128000; val_acc: 0.151000\n",
      "(Iteration 56701 / 61200) loss: 2.304656\n",
      "(Iteration 56801 / 61200) loss: 2.304645\n",
      "(Iteration 56901 / 61200) loss: 2.304642\n",
      "(Iteration 57001 / 61200) loss: 2.304650\n",
      "(Iteration 57101 / 61200) loss: 2.304643\n",
      "(Iteration 57201 / 61200) loss: 2.304643\n",
      "(Iteration 57301 / 61200) loss: 2.304646\n",
      "(Epoch 75 / 80) train acc: 0.120000; val_acc: 0.151000\n",
      "(Iteration 57401 / 61200) loss: 2.304650\n",
      "(Iteration 57501 / 61200) loss: 2.304645\n",
      "(Iteration 57601 / 61200) loss: 2.304647\n",
      "(Iteration 57701 / 61200) loss: 2.304643\n",
      "(Iteration 57801 / 61200) loss: 2.304639\n",
      "(Iteration 57901 / 61200) loss: 2.304648\n",
      "(Iteration 58001 / 61200) loss: 2.304634\n",
      "(Iteration 58101 / 61200) loss: 2.304649\n",
      "(Epoch 76 / 80) train acc: 0.135000; val_acc: 0.151000\n",
      "(Iteration 58201 / 61200) loss: 2.304645\n",
      "(Iteration 58301 / 61200) loss: 2.304653\n",
      "(Iteration 58401 / 61200) loss: 2.304655\n",
      "(Iteration 58501 / 61200) loss: 2.304643\n",
      "(Iteration 58601 / 61200) loss: 2.304641\n",
      "(Iteration 58701 / 61200) loss: 2.304637\n",
      "(Iteration 58801 / 61200) loss: 2.304653\n",
      "(Iteration 58901 / 61200) loss: 2.304642\n",
      "(Epoch 77 / 80) train acc: 0.105000; val_acc: 0.151000\n",
      "(Iteration 59001 / 61200) loss: 2.304646\n",
      "(Iteration 59101 / 61200) loss: 2.304637\n",
      "(Iteration 59201 / 61200) loss: 2.304633\n",
      "(Iteration 59301 / 61200) loss: 2.304645\n",
      "(Iteration 59401 / 61200) loss: 2.304660\n",
      "(Iteration 59501 / 61200) loss: 2.304646\n",
      "(Iteration 59601 / 61200) loss: 2.304645\n",
      "(Epoch 78 / 80) train acc: 0.128000; val_acc: 0.151000\n",
      "(Iteration 59701 / 61200) loss: 2.304649\n",
      "(Iteration 59801 / 61200) loss: 2.304662\n",
      "(Iteration 59901 / 61200) loss: 2.304633\n",
      "(Iteration 60001 / 61200) loss: 2.304643\n",
      "(Iteration 60101 / 61200) loss: 2.304631\n",
      "(Iteration 60201 / 61200) loss: 2.304659\n",
      "(Iteration 60301 / 61200) loss: 2.304637\n",
      "(Iteration 60401 / 61200) loss: 2.304639\n",
      "(Epoch 79 / 80) train acc: 0.106000; val_acc: 0.151000\n",
      "(Iteration 60501 / 61200) loss: 2.304635\n",
      "(Iteration 60601 / 61200) loss: 2.304636\n",
      "(Iteration 60701 / 61200) loss: 2.304636\n",
      "(Iteration 60801 / 61200) loss: 2.304644\n",
      "(Iteration 60901 / 61200) loss: 2.304655\n",
      "(Iteration 61001 / 61200) loss: 2.304656\n",
      "(Iteration 61101 / 61200) loss: 2.304654\n",
      "(Epoch 80 / 80) train acc: 0.141000; val_acc: 0.151000\n",
      "New best model found with validation accuracy: 0.1509\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.304633\n",
      "(Epoch 0 / 80) train acc: 0.121000; val_acc: 0.110000\n",
      "(Iteration 101 / 30560) loss: 2.304632\n",
      "(Iteration 201 / 30560) loss: 2.304628\n",
      "(Iteration 301 / 30560) loss: 2.304631\n",
      "(Epoch 1 / 80) train acc: 0.137000; val_acc: 0.111000\n",
      "(Iteration 401 / 30560) loss: 2.304630\n",
      "(Iteration 501 / 30560) loss: 2.304639\n",
      "(Iteration 601 / 30560) loss: 2.304628\n",
      "(Iteration 701 / 30560) loss: 2.304626\n",
      "(Epoch 2 / 80) train acc: 0.124000; val_acc: 0.111000\n",
      "(Iteration 801 / 30560) loss: 2.304624\n",
      "(Iteration 901 / 30560) loss: 2.304634\n",
      "(Iteration 1001 / 30560) loss: 2.304629\n",
      "(Iteration 1101 / 30560) loss: 2.304624\n",
      "(Epoch 3 / 80) train acc: 0.113000; val_acc: 0.110000\n",
      "(Iteration 1201 / 30560) loss: 2.304631\n",
      "(Iteration 1301 / 30560) loss: 2.304621\n",
      "(Iteration 1401 / 30560) loss: 2.304629\n",
      "(Iteration 1501 / 30560) loss: 2.304632\n",
      "(Epoch 4 / 80) train acc: 0.125000; val_acc: 0.110000\n",
      "(Iteration 1601 / 30560) loss: 2.304630\n",
      "(Iteration 1701 / 30560) loss: 2.304627\n",
      "(Iteration 1801 / 30560) loss: 2.304627\n",
      "(Iteration 1901 / 30560) loss: 2.304632\n",
      "(Epoch 5 / 80) train acc: 0.122000; val_acc: 0.110000\n",
      "(Iteration 2001 / 30560) loss: 2.304632\n",
      "(Iteration 2101 / 30560) loss: 2.304624\n",
      "(Iteration 2201 / 30560) loss: 2.304627\n",
      "(Epoch 6 / 80) train acc: 0.122000; val_acc: 0.110000\n",
      "(Iteration 2301 / 30560) loss: 2.304637\n",
      "(Iteration 2401 / 30560) loss: 2.304628\n",
      "(Iteration 2501 / 30560) loss: 2.304626\n",
      "(Iteration 2601 / 30560) loss: 2.304628\n",
      "(Epoch 7 / 80) train acc: 0.129000; val_acc: 0.110000\n",
      "(Iteration 2701 / 30560) loss: 2.304629\n",
      "(Iteration 2801 / 30560) loss: 2.304631\n",
      "(Iteration 2901 / 30560) loss: 2.304629\n",
      "(Iteration 3001 / 30560) loss: 2.304627\n",
      "(Epoch 8 / 80) train acc: 0.115000; val_acc: 0.110000\n",
      "(Iteration 3101 / 30560) loss: 2.304631\n",
      "(Iteration 3201 / 30560) loss: 2.304625\n",
      "(Iteration 3301 / 30560) loss: 2.304634\n",
      "(Iteration 3401 / 30560) loss: 2.304633\n",
      "(Epoch 9 / 80) train acc: 0.129000; val_acc: 0.110000\n",
      "(Iteration 3501 / 30560) loss: 2.304623\n",
      "(Iteration 3601 / 30560) loss: 2.304637\n",
      "(Iteration 3701 / 30560) loss: 2.304638\n",
      "(Iteration 3801 / 30560) loss: 2.304637\n",
      "(Epoch 10 / 80) train acc: 0.118000; val_acc: 0.110000\n",
      "(Iteration 3901 / 30560) loss: 2.304630\n",
      "(Iteration 4001 / 30560) loss: 2.304639\n",
      "(Iteration 4101 / 30560) loss: 2.304632\n",
      "(Iteration 4201 / 30560) loss: 2.304628\n",
      "(Epoch 11 / 80) train acc: 0.119000; val_acc: 0.110000\n",
      "(Iteration 4301 / 30560) loss: 2.304633\n",
      "(Iteration 4401 / 30560) loss: 2.304625\n",
      "(Iteration 4501 / 30560) loss: 2.304625\n",
      "(Epoch 12 / 80) train acc: 0.150000; val_acc: 0.110000\n",
      "(Iteration 4601 / 30560) loss: 2.304633\n",
      "(Iteration 4701 / 30560) loss: 2.304631\n",
      "(Iteration 4801 / 30560) loss: 2.304629\n",
      "(Iteration 4901 / 30560) loss: 2.304636\n",
      "(Epoch 13 / 80) train acc: 0.135000; val_acc: 0.110000\n",
      "(Iteration 5001 / 30560) loss: 2.304635\n",
      "(Iteration 5101 / 30560) loss: 2.304625\n",
      "(Iteration 5201 / 30560) loss: 2.304625\n",
      "(Iteration 5301 / 30560) loss: 2.304632\n",
      "(Epoch 14 / 80) train acc: 0.137000; val_acc: 0.110000\n",
      "(Iteration 5401 / 30560) loss: 2.304628\n",
      "(Iteration 5501 / 30560) loss: 2.304619\n",
      "(Iteration 5601 / 30560) loss: 2.304631\n",
      "(Iteration 5701 / 30560) loss: 2.304624\n",
      "(Epoch 15 / 80) train acc: 0.123000; val_acc: 0.110000\n",
      "(Iteration 5801 / 30560) loss: 2.304636\n",
      "(Iteration 5901 / 30560) loss: 2.304628\n",
      "(Iteration 6001 / 30560) loss: 2.304628\n",
      "(Iteration 6101 / 30560) loss: 2.304626\n",
      "(Epoch 16 / 80) train acc: 0.139000; val_acc: 0.110000\n",
      "(Iteration 6201 / 30560) loss: 2.304631\n",
      "(Iteration 6301 / 30560) loss: 2.304630\n",
      "(Iteration 6401 / 30560) loss: 2.304626\n",
      "(Epoch 17 / 80) train acc: 0.141000; val_acc: 0.110000\n",
      "(Iteration 6501 / 30560) loss: 2.304629\n",
      "(Iteration 6601 / 30560) loss: 2.304632\n",
      "(Iteration 6701 / 30560) loss: 2.304635\n",
      "(Iteration 6801 / 30560) loss: 2.304630\n",
      "(Epoch 18 / 80) train acc: 0.114000; val_acc: 0.110000\n",
      "(Iteration 6901 / 30560) loss: 2.304631\n",
      "(Iteration 7001 / 30560) loss: 2.304637\n",
      "(Iteration 7101 / 30560) loss: 2.304630\n",
      "(Iteration 7201 / 30560) loss: 2.304636\n",
      "(Epoch 19 / 80) train acc: 0.120000; val_acc: 0.110000\n",
      "(Iteration 7301 / 30560) loss: 2.304630\n",
      "(Iteration 7401 / 30560) loss: 2.304622\n",
      "(Iteration 7501 / 30560) loss: 2.304635\n",
      "(Iteration 7601 / 30560) loss: 2.304624\n",
      "(Epoch 20 / 80) train acc: 0.118000; val_acc: 0.110000\n",
      "(Iteration 7701 / 30560) loss: 2.304622\n",
      "(Iteration 7801 / 30560) loss: 2.304635\n",
      "(Iteration 7901 / 30560) loss: 2.304634\n",
      "(Iteration 8001 / 30560) loss: 2.304625\n",
      "(Epoch 21 / 80) train acc: 0.092000; val_acc: 0.110000\n",
      "(Iteration 8101 / 30560) loss: 2.304622\n",
      "(Iteration 8201 / 30560) loss: 2.304627\n",
      "(Iteration 8301 / 30560) loss: 2.304625\n",
      "(Iteration 8401 / 30560) loss: 2.304620\n",
      "(Epoch 22 / 80) train acc: 0.129000; val_acc: 0.110000\n",
      "(Iteration 8501 / 30560) loss: 2.304634\n",
      "(Iteration 8601 / 30560) loss: 2.304629\n",
      "(Iteration 8701 / 30560) loss: 2.304633\n",
      "(Epoch 23 / 80) train acc: 0.125000; val_acc: 0.110000\n",
      "(Iteration 8801 / 30560) loss: 2.304621\n",
      "(Iteration 8901 / 30560) loss: 2.304625\n",
      "(Iteration 9001 / 30560) loss: 2.304628\n",
      "(Iteration 9101 / 30560) loss: 2.304623\n",
      "(Epoch 24 / 80) train acc: 0.109000; val_acc: 0.110000\n",
      "(Iteration 9201 / 30560) loss: 2.304632\n",
      "(Iteration 9301 / 30560) loss: 2.304626\n",
      "(Iteration 9401 / 30560) loss: 2.304630\n",
      "(Iteration 9501 / 30560) loss: 2.304625\n",
      "(Epoch 25 / 80) train acc: 0.127000; val_acc: 0.110000\n",
      "(Iteration 9601 / 30560) loss: 2.304626\n",
      "(Iteration 9701 / 30560) loss: 2.304631\n",
      "(Iteration 9801 / 30560) loss: 2.304630\n",
      "(Iteration 9901 / 30560) loss: 2.304625\n",
      "(Epoch 26 / 80) train acc: 0.125000; val_acc: 0.110000\n",
      "(Iteration 10001 / 30560) loss: 2.304631\n",
      "(Iteration 10101 / 30560) loss: 2.304620\n",
      "(Iteration 10201 / 30560) loss: 2.304636\n",
      "(Iteration 10301 / 30560) loss: 2.304626\n",
      "(Epoch 27 / 80) train acc: 0.139000; val_acc: 0.110000\n",
      "(Iteration 10401 / 30560) loss: 2.304620\n",
      "(Iteration 10501 / 30560) loss: 2.304625\n",
      "(Iteration 10601 / 30560) loss: 2.304629\n",
      "(Epoch 28 / 80) train acc: 0.123000; val_acc: 0.110000\n",
      "(Iteration 10701 / 30560) loss: 2.304636\n",
      "(Iteration 10801 / 30560) loss: 2.304628\n",
      "(Iteration 10901 / 30560) loss: 2.304634\n",
      "(Iteration 11001 / 30560) loss: 2.304627\n",
      "(Epoch 29 / 80) train acc: 0.132000; val_acc: 0.110000\n",
      "(Iteration 11101 / 30560) loss: 2.304630\n",
      "(Iteration 11201 / 30560) loss: 2.304627\n",
      "(Iteration 11301 / 30560) loss: 2.304632\n",
      "(Iteration 11401 / 30560) loss: 2.304634\n",
      "(Epoch 30 / 80) train acc: 0.136000; val_acc: 0.110000\n",
      "(Iteration 11501 / 30560) loss: 2.304632\n",
      "(Iteration 11601 / 30560) loss: 2.304623\n",
      "(Iteration 11701 / 30560) loss: 2.304634\n",
      "(Iteration 11801 / 30560) loss: 2.304631\n",
      "(Epoch 31 / 80) train acc: 0.129000; val_acc: 0.110000\n",
      "(Iteration 11901 / 30560) loss: 2.304625\n",
      "(Iteration 12001 / 30560) loss: 2.304620\n",
      "(Iteration 12101 / 30560) loss: 2.304625\n",
      "(Iteration 12201 / 30560) loss: 2.304637\n",
      "(Epoch 32 / 80) train acc: 0.106000; val_acc: 0.110000\n",
      "(Iteration 12301 / 30560) loss: 2.304631\n",
      "(Iteration 12401 / 30560) loss: 2.304634\n",
      "(Iteration 12501 / 30560) loss: 2.304622\n",
      "(Iteration 12601 / 30560) loss: 2.304627\n",
      "(Epoch 33 / 80) train acc: 0.116000; val_acc: 0.110000\n",
      "(Iteration 12701 / 30560) loss: 2.304630\n",
      "(Iteration 12801 / 30560) loss: 2.304629\n",
      "(Iteration 12901 / 30560) loss: 2.304631\n",
      "(Epoch 34 / 80) train acc: 0.114000; val_acc: 0.110000\n",
      "(Iteration 13001 / 30560) loss: 2.304623\n",
      "(Iteration 13101 / 30560) loss: 2.304627\n",
      "(Iteration 13201 / 30560) loss: 2.304637\n",
      "(Iteration 13301 / 30560) loss: 2.304625\n",
      "(Epoch 35 / 80) train acc: 0.131000; val_acc: 0.110000\n",
      "(Iteration 13401 / 30560) loss: 2.304629\n",
      "(Iteration 13501 / 30560) loss: 2.304624\n",
      "(Iteration 13601 / 30560) loss: 2.304626\n",
      "(Iteration 13701 / 30560) loss: 2.304623\n",
      "(Epoch 36 / 80) train acc: 0.123000; val_acc: 0.110000\n",
      "(Iteration 13801 / 30560) loss: 2.304626\n",
      "(Iteration 13901 / 30560) loss: 2.304635\n",
      "(Iteration 14001 / 30560) loss: 2.304630\n",
      "(Iteration 14101 / 30560) loss: 2.304626\n",
      "(Epoch 37 / 80) train acc: 0.138000; val_acc: 0.110000\n",
      "(Iteration 14201 / 30560) loss: 2.304636\n",
      "(Iteration 14301 / 30560) loss: 2.304625\n",
      "(Iteration 14401 / 30560) loss: 2.304629\n",
      "(Iteration 14501 / 30560) loss: 2.304631\n",
      "(Epoch 38 / 80) train acc: 0.120000; val_acc: 0.110000\n",
      "(Iteration 14601 / 30560) loss: 2.304639\n",
      "(Iteration 14701 / 30560) loss: 2.304624\n",
      "(Iteration 14801 / 30560) loss: 2.304624\n",
      "(Epoch 39 / 80) train acc: 0.127000; val_acc: 0.110000\n",
      "(Iteration 14901 / 30560) loss: 2.304628\n",
      "(Iteration 15001 / 30560) loss: 2.304630\n",
      "(Iteration 15101 / 30560) loss: 2.304631\n",
      "(Iteration 15201 / 30560) loss: 2.304618\n",
      "(Epoch 40 / 80) train acc: 0.120000; val_acc: 0.110000\n",
      "(Iteration 15301 / 30560) loss: 2.304636\n",
      "(Iteration 15401 / 30560) loss: 2.304629\n",
      "(Iteration 15501 / 30560) loss: 2.304630\n",
      "(Iteration 15601 / 30560) loss: 2.304625\n",
      "(Epoch 41 / 80) train acc: 0.123000; val_acc: 0.110000\n",
      "(Iteration 15701 / 30560) loss: 2.304627\n",
      "(Iteration 15801 / 30560) loss: 2.304634\n",
      "(Iteration 15901 / 30560) loss: 2.304636\n",
      "(Iteration 16001 / 30560) loss: 2.304633\n",
      "(Epoch 42 / 80) train acc: 0.141000; val_acc: 0.110000\n",
      "(Iteration 16101 / 30560) loss: 2.304628\n",
      "(Iteration 16201 / 30560) loss: 2.304633\n",
      "(Iteration 16301 / 30560) loss: 2.304631\n",
      "(Iteration 16401 / 30560) loss: 2.304628\n",
      "(Epoch 43 / 80) train acc: 0.108000; val_acc: 0.110000\n",
      "(Iteration 16501 / 30560) loss: 2.304631\n",
      "(Iteration 16601 / 30560) loss: 2.304632\n",
      "(Iteration 16701 / 30560) loss: 2.304623\n",
      "(Iteration 16801 / 30560) loss: 2.304623\n",
      "(Epoch 44 / 80) train acc: 0.124000; val_acc: 0.110000\n",
      "(Iteration 16901 / 30560) loss: 2.304627\n",
      "(Iteration 17001 / 30560) loss: 2.304631\n",
      "(Iteration 17101 / 30560) loss: 2.304636\n",
      "(Epoch 45 / 80) train acc: 0.125000; val_acc: 0.110000\n",
      "(Iteration 17201 / 30560) loss: 2.304627\n",
      "(Iteration 17301 / 30560) loss: 2.304630\n",
      "(Iteration 17401 / 30560) loss: 2.304630\n",
      "(Iteration 17501 / 30560) loss: 2.304637\n",
      "(Epoch 46 / 80) train acc: 0.131000; val_acc: 0.110000\n",
      "(Iteration 17601 / 30560) loss: 2.304626\n",
      "(Iteration 17701 / 30560) loss: 2.304621\n",
      "(Iteration 17801 / 30560) loss: 2.304627\n",
      "(Iteration 17901 / 30560) loss: 2.304640\n",
      "(Epoch 47 / 80) train acc: 0.119000; val_acc: 0.110000\n",
      "(Iteration 18001 / 30560) loss: 2.304630\n",
      "(Iteration 18101 / 30560) loss: 2.304627\n",
      "(Iteration 18201 / 30560) loss: 2.304633\n",
      "(Iteration 18301 / 30560) loss: 2.304634\n",
      "(Epoch 48 / 80) train acc: 0.114000; val_acc: 0.110000\n",
      "(Iteration 18401 / 30560) loss: 2.304625\n",
      "(Iteration 18501 / 30560) loss: 2.304631\n",
      "(Iteration 18601 / 30560) loss: 2.304637\n",
      "(Iteration 18701 / 30560) loss: 2.304625\n",
      "(Epoch 49 / 80) train acc: 0.124000; val_acc: 0.110000\n",
      "(Iteration 18801 / 30560) loss: 2.304634\n",
      "(Iteration 18901 / 30560) loss: 2.304621\n",
      "(Iteration 19001 / 30560) loss: 2.304620\n",
      "(Epoch 50 / 80) train acc: 0.126000; val_acc: 0.110000\n",
      "(Iteration 19101 / 30560) loss: 2.304626\n",
      "(Iteration 19201 / 30560) loss: 2.304639\n",
      "(Iteration 19301 / 30560) loss: 2.304627\n",
      "(Iteration 19401 / 30560) loss: 2.304635\n",
      "(Epoch 51 / 80) train acc: 0.116000; val_acc: 0.110000\n",
      "(Iteration 19501 / 30560) loss: 2.304633\n",
      "(Iteration 19601 / 30560) loss: 2.304635\n",
      "(Iteration 19701 / 30560) loss: 2.304640\n",
      "(Iteration 19801 / 30560) loss: 2.304628\n",
      "(Epoch 52 / 80) train acc: 0.127000; val_acc: 0.110000\n",
      "(Iteration 19901 / 30560) loss: 2.304626\n",
      "(Iteration 20001 / 30560) loss: 2.304620\n",
      "(Iteration 20101 / 30560) loss: 2.304619\n",
      "(Iteration 20201 / 30560) loss: 2.304626\n",
      "(Epoch 53 / 80) train acc: 0.134000; val_acc: 0.110000\n",
      "(Iteration 20301 / 30560) loss: 2.304634\n",
      "(Iteration 20401 / 30560) loss: 2.304628\n",
      "(Iteration 20501 / 30560) loss: 2.304629\n",
      "(Iteration 20601 / 30560) loss: 2.304626\n",
      "(Epoch 54 / 80) train acc: 0.127000; val_acc: 0.110000\n",
      "(Iteration 20701 / 30560) loss: 2.304628\n",
      "(Iteration 20801 / 30560) loss: 2.304640\n",
      "(Iteration 20901 / 30560) loss: 2.304626\n",
      "(Iteration 21001 / 30560) loss: 2.304626\n",
      "(Epoch 55 / 80) train acc: 0.118000; val_acc: 0.110000\n",
      "(Iteration 21101 / 30560) loss: 2.304628\n",
      "(Iteration 21201 / 30560) loss: 2.304634\n",
      "(Iteration 21301 / 30560) loss: 2.304632\n",
      "(Epoch 56 / 80) train acc: 0.121000; val_acc: 0.110000\n",
      "(Iteration 21401 / 30560) loss: 2.304622\n",
      "(Iteration 21501 / 30560) loss: 2.304629\n",
      "(Iteration 21601 / 30560) loss: 2.304628\n",
      "(Iteration 21701 / 30560) loss: 2.304631\n",
      "(Epoch 57 / 80) train acc: 0.112000; val_acc: 0.110000\n",
      "(Iteration 21801 / 30560) loss: 2.304627\n",
      "(Iteration 21901 / 30560) loss: 2.304634\n",
      "(Iteration 22001 / 30560) loss: 2.304631\n",
      "(Iteration 22101 / 30560) loss: 2.304625\n",
      "(Epoch 58 / 80) train acc: 0.127000; val_acc: 0.110000\n",
      "(Iteration 22201 / 30560) loss: 2.304633\n",
      "(Iteration 22301 / 30560) loss: 2.304620\n",
      "(Iteration 22401 / 30560) loss: 2.304625\n",
      "(Iteration 22501 / 30560) loss: 2.304628\n",
      "(Epoch 59 / 80) train acc: 0.133000; val_acc: 0.110000\n",
      "(Iteration 22601 / 30560) loss: 2.304632\n",
      "(Iteration 22701 / 30560) loss: 2.304627\n",
      "(Iteration 22801 / 30560) loss: 2.304631\n",
      "(Iteration 22901 / 30560) loss: 2.304633\n",
      "(Epoch 60 / 80) train acc: 0.111000; val_acc: 0.110000\n",
      "(Iteration 23001 / 30560) loss: 2.304625\n",
      "(Iteration 23101 / 30560) loss: 2.304626\n",
      "(Iteration 23201 / 30560) loss: 2.304636\n",
      "(Iteration 23301 / 30560) loss: 2.304637\n",
      "(Epoch 61 / 80) train acc: 0.120000; val_acc: 0.110000\n",
      "(Iteration 23401 / 30560) loss: 2.304637\n",
      "(Iteration 23501 / 30560) loss: 2.304623\n",
      "(Iteration 23601 / 30560) loss: 2.304621\n",
      "(Epoch 62 / 80) train acc: 0.135000; val_acc: 0.110000\n",
      "(Iteration 23701 / 30560) loss: 2.304625\n",
      "(Iteration 23801 / 30560) loss: 2.304628\n",
      "(Iteration 23901 / 30560) loss: 2.304626\n",
      "(Iteration 24001 / 30560) loss: 2.304633\n",
      "(Epoch 63 / 80) train acc: 0.141000; val_acc: 0.110000\n",
      "(Iteration 24101 / 30560) loss: 2.304633\n",
      "(Iteration 24201 / 30560) loss: 2.304629\n",
      "(Iteration 24301 / 30560) loss: 2.304632\n",
      "(Iteration 24401 / 30560) loss: 2.304617\n",
      "(Epoch 64 / 80) train acc: 0.121000; val_acc: 0.109000\n",
      "(Iteration 24501 / 30560) loss: 2.304628\n",
      "(Iteration 24601 / 30560) loss: 2.304635\n",
      "(Iteration 24701 / 30560) loss: 2.304626\n",
      "(Iteration 24801 / 30560) loss: 2.304625\n",
      "(Epoch 65 / 80) train acc: 0.140000; val_acc: 0.109000\n",
      "(Iteration 24901 / 30560) loss: 2.304634\n",
      "(Iteration 25001 / 30560) loss: 2.304630\n",
      "(Iteration 25101 / 30560) loss: 2.304625\n",
      "(Iteration 25201 / 30560) loss: 2.304626\n",
      "(Epoch 66 / 80) train acc: 0.131000; val_acc: 0.109000\n",
      "(Iteration 25301 / 30560) loss: 2.304634\n",
      "(Iteration 25401 / 30560) loss: 2.304623\n",
      "(Iteration 25501 / 30560) loss: 2.304636\n",
      "(Epoch 67 / 80) train acc: 0.120000; val_acc: 0.109000\n",
      "(Iteration 25601 / 30560) loss: 2.304633\n",
      "(Iteration 25701 / 30560) loss: 2.304633\n",
      "(Iteration 25801 / 30560) loss: 2.304626\n",
      "(Iteration 25901 / 30560) loss: 2.304635\n",
      "(Epoch 68 / 80) train acc: 0.118000; val_acc: 0.109000\n",
      "(Iteration 26001 / 30560) loss: 2.304619\n",
      "(Iteration 26101 / 30560) loss: 2.304636\n",
      "(Iteration 26201 / 30560) loss: 2.304628\n",
      "(Iteration 26301 / 30560) loss: 2.304636\n",
      "(Epoch 69 / 80) train acc: 0.116000; val_acc: 0.109000\n",
      "(Iteration 26401 / 30560) loss: 2.304632\n",
      "(Iteration 26501 / 30560) loss: 2.304638\n",
      "(Iteration 26601 / 30560) loss: 2.304630\n",
      "(Iteration 26701 / 30560) loss: 2.304620\n",
      "(Epoch 70 / 80) train acc: 0.104000; val_acc: 0.109000\n",
      "(Iteration 26801 / 30560) loss: 2.304634\n",
      "(Iteration 26901 / 30560) loss: 2.304637\n",
      "(Iteration 27001 / 30560) loss: 2.304630\n",
      "(Iteration 27101 / 30560) loss: 2.304629\n",
      "(Epoch 71 / 80) train acc: 0.138000; val_acc: 0.109000\n",
      "(Iteration 27201 / 30560) loss: 2.304625\n",
      "(Iteration 27301 / 30560) loss: 2.304617\n",
      "(Iteration 27401 / 30560) loss: 2.304632\n",
      "(Iteration 27501 / 30560) loss: 2.304624\n",
      "(Epoch 72 / 80) train acc: 0.130000; val_acc: 0.109000\n",
      "(Iteration 27601 / 30560) loss: 2.304641\n",
      "(Iteration 27701 / 30560) loss: 2.304624\n",
      "(Iteration 27801 / 30560) loss: 2.304629\n",
      "(Epoch 73 / 80) train acc: 0.107000; val_acc: 0.109000\n",
      "(Iteration 27901 / 30560) loss: 2.304633\n",
      "(Iteration 28001 / 30560) loss: 2.304628\n",
      "(Iteration 28101 / 30560) loss: 2.304630\n",
      "(Iteration 28201 / 30560) loss: 2.304633\n",
      "(Epoch 74 / 80) train acc: 0.106000; val_acc: 0.109000\n",
      "(Iteration 28301 / 30560) loss: 2.304627\n",
      "(Iteration 28401 / 30560) loss: 2.304631\n",
      "(Iteration 28501 / 30560) loss: 2.304630\n",
      "(Iteration 28601 / 30560) loss: 2.304629\n",
      "(Epoch 75 / 80) train acc: 0.130000; val_acc: 0.109000\n",
      "(Iteration 28701 / 30560) loss: 2.304632\n",
      "(Iteration 28801 / 30560) loss: 2.304637\n",
      "(Iteration 28901 / 30560) loss: 2.304635\n",
      "(Iteration 29001 / 30560) loss: 2.304630\n",
      "(Epoch 76 / 80) train acc: 0.125000; val_acc: 0.109000\n",
      "(Iteration 29101 / 30560) loss: 2.304623\n",
      "(Iteration 29201 / 30560) loss: 2.304617\n",
      "(Iteration 29301 / 30560) loss: 2.304629\n",
      "(Iteration 29401 / 30560) loss: 2.304624\n",
      "(Epoch 77 / 80) train acc: 0.128000; val_acc: 0.109000\n",
      "(Iteration 29501 / 30560) loss: 2.304624\n",
      "(Iteration 29601 / 30560) loss: 2.304631\n",
      "(Iteration 29701 / 30560) loss: 2.304626\n",
      "(Epoch 78 / 80) train acc: 0.123000; val_acc: 0.109000\n",
      "(Iteration 29801 / 30560) loss: 2.304630\n",
      "(Iteration 29901 / 30560) loss: 2.304624\n",
      "(Iteration 30001 / 30560) loss: 2.304629\n",
      "(Iteration 30101 / 30560) loss: 2.304631\n",
      "(Epoch 79 / 80) train acc: 0.111000; val_acc: 0.109000\n",
      "(Iteration 30201 / 30560) loss: 2.304622\n",
      "(Iteration 30301 / 30560) loss: 2.304635\n",
      "(Iteration 30401 / 30560) loss: 2.304631\n",
      "(Iteration 30501 / 30560) loss: 2.304627\n",
      "(Epoch 80 / 80) train acc: 0.128000; val_acc: 0.109000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.305509\n",
      "(Epoch 0 / 80) train acc: 0.089000; val_acc: 0.101000\n",
      "(Iteration 101 / 61200) loss: 2.305526\n",
      "(Iteration 201 / 61200) loss: 2.305515\n",
      "(Iteration 301 / 61200) loss: 2.305524\n",
      "(Iteration 401 / 61200) loss: 2.305532\n",
      "(Iteration 501 / 61200) loss: 2.305519\n",
      "(Iteration 601 / 61200) loss: 2.305518\n",
      "(Iteration 701 / 61200) loss: 2.305519\n",
      "(Epoch 1 / 80) train acc: 0.083000; val_acc: 0.101000\n",
      "(Iteration 801 / 61200) loss: 2.305514\n",
      "(Iteration 901 / 61200) loss: 2.305522\n",
      "(Iteration 1001 / 61200) loss: 2.305522\n",
      "(Iteration 1101 / 61200) loss: 2.305527\n",
      "(Iteration 1201 / 61200) loss: 2.305507\n",
      "(Iteration 1301 / 61200) loss: 2.305518\n",
      "(Iteration 1401 / 61200) loss: 2.305519\n",
      "(Iteration 1501 / 61200) loss: 2.305526\n",
      "(Epoch 2 / 80) train acc: 0.095000; val_acc: 0.102000\n",
      "(Iteration 1601 / 61200) loss: 2.305523\n",
      "(Iteration 1701 / 61200) loss: 2.305521\n",
      "(Iteration 1801 / 61200) loss: 2.305519\n",
      "(Iteration 1901 / 61200) loss: 2.305520\n",
      "(Iteration 2001 / 61200) loss: 2.305509\n",
      "(Iteration 2101 / 61200) loss: 2.305514\n",
      "(Iteration 2201 / 61200) loss: 2.305518\n",
      "(Epoch 3 / 80) train acc: 0.069000; val_acc: 0.101000\n",
      "(Iteration 2301 / 61200) loss: 2.305520\n",
      "(Iteration 2401 / 61200) loss: 2.305525\n",
      "(Iteration 2501 / 61200) loss: 2.305528\n",
      "(Iteration 2601 / 61200) loss: 2.305520\n",
      "(Iteration 2701 / 61200) loss: 2.305518\n",
      "(Iteration 2801 / 61200) loss: 2.305517\n",
      "(Iteration 2901 / 61200) loss: 2.305525\n",
      "(Iteration 3001 / 61200) loss: 2.305521\n",
      "(Epoch 4 / 80) train acc: 0.088000; val_acc: 0.101000\n",
      "(Iteration 3101 / 61200) loss: 2.305516\n",
      "(Iteration 3201 / 61200) loss: 2.305514\n",
      "(Iteration 3301 / 61200) loss: 2.305531\n",
      "(Iteration 3401 / 61200) loss: 2.305519\n",
      "(Iteration 3501 / 61200) loss: 2.305516\n",
      "(Iteration 3601 / 61200) loss: 2.305514\n",
      "(Iteration 3701 / 61200) loss: 2.305512\n",
      "(Iteration 3801 / 61200) loss: 2.305533\n",
      "(Epoch 5 / 80) train acc: 0.104000; val_acc: 0.101000\n",
      "(Iteration 3901 / 61200) loss: 2.305520\n",
      "(Iteration 4001 / 61200) loss: 2.305500\n",
      "(Iteration 4101 / 61200) loss: 2.305515\n",
      "(Iteration 4201 / 61200) loss: 2.305522\n",
      "(Iteration 4301 / 61200) loss: 2.305516\n",
      "(Iteration 4401 / 61200) loss: 2.305525\n",
      "(Iteration 4501 / 61200) loss: 2.305527\n",
      "(Epoch 6 / 80) train acc: 0.085000; val_acc: 0.101000\n",
      "(Iteration 4601 / 61200) loss: 2.305520\n",
      "(Iteration 4701 / 61200) loss: 2.305506\n",
      "(Iteration 4801 / 61200) loss: 2.305513\n",
      "(Iteration 4901 / 61200) loss: 2.305518\n",
      "(Iteration 5001 / 61200) loss: 2.305512\n",
      "(Iteration 5101 / 61200) loss: 2.305526\n",
      "(Iteration 5201 / 61200) loss: 2.305511\n",
      "(Iteration 5301 / 61200) loss: 2.305514\n",
      "(Epoch 7 / 80) train acc: 0.091000; val_acc: 0.101000\n",
      "(Iteration 5401 / 61200) loss: 2.305505\n",
      "(Iteration 5501 / 61200) loss: 2.305515\n",
      "(Iteration 5601 / 61200) loss: 2.305529\n",
      "(Iteration 5701 / 61200) loss: 2.305515\n",
      "(Iteration 5801 / 61200) loss: 2.305511\n",
      "(Iteration 5901 / 61200) loss: 2.305512\n",
      "(Iteration 6001 / 61200) loss: 2.305517\n",
      "(Iteration 6101 / 61200) loss: 2.305528\n",
      "(Epoch 8 / 80) train acc: 0.084000; val_acc: 0.101000\n",
      "(Iteration 6201 / 61200) loss: 2.305532\n",
      "(Iteration 6301 / 61200) loss: 2.305518\n",
      "(Iteration 6401 / 61200) loss: 2.305527\n",
      "(Iteration 6501 / 61200) loss: 2.305513\n",
      "(Iteration 6601 / 61200) loss: 2.305525\n",
      "(Iteration 6701 / 61200) loss: 2.305515\n",
      "(Iteration 6801 / 61200) loss: 2.305513\n",
      "(Epoch 9 / 80) train acc: 0.084000; val_acc: 0.101000\n",
      "(Iteration 6901 / 61200) loss: 2.305526\n",
      "(Iteration 7001 / 61200) loss: 2.305510\n",
      "(Iteration 7101 / 61200) loss: 2.305518\n",
      "(Iteration 7201 / 61200) loss: 2.305529\n",
      "(Iteration 7301 / 61200) loss: 2.305526\n",
      "(Iteration 7401 / 61200) loss: 2.305522\n",
      "(Iteration 7501 / 61200) loss: 2.305521\n",
      "(Iteration 7601 / 61200) loss: 2.305521\n",
      "(Epoch 10 / 80) train acc: 0.072000; val_acc: 0.101000\n",
      "(Iteration 7701 / 61200) loss: 2.305528\n",
      "(Iteration 7801 / 61200) loss: 2.305522\n",
      "(Iteration 7901 / 61200) loss: 2.305509\n",
      "(Iteration 8001 / 61200) loss: 2.305509\n",
      "(Iteration 8101 / 61200) loss: 2.305512\n",
      "(Iteration 8201 / 61200) loss: 2.305526\n",
      "(Iteration 8301 / 61200) loss: 2.305519\n",
      "(Iteration 8401 / 61200) loss: 2.305520\n",
      "(Epoch 11 / 80) train acc: 0.092000; val_acc: 0.101000\n",
      "(Iteration 8501 / 61200) loss: 2.305518\n",
      "(Iteration 8601 / 61200) loss: 2.305531\n",
      "(Iteration 8701 / 61200) loss: 2.305519\n",
      "(Iteration 8801 / 61200) loss: 2.305523\n",
      "(Iteration 8901 / 61200) loss: 2.305502\n",
      "(Iteration 9001 / 61200) loss: 2.305509\n",
      "(Iteration 9101 / 61200) loss: 2.305509\n",
      "(Epoch 12 / 80) train acc: 0.081000; val_acc: 0.101000\n",
      "(Iteration 9201 / 61200) loss: 2.305520\n",
      "(Iteration 9301 / 61200) loss: 2.305516\n",
      "(Iteration 9401 / 61200) loss: 2.305525\n",
      "(Iteration 9501 / 61200) loss: 2.305517\n",
      "(Iteration 9601 / 61200) loss: 2.305520\n",
      "(Iteration 9701 / 61200) loss: 2.305518\n",
      "(Iteration 9801 / 61200) loss: 2.305513\n",
      "(Iteration 9901 / 61200) loss: 2.305513\n",
      "(Epoch 13 / 80) train acc: 0.077000; val_acc: 0.100000\n",
      "(Iteration 10001 / 61200) loss: 2.305531\n",
      "(Iteration 10101 / 61200) loss: 2.305513\n",
      "(Iteration 10201 / 61200) loss: 2.305518\n",
      "(Iteration 10301 / 61200) loss: 2.305527\n",
      "(Iteration 10401 / 61200) loss: 2.305514\n",
      "(Iteration 10501 / 61200) loss: 2.305518\n",
      "(Iteration 10601 / 61200) loss: 2.305512\n",
      "(Iteration 10701 / 61200) loss: 2.305521\n",
      "(Epoch 14 / 80) train acc: 0.088000; val_acc: 0.100000\n",
      "(Iteration 10801 / 61200) loss: 2.305518\n",
      "(Iteration 10901 / 61200) loss: 2.305515\n",
      "(Iteration 11001 / 61200) loss: 2.305519\n",
      "(Iteration 11101 / 61200) loss: 2.305536\n",
      "(Iteration 11201 / 61200) loss: 2.305514\n",
      "(Iteration 11301 / 61200) loss: 2.305516\n",
      "(Iteration 11401 / 61200) loss: 2.305509\n",
      "(Epoch 15 / 80) train acc: 0.092000; val_acc: 0.100000\n",
      "(Iteration 11501 / 61200) loss: 2.305517\n",
      "(Iteration 11601 / 61200) loss: 2.305513\n",
      "(Iteration 11701 / 61200) loss: 2.305510\n",
      "(Iteration 11801 / 61200) loss: 2.305514\n",
      "(Iteration 11901 / 61200) loss: 2.305529\n",
      "(Iteration 12001 / 61200) loss: 2.305507\n",
      "(Iteration 12101 / 61200) loss: 2.305520\n",
      "(Iteration 12201 / 61200) loss: 2.305511\n",
      "(Epoch 16 / 80) train acc: 0.087000; val_acc: 0.100000\n",
      "(Iteration 12301 / 61200) loss: 2.305520\n",
      "(Iteration 12401 / 61200) loss: 2.305509\n",
      "(Iteration 12501 / 61200) loss: 2.305512\n",
      "(Iteration 12601 / 61200) loss: 2.305507\n",
      "(Iteration 12701 / 61200) loss: 2.305519\n",
      "(Iteration 12801 / 61200) loss: 2.305505\n",
      "(Iteration 12901 / 61200) loss: 2.305508\n",
      "(Iteration 13001 / 61200) loss: 2.305518\n",
      "(Epoch 17 / 80) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 13101 / 61200) loss: 2.305513\n",
      "(Iteration 13201 / 61200) loss: 2.305524\n",
      "(Iteration 13301 / 61200) loss: 2.305516\n",
      "(Iteration 13401 / 61200) loss: 2.305524\n",
      "(Iteration 13501 / 61200) loss: 2.305515\n",
      "(Iteration 13601 / 61200) loss: 2.305512\n",
      "(Iteration 13701 / 61200) loss: 2.305529\n",
      "(Epoch 18 / 80) train acc: 0.100000; val_acc: 0.100000\n",
      "(Iteration 13801 / 61200) loss: 2.305516\n",
      "(Iteration 13901 / 61200) loss: 2.305522\n",
      "(Iteration 14001 / 61200) loss: 2.305523\n",
      "(Iteration 14101 / 61200) loss: 2.305517\n",
      "(Iteration 14201 / 61200) loss: 2.305509\n",
      "(Iteration 14301 / 61200) loss: 2.305518\n",
      "(Iteration 14401 / 61200) loss: 2.305525\n",
      "(Iteration 14501 / 61200) loss: 2.305518\n",
      "(Epoch 19 / 80) train acc: 0.079000; val_acc: 0.100000\n",
      "(Iteration 14601 / 61200) loss: 2.305510\n",
      "(Iteration 14701 / 61200) loss: 2.305529\n",
      "(Iteration 14801 / 61200) loss: 2.305514\n",
      "(Iteration 14901 / 61200) loss: 2.305517\n",
      "(Iteration 15001 / 61200) loss: 2.305522\n",
      "(Iteration 15101 / 61200) loss: 2.305532\n",
      "(Iteration 15201 / 61200) loss: 2.305514\n",
      "(Epoch 20 / 80) train acc: 0.091000; val_acc: 0.100000\n",
      "(Iteration 15301 / 61200) loss: 2.305520\n",
      "(Iteration 15401 / 61200) loss: 2.305511\n",
      "(Iteration 15501 / 61200) loss: 2.305527\n",
      "(Iteration 15601 / 61200) loss: 2.305512\n",
      "(Iteration 15701 / 61200) loss: 2.305515\n",
      "(Iteration 15801 / 61200) loss: 2.305512\n",
      "(Iteration 15901 / 61200) loss: 2.305518\n",
      "(Iteration 16001 / 61200) loss: 2.305517\n",
      "(Epoch 21 / 80) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 16101 / 61200) loss: 2.305516\n",
      "(Iteration 16201 / 61200) loss: 2.305517\n",
      "(Iteration 16301 / 61200) loss: 2.305516\n",
      "(Iteration 16401 / 61200) loss: 2.305513\n",
      "(Iteration 16501 / 61200) loss: 2.305526\n",
      "(Iteration 16601 / 61200) loss: 2.305529\n",
      "(Iteration 16701 / 61200) loss: 2.305517\n",
      "(Iteration 16801 / 61200) loss: 2.305528\n",
      "(Epoch 22 / 80) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 16901 / 61200) loss: 2.305512\n",
      "(Iteration 17001 / 61200) loss: 2.305517\n",
      "(Iteration 17101 / 61200) loss: 2.305504\n",
      "(Iteration 17201 / 61200) loss: 2.305508\n",
      "(Iteration 17301 / 61200) loss: 2.305514\n",
      "(Iteration 17401 / 61200) loss: 2.305517\n",
      "(Iteration 17501 / 61200) loss: 2.305514\n",
      "(Epoch 23 / 80) train acc: 0.100000; val_acc: 0.100000\n",
      "(Iteration 17601 / 61200) loss: 2.305513\n",
      "(Iteration 17701 / 61200) loss: 2.305529\n",
      "(Iteration 17801 / 61200) loss: 2.305520\n",
      "(Iteration 17901 / 61200) loss: 2.305528\n",
      "(Iteration 18001 / 61200) loss: 2.305515\n",
      "(Iteration 18101 / 61200) loss: 2.305507\n",
      "(Iteration 18201 / 61200) loss: 2.305509\n",
      "(Iteration 18301 / 61200) loss: 2.305527\n",
      "(Epoch 24 / 80) train acc: 0.092000; val_acc: 0.100000\n",
      "(Iteration 18401 / 61200) loss: 2.305514\n",
      "(Iteration 18501 / 61200) loss: 2.305507\n",
      "(Iteration 18601 / 61200) loss: 2.305522\n",
      "(Iteration 18701 / 61200) loss: 2.305510\n",
      "(Iteration 18801 / 61200) loss: 2.305518\n",
      "(Iteration 18901 / 61200) loss: 2.305522\n",
      "(Iteration 19001 / 61200) loss: 2.305511\n",
      "(Iteration 19101 / 61200) loss: 2.305508\n",
      "(Epoch 25 / 80) train acc: 0.085000; val_acc: 0.100000\n",
      "(Iteration 19201 / 61200) loss: 2.305510\n",
      "(Iteration 19301 / 61200) loss: 2.305515\n",
      "(Iteration 19401 / 61200) loss: 2.305522\n",
      "(Iteration 19501 / 61200) loss: 2.305529\n",
      "(Iteration 19601 / 61200) loss: 2.305518\n",
      "(Iteration 19701 / 61200) loss: 2.305511\n",
      "(Iteration 19801 / 61200) loss: 2.305508\n",
      "(Epoch 26 / 80) train acc: 0.089000; val_acc: 0.100000\n",
      "(Iteration 19901 / 61200) loss: 2.305522\n",
      "(Iteration 20001 / 61200) loss: 2.305513\n",
      "(Iteration 20101 / 61200) loss: 2.305512\n",
      "(Iteration 20201 / 61200) loss: 2.305524\n",
      "(Iteration 20301 / 61200) loss: 2.305524\n",
      "(Iteration 20401 / 61200) loss: 2.305515\n",
      "(Iteration 20501 / 61200) loss: 2.305522\n",
      "(Iteration 20601 / 61200) loss: 2.305514\n",
      "(Epoch 27 / 80) train acc: 0.092000; val_acc: 0.100000\n",
      "(Iteration 20701 / 61200) loss: 2.305515\n",
      "(Iteration 20801 / 61200) loss: 2.305517\n",
      "(Iteration 20901 / 61200) loss: 2.305512\n",
      "(Iteration 21001 / 61200) loss: 2.305510\n",
      "(Iteration 21101 / 61200) loss: 2.305499\n",
      "(Iteration 21201 / 61200) loss: 2.305520\n",
      "(Iteration 21301 / 61200) loss: 2.305495\n",
      "(Iteration 21401 / 61200) loss: 2.305523\n",
      "(Epoch 28 / 80) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 21501 / 61200) loss: 2.305514\n",
      "(Iteration 21601 / 61200) loss: 2.305518\n",
      "(Iteration 21701 / 61200) loss: 2.305515\n",
      "(Iteration 21801 / 61200) loss: 2.305510\n",
      "(Iteration 21901 / 61200) loss: 2.305527\n",
      "(Iteration 22001 / 61200) loss: 2.305513\n",
      "(Iteration 22101 / 61200) loss: 2.305529\n",
      "(Epoch 29 / 80) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 22201 / 61200) loss: 2.305518\n",
      "(Iteration 22301 / 61200) loss: 2.305516\n",
      "(Iteration 22401 / 61200) loss: 2.305524\n",
      "(Iteration 22501 / 61200) loss: 2.305508\n",
      "(Iteration 22601 / 61200) loss: 2.305524\n",
      "(Iteration 22701 / 61200) loss: 2.305520\n",
      "(Iteration 22801 / 61200) loss: 2.305520\n",
      "(Iteration 22901 / 61200) loss: 2.305517\n",
      "(Epoch 30 / 80) train acc: 0.088000; val_acc: 0.100000\n",
      "(Iteration 23001 / 61200) loss: 2.305511\n",
      "(Iteration 23101 / 61200) loss: 2.305514\n",
      "(Iteration 23201 / 61200) loss: 2.305517\n",
      "(Iteration 23301 / 61200) loss: 2.305532\n",
      "(Iteration 23401 / 61200) loss: 2.305517\n",
      "(Iteration 23501 / 61200) loss: 2.305519\n",
      "(Iteration 23601 / 61200) loss: 2.305519\n",
      "(Iteration 23701 / 61200) loss: 2.305519\n",
      "(Epoch 31 / 80) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 23801 / 61200) loss: 2.305524\n",
      "(Iteration 23901 / 61200) loss: 2.305522\n",
      "(Iteration 24001 / 61200) loss: 2.305508\n",
      "(Iteration 24101 / 61200) loss: 2.305517\n",
      "(Iteration 24201 / 61200) loss: 2.305520\n",
      "(Iteration 24301 / 61200) loss: 2.305523\n",
      "(Iteration 24401 / 61200) loss: 2.305514\n",
      "(Epoch 32 / 80) train acc: 0.087000; val_acc: 0.100000\n",
      "(Iteration 24501 / 61200) loss: 2.305516\n",
      "(Iteration 24601 / 61200) loss: 2.305524\n",
      "(Iteration 24701 / 61200) loss: 2.305524\n",
      "(Iteration 24801 / 61200) loss: 2.305518\n",
      "(Iteration 24901 / 61200) loss: 2.305525\n",
      "(Iteration 25001 / 61200) loss: 2.305524\n",
      "(Iteration 25101 / 61200) loss: 2.305513\n",
      "(Iteration 25201 / 61200) loss: 2.305509\n",
      "(Epoch 33 / 80) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 25301 / 61200) loss: 2.305516\n",
      "(Iteration 25401 / 61200) loss: 2.305516\n",
      "(Iteration 25501 / 61200) loss: 2.305517\n",
      "(Iteration 25601 / 61200) loss: 2.305513\n",
      "(Iteration 25701 / 61200) loss: 2.305496\n",
      "(Iteration 25801 / 61200) loss: 2.305514\n",
      "(Iteration 25901 / 61200) loss: 2.305505\n",
      "(Iteration 26001 / 61200) loss: 2.305511\n",
      "(Epoch 34 / 80) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 26101 / 61200) loss: 2.305509\n",
      "(Iteration 26201 / 61200) loss: 2.305518\n",
      "(Iteration 26301 / 61200) loss: 2.305518\n",
      "(Iteration 26401 / 61200) loss: 2.305526\n",
      "(Iteration 26501 / 61200) loss: 2.305511\n",
      "(Iteration 26601 / 61200) loss: 2.305523\n",
      "(Iteration 26701 / 61200) loss: 2.305513\n",
      "(Epoch 35 / 80) train acc: 0.084000; val_acc: 0.100000\n",
      "(Iteration 26801 / 61200) loss: 2.305520\n",
      "(Iteration 26901 / 61200) loss: 2.305507\n",
      "(Iteration 27001 / 61200) loss: 2.305515\n",
      "(Iteration 27101 / 61200) loss: 2.305519\n",
      "(Iteration 27201 / 61200) loss: 2.305531\n",
      "(Iteration 27301 / 61200) loss: 2.305506\n",
      "(Iteration 27401 / 61200) loss: 2.305518\n",
      "(Iteration 27501 / 61200) loss: 2.305507\n",
      "(Epoch 36 / 80) train acc: 0.081000; val_acc: 0.100000\n",
      "(Iteration 27601 / 61200) loss: 2.305512\n",
      "(Iteration 27701 / 61200) loss: 2.305507\n",
      "(Iteration 27801 / 61200) loss: 2.305516\n",
      "(Iteration 27901 / 61200) loss: 2.305533\n",
      "(Iteration 28001 / 61200) loss: 2.305506\n",
      "(Iteration 28101 / 61200) loss: 2.305503\n",
      "(Iteration 28201 / 61200) loss: 2.305513\n",
      "(Iteration 28301 / 61200) loss: 2.305527\n",
      "(Epoch 37 / 80) train acc: 0.069000; val_acc: 0.100000\n",
      "(Iteration 28401 / 61200) loss: 2.305508\n",
      "(Iteration 28501 / 61200) loss: 2.305519\n",
      "(Iteration 28601 / 61200) loss: 2.305520\n",
      "(Iteration 28701 / 61200) loss: 2.305512\n",
      "(Iteration 28801 / 61200) loss: 2.305520\n",
      "(Iteration 28901 / 61200) loss: 2.305520\n",
      "(Iteration 29001 / 61200) loss: 2.305522\n",
      "(Epoch 38 / 80) train acc: 0.111000; val_acc: 0.100000\n",
      "(Iteration 29101 / 61200) loss: 2.305498\n",
      "(Iteration 29201 / 61200) loss: 2.305520\n",
      "(Iteration 29301 / 61200) loss: 2.305516\n",
      "(Iteration 29401 / 61200) loss: 2.305521\n",
      "(Iteration 29501 / 61200) loss: 2.305511\n",
      "(Iteration 29601 / 61200) loss: 2.305516\n",
      "(Iteration 29701 / 61200) loss: 2.305518\n",
      "(Iteration 29801 / 61200) loss: 2.305522\n",
      "(Epoch 39 / 80) train acc: 0.081000; val_acc: 0.100000\n",
      "(Iteration 29901 / 61200) loss: 2.305519\n",
      "(Iteration 30001 / 61200) loss: 2.305510\n",
      "(Iteration 30101 / 61200) loss: 2.305521\n",
      "(Iteration 30201 / 61200) loss: 2.305513\n",
      "(Iteration 30301 / 61200) loss: 2.305503\n",
      "(Iteration 30401 / 61200) loss: 2.305514\n",
      "(Iteration 30501 / 61200) loss: 2.305520\n",
      "(Epoch 40 / 80) train acc: 0.078000; val_acc: 0.100000\n",
      "(Iteration 30601 / 61200) loss: 2.305522\n",
      "(Iteration 30701 / 61200) loss: 2.305517\n",
      "(Iteration 30801 / 61200) loss: 2.305514\n",
      "(Iteration 30901 / 61200) loss: 2.305526\n",
      "(Iteration 31001 / 61200) loss: 2.305522\n",
      "(Iteration 31101 / 61200) loss: 2.305507\n",
      "(Iteration 31201 / 61200) loss: 2.305516\n",
      "(Iteration 31301 / 61200) loss: 2.305503\n",
      "(Epoch 41 / 80) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 31401 / 61200) loss: 2.305520\n",
      "(Iteration 31501 / 61200) loss: 2.305531\n",
      "(Iteration 31601 / 61200) loss: 2.305508\n",
      "(Iteration 31701 / 61200) loss: 2.305527\n",
      "(Iteration 31801 / 61200) loss: 2.305504\n",
      "(Iteration 31901 / 61200) loss: 2.305528\n",
      "(Iteration 32001 / 61200) loss: 2.305512\n",
      "(Iteration 32101 / 61200) loss: 2.305518\n",
      "(Epoch 42 / 80) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 32201 / 61200) loss: 2.305515\n",
      "(Iteration 32301 / 61200) loss: 2.305517\n",
      "(Iteration 32401 / 61200) loss: 2.305521\n",
      "(Iteration 32501 / 61200) loss: 2.305519\n",
      "(Iteration 32601 / 61200) loss: 2.305516\n",
      "(Iteration 32701 / 61200) loss: 2.305516\n",
      "(Iteration 32801 / 61200) loss: 2.305508\n",
      "(Epoch 43 / 80) train acc: 0.086000; val_acc: 0.100000\n",
      "(Iteration 32901 / 61200) loss: 2.305522\n",
      "(Iteration 33001 / 61200) loss: 2.305500\n",
      "(Iteration 33101 / 61200) loss: 2.305512\n",
      "(Iteration 33201 / 61200) loss: 2.305519\n",
      "(Iteration 33301 / 61200) loss: 2.305512\n",
      "(Iteration 33401 / 61200) loss: 2.305513\n",
      "(Iteration 33501 / 61200) loss: 2.305526\n",
      "(Iteration 33601 / 61200) loss: 2.305513\n",
      "(Epoch 44 / 80) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 33701 / 61200) loss: 2.305524\n",
      "(Iteration 33801 / 61200) loss: 2.305507\n",
      "(Iteration 33901 / 61200) loss: 2.305515\n",
      "(Iteration 34001 / 61200) loss: 2.305508\n",
      "(Iteration 34101 / 61200) loss: 2.305527\n",
      "(Iteration 34201 / 61200) loss: 2.305511\n",
      "(Iteration 34301 / 61200) loss: 2.305502\n",
      "(Iteration 34401 / 61200) loss: 2.305523\n",
      "(Epoch 45 / 80) train acc: 0.111000; val_acc: 0.100000\n",
      "(Iteration 34501 / 61200) loss: 2.305512\n",
      "(Iteration 34601 / 61200) loss: 2.305511\n",
      "(Iteration 34701 / 61200) loss: 2.305515\n",
      "(Iteration 34801 / 61200) loss: 2.305510\n",
      "(Iteration 34901 / 61200) loss: 2.305512\n",
      "(Iteration 35001 / 61200) loss: 2.305519\n",
      "(Iteration 35101 / 61200) loss: 2.305517\n",
      "(Epoch 46 / 80) train acc: 0.105000; val_acc: 0.100000\n",
      "(Iteration 35201 / 61200) loss: 2.305504\n",
      "(Iteration 35301 / 61200) loss: 2.305507\n",
      "(Iteration 35401 / 61200) loss: 2.305515\n",
      "(Iteration 35501 / 61200) loss: 2.305506\n",
      "(Iteration 35601 / 61200) loss: 2.305518\n",
      "(Iteration 35701 / 61200) loss: 2.305518\n",
      "(Iteration 35801 / 61200) loss: 2.305509\n",
      "(Iteration 35901 / 61200) loss: 2.305513\n",
      "(Epoch 47 / 80) train acc: 0.092000; val_acc: 0.100000\n",
      "(Iteration 36001 / 61200) loss: 2.305512\n",
      "(Iteration 36101 / 61200) loss: 2.305517\n",
      "(Iteration 36201 / 61200) loss: 2.305528\n",
      "(Iteration 36301 / 61200) loss: 2.305533\n",
      "(Iteration 36401 / 61200) loss: 2.305519\n",
      "(Iteration 36501 / 61200) loss: 2.305514\n",
      "(Iteration 36601 / 61200) loss: 2.305518\n",
      "(Iteration 36701 / 61200) loss: 2.305524\n",
      "(Epoch 48 / 80) train acc: 0.092000; val_acc: 0.100000\n",
      "(Iteration 36801 / 61200) loss: 2.305523\n",
      "(Iteration 36901 / 61200) loss: 2.305525\n",
      "(Iteration 37001 / 61200) loss: 2.305524\n",
      "(Iteration 37101 / 61200) loss: 2.305518\n",
      "(Iteration 37201 / 61200) loss: 2.305531\n",
      "(Iteration 37301 / 61200) loss: 2.305521\n",
      "(Iteration 37401 / 61200) loss: 2.305517\n",
      "(Epoch 49 / 80) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 37501 / 61200) loss: 2.305526\n",
      "(Iteration 37601 / 61200) loss: 2.305514\n",
      "(Iteration 37701 / 61200) loss: 2.305506\n",
      "(Iteration 37801 / 61200) loss: 2.305533\n",
      "(Iteration 37901 / 61200) loss: 2.305516\n",
      "(Iteration 38001 / 61200) loss: 2.305519\n",
      "(Iteration 38101 / 61200) loss: 2.305530\n",
      "(Iteration 38201 / 61200) loss: 2.305511\n",
      "(Epoch 50 / 80) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 38301 / 61200) loss: 2.305518\n",
      "(Iteration 38401 / 61200) loss: 2.305519\n",
      "(Iteration 38501 / 61200) loss: 2.305516\n",
      "(Iteration 38601 / 61200) loss: 2.305520\n",
      "(Iteration 38701 / 61200) loss: 2.305511\n",
      "(Iteration 38801 / 61200) loss: 2.305512\n",
      "(Iteration 38901 / 61200) loss: 2.305516\n",
      "(Iteration 39001 / 61200) loss: 2.305508\n",
      "(Epoch 51 / 80) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 39101 / 61200) loss: 2.305514\n",
      "(Iteration 39201 / 61200) loss: 2.305523\n",
      "(Iteration 39301 / 61200) loss: 2.305517\n",
      "(Iteration 39401 / 61200) loss: 2.305507\n",
      "(Iteration 39501 / 61200) loss: 2.305520\n",
      "(Iteration 39601 / 61200) loss: 2.305510\n",
      "(Iteration 39701 / 61200) loss: 2.305525\n",
      "(Epoch 52 / 80) train acc: 0.085000; val_acc: 0.100000\n",
      "(Iteration 39801 / 61200) loss: 2.305514\n",
      "(Iteration 39901 / 61200) loss: 2.305514\n",
      "(Iteration 40001 / 61200) loss: 2.305517\n",
      "(Iteration 40101 / 61200) loss: 2.305521\n",
      "(Iteration 40201 / 61200) loss: 2.305527\n",
      "(Iteration 40301 / 61200) loss: 2.305509\n",
      "(Iteration 40401 / 61200) loss: 2.305524\n",
      "(Iteration 40501 / 61200) loss: 2.305509\n",
      "(Epoch 53 / 80) train acc: 0.083000; val_acc: 0.100000\n",
      "(Iteration 40601 / 61200) loss: 2.305520\n",
      "(Iteration 40701 / 61200) loss: 2.305513\n",
      "(Iteration 40801 / 61200) loss: 2.305515\n",
      "(Iteration 40901 / 61200) loss: 2.305527\n",
      "(Iteration 41001 / 61200) loss: 2.305522\n",
      "(Iteration 41101 / 61200) loss: 2.305521\n",
      "(Iteration 41201 / 61200) loss: 2.305515\n",
      "(Iteration 41301 / 61200) loss: 2.305524\n",
      "(Epoch 54 / 80) train acc: 0.088000; val_acc: 0.100000\n",
      "(Iteration 41401 / 61200) loss: 2.305501\n",
      "(Iteration 41501 / 61200) loss: 2.305532\n",
      "(Iteration 41601 / 61200) loss: 2.305525\n",
      "(Iteration 41701 / 61200) loss: 2.305514\n",
      "(Iteration 41801 / 61200) loss: 2.305516\n",
      "(Iteration 41901 / 61200) loss: 2.305509\n",
      "(Iteration 42001 / 61200) loss: 2.305507\n",
      "(Epoch 55 / 80) train acc: 0.103000; val_acc: 0.100000\n",
      "(Iteration 42101 / 61200) loss: 2.305521\n",
      "(Iteration 42201 / 61200) loss: 2.305509\n",
      "(Iteration 42301 / 61200) loss: 2.305522\n",
      "(Iteration 42401 / 61200) loss: 2.305505\n",
      "(Iteration 42501 / 61200) loss: 2.305515\n",
      "(Iteration 42601 / 61200) loss: 2.305519\n",
      "(Iteration 42701 / 61200) loss: 2.305516\n",
      "(Iteration 42801 / 61200) loss: 2.305507\n",
      "(Epoch 56 / 80) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 42901 / 61200) loss: 2.305513\n",
      "(Iteration 43001 / 61200) loss: 2.305513\n",
      "(Iteration 43101 / 61200) loss: 2.305521\n",
      "(Iteration 43201 / 61200) loss: 2.305519\n",
      "(Iteration 43301 / 61200) loss: 2.305499\n",
      "(Iteration 43401 / 61200) loss: 2.305507\n",
      "(Iteration 43501 / 61200) loss: 2.305514\n",
      "(Iteration 43601 / 61200) loss: 2.305523\n",
      "(Epoch 57 / 80) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 43701 / 61200) loss: 2.305516\n",
      "(Iteration 43801 / 61200) loss: 2.305514\n",
      "(Iteration 43901 / 61200) loss: 2.305493\n",
      "(Iteration 44001 / 61200) loss: 2.305518\n",
      "(Iteration 44101 / 61200) loss: 2.305518\n",
      "(Iteration 44201 / 61200) loss: 2.305512\n",
      "(Iteration 44301 / 61200) loss: 2.305521\n",
      "(Epoch 58 / 80) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 44401 / 61200) loss: 2.305515\n",
      "(Iteration 44501 / 61200) loss: 2.305519\n",
      "(Iteration 44601 / 61200) loss: 2.305504\n",
      "(Iteration 44701 / 61200) loss: 2.305510\n",
      "(Iteration 44801 / 61200) loss: 2.305515\n",
      "(Iteration 44901 / 61200) loss: 2.305512\n",
      "(Iteration 45001 / 61200) loss: 2.305504\n",
      "(Iteration 45101 / 61200) loss: 2.305507\n",
      "(Epoch 59 / 80) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 45201 / 61200) loss: 2.305524\n",
      "(Iteration 45301 / 61200) loss: 2.305510\n",
      "(Iteration 45401 / 61200) loss: 2.305514\n",
      "(Iteration 45501 / 61200) loss: 2.305512\n",
      "(Iteration 45601 / 61200) loss: 2.305508\n",
      "(Iteration 45701 / 61200) loss: 2.305529\n",
      "(Iteration 45801 / 61200) loss: 2.305511\n",
      "(Epoch 60 / 80) train acc: 0.071000; val_acc: 0.100000\n",
      "(Iteration 45901 / 61200) loss: 2.305520\n",
      "(Iteration 46001 / 61200) loss: 2.305514\n",
      "(Iteration 46101 / 61200) loss: 2.305515\n",
      "(Iteration 46201 / 61200) loss: 2.305514\n",
      "(Iteration 46301 / 61200) loss: 2.305521\n",
      "(Iteration 46401 / 61200) loss: 2.305505\n",
      "(Iteration 46501 / 61200) loss: 2.305513\n",
      "(Iteration 46601 / 61200) loss: 2.305509\n",
      "(Epoch 61 / 80) train acc: 0.111000; val_acc: 0.100000\n",
      "(Iteration 46701 / 61200) loss: 2.305522\n",
      "(Iteration 46801 / 61200) loss: 2.305524\n",
      "(Iteration 46901 / 61200) loss: 2.305511\n",
      "(Iteration 47001 / 61200) loss: 2.305505\n",
      "(Iteration 47101 / 61200) loss: 2.305517\n",
      "(Iteration 47201 / 61200) loss: 2.305518\n",
      "(Iteration 47301 / 61200) loss: 2.305520\n",
      "(Iteration 47401 / 61200) loss: 2.305517\n",
      "(Epoch 62 / 80) train acc: 0.087000; val_acc: 0.100000\n",
      "(Iteration 47501 / 61200) loss: 2.305519\n",
      "(Iteration 47601 / 61200) loss: 2.305507\n",
      "(Iteration 47701 / 61200) loss: 2.305511\n",
      "(Iteration 47801 / 61200) loss: 2.305509\n",
      "(Iteration 47901 / 61200) loss: 2.305512\n",
      "(Iteration 48001 / 61200) loss: 2.305524\n",
      "(Iteration 48101 / 61200) loss: 2.305521\n",
      "(Epoch 63 / 80) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 48201 / 61200) loss: 2.305513\n",
      "(Iteration 48301 / 61200) loss: 2.305513\n",
      "(Iteration 48401 / 61200) loss: 2.305514\n",
      "(Iteration 48501 / 61200) loss: 2.305518\n",
      "(Iteration 48601 / 61200) loss: 2.305509\n",
      "(Iteration 48701 / 61200) loss: 2.305526\n",
      "(Iteration 48801 / 61200) loss: 2.305518\n",
      "(Iteration 48901 / 61200) loss: 2.305523\n",
      "(Epoch 64 / 80) train acc: 0.081000; val_acc: 0.100000\n",
      "(Iteration 49001 / 61200) loss: 2.305512\n",
      "(Iteration 49101 / 61200) loss: 2.305520\n",
      "(Iteration 49201 / 61200) loss: 2.305519\n",
      "(Iteration 49301 / 61200) loss: 2.305516\n",
      "(Iteration 49401 / 61200) loss: 2.305514\n",
      "(Iteration 49501 / 61200) loss: 2.305515\n",
      "(Iteration 49601 / 61200) loss: 2.305511\n",
      "(Iteration 49701 / 61200) loss: 2.305524\n",
      "(Epoch 65 / 80) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 49801 / 61200) loss: 2.305521\n",
      "(Iteration 49901 / 61200) loss: 2.305509\n",
      "(Iteration 50001 / 61200) loss: 2.305508\n",
      "(Iteration 50101 / 61200) loss: 2.305526\n",
      "(Iteration 50201 / 61200) loss: 2.305510\n",
      "(Iteration 50301 / 61200) loss: 2.305523\n",
      "(Iteration 50401 / 61200) loss: 2.305518\n",
      "(Epoch 66 / 80) train acc: 0.091000; val_acc: 0.100000\n",
      "(Iteration 50501 / 61200) loss: 2.305525\n",
      "(Iteration 50601 / 61200) loss: 2.305506\n",
      "(Iteration 50701 / 61200) loss: 2.305524\n",
      "(Iteration 50801 / 61200) loss: 2.305518\n",
      "(Iteration 50901 / 61200) loss: 2.305510\n",
      "(Iteration 51001 / 61200) loss: 2.305525\n",
      "(Iteration 51101 / 61200) loss: 2.305523\n",
      "(Iteration 51201 / 61200) loss: 2.305509\n",
      "(Epoch 67 / 80) train acc: 0.102000; val_acc: 0.100000\n",
      "(Iteration 51301 / 61200) loss: 2.305526\n",
      "(Iteration 51401 / 61200) loss: 2.305517\n",
      "(Iteration 51501 / 61200) loss: 2.305518\n",
      "(Iteration 51601 / 61200) loss: 2.305522\n",
      "(Iteration 51701 / 61200) loss: 2.305522\n",
      "(Iteration 51801 / 61200) loss: 2.305518\n",
      "(Iteration 51901 / 61200) loss: 2.305514\n",
      "(Iteration 52001 / 61200) loss: 2.305500\n",
      "(Epoch 68 / 80) train acc: 0.107000; val_acc: 0.100000\n",
      "(Iteration 52101 / 61200) loss: 2.305528\n",
      "(Iteration 52201 / 61200) loss: 2.305523\n",
      "(Iteration 52301 / 61200) loss: 2.305512\n",
      "(Iteration 52401 / 61200) loss: 2.305516\n",
      "(Iteration 52501 / 61200) loss: 2.305519\n",
      "(Iteration 52601 / 61200) loss: 2.305508\n",
      "(Iteration 52701 / 61200) loss: 2.305510\n",
      "(Epoch 69 / 80) train acc: 0.084000; val_acc: 0.100000\n",
      "(Iteration 52801 / 61200) loss: 2.305511\n",
      "(Iteration 52901 / 61200) loss: 2.305501\n",
      "(Iteration 53001 / 61200) loss: 2.305525\n",
      "(Iteration 53101 / 61200) loss: 2.305511\n",
      "(Iteration 53201 / 61200) loss: 2.305521\n",
      "(Iteration 53301 / 61200) loss: 2.305513\n",
      "(Iteration 53401 / 61200) loss: 2.305508\n",
      "(Iteration 53501 / 61200) loss: 2.305524\n",
      "(Epoch 70 / 80) train acc: 0.081000; val_acc: 0.100000\n",
      "(Iteration 53601 / 61200) loss: 2.305522\n",
      "(Iteration 53701 / 61200) loss: 2.305527\n",
      "(Iteration 53801 / 61200) loss: 2.305515\n",
      "(Iteration 53901 / 61200) loss: 2.305516\n",
      "(Iteration 54001 / 61200) loss: 2.305515\n",
      "(Iteration 54101 / 61200) loss: 2.305521\n",
      "(Iteration 54201 / 61200) loss: 2.305518\n",
      "(Iteration 54301 / 61200) loss: 2.305513\n",
      "(Epoch 71 / 80) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 54401 / 61200) loss: 2.305510\n",
      "(Iteration 54501 / 61200) loss: 2.305518\n",
      "(Iteration 54601 / 61200) loss: 2.305522\n",
      "(Iteration 54701 / 61200) loss: 2.305496\n",
      "(Iteration 54801 / 61200) loss: 2.305506\n",
      "(Iteration 54901 / 61200) loss: 2.305519\n",
      "(Iteration 55001 / 61200) loss: 2.305525\n",
      "(Epoch 72 / 80) train acc: 0.102000; val_acc: 0.100000\n",
      "(Iteration 55101 / 61200) loss: 2.305518\n",
      "(Iteration 55201 / 61200) loss: 2.305523\n",
      "(Iteration 55301 / 61200) loss: 2.305506\n",
      "(Iteration 55401 / 61200) loss: 2.305516\n",
      "(Iteration 55501 / 61200) loss: 2.305510\n",
      "(Iteration 55601 / 61200) loss: 2.305527\n",
      "(Iteration 55701 / 61200) loss: 2.305507\n",
      "(Iteration 55801 / 61200) loss: 2.305516\n",
      "(Epoch 73 / 80) train acc: 0.074000; val_acc: 0.100000\n",
      "(Iteration 55901 / 61200) loss: 2.305517\n",
      "(Iteration 56001 / 61200) loss: 2.305517\n",
      "(Iteration 56101 / 61200) loss: 2.305517\n",
      "(Iteration 56201 / 61200) loss: 2.305505\n",
      "(Iteration 56301 / 61200) loss: 2.305518\n",
      "(Iteration 56401 / 61200) loss: 2.305511\n",
      "(Iteration 56501 / 61200) loss: 2.305524\n",
      "(Iteration 56601 / 61200) loss: 2.305533\n",
      "(Epoch 74 / 80) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 56701 / 61200) loss: 2.305516\n",
      "(Iteration 56801 / 61200) loss: 2.305508\n",
      "(Iteration 56901 / 61200) loss: 2.305512\n",
      "(Iteration 57001 / 61200) loss: 2.305523\n",
      "(Iteration 57101 / 61200) loss: 2.305512\n",
      "(Iteration 57201 / 61200) loss: 2.305516\n",
      "(Iteration 57301 / 61200) loss: 2.305521\n",
      "(Epoch 75 / 80) train acc: 0.087000; val_acc: 0.100000\n",
      "(Iteration 57401 / 61200) loss: 2.305525\n",
      "(Iteration 57501 / 61200) loss: 2.305517\n",
      "(Iteration 57601 / 61200) loss: 2.305520\n",
      "(Iteration 57701 / 61200) loss: 2.305514\n",
      "(Iteration 57801 / 61200) loss: 2.305507\n",
      "(Iteration 57901 / 61200) loss: 2.305513\n",
      "(Iteration 58001 / 61200) loss: 2.305516\n",
      "(Iteration 58101 / 61200) loss: 2.305513\n",
      "(Epoch 76 / 80) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 58201 / 61200) loss: 2.305525\n",
      "(Iteration 58301 / 61200) loss: 2.305502\n",
      "(Iteration 58401 / 61200) loss: 2.305523\n",
      "(Iteration 58501 / 61200) loss: 2.305522\n",
      "(Iteration 58601 / 61200) loss: 2.305514\n",
      "(Iteration 58701 / 61200) loss: 2.305506\n",
      "(Iteration 58801 / 61200) loss: 2.305515\n",
      "(Iteration 58901 / 61200) loss: 2.305512\n",
      "(Epoch 77 / 80) train acc: 0.103000; val_acc: 0.100000\n",
      "(Iteration 59001 / 61200) loss: 2.305513\n",
      "(Iteration 59101 / 61200) loss: 2.305519\n",
      "(Iteration 59201 / 61200) loss: 2.305521\n",
      "(Iteration 59301 / 61200) loss: 2.305519\n",
      "(Iteration 59401 / 61200) loss: 2.305509\n",
      "(Iteration 59501 / 61200) loss: 2.305516\n",
      "(Iteration 59601 / 61200) loss: 2.305508\n",
      "(Epoch 78 / 80) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 59701 / 61200) loss: 2.305514\n",
      "(Iteration 59801 / 61200) loss: 2.305519\n",
      "(Iteration 59901 / 61200) loss: 2.305515\n",
      "(Iteration 60001 / 61200) loss: 2.305513\n",
      "(Iteration 60101 / 61200) loss: 2.305516\n",
      "(Iteration 60201 / 61200) loss: 2.305513\n",
      "(Iteration 60301 / 61200) loss: 2.305509\n",
      "(Iteration 60401 / 61200) loss: 2.305521\n",
      "(Epoch 79 / 80) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 60501 / 61200) loss: 2.305532\n",
      "(Iteration 60601 / 61200) loss: 2.305524\n",
      "(Iteration 60701 / 61200) loss: 2.305518\n",
      "(Iteration 60801 / 61200) loss: 2.305519\n",
      "(Iteration 60901 / 61200) loss: 2.305514\n",
      "(Iteration 61001 / 61200) loss: 2.305516\n",
      "(Iteration 61101 / 61200) loss: 2.305519\n",
      "(Epoch 80 / 80) train acc: 0.101000; val_acc: 0.100000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.305506\n",
      "(Epoch 0 / 80) train acc: 0.080000; val_acc: 0.105000\n",
      "(Iteration 101 / 30560) loss: 2.305510\n",
      "(Iteration 201 / 30560) loss: 2.305512\n",
      "(Iteration 301 / 30560) loss: 2.305517\n",
      "(Epoch 1 / 80) train acc: 0.072000; val_acc: 0.105000\n",
      "(Iteration 401 / 30560) loss: 2.305518\n",
      "(Iteration 501 / 30560) loss: 2.305518\n",
      "(Iteration 601 / 30560) loss: 2.305508\n",
      "(Iteration 701 / 30560) loss: 2.305509\n",
      "(Epoch 2 / 80) train acc: 0.080000; val_acc: 0.106000\n",
      "(Iteration 801 / 30560) loss: 2.305514\n",
      "(Iteration 901 / 30560) loss: 2.305504\n",
      "(Iteration 1001 / 30560) loss: 2.305517\n",
      "(Iteration 1101 / 30560) loss: 2.305513\n",
      "(Epoch 3 / 80) train acc: 0.085000; val_acc: 0.105000\n",
      "(Iteration 1201 / 30560) loss: 2.305514\n",
      "(Iteration 1301 / 30560) loss: 2.305512\n",
      "(Iteration 1401 / 30560) loss: 2.305511\n",
      "(Iteration 1501 / 30560) loss: 2.305513\n",
      "(Epoch 4 / 80) train acc: 0.082000; val_acc: 0.106000\n",
      "(Iteration 1601 / 30560) loss: 2.305514\n",
      "(Iteration 1701 / 30560) loss: 2.305512\n",
      "(Iteration 1801 / 30560) loss: 2.305507\n",
      "(Iteration 1901 / 30560) loss: 2.305516\n",
      "(Epoch 5 / 80) train acc: 0.079000; val_acc: 0.106000\n",
      "(Iteration 2001 / 30560) loss: 2.305505\n",
      "(Iteration 2101 / 30560) loss: 2.305508\n",
      "(Iteration 2201 / 30560) loss: 2.305520\n",
      "(Epoch 6 / 80) train acc: 0.075000; val_acc: 0.106000\n",
      "(Iteration 2301 / 30560) loss: 2.305510\n",
      "(Iteration 2401 / 30560) loss: 2.305511\n",
      "(Iteration 2501 / 30560) loss: 2.305510\n",
      "(Iteration 2601 / 30560) loss: 2.305521\n",
      "(Epoch 7 / 80) train acc: 0.079000; val_acc: 0.106000\n",
      "(Iteration 2701 / 30560) loss: 2.305510\n",
      "(Iteration 2801 / 30560) loss: 2.305507\n",
      "(Iteration 2901 / 30560) loss: 2.305509\n",
      "(Iteration 3001 / 30560) loss: 2.305511\n",
      "(Epoch 8 / 80) train acc: 0.088000; val_acc: 0.106000\n",
      "(Iteration 3101 / 30560) loss: 2.305515\n",
      "(Iteration 3201 / 30560) loss: 2.305510\n",
      "(Iteration 3301 / 30560) loss: 2.305500\n",
      "(Iteration 3401 / 30560) loss: 2.305504\n",
      "(Epoch 9 / 80) train acc: 0.085000; val_acc: 0.106000\n",
      "(Iteration 3501 / 30560) loss: 2.305515\n",
      "(Iteration 3601 / 30560) loss: 2.305508\n",
      "(Iteration 3701 / 30560) loss: 2.305506\n",
      "(Iteration 3801 / 30560) loss: 2.305511\n",
      "(Epoch 10 / 80) train acc: 0.077000; val_acc: 0.106000\n",
      "(Iteration 3901 / 30560) loss: 2.305511\n",
      "(Iteration 4001 / 30560) loss: 2.305510\n",
      "(Iteration 4101 / 30560) loss: 2.305512\n",
      "(Iteration 4201 / 30560) loss: 2.305513\n",
      "(Epoch 11 / 80) train acc: 0.090000; val_acc: 0.105000\n",
      "(Iteration 4301 / 30560) loss: 2.305503\n",
      "(Iteration 4401 / 30560) loss: 2.305503\n",
      "(Iteration 4501 / 30560) loss: 2.305505\n",
      "(Epoch 12 / 80) train acc: 0.087000; val_acc: 0.105000\n",
      "(Iteration 4601 / 30560) loss: 2.305508\n",
      "(Iteration 4701 / 30560) loss: 2.305510\n",
      "(Iteration 4801 / 30560) loss: 2.305507\n",
      "(Iteration 4901 / 30560) loss: 2.305517\n",
      "(Epoch 13 / 80) train acc: 0.075000; val_acc: 0.106000\n",
      "(Iteration 5001 / 30560) loss: 2.305516\n",
      "(Iteration 5101 / 30560) loss: 2.305515\n",
      "(Iteration 5201 / 30560) loss: 2.305515\n",
      "(Iteration 5301 / 30560) loss: 2.305514\n",
      "(Epoch 14 / 80) train acc: 0.076000; val_acc: 0.105000\n",
      "(Iteration 5401 / 30560) loss: 2.305524\n",
      "(Iteration 5501 / 30560) loss: 2.305504\n",
      "(Iteration 5601 / 30560) loss: 2.305511\n",
      "(Iteration 5701 / 30560) loss: 2.305507\n",
      "(Epoch 15 / 80) train acc: 0.082000; val_acc: 0.105000\n",
      "(Iteration 5801 / 30560) loss: 2.305515\n",
      "(Iteration 5901 / 30560) loss: 2.305510\n",
      "(Iteration 6001 / 30560) loss: 2.305506\n",
      "(Iteration 6101 / 30560) loss: 2.305513\n",
      "(Epoch 16 / 80) train acc: 0.104000; val_acc: 0.105000\n",
      "(Iteration 6201 / 30560) loss: 2.305505\n",
      "(Iteration 6301 / 30560) loss: 2.305517\n",
      "(Iteration 6401 / 30560) loss: 2.305511\n",
      "(Epoch 17 / 80) train acc: 0.083000; val_acc: 0.105000\n",
      "(Iteration 6501 / 30560) loss: 2.305509\n",
      "(Iteration 6601 / 30560) loss: 2.305502\n",
      "(Iteration 6701 / 30560) loss: 2.305501\n",
      "(Iteration 6801 / 30560) loss: 2.305514\n",
      "(Epoch 18 / 80) train acc: 0.084000; val_acc: 0.105000\n",
      "(Iteration 6901 / 30560) loss: 2.305508\n",
      "(Iteration 7001 / 30560) loss: 2.305513\n",
      "(Iteration 7101 / 30560) loss: 2.305519\n",
      "(Iteration 7201 / 30560) loss: 2.305506\n",
      "(Epoch 19 / 80) train acc: 0.096000; val_acc: 0.105000\n",
      "(Iteration 7301 / 30560) loss: 2.305511\n",
      "(Iteration 7401 / 30560) loss: 2.305502\n",
      "(Iteration 7501 / 30560) loss: 2.305513\n",
      "(Iteration 7601 / 30560) loss: 2.305504\n",
      "(Epoch 20 / 80) train acc: 0.086000; val_acc: 0.105000\n",
      "(Iteration 7701 / 30560) loss: 2.305507\n",
      "(Iteration 7801 / 30560) loss: 2.305509\n",
      "(Iteration 7901 / 30560) loss: 2.305508\n",
      "(Iteration 8001 / 30560) loss: 2.305506\n",
      "(Epoch 21 / 80) train acc: 0.097000; val_acc: 0.105000\n",
      "(Iteration 8101 / 30560) loss: 2.305503\n",
      "(Iteration 8201 / 30560) loss: 2.305509\n",
      "(Iteration 8301 / 30560) loss: 2.305514\n",
      "(Iteration 8401 / 30560) loss: 2.305509\n",
      "(Epoch 22 / 80) train acc: 0.068000; val_acc: 0.105000\n",
      "(Iteration 8501 / 30560) loss: 2.305519\n",
      "(Iteration 8601 / 30560) loss: 2.305510\n",
      "(Iteration 8701 / 30560) loss: 2.305511\n",
      "(Epoch 23 / 80) train acc: 0.081000; val_acc: 0.105000\n",
      "(Iteration 8801 / 30560) loss: 2.305502\n",
      "(Iteration 8901 / 30560) loss: 2.305513\n",
      "(Iteration 9001 / 30560) loss: 2.305507\n",
      "(Iteration 9101 / 30560) loss: 2.305507\n",
      "(Epoch 24 / 80) train acc: 0.083000; val_acc: 0.105000\n",
      "(Iteration 9201 / 30560) loss: 2.305513\n",
      "(Iteration 9301 / 30560) loss: 2.305509\n",
      "(Iteration 9401 / 30560) loss: 2.305514\n",
      "(Iteration 9501 / 30560) loss: 2.305503\n",
      "(Epoch 25 / 80) train acc: 0.079000; val_acc: 0.105000\n",
      "(Iteration 9601 / 30560) loss: 2.305507\n",
      "(Iteration 9701 / 30560) loss: 2.305510\n",
      "(Iteration 9801 / 30560) loss: 2.305501\n",
      "(Iteration 9901 / 30560) loss: 2.305504\n",
      "(Epoch 26 / 80) train acc: 0.084000; val_acc: 0.105000\n",
      "(Iteration 10001 / 30560) loss: 2.305521\n",
      "(Iteration 10101 / 30560) loss: 2.305514\n",
      "(Iteration 10201 / 30560) loss: 2.305509\n",
      "(Iteration 10301 / 30560) loss: 2.305512\n",
      "(Epoch 27 / 80) train acc: 0.081000; val_acc: 0.105000\n",
      "(Iteration 10401 / 30560) loss: 2.305505\n",
      "(Iteration 10501 / 30560) loss: 2.305509\n",
      "(Iteration 10601 / 30560) loss: 2.305511\n",
      "(Epoch 28 / 80) train acc: 0.097000; val_acc: 0.105000\n",
      "(Iteration 10701 / 30560) loss: 2.305501\n",
      "(Iteration 10801 / 30560) loss: 2.305503\n",
      "(Iteration 10901 / 30560) loss: 2.305510\n",
      "(Iteration 11001 / 30560) loss: 2.305513\n",
      "(Epoch 29 / 80) train acc: 0.071000; val_acc: 0.105000\n",
      "(Iteration 11101 / 30560) loss: 2.305505\n",
      "(Iteration 11201 / 30560) loss: 2.305513\n",
      "(Iteration 11301 / 30560) loss: 2.305512\n",
      "(Iteration 11401 / 30560) loss: 2.305506\n",
      "(Epoch 30 / 80) train acc: 0.106000; val_acc: 0.105000\n",
      "(Iteration 11501 / 30560) loss: 2.305504\n",
      "(Iteration 11601 / 30560) loss: 2.305514\n",
      "(Iteration 11701 / 30560) loss: 2.305506\n",
      "(Iteration 11801 / 30560) loss: 2.305506\n",
      "(Epoch 31 / 80) train acc: 0.097000; val_acc: 0.105000\n",
      "(Iteration 11901 / 30560) loss: 2.305507\n",
      "(Iteration 12001 / 30560) loss: 2.305513\n",
      "(Iteration 12101 / 30560) loss: 2.305509\n",
      "(Iteration 12201 / 30560) loss: 2.305500\n",
      "(Epoch 32 / 80) train acc: 0.093000; val_acc: 0.105000\n",
      "(Iteration 12301 / 30560) loss: 2.305502\n",
      "(Iteration 12401 / 30560) loss: 2.305493\n",
      "(Iteration 12501 / 30560) loss: 2.305504\n",
      "(Iteration 12601 / 30560) loss: 2.305517\n",
      "(Epoch 33 / 80) train acc: 0.079000; val_acc: 0.105000\n",
      "(Iteration 12701 / 30560) loss: 2.305511\n",
      "(Iteration 12801 / 30560) loss: 2.305503\n",
      "(Iteration 12901 / 30560) loss: 2.305513\n",
      "(Epoch 34 / 80) train acc: 0.070000; val_acc: 0.105000\n",
      "(Iteration 13001 / 30560) loss: 2.305509\n",
      "(Iteration 13101 / 30560) loss: 2.305508\n",
      "(Iteration 13201 / 30560) loss: 2.305519\n",
      "(Iteration 13301 / 30560) loss: 2.305497\n",
      "(Epoch 35 / 80) train acc: 0.083000; val_acc: 0.105000\n",
      "(Iteration 13401 / 30560) loss: 2.305509\n",
      "(Iteration 13501 / 30560) loss: 2.305508\n",
      "(Iteration 13601 / 30560) loss: 2.305514\n",
      "(Iteration 13701 / 30560) loss: 2.305510\n",
      "(Epoch 36 / 80) train acc: 0.097000; val_acc: 0.105000\n",
      "(Iteration 13801 / 30560) loss: 2.305507\n",
      "(Iteration 13901 / 30560) loss: 2.305513\n",
      "(Iteration 14001 / 30560) loss: 2.305512\n",
      "(Iteration 14101 / 30560) loss: 2.305509\n",
      "(Epoch 37 / 80) train acc: 0.095000; val_acc: 0.105000\n",
      "(Iteration 14201 / 30560) loss: 2.305514\n",
      "(Iteration 14301 / 30560) loss: 2.305507\n",
      "(Iteration 14401 / 30560) loss: 2.305512\n",
      "(Iteration 14501 / 30560) loss: 2.305508\n",
      "(Epoch 38 / 80) train acc: 0.067000; val_acc: 0.105000\n",
      "(Iteration 14601 / 30560) loss: 2.305501\n",
      "(Iteration 14701 / 30560) loss: 2.305510\n",
      "(Iteration 14801 / 30560) loss: 2.305508\n",
      "(Epoch 39 / 80) train acc: 0.085000; val_acc: 0.105000\n",
      "(Iteration 14901 / 30560) loss: 2.305512\n",
      "(Iteration 15001 / 30560) loss: 2.305503\n",
      "(Iteration 15101 / 30560) loss: 2.305508\n",
      "(Iteration 15201 / 30560) loss: 2.305509\n",
      "(Epoch 40 / 80) train acc: 0.079000; val_acc: 0.105000\n",
      "(Iteration 15301 / 30560) loss: 2.305503\n",
      "(Iteration 15401 / 30560) loss: 2.305509\n",
      "(Iteration 15501 / 30560) loss: 2.305504\n",
      "(Iteration 15601 / 30560) loss: 2.305507\n",
      "(Epoch 41 / 80) train acc: 0.078000; val_acc: 0.105000\n",
      "(Iteration 15701 / 30560) loss: 2.305502\n",
      "(Iteration 15801 / 30560) loss: 2.305511\n",
      "(Iteration 15901 / 30560) loss: 2.305505\n",
      "(Iteration 16001 / 30560) loss: 2.305507\n",
      "(Epoch 42 / 80) train acc: 0.071000; val_acc: 0.105000\n",
      "(Iteration 16101 / 30560) loss: 2.305504\n",
      "(Iteration 16201 / 30560) loss: 2.305511\n",
      "(Iteration 16301 / 30560) loss: 2.305510\n",
      "(Iteration 16401 / 30560) loss: 2.305516\n",
      "(Epoch 43 / 80) train acc: 0.087000; val_acc: 0.105000\n",
      "(Iteration 16501 / 30560) loss: 2.305506\n",
      "(Iteration 16601 / 30560) loss: 2.305511\n",
      "(Iteration 16701 / 30560) loss: 2.305511\n",
      "(Iteration 16801 / 30560) loss: 2.305504\n",
      "(Epoch 44 / 80) train acc: 0.093000; val_acc: 0.105000\n",
      "(Iteration 16901 / 30560) loss: 2.305512\n",
      "(Iteration 17001 / 30560) loss: 2.305513\n",
      "(Iteration 17101 / 30560) loss: 2.305519\n",
      "(Epoch 45 / 80) train acc: 0.085000; val_acc: 0.105000\n",
      "(Iteration 17201 / 30560) loss: 2.305506\n",
      "(Iteration 17301 / 30560) loss: 2.305509\n",
      "(Iteration 17401 / 30560) loss: 2.305512\n",
      "(Iteration 17501 / 30560) loss: 2.305505\n",
      "(Epoch 46 / 80) train acc: 0.103000; val_acc: 0.105000\n",
      "(Iteration 17601 / 30560) loss: 2.305511\n",
      "(Iteration 17701 / 30560) loss: 2.305508\n",
      "(Iteration 17801 / 30560) loss: 2.305511\n",
      "(Iteration 17901 / 30560) loss: 2.305499\n",
      "(Epoch 47 / 80) train acc: 0.078000; val_acc: 0.105000\n",
      "(Iteration 18001 / 30560) loss: 2.305510\n",
      "(Iteration 18101 / 30560) loss: 2.305510\n",
      "(Iteration 18201 / 30560) loss: 2.305512\n",
      "(Iteration 18301 / 30560) loss: 2.305508\n",
      "(Epoch 48 / 80) train acc: 0.081000; val_acc: 0.105000\n",
      "(Iteration 18401 / 30560) loss: 2.305507\n",
      "(Iteration 18501 / 30560) loss: 2.305511\n",
      "(Iteration 18601 / 30560) loss: 2.305508\n",
      "(Iteration 18701 / 30560) loss: 2.305506\n",
      "(Epoch 49 / 80) train acc: 0.079000; val_acc: 0.105000\n",
      "(Iteration 18801 / 30560) loss: 2.305509\n",
      "(Iteration 18901 / 30560) loss: 2.305510\n",
      "(Iteration 19001 / 30560) loss: 2.305515\n",
      "(Epoch 50 / 80) train acc: 0.091000; val_acc: 0.105000\n",
      "(Iteration 19101 / 30560) loss: 2.305511\n",
      "(Iteration 19201 / 30560) loss: 2.305510\n",
      "(Iteration 19301 / 30560) loss: 2.305508\n",
      "(Iteration 19401 / 30560) loss: 2.305513\n",
      "(Epoch 51 / 80) train acc: 0.082000; val_acc: 0.105000\n",
      "(Iteration 19501 / 30560) loss: 2.305516\n",
      "(Iteration 19601 / 30560) loss: 2.305510\n",
      "(Iteration 19701 / 30560) loss: 2.305504\n",
      "(Iteration 19801 / 30560) loss: 2.305513\n",
      "(Epoch 52 / 80) train acc: 0.081000; val_acc: 0.105000\n",
      "(Iteration 19901 / 30560) loss: 2.305515\n",
      "(Iteration 20001 / 30560) loss: 2.305512\n",
      "(Iteration 20101 / 30560) loss: 2.305506\n",
      "(Iteration 20201 / 30560) loss: 2.305512\n",
      "(Epoch 53 / 80) train acc: 0.101000; val_acc: 0.105000\n",
      "(Iteration 20301 / 30560) loss: 2.305513\n",
      "(Iteration 20401 / 30560) loss: 2.305510\n",
      "(Iteration 20501 / 30560) loss: 2.305520\n",
      "(Iteration 20601 / 30560) loss: 2.305505\n",
      "(Epoch 54 / 80) train acc: 0.075000; val_acc: 0.105000\n",
      "(Iteration 20701 / 30560) loss: 2.305503\n",
      "(Iteration 20801 / 30560) loss: 2.305501\n",
      "(Iteration 20901 / 30560) loss: 2.305509\n",
      "(Iteration 21001 / 30560) loss: 2.305513\n",
      "(Epoch 55 / 80) train acc: 0.094000; val_acc: 0.105000\n",
      "(Iteration 21101 / 30560) loss: 2.305518\n",
      "(Iteration 21201 / 30560) loss: 2.305505\n",
      "(Iteration 21301 / 30560) loss: 2.305515\n",
      "(Epoch 56 / 80) train acc: 0.080000; val_acc: 0.105000\n",
      "(Iteration 21401 / 30560) loss: 2.305501\n",
      "(Iteration 21501 / 30560) loss: 2.305506\n",
      "(Iteration 21601 / 30560) loss: 2.305505\n",
      "(Iteration 21701 / 30560) loss: 2.305514\n",
      "(Epoch 57 / 80) train acc: 0.086000; val_acc: 0.105000\n",
      "(Iteration 21801 / 30560) loss: 2.305509\n",
      "(Iteration 21901 / 30560) loss: 2.305504\n",
      "(Iteration 22001 / 30560) loss: 2.305516\n",
      "(Iteration 22101 / 30560) loss: 2.305510\n",
      "(Epoch 58 / 80) train acc: 0.085000; val_acc: 0.105000\n",
      "(Iteration 22201 / 30560) loss: 2.305501\n",
      "(Iteration 22301 / 30560) loss: 2.305509\n",
      "(Iteration 22401 / 30560) loss: 2.305515\n",
      "(Iteration 22501 / 30560) loss: 2.305510\n",
      "(Epoch 59 / 80) train acc: 0.083000; val_acc: 0.105000\n",
      "(Iteration 22601 / 30560) loss: 2.305508\n",
      "(Iteration 22701 / 30560) loss: 2.305517\n",
      "(Iteration 22801 / 30560) loss: 2.305507\n",
      "(Iteration 22901 / 30560) loss: 2.305508\n",
      "(Epoch 60 / 80) train acc: 0.085000; val_acc: 0.105000\n",
      "(Iteration 23001 / 30560) loss: 2.305509\n",
      "(Iteration 23101 / 30560) loss: 2.305512\n",
      "(Iteration 23201 / 30560) loss: 2.305509\n",
      "(Iteration 23301 / 30560) loss: 2.305511\n",
      "(Epoch 61 / 80) train acc: 0.087000; val_acc: 0.105000\n",
      "(Iteration 23401 / 30560) loss: 2.305511\n",
      "(Iteration 23501 / 30560) loss: 2.305514\n",
      "(Iteration 23601 / 30560) loss: 2.305508\n",
      "(Epoch 62 / 80) train acc: 0.079000; val_acc: 0.105000\n",
      "(Iteration 23701 / 30560) loss: 2.305515\n",
      "(Iteration 23801 / 30560) loss: 2.305511\n",
      "(Iteration 23901 / 30560) loss: 2.305518\n",
      "(Iteration 24001 / 30560) loss: 2.305510\n",
      "(Epoch 63 / 80) train acc: 0.071000; val_acc: 0.105000\n",
      "(Iteration 24101 / 30560) loss: 2.305514\n",
      "(Iteration 24201 / 30560) loss: 2.305501\n",
      "(Iteration 24301 / 30560) loss: 2.305502\n",
      "(Iteration 24401 / 30560) loss: 2.305517\n",
      "(Epoch 64 / 80) train acc: 0.079000; val_acc: 0.105000\n",
      "(Iteration 24501 / 30560) loss: 2.305511\n",
      "(Iteration 24601 / 30560) loss: 2.305512\n",
      "(Iteration 24701 / 30560) loss: 2.305503\n",
      "(Iteration 24801 / 30560) loss: 2.305511\n",
      "(Epoch 65 / 80) train acc: 0.096000; val_acc: 0.105000\n",
      "(Iteration 24901 / 30560) loss: 2.305505\n",
      "(Iteration 25001 / 30560) loss: 2.305517\n",
      "(Iteration 25101 / 30560) loss: 2.305510\n",
      "(Iteration 25201 / 30560) loss: 2.305508\n",
      "(Epoch 66 / 80) train acc: 0.084000; val_acc: 0.105000\n",
      "(Iteration 25301 / 30560) loss: 2.305512\n",
      "(Iteration 25401 / 30560) loss: 2.305506\n",
      "(Iteration 25501 / 30560) loss: 2.305504\n",
      "(Epoch 67 / 80) train acc: 0.073000; val_acc: 0.105000\n",
      "(Iteration 25601 / 30560) loss: 2.305508\n",
      "(Iteration 25701 / 30560) loss: 2.305504\n",
      "(Iteration 25801 / 30560) loss: 2.305508\n",
      "(Iteration 25901 / 30560) loss: 2.305519\n",
      "(Epoch 68 / 80) train acc: 0.092000; val_acc: 0.105000\n",
      "(Iteration 26001 / 30560) loss: 2.305496\n",
      "(Iteration 26101 / 30560) loss: 2.305503\n",
      "(Iteration 26201 / 30560) loss: 2.305503\n",
      "(Iteration 26301 / 30560) loss: 2.305520\n",
      "(Epoch 69 / 80) train acc: 0.078000; val_acc: 0.105000\n",
      "(Iteration 26401 / 30560) loss: 2.305510\n",
      "(Iteration 26501 / 30560) loss: 2.305513\n",
      "(Iteration 26601 / 30560) loss: 2.305512\n",
      "(Iteration 26701 / 30560) loss: 2.305509\n",
      "(Epoch 70 / 80) train acc: 0.093000; val_acc: 0.105000\n",
      "(Iteration 26801 / 30560) loss: 2.305510\n",
      "(Iteration 26901 / 30560) loss: 2.305509\n",
      "(Iteration 27001 / 30560) loss: 2.305517\n",
      "(Iteration 27101 / 30560) loss: 2.305508\n",
      "(Epoch 71 / 80) train acc: 0.081000; val_acc: 0.105000\n",
      "(Iteration 27201 / 30560) loss: 2.305506\n",
      "(Iteration 27301 / 30560) loss: 2.305512\n",
      "(Iteration 27401 / 30560) loss: 2.305506\n",
      "(Iteration 27501 / 30560) loss: 2.305508\n",
      "(Epoch 72 / 80) train acc: 0.086000; val_acc: 0.105000\n",
      "(Iteration 27601 / 30560) loss: 2.305510\n",
      "(Iteration 27701 / 30560) loss: 2.305516\n",
      "(Iteration 27801 / 30560) loss: 2.305506\n",
      "(Epoch 73 / 80) train acc: 0.087000; val_acc: 0.105000\n",
      "(Iteration 27901 / 30560) loss: 2.305516\n",
      "(Iteration 28001 / 30560) loss: 2.305510\n",
      "(Iteration 28101 / 30560) loss: 2.305506\n",
      "(Iteration 28201 / 30560) loss: 2.305511\n",
      "(Epoch 74 / 80) train acc: 0.092000; val_acc: 0.105000\n",
      "(Iteration 28301 / 30560) loss: 2.305513\n",
      "(Iteration 28401 / 30560) loss: 2.305503\n",
      "(Iteration 28501 / 30560) loss: 2.305502\n",
      "(Iteration 28601 / 30560) loss: 2.305510\n",
      "(Epoch 75 / 80) train acc: 0.056000; val_acc: 0.105000\n",
      "(Iteration 28701 / 30560) loss: 2.305511\n",
      "(Iteration 28801 / 30560) loss: 2.305508\n",
      "(Iteration 28901 / 30560) loss: 2.305504\n",
      "(Iteration 29001 / 30560) loss: 2.305505\n",
      "(Epoch 76 / 80) train acc: 0.078000; val_acc: 0.105000\n",
      "(Iteration 29101 / 30560) loss: 2.305514\n",
      "(Iteration 29201 / 30560) loss: 2.305512\n",
      "(Iteration 29301 / 30560) loss: 2.305509\n",
      "(Iteration 29401 / 30560) loss: 2.305514\n",
      "(Epoch 77 / 80) train acc: 0.079000; val_acc: 0.105000\n",
      "(Iteration 29501 / 30560) loss: 2.305509\n",
      "(Iteration 29601 / 30560) loss: 2.305514\n",
      "(Iteration 29701 / 30560) loss: 2.305506\n",
      "(Epoch 78 / 80) train acc: 0.085000; val_acc: 0.105000\n",
      "(Iteration 29801 / 30560) loss: 2.305502\n",
      "(Iteration 29901 / 30560) loss: 2.305514\n",
      "(Iteration 30001 / 30560) loss: 2.305509\n",
      "(Iteration 30101 / 30560) loss: 2.305516\n",
      "(Epoch 79 / 80) train acc: 0.073000; val_acc: 0.105000\n",
      "(Iteration 30201 / 30560) loss: 2.305509\n",
      "(Iteration 30301 / 30560) loss: 2.305517\n",
      "(Iteration 30401 / 30560) loss: 2.305514\n",
      "(Iteration 30501 / 30560) loss: 2.305504\n",
      "(Epoch 80 / 80) train acc: 0.082000; val_acc: 0.105000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.305552\n",
      "(Epoch 0 / 80) train acc: 0.103000; val_acc: 0.096000\n",
      "(Iteration 101 / 61200) loss: 2.305528\n",
      "(Iteration 201 / 61200) loss: 2.305530\n",
      "(Iteration 301 / 61200) loss: 2.305540\n",
      "(Iteration 401 / 61200) loss: 2.305525\n",
      "(Iteration 501 / 61200) loss: 2.305544\n",
      "(Iteration 601 / 61200) loss: 2.305548\n",
      "(Iteration 701 / 61200) loss: 2.305544\n",
      "(Epoch 1 / 80) train acc: 0.116000; val_acc: 0.096000\n",
      "(Iteration 801 / 61200) loss: 2.305551\n",
      "(Iteration 901 / 61200) loss: 2.305536\n",
      "(Iteration 1001 / 61200) loss: 2.305523\n",
      "(Iteration 1101 / 61200) loss: 2.305555\n",
      "(Iteration 1201 / 61200) loss: 2.305538\n",
      "(Iteration 1301 / 61200) loss: 2.305523\n",
      "(Iteration 1401 / 61200) loss: 2.305538\n",
      "(Iteration 1501 / 61200) loss: 2.305554\n",
      "(Epoch 2 / 80) train acc: 0.107000; val_acc: 0.096000\n",
      "(Iteration 1601 / 61200) loss: 2.305536\n",
      "(Iteration 1701 / 61200) loss: 2.305539\n",
      "(Iteration 1801 / 61200) loss: 2.305519\n",
      "(Iteration 1901 / 61200) loss: 2.305542\n",
      "(Iteration 2001 / 61200) loss: 2.305536\n",
      "(Iteration 2101 / 61200) loss: 2.305528\n",
      "(Iteration 2201 / 61200) loss: 2.305544\n",
      "(Epoch 3 / 80) train acc: 0.118000; val_acc: 0.096000\n",
      "(Iteration 2301 / 61200) loss: 2.305533\n",
      "(Iteration 2401 / 61200) loss: 2.305541\n",
      "(Iteration 2501 / 61200) loss: 2.305530\n",
      "(Iteration 2601 / 61200) loss: 2.305528\n",
      "(Iteration 2701 / 61200) loss: 2.305530\n",
      "(Iteration 2801 / 61200) loss: 2.305521\n",
      "(Iteration 2901 / 61200) loss: 2.305544\n",
      "(Iteration 3001 / 61200) loss: 2.305537\n",
      "(Epoch 4 / 80) train acc: 0.118000; val_acc: 0.096000\n",
      "(Iteration 3101 / 61200) loss: 2.305538\n",
      "(Iteration 3201 / 61200) loss: 2.305529\n",
      "(Iteration 3301 / 61200) loss: 2.305520\n",
      "(Iteration 3401 / 61200) loss: 2.305545\n",
      "(Iteration 3501 / 61200) loss: 2.305533\n",
      "(Iteration 3601 / 61200) loss: 2.305545\n",
      "(Iteration 3701 / 61200) loss: 2.305537\n",
      "(Iteration 3801 / 61200) loss: 2.305525\n",
      "(Epoch 5 / 80) train acc: 0.122000; val_acc: 0.096000\n",
      "(Iteration 3901 / 61200) loss: 2.305528\n",
      "(Iteration 4001 / 61200) loss: 2.305532\n",
      "(Iteration 4101 / 61200) loss: 2.305537\n",
      "(Iteration 4201 / 61200) loss: 2.305546\n",
      "(Iteration 4301 / 61200) loss: 2.305552\n",
      "(Iteration 4401 / 61200) loss: 2.305538\n",
      "(Iteration 4501 / 61200) loss: 2.305532\n",
      "(Epoch 6 / 80) train acc: 0.110000; val_acc: 0.096000\n",
      "(Iteration 4601 / 61200) loss: 2.305530\n",
      "(Iteration 4701 / 61200) loss: 2.305521\n",
      "(Iteration 4801 / 61200) loss: 2.305534\n",
      "(Iteration 4901 / 61200) loss: 2.305529\n",
      "(Iteration 5001 / 61200) loss: 2.305527\n",
      "(Iteration 5101 / 61200) loss: 2.305532\n",
      "(Iteration 5201 / 61200) loss: 2.305524\n",
      "(Iteration 5301 / 61200) loss: 2.305541\n",
      "(Epoch 7 / 80) train acc: 0.109000; val_acc: 0.096000\n",
      "(Iteration 5401 / 61200) loss: 2.305529\n",
      "(Iteration 5501 / 61200) loss: 2.305561\n",
      "(Iteration 5601 / 61200) loss: 2.305550\n",
      "(Iteration 5701 / 61200) loss: 2.305563\n",
      "(Iteration 5801 / 61200) loss: 2.305520\n",
      "(Iteration 5901 / 61200) loss: 2.305540\n",
      "(Iteration 6001 / 61200) loss: 2.305547\n",
      "(Iteration 6101 / 61200) loss: 2.305533\n",
      "(Epoch 8 / 80) train acc: 0.120000; val_acc: 0.095000\n",
      "(Iteration 6201 / 61200) loss: 2.305539\n",
      "(Iteration 6301 / 61200) loss: 2.305541\n",
      "(Iteration 6401 / 61200) loss: 2.305524\n",
      "(Iteration 6501 / 61200) loss: 2.305540\n",
      "(Iteration 6601 / 61200) loss: 2.305537\n",
      "(Iteration 6701 / 61200) loss: 2.305550\n",
      "(Iteration 6801 / 61200) loss: 2.305539\n",
      "(Epoch 9 / 80) train acc: 0.112000; val_acc: 0.095000\n",
      "(Iteration 6901 / 61200) loss: 2.305527\n",
      "(Iteration 7001 / 61200) loss: 2.305525\n",
      "(Iteration 7101 / 61200) loss: 2.305539\n",
      "(Iteration 7201 / 61200) loss: 2.305528\n",
      "(Iteration 7301 / 61200) loss: 2.305530\n",
      "(Iteration 7401 / 61200) loss: 2.305532\n",
      "(Iteration 7501 / 61200) loss: 2.305523\n",
      "(Iteration 7601 / 61200) loss: 2.305540\n",
      "(Epoch 10 / 80) train acc: 0.109000; val_acc: 0.096000\n",
      "(Iteration 7701 / 61200) loss: 2.305535\n",
      "(Iteration 7801 / 61200) loss: 2.305530\n",
      "(Iteration 7901 / 61200) loss: 2.305536\n",
      "(Iteration 8001 / 61200) loss: 2.305529\n",
      "(Iteration 8101 / 61200) loss: 2.305544\n",
      "(Iteration 8201 / 61200) loss: 2.305520\n",
      "(Iteration 8301 / 61200) loss: 2.305533\n",
      "(Iteration 8401 / 61200) loss: 2.305530\n",
      "(Epoch 11 / 80) train acc: 0.104000; val_acc: 0.095000\n",
      "(Iteration 8501 / 61200) loss: 2.305544\n",
      "(Iteration 8601 / 61200) loss: 2.305527\n",
      "(Iteration 8701 / 61200) loss: 2.305523\n",
      "(Iteration 8801 / 61200) loss: 2.305540\n",
      "(Iteration 8901 / 61200) loss: 2.305536\n",
      "(Iteration 9001 / 61200) loss: 2.305525\n",
      "(Iteration 9101 / 61200) loss: 2.305524\n",
      "(Epoch 12 / 80) train acc: 0.114000; val_acc: 0.095000\n",
      "(Iteration 9201 / 61200) loss: 2.305526\n",
      "(Iteration 9301 / 61200) loss: 2.305535\n",
      "(Iteration 9401 / 61200) loss: 2.305527\n",
      "(Iteration 9501 / 61200) loss: 2.305541\n",
      "(Iteration 9601 / 61200) loss: 2.305541\n",
      "(Iteration 9701 / 61200) loss: 2.305532\n",
      "(Iteration 9801 / 61200) loss: 2.305529\n",
      "(Iteration 9901 / 61200) loss: 2.305540\n",
      "(Epoch 13 / 80) train acc: 0.109000; val_acc: 0.096000\n",
      "(Iteration 10001 / 61200) loss: 2.305534\n",
      "(Iteration 10101 / 61200) loss: 2.305537\n",
      "(Iteration 10201 / 61200) loss: 2.305544\n",
      "(Iteration 10301 / 61200) loss: 2.305550\n",
      "(Iteration 10401 / 61200) loss: 2.305539\n",
      "(Iteration 10501 / 61200) loss: 2.305526\n",
      "(Iteration 10601 / 61200) loss: 2.305519\n",
      "(Iteration 10701 / 61200) loss: 2.305510\n",
      "(Epoch 14 / 80) train acc: 0.105000; val_acc: 0.096000\n",
      "(Iteration 10801 / 61200) loss: 2.305543\n",
      "(Iteration 10901 / 61200) loss: 2.305531\n",
      "(Iteration 11001 / 61200) loss: 2.305524\n",
      "(Iteration 11101 / 61200) loss: 2.305539\n",
      "(Iteration 11201 / 61200) loss: 2.305546\n",
      "(Iteration 11301 / 61200) loss: 2.305530\n",
      "(Iteration 11401 / 61200) loss: 2.305536\n",
      "(Epoch 15 / 80) train acc: 0.116000; val_acc: 0.096000\n",
      "(Iteration 11501 / 61200) loss: 2.305531\n",
      "(Iteration 11601 / 61200) loss: 2.305548\n",
      "(Iteration 11701 / 61200) loss: 2.305535\n",
      "(Iteration 11801 / 61200) loss: 2.305526\n",
      "(Iteration 11901 / 61200) loss: 2.305539\n",
      "(Iteration 12001 / 61200) loss: 2.305537\n",
      "(Iteration 12101 / 61200) loss: 2.305530\n",
      "(Iteration 12201 / 61200) loss: 2.305530\n",
      "(Epoch 16 / 80) train acc: 0.121000; val_acc: 0.096000\n",
      "(Iteration 12301 / 61200) loss: 2.305541\n",
      "(Iteration 12401 / 61200) loss: 2.305526\n",
      "(Iteration 12501 / 61200) loss: 2.305543\n",
      "(Iteration 12601 / 61200) loss: 2.305530\n",
      "(Iteration 12701 / 61200) loss: 2.305532\n",
      "(Iteration 12801 / 61200) loss: 2.305538\n",
      "(Iteration 12901 / 61200) loss: 2.305506\n",
      "(Iteration 13001 / 61200) loss: 2.305527\n",
      "(Epoch 17 / 80) train acc: 0.108000; val_acc: 0.096000\n",
      "(Iteration 13101 / 61200) loss: 2.305536\n",
      "(Iteration 13201 / 61200) loss: 2.305527\n",
      "(Iteration 13301 / 61200) loss: 2.305517\n",
      "(Iteration 13401 / 61200) loss: 2.305538\n",
      "(Iteration 13501 / 61200) loss: 2.305537\n",
      "(Iteration 13601 / 61200) loss: 2.305523\n",
      "(Iteration 13701 / 61200) loss: 2.305524\n",
      "(Epoch 18 / 80) train acc: 0.124000; val_acc: 0.096000\n",
      "(Iteration 13801 / 61200) loss: 2.305543\n",
      "(Iteration 13901 / 61200) loss: 2.305540\n",
      "(Iteration 14001 / 61200) loss: 2.305529\n",
      "(Iteration 14101 / 61200) loss: 2.305545\n",
      "(Iteration 14201 / 61200) loss: 2.305527\n",
      "(Iteration 14301 / 61200) loss: 2.305535\n",
      "(Iteration 14401 / 61200) loss: 2.305521\n",
      "(Iteration 14501 / 61200) loss: 2.305533\n",
      "(Epoch 19 / 80) train acc: 0.101000; val_acc: 0.096000\n",
      "(Iteration 14601 / 61200) loss: 2.305529\n",
      "(Iteration 14701 / 61200) loss: 2.305528\n",
      "(Iteration 14801 / 61200) loss: 2.305517\n",
      "(Iteration 14901 / 61200) loss: 2.305523\n",
      "(Iteration 15001 / 61200) loss: 2.305535\n",
      "(Iteration 15101 / 61200) loss: 2.305540\n",
      "(Iteration 15201 / 61200) loss: 2.305528\n",
      "(Epoch 20 / 80) train acc: 0.108000; val_acc: 0.096000\n",
      "(Iteration 15301 / 61200) loss: 2.305522\n",
      "(Iteration 15401 / 61200) loss: 2.305534\n",
      "(Iteration 15501 / 61200) loss: 2.305538\n",
      "(Iteration 15601 / 61200) loss: 2.305538\n",
      "(Iteration 15701 / 61200) loss: 2.305516\n",
      "(Iteration 15801 / 61200) loss: 2.305520\n",
      "(Iteration 15901 / 61200) loss: 2.305533\n",
      "(Iteration 16001 / 61200) loss: 2.305538\n",
      "(Epoch 21 / 80) train acc: 0.110000; val_acc: 0.096000\n",
      "(Iteration 16101 / 61200) loss: 2.305539\n",
      "(Iteration 16201 / 61200) loss: 2.305525\n",
      "(Iteration 16301 / 61200) loss: 2.305540\n",
      "(Iteration 16401 / 61200) loss: 2.305531\n",
      "(Iteration 16501 / 61200) loss: 2.305524\n",
      "(Iteration 16601 / 61200) loss: 2.305525\n",
      "(Iteration 16701 / 61200) loss: 2.305526\n",
      "(Iteration 16801 / 61200) loss: 2.305533\n",
      "(Epoch 22 / 80) train acc: 0.114000; val_acc: 0.096000\n",
      "(Iteration 16901 / 61200) loss: 2.305530\n",
      "(Iteration 17001 / 61200) loss: 2.305535\n",
      "(Iteration 17101 / 61200) loss: 2.305530\n",
      "(Iteration 17201 / 61200) loss: 2.305533\n",
      "(Iteration 17301 / 61200) loss: 2.305514\n",
      "(Iteration 17401 / 61200) loss: 2.305532\n",
      "(Iteration 17501 / 61200) loss: 2.305528\n",
      "(Epoch 23 / 80) train acc: 0.118000; val_acc: 0.096000\n",
      "(Iteration 17601 / 61200) loss: 2.305519\n",
      "(Iteration 17701 / 61200) loss: 2.305540\n",
      "(Iteration 17801 / 61200) loss: 2.305523\n",
      "(Iteration 17901 / 61200) loss: 2.305530\n",
      "(Iteration 18001 / 61200) loss: 2.305521\n",
      "(Iteration 18101 / 61200) loss: 2.305527\n",
      "(Iteration 18201 / 61200) loss: 2.305528\n",
      "(Iteration 18301 / 61200) loss: 2.305534\n",
      "(Epoch 24 / 80) train acc: 0.132000; val_acc: 0.096000\n",
      "(Iteration 18401 / 61200) loss: 2.305539\n",
      "(Iteration 18501 / 61200) loss: 2.305547\n",
      "(Iteration 18601 / 61200) loss: 2.305532\n",
      "(Iteration 18701 / 61200) loss: 2.305528\n",
      "(Iteration 18801 / 61200) loss: 2.305529\n",
      "(Iteration 18901 / 61200) loss: 2.305540\n",
      "(Iteration 19001 / 61200) loss: 2.305530\n",
      "(Iteration 19101 / 61200) loss: 2.305536\n",
      "(Epoch 25 / 80) train acc: 0.107000; val_acc: 0.096000\n",
      "(Iteration 19201 / 61200) loss: 2.305524\n",
      "(Iteration 19301 / 61200) loss: 2.305512\n",
      "(Iteration 19401 / 61200) loss: 2.305529\n",
      "(Iteration 19501 / 61200) loss: 2.305531\n",
      "(Iteration 19601 / 61200) loss: 2.305523\n",
      "(Iteration 19701 / 61200) loss: 2.305537\n",
      "(Iteration 19801 / 61200) loss: 2.305530\n",
      "(Epoch 26 / 80) train acc: 0.111000; val_acc: 0.096000\n",
      "(Iteration 19901 / 61200) loss: 2.305517\n",
      "(Iteration 20001 / 61200) loss: 2.305530\n",
      "(Iteration 20101 / 61200) loss: 2.305522\n",
      "(Iteration 20201 / 61200) loss: 2.305528\n",
      "(Iteration 20301 / 61200) loss: 2.305534\n",
      "(Iteration 20401 / 61200) loss: 2.305522\n",
      "(Iteration 20501 / 61200) loss: 2.305534\n",
      "(Iteration 20601 / 61200) loss: 2.305531\n",
      "(Epoch 27 / 80) train acc: 0.129000; val_acc: 0.095000\n",
      "(Iteration 20701 / 61200) loss: 2.305538\n",
      "(Iteration 20801 / 61200) loss: 2.305542\n",
      "(Iteration 20901 / 61200) loss: 2.305537\n",
      "(Iteration 21001 / 61200) loss: 2.305536\n",
      "(Iteration 21101 / 61200) loss: 2.305531\n",
      "(Iteration 21201 / 61200) loss: 2.305530\n",
      "(Iteration 21301 / 61200) loss: 2.305537\n",
      "(Iteration 21401 / 61200) loss: 2.305535\n",
      "(Epoch 28 / 80) train acc: 0.100000; val_acc: 0.095000\n",
      "(Iteration 21501 / 61200) loss: 2.305528\n",
      "(Iteration 21601 / 61200) loss: 2.305527\n",
      "(Iteration 21701 / 61200) loss: 2.305543\n",
      "(Iteration 21801 / 61200) loss: 2.305545\n",
      "(Iteration 21901 / 61200) loss: 2.305532\n",
      "(Iteration 22001 / 61200) loss: 2.305528\n",
      "(Iteration 22101 / 61200) loss: 2.305541\n",
      "(Epoch 29 / 80) train acc: 0.113000; val_acc: 0.095000\n",
      "(Iteration 22201 / 61200) loss: 2.305529\n",
      "(Iteration 22301 / 61200) loss: 2.305524\n",
      "(Iteration 22401 / 61200) loss: 2.305539\n",
      "(Iteration 22501 / 61200) loss: 2.305539\n",
      "(Iteration 22601 / 61200) loss: 2.305538\n",
      "(Iteration 22701 / 61200) loss: 2.305539\n",
      "(Iteration 22801 / 61200) loss: 2.305521\n",
      "(Iteration 22901 / 61200) loss: 2.305530\n",
      "(Epoch 30 / 80) train acc: 0.095000; val_acc: 0.095000\n",
      "(Iteration 23001 / 61200) loss: 2.305536\n",
      "(Iteration 23101 / 61200) loss: 2.305542\n",
      "(Iteration 23201 / 61200) loss: 2.305530\n",
      "(Iteration 23301 / 61200) loss: 2.305540\n",
      "(Iteration 23401 / 61200) loss: 2.305537\n",
      "(Iteration 23501 / 61200) loss: 2.305531\n",
      "(Iteration 23601 / 61200) loss: 2.305529\n",
      "(Iteration 23701 / 61200) loss: 2.305527\n",
      "(Epoch 31 / 80) train acc: 0.124000; val_acc: 0.096000\n",
      "(Iteration 23801 / 61200) loss: 2.305516\n",
      "(Iteration 23901 / 61200) loss: 2.305549\n",
      "(Iteration 24001 / 61200) loss: 2.305545\n",
      "(Iteration 24101 / 61200) loss: 2.305521\n",
      "(Iteration 24201 / 61200) loss: 2.305529\n",
      "(Iteration 24301 / 61200) loss: 2.305525\n",
      "(Iteration 24401 / 61200) loss: 2.305530\n",
      "(Epoch 32 / 80) train acc: 0.130000; val_acc: 0.096000\n",
      "(Iteration 24501 / 61200) loss: 2.305536\n",
      "(Iteration 24601 / 61200) loss: 2.305512\n",
      "(Iteration 24701 / 61200) loss: 2.305541\n",
      "(Iteration 24801 / 61200) loss: 2.305525\n",
      "(Iteration 24901 / 61200) loss: 2.305534\n",
      "(Iteration 25001 / 61200) loss: 2.305544\n",
      "(Iteration 25101 / 61200) loss: 2.305532\n",
      "(Iteration 25201 / 61200) loss: 2.305521\n",
      "(Epoch 33 / 80) train acc: 0.108000; val_acc: 0.096000\n",
      "(Iteration 25301 / 61200) loss: 2.305526\n",
      "(Iteration 25401 / 61200) loss: 2.305538\n",
      "(Iteration 25501 / 61200) loss: 2.305531\n",
      "(Iteration 25601 / 61200) loss: 2.305525\n",
      "(Iteration 25701 / 61200) loss: 2.305536\n",
      "(Iteration 25801 / 61200) loss: 2.305519\n",
      "(Iteration 25901 / 61200) loss: 2.305550\n",
      "(Iteration 26001 / 61200) loss: 2.305542\n",
      "(Epoch 34 / 80) train acc: 0.123000; val_acc: 0.096000\n",
      "(Iteration 26101 / 61200) loss: 2.305523\n",
      "(Iteration 26201 / 61200) loss: 2.305536\n",
      "(Iteration 26301 / 61200) loss: 2.305523\n",
      "(Iteration 26401 / 61200) loss: 2.305518\n",
      "(Iteration 26501 / 61200) loss: 2.305527\n",
      "(Iteration 26601 / 61200) loss: 2.305526\n",
      "(Iteration 26701 / 61200) loss: 2.305521\n",
      "(Epoch 35 / 80) train acc: 0.109000; val_acc: 0.096000\n",
      "(Iteration 26801 / 61200) loss: 2.305545\n",
      "(Iteration 26901 / 61200) loss: 2.305538\n",
      "(Iteration 27001 / 61200) loss: 2.305529\n",
      "(Iteration 27101 / 61200) loss: 2.305517\n",
      "(Iteration 27201 / 61200) loss: 2.305521\n",
      "(Iteration 27301 / 61200) loss: 2.305516\n",
      "(Iteration 27401 / 61200) loss: 2.305521\n",
      "(Iteration 27501 / 61200) loss: 2.305537\n",
      "(Epoch 36 / 80) train acc: 0.120000; val_acc: 0.096000\n",
      "(Iteration 27601 / 61200) loss: 2.305513\n",
      "(Iteration 27701 / 61200) loss: 2.305525\n",
      "(Iteration 27801 / 61200) loss: 2.305519\n",
      "(Iteration 27901 / 61200) loss: 2.305531\n",
      "(Iteration 28001 / 61200) loss: 2.305532\n",
      "(Iteration 28101 / 61200) loss: 2.305538\n",
      "(Iteration 28201 / 61200) loss: 2.305530\n",
      "(Iteration 28301 / 61200) loss: 2.305512\n",
      "(Epoch 37 / 80) train acc: 0.112000; val_acc: 0.096000\n",
      "(Iteration 28401 / 61200) loss: 2.305530\n",
      "(Iteration 28501 / 61200) loss: 2.305533\n",
      "(Iteration 28601 / 61200) loss: 2.305534\n",
      "(Iteration 28701 / 61200) loss: 2.305536\n",
      "(Iteration 28801 / 61200) loss: 2.305524\n",
      "(Iteration 28901 / 61200) loss: 2.305534\n",
      "(Iteration 29001 / 61200) loss: 2.305527\n",
      "(Epoch 38 / 80) train acc: 0.120000; val_acc: 0.096000\n",
      "(Iteration 29101 / 61200) loss: 2.305539\n",
      "(Iteration 29201 / 61200) loss: 2.305556\n",
      "(Iteration 29301 / 61200) loss: 2.305533\n",
      "(Iteration 29401 / 61200) loss: 2.305524\n",
      "(Iteration 29501 / 61200) loss: 2.305524\n",
      "(Iteration 29601 / 61200) loss: 2.305518\n",
      "(Iteration 29701 / 61200) loss: 2.305527\n",
      "(Iteration 29801 / 61200) loss: 2.305532\n",
      "(Epoch 39 / 80) train acc: 0.110000; val_acc: 0.096000\n",
      "(Iteration 29901 / 61200) loss: 2.305532\n",
      "(Iteration 30001 / 61200) loss: 2.305536\n",
      "(Iteration 30101 / 61200) loss: 2.305525\n",
      "(Iteration 30201 / 61200) loss: 2.305536\n",
      "(Iteration 30301 / 61200) loss: 2.305536\n",
      "(Iteration 30401 / 61200) loss: 2.305523\n",
      "(Iteration 30501 / 61200) loss: 2.305533\n",
      "(Epoch 40 / 80) train acc: 0.125000; val_acc: 0.096000\n",
      "(Iteration 30601 / 61200) loss: 2.305528\n",
      "(Iteration 30701 / 61200) loss: 2.305534\n",
      "(Iteration 30801 / 61200) loss: 2.305531\n",
      "(Iteration 30901 / 61200) loss: 2.305523\n",
      "(Iteration 31001 / 61200) loss: 2.305520\n",
      "(Iteration 31101 / 61200) loss: 2.305531\n",
      "(Iteration 31201 / 61200) loss: 2.305536\n",
      "(Iteration 31301 / 61200) loss: 2.305533\n",
      "(Epoch 41 / 80) train acc: 0.107000; val_acc: 0.096000\n",
      "(Iteration 31401 / 61200) loss: 2.305525\n",
      "(Iteration 31501 / 61200) loss: 2.305537\n",
      "(Iteration 31601 / 61200) loss: 2.305534\n",
      "(Iteration 31701 / 61200) loss: 2.305531\n",
      "(Iteration 31801 / 61200) loss: 2.305541\n",
      "(Iteration 31901 / 61200) loss: 2.305525\n",
      "(Iteration 32001 / 61200) loss: 2.305533\n",
      "(Iteration 32101 / 61200) loss: 2.305536\n",
      "(Epoch 42 / 80) train acc: 0.107000; val_acc: 0.096000\n",
      "(Iteration 32201 / 61200) loss: 2.305528\n",
      "(Iteration 32301 / 61200) loss: 2.305538\n",
      "(Iteration 32401 / 61200) loss: 2.305523\n",
      "(Iteration 32501 / 61200) loss: 2.305502\n",
      "(Iteration 32601 / 61200) loss: 2.305519\n",
      "(Iteration 32701 / 61200) loss: 2.305536\n",
      "(Iteration 32801 / 61200) loss: 2.305532\n",
      "(Epoch 43 / 80) train acc: 0.115000; val_acc: 0.096000\n",
      "(Iteration 32901 / 61200) loss: 2.305525\n",
      "(Iteration 33001 / 61200) loss: 2.305542\n",
      "(Iteration 33101 / 61200) loss: 2.305542\n",
      "(Iteration 33201 / 61200) loss: 2.305524\n",
      "(Iteration 33301 / 61200) loss: 2.305527\n",
      "(Iteration 33401 / 61200) loss: 2.305533\n",
      "(Iteration 33501 / 61200) loss: 2.305523\n",
      "(Iteration 33601 / 61200) loss: 2.305524\n",
      "(Epoch 44 / 80) train acc: 0.106000; val_acc: 0.096000\n",
      "(Iteration 33701 / 61200) loss: 2.305538\n",
      "(Iteration 33801 / 61200) loss: 2.305540\n",
      "(Iteration 33901 / 61200) loss: 2.305542\n",
      "(Iteration 34001 / 61200) loss: 2.305540\n",
      "(Iteration 34101 / 61200) loss: 2.305531\n",
      "(Iteration 34201 / 61200) loss: 2.305518\n",
      "(Iteration 34301 / 61200) loss: 2.305525\n",
      "(Iteration 34401 / 61200) loss: 2.305528\n",
      "(Epoch 45 / 80) train acc: 0.112000; val_acc: 0.096000\n",
      "(Iteration 34501 / 61200) loss: 2.305550\n",
      "(Iteration 34601 / 61200) loss: 2.305523\n",
      "(Iteration 34701 / 61200) loss: 2.305535\n",
      "(Iteration 34801 / 61200) loss: 2.305522\n",
      "(Iteration 34901 / 61200) loss: 2.305527\n",
      "(Iteration 35001 / 61200) loss: 2.305529\n",
      "(Iteration 35101 / 61200) loss: 2.305543\n",
      "(Epoch 46 / 80) train acc: 0.120000; val_acc: 0.096000\n",
      "(Iteration 35201 / 61200) loss: 2.305530\n",
      "(Iteration 35301 / 61200) loss: 2.305538\n",
      "(Iteration 35401 / 61200) loss: 2.305530\n",
      "(Iteration 35501 / 61200) loss: 2.305533\n",
      "(Iteration 35601 / 61200) loss: 2.305514\n",
      "(Iteration 35701 / 61200) loss: 2.305541\n",
      "(Iteration 35801 / 61200) loss: 2.305542\n",
      "(Iteration 35901 / 61200) loss: 2.305531\n",
      "(Epoch 47 / 80) train acc: 0.097000; val_acc: 0.096000\n",
      "(Iteration 36001 / 61200) loss: 2.305532\n",
      "(Iteration 36101 / 61200) loss: 2.305528\n",
      "(Iteration 36201 / 61200) loss: 2.305517\n",
      "(Iteration 36301 / 61200) loss: 2.305542\n",
      "(Iteration 36401 / 61200) loss: 2.305520\n",
      "(Iteration 36501 / 61200) loss: 2.305539\n",
      "(Iteration 36601 / 61200) loss: 2.305536\n",
      "(Iteration 36701 / 61200) loss: 2.305525\n",
      "(Epoch 48 / 80) train acc: 0.128000; val_acc: 0.096000\n",
      "(Iteration 36801 / 61200) loss: 2.305538\n",
      "(Iteration 36901 / 61200) loss: 2.305534\n",
      "(Iteration 37001 / 61200) loss: 2.305529\n",
      "(Iteration 37101 / 61200) loss: 2.305535\n",
      "(Iteration 37201 / 61200) loss: 2.305524\n",
      "(Iteration 37301 / 61200) loss: 2.305526\n",
      "(Iteration 37401 / 61200) loss: 2.305532\n",
      "(Epoch 49 / 80) train acc: 0.107000; val_acc: 0.096000\n",
      "(Iteration 37501 / 61200) loss: 2.305539\n",
      "(Iteration 37601 / 61200) loss: 2.305520\n",
      "(Iteration 37701 / 61200) loss: 2.305530\n",
      "(Iteration 37801 / 61200) loss: 2.305533\n",
      "(Iteration 37901 / 61200) loss: 2.305528\n",
      "(Iteration 38001 / 61200) loss: 2.305536\n",
      "(Iteration 38101 / 61200) loss: 2.305530\n",
      "(Iteration 38201 / 61200) loss: 2.305520\n",
      "(Epoch 50 / 80) train acc: 0.106000; val_acc: 0.096000\n",
      "(Iteration 38301 / 61200) loss: 2.305519\n",
      "(Iteration 38401 / 61200) loss: 2.305546\n",
      "(Iteration 38501 / 61200) loss: 2.305519\n",
      "(Iteration 38601 / 61200) loss: 2.305524\n",
      "(Iteration 38701 / 61200) loss: 2.305526\n",
      "(Iteration 38801 / 61200) loss: 2.305530\n",
      "(Iteration 38901 / 61200) loss: 2.305531\n",
      "(Iteration 39001 / 61200) loss: 2.305523\n",
      "(Epoch 51 / 80) train acc: 0.102000; val_acc: 0.096000\n",
      "(Iteration 39101 / 61200) loss: 2.305528\n",
      "(Iteration 39201 / 61200) loss: 2.305528\n",
      "(Iteration 39301 / 61200) loss: 2.305526\n",
      "(Iteration 39401 / 61200) loss: 2.305533\n",
      "(Iteration 39501 / 61200) loss: 2.305527\n",
      "(Iteration 39601 / 61200) loss: 2.305538\n",
      "(Iteration 39701 / 61200) loss: 2.305528\n",
      "(Epoch 52 / 80) train acc: 0.115000; val_acc: 0.096000\n",
      "(Iteration 39801 / 61200) loss: 2.305529\n",
      "(Iteration 39901 / 61200) loss: 2.305545\n",
      "(Iteration 40001 / 61200) loss: 2.305536\n",
      "(Iteration 40101 / 61200) loss: 2.305531\n",
      "(Iteration 40201 / 61200) loss: 2.305547\n",
      "(Iteration 40301 / 61200) loss: 2.305522\n",
      "(Iteration 40401 / 61200) loss: 2.305522\n",
      "(Iteration 40501 / 61200) loss: 2.305535\n",
      "(Epoch 53 / 80) train acc: 0.108000; val_acc: 0.096000\n",
      "(Iteration 40601 / 61200) loss: 2.305523\n",
      "(Iteration 40701 / 61200) loss: 2.305527\n",
      "(Iteration 40801 / 61200) loss: 2.305540\n",
      "(Iteration 40901 / 61200) loss: 2.305528\n",
      "(Iteration 41001 / 61200) loss: 2.305535\n",
      "(Iteration 41101 / 61200) loss: 2.305530\n",
      "(Iteration 41201 / 61200) loss: 2.305532\n",
      "(Iteration 41301 / 61200) loss: 2.305526\n",
      "(Epoch 54 / 80) train acc: 0.117000; val_acc: 0.096000\n",
      "(Iteration 41401 / 61200) loss: 2.305540\n",
      "(Iteration 41501 / 61200) loss: 2.305534\n",
      "(Iteration 41601 / 61200) loss: 2.305528\n",
      "(Iteration 41701 / 61200) loss: 2.305543\n",
      "(Iteration 41801 / 61200) loss: 2.305540\n",
      "(Iteration 41901 / 61200) loss: 2.305544\n",
      "(Iteration 42001 / 61200) loss: 2.305527\n",
      "(Epoch 55 / 80) train acc: 0.110000; val_acc: 0.096000\n",
      "(Iteration 42101 / 61200) loss: 2.305522\n",
      "(Iteration 42201 / 61200) loss: 2.305532\n",
      "(Iteration 42301 / 61200) loss: 2.305535\n",
      "(Iteration 42401 / 61200) loss: 2.305525\n",
      "(Iteration 42501 / 61200) loss: 2.305534\n",
      "(Iteration 42601 / 61200) loss: 2.305521\n",
      "(Iteration 42701 / 61200) loss: 2.305532\n",
      "(Iteration 42801 / 61200) loss: 2.305521\n",
      "(Epoch 56 / 80) train acc: 0.129000; val_acc: 0.096000\n",
      "(Iteration 42901 / 61200) loss: 2.305531\n",
      "(Iteration 43001 / 61200) loss: 2.305528\n",
      "(Iteration 43101 / 61200) loss: 2.305527\n",
      "(Iteration 43201 / 61200) loss: 2.305536\n",
      "(Iteration 43301 / 61200) loss: 2.305533\n",
      "(Iteration 43401 / 61200) loss: 2.305519\n",
      "(Iteration 43501 / 61200) loss: 2.305519\n",
      "(Iteration 43601 / 61200) loss: 2.305531\n",
      "(Epoch 57 / 80) train acc: 0.093000; val_acc: 0.096000\n",
      "(Iteration 43701 / 61200) loss: 2.305540\n",
      "(Iteration 43801 / 61200) loss: 2.305527\n",
      "(Iteration 43901 / 61200) loss: 2.305535\n",
      "(Iteration 44001 / 61200) loss: 2.305531\n",
      "(Iteration 44101 / 61200) loss: 2.305509\n",
      "(Iteration 44201 / 61200) loss: 2.305543\n",
      "(Iteration 44301 / 61200) loss: 2.305534\n",
      "(Epoch 58 / 80) train acc: 0.107000; val_acc: 0.096000\n",
      "(Iteration 44401 / 61200) loss: 2.305517\n",
      "(Iteration 44501 / 61200) loss: 2.305529\n",
      "(Iteration 44601 / 61200) loss: 2.305513\n",
      "(Iteration 44701 / 61200) loss: 2.305541\n",
      "(Iteration 44801 / 61200) loss: 2.305517\n",
      "(Iteration 44901 / 61200) loss: 2.305530\n",
      "(Iteration 45001 / 61200) loss: 2.305539\n",
      "(Iteration 45101 / 61200) loss: 2.305521\n",
      "(Epoch 59 / 80) train acc: 0.115000; val_acc: 0.096000\n",
      "(Iteration 45201 / 61200) loss: 2.305511\n",
      "(Iteration 45301 / 61200) loss: 2.305535\n",
      "(Iteration 45401 / 61200) loss: 2.305523\n",
      "(Iteration 45501 / 61200) loss: 2.305542\n",
      "(Iteration 45601 / 61200) loss: 2.305526\n",
      "(Iteration 45701 / 61200) loss: 2.305527\n",
      "(Iteration 45801 / 61200) loss: 2.305536\n",
      "(Epoch 60 / 80) train acc: 0.111000; val_acc: 0.096000\n",
      "(Iteration 45901 / 61200) loss: 2.305543\n",
      "(Iteration 46001 / 61200) loss: 2.305538\n",
      "(Iteration 46101 / 61200) loss: 2.305534\n",
      "(Iteration 46201 / 61200) loss: 2.305529\n",
      "(Iteration 46301 / 61200) loss: 2.305527\n",
      "(Iteration 46401 / 61200) loss: 2.305522\n",
      "(Iteration 46501 / 61200) loss: 2.305548\n",
      "(Iteration 46601 / 61200) loss: 2.305533\n",
      "(Epoch 61 / 80) train acc: 0.119000; val_acc: 0.096000\n",
      "(Iteration 46701 / 61200) loss: 2.305525\n",
      "(Iteration 46801 / 61200) loss: 2.305540\n",
      "(Iteration 46901 / 61200) loss: 2.305533\n",
      "(Iteration 47001 / 61200) loss: 2.305517\n",
      "(Iteration 47101 / 61200) loss: 2.305545\n",
      "(Iteration 47201 / 61200) loss: 2.305535\n",
      "(Iteration 47301 / 61200) loss: 2.305537\n",
      "(Iteration 47401 / 61200) loss: 2.305525\n",
      "(Epoch 62 / 80) train acc: 0.123000; val_acc: 0.096000\n",
      "(Iteration 47501 / 61200) loss: 2.305552\n",
      "(Iteration 47601 / 61200) loss: 2.305532\n",
      "(Iteration 47701 / 61200) loss: 2.305526\n",
      "(Iteration 47801 / 61200) loss: 2.305538\n",
      "(Iteration 47901 / 61200) loss: 2.305532\n",
      "(Iteration 48001 / 61200) loss: 2.305529\n",
      "(Iteration 48101 / 61200) loss: 2.305534\n",
      "(Epoch 63 / 80) train acc: 0.092000; val_acc: 0.096000\n",
      "(Iteration 48201 / 61200) loss: 2.305541\n",
      "(Iteration 48301 / 61200) loss: 2.305544\n",
      "(Iteration 48401 / 61200) loss: 2.305528\n",
      "(Iteration 48501 / 61200) loss: 2.305544\n",
      "(Iteration 48601 / 61200) loss: 2.305527\n",
      "(Iteration 48701 / 61200) loss: 2.305537\n",
      "(Iteration 48801 / 61200) loss: 2.305528\n",
      "(Iteration 48901 / 61200) loss: 2.305544\n",
      "(Epoch 64 / 80) train acc: 0.117000; val_acc: 0.096000\n",
      "(Iteration 49001 / 61200) loss: 2.305537\n",
      "(Iteration 49101 / 61200) loss: 2.305526\n",
      "(Iteration 49201 / 61200) loss: 2.305531\n",
      "(Iteration 49301 / 61200) loss: 2.305527\n",
      "(Iteration 49401 / 61200) loss: 2.305545\n",
      "(Iteration 49501 / 61200) loss: 2.305530\n",
      "(Iteration 49601 / 61200) loss: 2.305538\n",
      "(Iteration 49701 / 61200) loss: 2.305520\n",
      "(Epoch 65 / 80) train acc: 0.108000; val_acc: 0.096000\n",
      "(Iteration 49801 / 61200) loss: 2.305519\n",
      "(Iteration 49901 / 61200) loss: 2.305531\n",
      "(Iteration 50001 / 61200) loss: 2.305550\n",
      "(Iteration 50101 / 61200) loss: 2.305526\n",
      "(Iteration 50201 / 61200) loss: 2.305537\n",
      "(Iteration 50301 / 61200) loss: 2.305517\n",
      "(Iteration 50401 / 61200) loss: 2.305532\n",
      "(Epoch 66 / 80) train acc: 0.111000; val_acc: 0.096000\n",
      "(Iteration 50501 / 61200) loss: 2.305523\n",
      "(Iteration 50601 / 61200) loss: 2.305525\n",
      "(Iteration 50701 / 61200) loss: 2.305539\n",
      "(Iteration 50801 / 61200) loss: 2.305521\n",
      "(Iteration 50901 / 61200) loss: 2.305536\n",
      "(Iteration 51001 / 61200) loss: 2.305524\n",
      "(Iteration 51101 / 61200) loss: 2.305527\n",
      "(Iteration 51201 / 61200) loss: 2.305524\n",
      "(Epoch 67 / 80) train acc: 0.104000; val_acc: 0.096000\n",
      "(Iteration 51301 / 61200) loss: 2.305508\n",
      "(Iteration 51401 / 61200) loss: 2.305541\n",
      "(Iteration 51501 / 61200) loss: 2.305530\n",
      "(Iteration 51601 / 61200) loss: 2.305527\n",
      "(Iteration 51701 / 61200) loss: 2.305541\n",
      "(Iteration 51801 / 61200) loss: 2.305541\n",
      "(Iteration 51901 / 61200) loss: 2.305529\n",
      "(Iteration 52001 / 61200) loss: 2.305538\n",
      "(Epoch 68 / 80) train acc: 0.085000; val_acc: 0.096000\n",
      "(Iteration 52101 / 61200) loss: 2.305539\n",
      "(Iteration 52201 / 61200) loss: 2.305524\n",
      "(Iteration 52301 / 61200) loss: 2.305531\n",
      "(Iteration 52401 / 61200) loss: 2.305545\n",
      "(Iteration 52501 / 61200) loss: 2.305534\n",
      "(Iteration 52601 / 61200) loss: 2.305544\n",
      "(Iteration 52701 / 61200) loss: 2.305533\n",
      "(Epoch 69 / 80) train acc: 0.137000; val_acc: 0.096000\n",
      "(Iteration 52801 / 61200) loss: 2.305538\n",
      "(Iteration 52901 / 61200) loss: 2.305528\n",
      "(Iteration 53001 / 61200) loss: 2.305517\n",
      "(Iteration 53101 / 61200) loss: 2.305522\n",
      "(Iteration 53201 / 61200) loss: 2.305545\n",
      "(Iteration 53301 / 61200) loss: 2.305526\n",
      "(Iteration 53401 / 61200) loss: 2.305527\n",
      "(Iteration 53501 / 61200) loss: 2.305532\n",
      "(Epoch 70 / 80) train acc: 0.111000; val_acc: 0.096000\n",
      "(Iteration 53601 / 61200) loss: 2.305528\n",
      "(Iteration 53701 / 61200) loss: 2.305526\n",
      "(Iteration 53801 / 61200) loss: 2.305525\n",
      "(Iteration 53901 / 61200) loss: 2.305525\n",
      "(Iteration 54001 / 61200) loss: 2.305533\n",
      "(Iteration 54101 / 61200) loss: 2.305526\n",
      "(Iteration 54201 / 61200) loss: 2.305531\n",
      "(Iteration 54301 / 61200) loss: 2.305525\n",
      "(Epoch 71 / 80) train acc: 0.103000; val_acc: 0.096000\n",
      "(Iteration 54401 / 61200) loss: 2.305530\n",
      "(Iteration 54501 / 61200) loss: 2.305526\n",
      "(Iteration 54601 / 61200) loss: 2.305549\n",
      "(Iteration 54701 / 61200) loss: 2.305546\n",
      "(Iteration 54801 / 61200) loss: 2.305544\n",
      "(Iteration 54901 / 61200) loss: 2.305531\n",
      "(Iteration 55001 / 61200) loss: 2.305522\n",
      "(Epoch 72 / 80) train acc: 0.129000; val_acc: 0.096000\n",
      "(Iteration 55101 / 61200) loss: 2.305510\n",
      "(Iteration 55201 / 61200) loss: 2.305530\n",
      "(Iteration 55301 / 61200) loss: 2.305524\n",
      "(Iteration 55401 / 61200) loss: 2.305515\n",
      "(Iteration 55501 / 61200) loss: 2.305523\n",
      "(Iteration 55601 / 61200) loss: 2.305514\n",
      "(Iteration 55701 / 61200) loss: 2.305524\n",
      "(Iteration 55801 / 61200) loss: 2.305538\n",
      "(Epoch 73 / 80) train acc: 0.109000; val_acc: 0.096000\n",
      "(Iteration 55901 / 61200) loss: 2.305543\n",
      "(Iteration 56001 / 61200) loss: 2.305526\n",
      "(Iteration 56101 / 61200) loss: 2.305532\n",
      "(Iteration 56201 / 61200) loss: 2.305534\n",
      "(Iteration 56301 / 61200) loss: 2.305528\n",
      "(Iteration 56401 / 61200) loss: 2.305516\n",
      "(Iteration 56501 / 61200) loss: 2.305511\n",
      "(Iteration 56601 / 61200) loss: 2.305504\n",
      "(Epoch 74 / 80) train acc: 0.101000; val_acc: 0.096000\n",
      "(Iteration 56701 / 61200) loss: 2.305528\n",
      "(Iteration 56801 / 61200) loss: 2.305531\n",
      "(Iteration 56901 / 61200) loss: 2.305525\n",
      "(Iteration 57001 / 61200) loss: 2.305537\n",
      "(Iteration 57101 / 61200) loss: 2.305532\n",
      "(Iteration 57201 / 61200) loss: 2.305525\n",
      "(Iteration 57301 / 61200) loss: 2.305517\n",
      "(Epoch 75 / 80) train acc: 0.105000; val_acc: 0.096000\n",
      "(Iteration 57401 / 61200) loss: 2.305517\n",
      "(Iteration 57501 / 61200) loss: 2.305532\n",
      "(Iteration 57601 / 61200) loss: 2.305543\n",
      "(Iteration 57701 / 61200) loss: 2.305521\n",
      "(Iteration 57801 / 61200) loss: 2.305533\n",
      "(Iteration 57901 / 61200) loss: 2.305534\n",
      "(Iteration 58001 / 61200) loss: 2.305535\n",
      "(Iteration 58101 / 61200) loss: 2.305532\n",
      "(Epoch 76 / 80) train acc: 0.107000; val_acc: 0.096000\n",
      "(Iteration 58201 / 61200) loss: 2.305515\n",
      "(Iteration 58301 / 61200) loss: 2.305535\n",
      "(Iteration 58401 / 61200) loss: 2.305529\n",
      "(Iteration 58501 / 61200) loss: 2.305515\n",
      "(Iteration 58601 / 61200) loss: 2.305528\n",
      "(Iteration 58701 / 61200) loss: 2.305534\n",
      "(Iteration 58801 / 61200) loss: 2.305536\n",
      "(Iteration 58901 / 61200) loss: 2.305519\n",
      "(Epoch 77 / 80) train acc: 0.114000; val_acc: 0.096000\n",
      "(Iteration 59001 / 61200) loss: 2.305542\n",
      "(Iteration 59101 / 61200) loss: 2.305526\n",
      "(Iteration 59201 / 61200) loss: 2.305523\n",
      "(Iteration 59301 / 61200) loss: 2.305529\n",
      "(Iteration 59401 / 61200) loss: 2.305544\n",
      "(Iteration 59501 / 61200) loss: 2.305537\n",
      "(Iteration 59601 / 61200) loss: 2.305533\n",
      "(Epoch 78 / 80) train acc: 0.105000; val_acc: 0.096000\n",
      "(Iteration 59701 / 61200) loss: 2.305532\n",
      "(Iteration 59801 / 61200) loss: 2.305529\n",
      "(Iteration 59901 / 61200) loss: 2.305535\n",
      "(Iteration 60001 / 61200) loss: 2.305515\n",
      "(Iteration 60101 / 61200) loss: 2.305526\n",
      "(Iteration 60201 / 61200) loss: 2.305544\n",
      "(Iteration 60301 / 61200) loss: 2.305522\n",
      "(Iteration 60401 / 61200) loss: 2.305542\n",
      "(Epoch 79 / 80) train acc: 0.098000; val_acc: 0.096000\n",
      "(Iteration 60501 / 61200) loss: 2.305540\n",
      "(Iteration 60601 / 61200) loss: 2.305530\n",
      "(Iteration 60701 / 61200) loss: 2.305536\n",
      "(Iteration 60801 / 61200) loss: 2.305521\n",
      "(Iteration 60901 / 61200) loss: 2.305530\n",
      "(Iteration 61001 / 61200) loss: 2.305532\n",
      "(Iteration 61101 / 61200) loss: 2.305535\n",
      "(Epoch 80 / 80) train acc: 0.103000; val_acc: 0.096000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.305480\n",
      "(Epoch 0 / 80) train acc: 0.081000; val_acc: 0.099000\n",
      "(Iteration 101 / 30560) loss: 2.305483\n",
      "(Iteration 201 / 30560) loss: 2.305467\n",
      "(Iteration 301 / 30560) loss: 2.305476\n",
      "(Epoch 1 / 80) train acc: 0.068000; val_acc: 0.099000\n",
      "(Iteration 401 / 30560) loss: 2.305474\n",
      "(Iteration 501 / 30560) loss: 2.305469\n",
      "(Iteration 601 / 30560) loss: 2.305466\n",
      "(Iteration 701 / 30560) loss: 2.305472\n",
      "(Epoch 2 / 80) train acc: 0.069000; val_acc: 0.099000\n",
      "(Iteration 801 / 30560) loss: 2.305472\n",
      "(Iteration 901 / 30560) loss: 2.305472\n",
      "(Iteration 1001 / 30560) loss: 2.305477\n",
      "(Iteration 1101 / 30560) loss: 2.305480\n",
      "(Epoch 3 / 80) train acc: 0.084000; val_acc: 0.099000\n",
      "(Iteration 1201 / 30560) loss: 2.305465\n",
      "(Iteration 1301 / 30560) loss: 2.305467\n",
      "(Iteration 1401 / 30560) loss: 2.305461\n",
      "(Iteration 1501 / 30560) loss: 2.305457\n",
      "(Epoch 4 / 80) train acc: 0.083000; val_acc: 0.099000\n",
      "(Iteration 1601 / 30560) loss: 2.305476\n",
      "(Iteration 1701 / 30560) loss: 2.305478\n",
      "(Iteration 1801 / 30560) loss: 2.305475\n",
      "(Iteration 1901 / 30560) loss: 2.305481\n",
      "(Epoch 5 / 80) train acc: 0.090000; val_acc: 0.099000\n",
      "(Iteration 2001 / 30560) loss: 2.305484\n",
      "(Iteration 2101 / 30560) loss: 2.305491\n",
      "(Iteration 2201 / 30560) loss: 2.305472\n",
      "(Epoch 6 / 80) train acc: 0.091000; val_acc: 0.099000\n",
      "(Iteration 2301 / 30560) loss: 2.305473\n",
      "(Iteration 2401 / 30560) loss: 2.305479\n",
      "(Iteration 2501 / 30560) loss: 2.305473\n",
      "(Iteration 2601 / 30560) loss: 2.305481\n",
      "(Epoch 7 / 80) train acc: 0.099000; val_acc: 0.099000\n",
      "(Iteration 2701 / 30560) loss: 2.305473\n",
      "(Iteration 2801 / 30560) loss: 2.305472\n",
      "(Iteration 2901 / 30560) loss: 2.305482\n",
      "(Iteration 3001 / 30560) loss: 2.305471\n",
      "(Epoch 8 / 80) train acc: 0.087000; val_acc: 0.099000\n",
      "(Iteration 3101 / 30560) loss: 2.305471\n",
      "(Iteration 3201 / 30560) loss: 2.305468\n",
      "(Iteration 3301 / 30560) loss: 2.305491\n",
      "(Iteration 3401 / 30560) loss: 2.305483\n",
      "(Epoch 9 / 80) train acc: 0.084000; val_acc: 0.100000\n",
      "(Iteration 3501 / 30560) loss: 2.305481\n",
      "(Iteration 3601 / 30560) loss: 2.305463\n",
      "(Iteration 3701 / 30560) loss: 2.305471\n",
      "(Iteration 3801 / 30560) loss: 2.305473\n",
      "(Epoch 10 / 80) train acc: 0.102000; val_acc: 0.100000\n",
      "(Iteration 3901 / 30560) loss: 2.305479\n",
      "(Iteration 4001 / 30560) loss: 2.305469\n",
      "(Iteration 4101 / 30560) loss: 2.305486\n",
      "(Iteration 4201 / 30560) loss: 2.305478\n",
      "(Epoch 11 / 80) train acc: 0.102000; val_acc: 0.100000\n",
      "(Iteration 4301 / 30560) loss: 2.305484\n",
      "(Iteration 4401 / 30560) loss: 2.305482\n",
      "(Iteration 4501 / 30560) loss: 2.305482\n",
      "(Epoch 12 / 80) train acc: 0.091000; val_acc: 0.100000\n",
      "(Iteration 4601 / 30560) loss: 2.305484\n",
      "(Iteration 4701 / 30560) loss: 2.305475\n",
      "(Iteration 4801 / 30560) loss: 2.305471\n",
      "(Iteration 4901 / 30560) loss: 2.305477\n",
      "(Epoch 13 / 80) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 5001 / 30560) loss: 2.305478\n",
      "(Iteration 5101 / 30560) loss: 2.305466\n",
      "(Iteration 5201 / 30560) loss: 2.305467\n",
      "(Iteration 5301 / 30560) loss: 2.305483\n",
      "(Epoch 14 / 80) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 5401 / 30560) loss: 2.305477\n",
      "(Iteration 5501 / 30560) loss: 2.305472\n",
      "(Iteration 5601 / 30560) loss: 2.305481\n",
      "(Iteration 5701 / 30560) loss: 2.305476\n",
      "(Epoch 15 / 80) train acc: 0.092000; val_acc: 0.100000\n",
      "(Iteration 5801 / 30560) loss: 2.305481\n",
      "(Iteration 5901 / 30560) loss: 2.305475\n",
      "(Iteration 6001 / 30560) loss: 2.305471\n",
      "(Iteration 6101 / 30560) loss: 2.305471\n",
      "(Epoch 16 / 80) train acc: 0.076000; val_acc: 0.100000\n",
      "(Iteration 6201 / 30560) loss: 2.305473\n",
      "(Iteration 6301 / 30560) loss: 2.305481\n",
      "(Iteration 6401 / 30560) loss: 2.305476\n",
      "(Epoch 17 / 80) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 6501 / 30560) loss: 2.305476\n",
      "(Iteration 6601 / 30560) loss: 2.305479\n",
      "(Iteration 6701 / 30560) loss: 2.305484\n",
      "(Iteration 6801 / 30560) loss: 2.305473\n",
      "(Epoch 18 / 80) train acc: 0.078000; val_acc: 0.100000\n",
      "(Iteration 6901 / 30560) loss: 2.305473\n",
      "(Iteration 7001 / 30560) loss: 2.305465\n",
      "(Iteration 7101 / 30560) loss: 2.305483\n",
      "(Iteration 7201 / 30560) loss: 2.305465\n",
      "(Epoch 19 / 80) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 7301 / 30560) loss: 2.305467\n",
      "(Iteration 7401 / 30560) loss: 2.305470\n",
      "(Iteration 7501 / 30560) loss: 2.305471\n",
      "(Iteration 7601 / 30560) loss: 2.305475\n",
      "(Epoch 20 / 80) train acc: 0.084000; val_acc: 0.100000\n",
      "(Iteration 7701 / 30560) loss: 2.305476\n",
      "(Iteration 7801 / 30560) loss: 2.305479\n",
      "(Iteration 7901 / 30560) loss: 2.305487\n",
      "(Iteration 8001 / 30560) loss: 2.305469\n",
      "(Epoch 21 / 80) train acc: 0.096000; val_acc: 0.101000\n",
      "(Iteration 8101 / 30560) loss: 2.305477\n",
      "(Iteration 8201 / 30560) loss: 2.305466\n",
      "(Iteration 8301 / 30560) loss: 2.305481\n",
      "(Iteration 8401 / 30560) loss: 2.305471\n",
      "(Epoch 22 / 80) train acc: 0.068000; val_acc: 0.101000\n",
      "(Iteration 8501 / 30560) loss: 2.305473\n",
      "(Iteration 8601 / 30560) loss: 2.305474\n",
      "(Iteration 8701 / 30560) loss: 2.305473\n",
      "(Epoch 23 / 80) train acc: 0.092000; val_acc: 0.101000\n",
      "(Iteration 8801 / 30560) loss: 2.305472\n",
      "(Iteration 8901 / 30560) loss: 2.305467\n",
      "(Iteration 9001 / 30560) loss: 2.305490\n",
      "(Iteration 9101 / 30560) loss: 2.305481\n",
      "(Epoch 24 / 80) train acc: 0.084000; val_acc: 0.101000\n",
      "(Iteration 9201 / 30560) loss: 2.305461\n",
      "(Iteration 9301 / 30560) loss: 2.305479\n",
      "(Iteration 9401 / 30560) loss: 2.305480\n",
      "(Iteration 9501 / 30560) loss: 2.305469\n",
      "(Epoch 25 / 80) train acc: 0.082000; val_acc: 0.100000\n",
      "(Iteration 9601 / 30560) loss: 2.305478\n",
      "(Iteration 9701 / 30560) loss: 2.305479\n",
      "(Iteration 9801 / 30560) loss: 2.305474\n",
      "(Iteration 9901 / 30560) loss: 2.305483\n",
      "(Epoch 26 / 80) train acc: 0.078000; val_acc: 0.101000\n",
      "(Iteration 10001 / 30560) loss: 2.305463\n",
      "(Iteration 10101 / 30560) loss: 2.305481\n",
      "(Iteration 10201 / 30560) loss: 2.305480\n",
      "(Iteration 10301 / 30560) loss: 2.305472\n",
      "(Epoch 27 / 80) train acc: 0.103000; val_acc: 0.101000\n",
      "(Iteration 10401 / 30560) loss: 2.305480\n",
      "(Iteration 10501 / 30560) loss: 2.305473\n",
      "(Iteration 10601 / 30560) loss: 2.305469\n",
      "(Epoch 28 / 80) train acc: 0.083000; val_acc: 0.100000\n",
      "(Iteration 10701 / 30560) loss: 2.305490\n",
      "(Iteration 10801 / 30560) loss: 2.305474\n",
      "(Iteration 10901 / 30560) loss: 2.305482\n",
      "(Iteration 11001 / 30560) loss: 2.305477\n",
      "(Epoch 29 / 80) train acc: 0.067000; val_acc: 0.100000\n",
      "(Iteration 11101 / 30560) loss: 2.305480\n",
      "(Iteration 11201 / 30560) loss: 2.305473\n",
      "(Iteration 11301 / 30560) loss: 2.305478\n",
      "(Iteration 11401 / 30560) loss: 2.305471\n",
      "(Epoch 30 / 80) train acc: 0.074000; val_acc: 0.100000\n",
      "(Iteration 11501 / 30560) loss: 2.305475\n",
      "(Iteration 11601 / 30560) loss: 2.305469\n",
      "(Iteration 11701 / 30560) loss: 2.305462\n",
      "(Iteration 11801 / 30560) loss: 2.305474\n",
      "(Epoch 31 / 80) train acc: 0.075000; val_acc: 0.101000\n",
      "(Iteration 11901 / 30560) loss: 2.305481\n",
      "(Iteration 12001 / 30560) loss: 2.305469\n",
      "(Iteration 12101 / 30560) loss: 2.305472\n",
      "(Iteration 12201 / 30560) loss: 2.305476\n",
      "(Epoch 32 / 80) train acc: 0.102000; val_acc: 0.101000\n",
      "(Iteration 12301 / 30560) loss: 2.305487\n",
      "(Iteration 12401 / 30560) loss: 2.305488\n",
      "(Iteration 12501 / 30560) loss: 2.305472\n",
      "(Iteration 12601 / 30560) loss: 2.305471\n",
      "(Epoch 33 / 80) train acc: 0.089000; val_acc: 0.101000\n",
      "(Iteration 12701 / 30560) loss: 2.305481\n",
      "(Iteration 12801 / 30560) loss: 2.305472\n",
      "(Iteration 12901 / 30560) loss: 2.305460\n",
      "(Epoch 34 / 80) train acc: 0.085000; val_acc: 0.101000\n",
      "(Iteration 13001 / 30560) loss: 2.305483\n",
      "(Iteration 13101 / 30560) loss: 2.305478\n",
      "(Iteration 13201 / 30560) loss: 2.305476\n",
      "(Iteration 13301 / 30560) loss: 2.305470\n",
      "(Epoch 35 / 80) train acc: 0.095000; val_acc: 0.101000\n",
      "(Iteration 13401 / 30560) loss: 2.305476\n",
      "(Iteration 13501 / 30560) loss: 2.305478\n",
      "(Iteration 13601 / 30560) loss: 2.305477\n",
      "(Iteration 13701 / 30560) loss: 2.305473\n",
      "(Epoch 36 / 80) train acc: 0.106000; val_acc: 0.101000\n",
      "(Iteration 13801 / 30560) loss: 2.305470\n",
      "(Iteration 13901 / 30560) loss: 2.305477\n",
      "(Iteration 14001 / 30560) loss: 2.305474\n",
      "(Iteration 14101 / 30560) loss: 2.305471\n",
      "(Epoch 37 / 80) train acc: 0.095000; val_acc: 0.101000\n",
      "(Iteration 14201 / 30560) loss: 2.305471\n",
      "(Iteration 14301 / 30560) loss: 2.305470\n",
      "(Iteration 14401 / 30560) loss: 2.305477\n",
      "(Iteration 14501 / 30560) loss: 2.305475\n",
      "(Epoch 38 / 80) train acc: 0.074000; val_acc: 0.101000\n",
      "(Iteration 14601 / 30560) loss: 2.305474\n",
      "(Iteration 14701 / 30560) loss: 2.305468\n",
      "(Iteration 14801 / 30560) loss: 2.305474\n",
      "(Epoch 39 / 80) train acc: 0.078000; val_acc: 0.101000\n",
      "(Iteration 14901 / 30560) loss: 2.305467\n",
      "(Iteration 15001 / 30560) loss: 2.305481\n",
      "(Iteration 15101 / 30560) loss: 2.305471\n",
      "(Iteration 15201 / 30560) loss: 2.305468\n",
      "(Epoch 40 / 80) train acc: 0.096000; val_acc: 0.101000\n",
      "(Iteration 15301 / 30560) loss: 2.305485\n",
      "(Iteration 15401 / 30560) loss: 2.305464\n",
      "(Iteration 15501 / 30560) loss: 2.305469\n",
      "(Iteration 15601 / 30560) loss: 2.305473\n",
      "(Epoch 41 / 80) train acc: 0.091000; val_acc: 0.101000\n",
      "(Iteration 15701 / 30560) loss: 2.305470\n",
      "(Iteration 15801 / 30560) loss: 2.305481\n",
      "(Iteration 15901 / 30560) loss: 2.305472\n",
      "(Iteration 16001 / 30560) loss: 2.305481\n",
      "(Epoch 42 / 80) train acc: 0.084000; val_acc: 0.100000\n",
      "(Iteration 16101 / 30560) loss: 2.305474\n",
      "(Iteration 16201 / 30560) loss: 2.305480\n",
      "(Iteration 16301 / 30560) loss: 2.305484\n",
      "(Iteration 16401 / 30560) loss: 2.305470\n",
      "(Epoch 43 / 80) train acc: 0.077000; val_acc: 0.100000\n",
      "(Iteration 16501 / 30560) loss: 2.305475\n",
      "(Iteration 16601 / 30560) loss: 2.305476\n",
      "(Iteration 16701 / 30560) loss: 2.305466\n",
      "(Iteration 16801 / 30560) loss: 2.305468\n",
      "(Epoch 44 / 80) train acc: 0.091000; val_acc: 0.100000\n",
      "(Iteration 16901 / 30560) loss: 2.305481\n",
      "(Iteration 17001 / 30560) loss: 2.305478\n",
      "(Iteration 17101 / 30560) loss: 2.305482\n",
      "(Epoch 45 / 80) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 17201 / 30560) loss: 2.305473\n",
      "(Iteration 17301 / 30560) loss: 2.305470\n",
      "(Iteration 17401 / 30560) loss: 2.305466\n",
      "(Iteration 17501 / 30560) loss: 2.305470\n",
      "(Epoch 46 / 80) train acc: 0.091000; val_acc: 0.100000\n",
      "(Iteration 17601 / 30560) loss: 2.305480\n",
      "(Iteration 17701 / 30560) loss: 2.305485\n",
      "(Iteration 17801 / 30560) loss: 2.305476\n",
      "(Iteration 17901 / 30560) loss: 2.305464\n",
      "(Epoch 47 / 80) train acc: 0.088000; val_acc: 0.101000\n",
      "(Iteration 18001 / 30560) loss: 2.305485\n",
      "(Iteration 18101 / 30560) loss: 2.305473\n",
      "(Iteration 18201 / 30560) loss: 2.305471\n",
      "(Iteration 18301 / 30560) loss: 2.305483\n",
      "(Epoch 48 / 80) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 18401 / 30560) loss: 2.305475\n",
      "(Iteration 18501 / 30560) loss: 2.305471\n",
      "(Iteration 18601 / 30560) loss: 2.305474\n",
      "(Iteration 18701 / 30560) loss: 2.305459\n",
      "(Epoch 49 / 80) train acc: 0.078000; val_acc: 0.100000\n",
      "(Iteration 18801 / 30560) loss: 2.305477\n",
      "(Iteration 18901 / 30560) loss: 2.305469\n",
      "(Iteration 19001 / 30560) loss: 2.305473\n",
      "(Epoch 50 / 80) train acc: 0.084000; val_acc: 0.100000\n",
      "(Iteration 19101 / 30560) loss: 2.305484\n",
      "(Iteration 19201 / 30560) loss: 2.305481\n",
      "(Iteration 19301 / 30560) loss: 2.305471\n",
      "(Iteration 19401 / 30560) loss: 2.305479\n",
      "(Epoch 51 / 80) train acc: 0.090000; val_acc: 0.101000\n",
      "(Iteration 19501 / 30560) loss: 2.305473\n",
      "(Iteration 19601 / 30560) loss: 2.305479\n",
      "(Iteration 19701 / 30560) loss: 2.305476\n",
      "(Iteration 19801 / 30560) loss: 2.305462\n",
      "(Epoch 52 / 80) train acc: 0.077000; val_acc: 0.101000\n",
      "(Iteration 19901 / 30560) loss: 2.305467\n",
      "(Iteration 20001 / 30560) loss: 2.305484\n",
      "(Iteration 20101 / 30560) loss: 2.305479\n",
      "(Iteration 20201 / 30560) loss: 2.305468\n",
      "(Epoch 53 / 80) train acc: 0.084000; val_acc: 0.101000\n",
      "(Iteration 20301 / 30560) loss: 2.305487\n",
      "(Iteration 20401 / 30560) loss: 2.305488\n",
      "(Iteration 20501 / 30560) loss: 2.305476\n",
      "(Iteration 20601 / 30560) loss: 2.305475\n",
      "(Epoch 54 / 80) train acc: 0.069000; val_acc: 0.101000\n",
      "(Iteration 20701 / 30560) loss: 2.305472\n",
      "(Iteration 20801 / 30560) loss: 2.305467\n",
      "(Iteration 20901 / 30560) loss: 2.305473\n",
      "(Iteration 21001 / 30560) loss: 2.305475\n",
      "(Epoch 55 / 80) train acc: 0.073000; val_acc: 0.101000\n",
      "(Iteration 21101 / 30560) loss: 2.305470\n",
      "(Iteration 21201 / 30560) loss: 2.305476\n",
      "(Iteration 21301 / 30560) loss: 2.305476\n",
      "(Epoch 56 / 80) train acc: 0.091000; val_acc: 0.101000\n",
      "(Iteration 21401 / 30560) loss: 2.305471\n",
      "(Iteration 21501 / 30560) loss: 2.305467\n",
      "(Iteration 21601 / 30560) loss: 2.305474\n",
      "(Iteration 21701 / 30560) loss: 2.305465\n",
      "(Epoch 57 / 80) train acc: 0.099000; val_acc: 0.101000\n",
      "(Iteration 21801 / 30560) loss: 2.305484\n",
      "(Iteration 21901 / 30560) loss: 2.305476\n",
      "(Iteration 22001 / 30560) loss: 2.305460\n",
      "(Iteration 22101 / 30560) loss: 2.305486\n",
      "(Epoch 58 / 80) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 22201 / 30560) loss: 2.305470\n",
      "(Iteration 22301 / 30560) loss: 2.305470\n",
      "(Iteration 22401 / 30560) loss: 2.305469\n",
      "(Iteration 22501 / 30560) loss: 2.305470\n",
      "(Epoch 59 / 80) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 22601 / 30560) loss: 2.305460\n",
      "(Iteration 22701 / 30560) loss: 2.305482\n",
      "(Iteration 22801 / 30560) loss: 2.305478\n",
      "(Iteration 22901 / 30560) loss: 2.305477\n",
      "(Epoch 60 / 80) train acc: 0.108000; val_acc: 0.100000\n",
      "(Iteration 23001 / 30560) loss: 2.305476\n",
      "(Iteration 23101 / 30560) loss: 2.305474\n",
      "(Iteration 23201 / 30560) loss: 2.305471\n",
      "(Iteration 23301 / 30560) loss: 2.305479\n",
      "(Epoch 61 / 80) train acc: 0.073000; val_acc: 0.100000\n",
      "(Iteration 23401 / 30560) loss: 2.305474\n",
      "(Iteration 23501 / 30560) loss: 2.305473\n",
      "(Iteration 23601 / 30560) loss: 2.305482\n",
      "(Epoch 62 / 80) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 23701 / 30560) loss: 2.305482\n",
      "(Iteration 23801 / 30560) loss: 2.305469\n",
      "(Iteration 23901 / 30560) loss: 2.305476\n",
      "(Iteration 24001 / 30560) loss: 2.305477\n",
      "(Epoch 63 / 80) train acc: 0.082000; val_acc: 0.100000\n",
      "(Iteration 24101 / 30560) loss: 2.305479\n",
      "(Iteration 24201 / 30560) loss: 2.305487\n",
      "(Iteration 24301 / 30560) loss: 2.305467\n",
      "(Iteration 24401 / 30560) loss: 2.305485\n",
      "(Epoch 64 / 80) train acc: 0.083000; val_acc: 0.100000\n",
      "(Iteration 24501 / 30560) loss: 2.305474\n",
      "(Iteration 24601 / 30560) loss: 2.305480\n",
      "(Iteration 24701 / 30560) loss: 2.305466\n",
      "(Iteration 24801 / 30560) loss: 2.305485\n",
      "(Epoch 65 / 80) train acc: 0.084000; val_acc: 0.100000\n",
      "(Iteration 24901 / 30560) loss: 2.305473\n",
      "(Iteration 25001 / 30560) loss: 2.305475\n",
      "(Iteration 25101 / 30560) loss: 2.305466\n",
      "(Iteration 25201 / 30560) loss: 2.305469\n",
      "(Epoch 66 / 80) train acc: 0.066000; val_acc: 0.100000\n",
      "(Iteration 25301 / 30560) loss: 2.305479\n",
      "(Iteration 25401 / 30560) loss: 2.305474\n",
      "(Iteration 25501 / 30560) loss: 2.305477\n",
      "(Epoch 67 / 80) train acc: 0.076000; val_acc: 0.100000\n",
      "(Iteration 25601 / 30560) loss: 2.305470\n",
      "(Iteration 25701 / 30560) loss: 2.305474\n",
      "(Iteration 25801 / 30560) loss: 2.305475\n",
      "(Iteration 25901 / 30560) loss: 2.305481\n",
      "(Epoch 68 / 80) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 26001 / 30560) loss: 2.305478\n",
      "(Iteration 26101 / 30560) loss: 2.305479\n",
      "(Iteration 26201 / 30560) loss: 2.305467\n",
      "(Iteration 26301 / 30560) loss: 2.305478\n",
      "(Epoch 69 / 80) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 26401 / 30560) loss: 2.305484\n",
      "(Iteration 26501 / 30560) loss: 2.305478\n",
      "(Iteration 26601 / 30560) loss: 2.305473\n",
      "(Iteration 26701 / 30560) loss: 2.305469\n",
      "(Epoch 70 / 80) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 26801 / 30560) loss: 2.305464\n",
      "(Iteration 26901 / 30560) loss: 2.305470\n",
      "(Iteration 27001 / 30560) loss: 2.305470\n",
      "(Iteration 27101 / 30560) loss: 2.305471\n",
      "(Epoch 71 / 80) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 27201 / 30560) loss: 2.305486\n",
      "(Iteration 27301 / 30560) loss: 2.305478\n",
      "(Iteration 27401 / 30560) loss: 2.305475\n",
      "(Iteration 27501 / 30560) loss: 2.305469\n",
      "(Epoch 72 / 80) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 27601 / 30560) loss: 2.305484\n",
      "(Iteration 27701 / 30560) loss: 2.305471\n",
      "(Iteration 27801 / 30560) loss: 2.305477\n",
      "(Epoch 73 / 80) train acc: 0.083000; val_acc: 0.100000\n",
      "(Iteration 27901 / 30560) loss: 2.305467\n",
      "(Iteration 28001 / 30560) loss: 2.305482\n",
      "(Iteration 28101 / 30560) loss: 2.305476\n",
      "(Iteration 28201 / 30560) loss: 2.305471\n",
      "(Epoch 74 / 80) train acc: 0.075000; val_acc: 0.100000\n",
      "(Iteration 28301 / 30560) loss: 2.305488\n",
      "(Iteration 28401 / 30560) loss: 2.305471\n",
      "(Iteration 28501 / 30560) loss: 2.305470\n",
      "(Iteration 28601 / 30560) loss: 2.305478\n",
      "(Epoch 75 / 80) train acc: 0.085000; val_acc: 0.100000\n",
      "(Iteration 28701 / 30560) loss: 2.305467\n",
      "(Iteration 28801 / 30560) loss: 2.305480\n",
      "(Iteration 28901 / 30560) loss: 2.305474\n",
      "(Iteration 29001 / 30560) loss: 2.305476\n",
      "(Epoch 76 / 80) train acc: 0.077000; val_acc: 0.100000\n",
      "(Iteration 29101 / 30560) loss: 2.305474\n",
      "(Iteration 29201 / 30560) loss: 2.305469\n",
      "(Iteration 29301 / 30560) loss: 2.305477\n",
      "(Iteration 29401 / 30560) loss: 2.305476\n",
      "(Epoch 77 / 80) train acc: 0.073000; val_acc: 0.100000\n",
      "(Iteration 29501 / 30560) loss: 2.305469\n",
      "(Iteration 29601 / 30560) loss: 2.305472\n",
      "(Iteration 29701 / 30560) loss: 2.305465\n",
      "(Epoch 78 / 80) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 29801 / 30560) loss: 2.305477\n",
      "(Iteration 29901 / 30560) loss: 2.305467\n",
      "(Iteration 30001 / 30560) loss: 2.305486\n",
      "(Iteration 30101 / 30560) loss: 2.305464\n",
      "(Epoch 79 / 80) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 30201 / 30560) loss: 2.305476\n",
      "(Iteration 30301 / 30560) loss: 2.305463\n",
      "(Iteration 30401 / 30560) loss: 2.305483\n",
      "(Iteration 30501 / 30560) loss: 2.305483\n",
      "(Epoch 80 / 80) train acc: 0.088000; val_acc: 0.100000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.304656\n",
      "(Epoch 0 / 100) train acc: 0.095000; val_acc: 0.099000\n",
      "(Iteration 101 / 76500) loss: 2.304665\n",
      "(Iteration 201 / 76500) loss: 2.304663\n",
      "(Iteration 301 / 76500) loss: 2.304651\n",
      "(Iteration 401 / 76500) loss: 2.304656\n",
      "(Iteration 501 / 76500) loss: 2.304648\n",
      "(Iteration 601 / 76500) loss: 2.304670\n",
      "(Iteration 701 / 76500) loss: 2.304660\n",
      "(Epoch 1 / 100) train acc: 0.093000; val_acc: 0.099000\n",
      "(Iteration 801 / 76500) loss: 2.304659\n",
      "(Iteration 901 / 76500) loss: 2.304662\n",
      "(Iteration 1001 / 76500) loss: 2.304655\n",
      "(Iteration 1101 / 76500) loss: 2.304656\n",
      "(Iteration 1201 / 76500) loss: 2.304640\n",
      "(Iteration 1301 / 76500) loss: 2.304651\n",
      "(Iteration 1401 / 76500) loss: 2.304658\n",
      "(Iteration 1501 / 76500) loss: 2.304660\n",
      "(Epoch 2 / 100) train acc: 0.107000; val_acc: 0.099000\n",
      "(Iteration 1601 / 76500) loss: 2.304651\n",
      "(Iteration 1701 / 76500) loss: 2.304658\n",
      "(Iteration 1801 / 76500) loss: 2.304651\n",
      "(Iteration 1901 / 76500) loss: 2.304652\n",
      "(Iteration 2001 / 76500) loss: 2.304656\n",
      "(Iteration 2101 / 76500) loss: 2.304647\n",
      "(Iteration 2201 / 76500) loss: 2.304651\n",
      "(Epoch 3 / 100) train acc: 0.103000; val_acc: 0.099000\n",
      "(Iteration 2301 / 76500) loss: 2.304658\n",
      "(Iteration 2401 / 76500) loss: 2.304646\n",
      "(Iteration 2501 / 76500) loss: 2.304646\n",
      "(Iteration 2601 / 76500) loss: 2.304649\n",
      "(Iteration 2701 / 76500) loss: 2.304667\n",
      "(Iteration 2801 / 76500) loss: 2.304650\n",
      "(Iteration 2901 / 76500) loss: 2.304660\n",
      "(Iteration 3001 / 76500) loss: 2.304646\n",
      "(Epoch 4 / 100) train acc: 0.090000; val_acc: 0.099000\n",
      "(Iteration 3101 / 76500) loss: 2.304660\n",
      "(Iteration 3201 / 76500) loss: 2.304657\n",
      "(Iteration 3301 / 76500) loss: 2.304649\n",
      "(Iteration 3401 / 76500) loss: 2.304668\n",
      "(Iteration 3501 / 76500) loss: 2.304646\n",
      "(Iteration 3601 / 76500) loss: 2.304660\n",
      "(Iteration 3701 / 76500) loss: 2.304651\n",
      "(Iteration 3801 / 76500) loss: 2.304649\n",
      "(Epoch 5 / 100) train acc: 0.101000; val_acc: 0.099000\n",
      "(Iteration 3901 / 76500) loss: 2.304651\n",
      "(Iteration 4001 / 76500) loss: 2.304650\n",
      "(Iteration 4101 / 76500) loss: 2.304644\n",
      "(Iteration 4201 / 76500) loss: 2.304647\n",
      "(Iteration 4301 / 76500) loss: 2.304656\n",
      "(Iteration 4401 / 76500) loss: 2.304651\n",
      "(Iteration 4501 / 76500) loss: 2.304659\n",
      "(Epoch 6 / 100) train acc: 0.098000; val_acc: 0.099000\n",
      "(Iteration 4601 / 76500) loss: 2.304646\n",
      "(Iteration 4701 / 76500) loss: 2.304654\n",
      "(Iteration 4801 / 76500) loss: 2.304647\n",
      "(Iteration 4901 / 76500) loss: 2.304658\n",
      "(Iteration 5001 / 76500) loss: 2.304650\n",
      "(Iteration 5101 / 76500) loss: 2.304665\n",
      "(Iteration 5201 / 76500) loss: 2.304658\n",
      "(Iteration 5301 / 76500) loss: 2.304656\n",
      "(Epoch 7 / 100) train acc: 0.099000; val_acc: 0.099000\n",
      "(Iteration 5401 / 76500) loss: 2.304643\n",
      "(Iteration 5501 / 76500) loss: 2.304655\n",
      "(Iteration 5601 / 76500) loss: 2.304656\n",
      "(Iteration 5701 / 76500) loss: 2.304659\n",
      "(Iteration 5801 / 76500) loss: 2.304646\n",
      "(Iteration 5901 / 76500) loss: 2.304644\n",
      "(Iteration 6001 / 76500) loss: 2.304671\n",
      "(Iteration 6101 / 76500) loss: 2.304660\n",
      "(Epoch 8 / 100) train acc: 0.095000; val_acc: 0.099000\n",
      "(Iteration 6201 / 76500) loss: 2.304664\n",
      "(Iteration 6301 / 76500) loss: 2.304656\n",
      "(Iteration 6401 / 76500) loss: 2.304645\n",
      "(Iteration 6501 / 76500) loss: 2.304656\n",
      "(Iteration 6601 / 76500) loss: 2.304647\n",
      "(Iteration 6701 / 76500) loss: 2.304648\n",
      "(Iteration 6801 / 76500) loss: 2.304667\n",
      "(Epoch 9 / 100) train acc: 0.109000; val_acc: 0.099000\n",
      "(Iteration 6901 / 76500) loss: 2.304658\n",
      "(Iteration 7001 / 76500) loss: 2.304648\n",
      "(Iteration 7101 / 76500) loss: 2.304664\n",
      "(Iteration 7201 / 76500) loss: 2.304663\n",
      "(Iteration 7301 / 76500) loss: 2.304660\n",
      "(Iteration 7401 / 76500) loss: 2.304661\n",
      "(Iteration 7501 / 76500) loss: 2.304651\n",
      "(Iteration 7601 / 76500) loss: 2.304663\n",
      "(Epoch 10 / 100) train acc: 0.097000; val_acc: 0.099000\n",
      "(Iteration 7701 / 76500) loss: 2.304657\n",
      "(Iteration 7801 / 76500) loss: 2.304659\n",
      "(Iteration 7901 / 76500) loss: 2.304652\n",
      "(Iteration 8001 / 76500) loss: 2.304649\n",
      "(Iteration 8101 / 76500) loss: 2.304661\n",
      "(Iteration 8201 / 76500) loss: 2.304651\n",
      "(Iteration 8301 / 76500) loss: 2.304660\n",
      "(Iteration 8401 / 76500) loss: 2.304658\n",
      "(Epoch 11 / 100) train acc: 0.110000; val_acc: 0.099000\n",
      "(Iteration 8501 / 76500) loss: 2.304648\n",
      "(Iteration 8601 / 76500) loss: 2.304645\n",
      "(Iteration 8701 / 76500) loss: 2.304659\n",
      "(Iteration 8801 / 76500) loss: 2.304647\n",
      "(Iteration 8901 / 76500) loss: 2.304657\n",
      "(Iteration 9001 / 76500) loss: 2.304654\n",
      "(Iteration 9101 / 76500) loss: 2.304656\n",
      "(Epoch 12 / 100) train acc: 0.105000; val_acc: 0.099000\n",
      "(Iteration 9201 / 76500) loss: 2.304656\n",
      "(Iteration 9301 / 76500) loss: 2.304669\n",
      "(Iteration 9401 / 76500) loss: 2.304656\n",
      "(Iteration 9501 / 76500) loss: 2.304651\n",
      "(Iteration 9601 / 76500) loss: 2.304647\n",
      "(Iteration 9701 / 76500) loss: 2.304650\n",
      "(Iteration 9801 / 76500) loss: 2.304667\n",
      "(Iteration 9901 / 76500) loss: 2.304651\n",
      "(Epoch 13 / 100) train acc: 0.099000; val_acc: 0.099000\n",
      "(Iteration 10001 / 76500) loss: 2.304646\n",
      "(Iteration 10101 / 76500) loss: 2.304649\n",
      "(Iteration 10201 / 76500) loss: 2.304659\n",
      "(Iteration 10301 / 76500) loss: 2.304657\n",
      "(Iteration 10401 / 76500) loss: 2.304655\n",
      "(Iteration 10501 / 76500) loss: 2.304653\n",
      "(Iteration 10601 / 76500) loss: 2.304643\n",
      "(Iteration 10701 / 76500) loss: 2.304653\n",
      "(Epoch 14 / 100) train acc: 0.107000; val_acc: 0.099000\n",
      "(Iteration 10801 / 76500) loss: 2.304658\n",
      "(Iteration 10901 / 76500) loss: 2.304644\n",
      "(Iteration 11001 / 76500) loss: 2.304653\n",
      "(Iteration 11101 / 76500) loss: 2.304661\n",
      "(Iteration 11201 / 76500) loss: 2.304657\n",
      "(Iteration 11301 / 76500) loss: 2.304660\n",
      "(Iteration 11401 / 76500) loss: 2.304646\n",
      "(Epoch 15 / 100) train acc: 0.094000; val_acc: 0.099000\n",
      "(Iteration 11501 / 76500) loss: 2.304665\n",
      "(Iteration 11601 / 76500) loss: 2.304650\n",
      "(Iteration 11701 / 76500) loss: 2.304654\n",
      "(Iteration 11801 / 76500) loss: 2.304656\n",
      "(Iteration 11901 / 76500) loss: 2.304646\n",
      "(Iteration 12001 / 76500) loss: 2.304661\n",
      "(Iteration 12101 / 76500) loss: 2.304657\n",
      "(Iteration 12201 / 76500) loss: 2.304643\n",
      "(Epoch 16 / 100) train acc: 0.086000; val_acc: 0.099000\n",
      "(Iteration 12301 / 76500) loss: 2.304653\n",
      "(Iteration 12401 / 76500) loss: 2.304641\n",
      "(Iteration 12501 / 76500) loss: 2.304657\n",
      "(Iteration 12601 / 76500) loss: 2.304656\n",
      "(Iteration 12701 / 76500) loss: 2.304644\n",
      "(Iteration 12801 / 76500) loss: 2.304644\n",
      "(Iteration 12901 / 76500) loss: 2.304662\n",
      "(Iteration 13001 / 76500) loss: 2.304645\n",
      "(Epoch 17 / 100) train acc: 0.086000; val_acc: 0.099000\n",
      "(Iteration 13101 / 76500) loss: 2.304670\n",
      "(Iteration 13201 / 76500) loss: 2.304658\n",
      "(Iteration 13301 / 76500) loss: 2.304648\n",
      "(Iteration 13401 / 76500) loss: 2.304647\n",
      "(Iteration 13501 / 76500) loss: 2.304655\n",
      "(Iteration 13601 / 76500) loss: 2.304661\n",
      "(Iteration 13701 / 76500) loss: 2.304649\n",
      "(Epoch 18 / 100) train acc: 0.090000; val_acc: 0.099000\n",
      "(Iteration 13801 / 76500) loss: 2.304643\n",
      "(Iteration 13901 / 76500) loss: 2.304662\n",
      "(Iteration 14001 / 76500) loss: 2.304656\n",
      "(Iteration 14101 / 76500) loss: 2.304658\n",
      "(Iteration 14201 / 76500) loss: 2.304653\n",
      "(Iteration 14301 / 76500) loss: 2.304654\n",
      "(Iteration 14401 / 76500) loss: 2.304648\n",
      "(Iteration 14501 / 76500) loss: 2.304661\n",
      "(Epoch 19 / 100) train acc: 0.106000; val_acc: 0.099000\n",
      "(Iteration 14601 / 76500) loss: 2.304650\n",
      "(Iteration 14701 / 76500) loss: 2.304651\n",
      "(Iteration 14801 / 76500) loss: 2.304644\n",
      "(Iteration 14901 / 76500) loss: 2.304666\n",
      "(Iteration 15001 / 76500) loss: 2.304660\n",
      "(Iteration 15101 / 76500) loss: 2.304668\n",
      "(Iteration 15201 / 76500) loss: 2.304647\n",
      "(Epoch 20 / 100) train acc: 0.086000; val_acc: 0.099000\n",
      "(Iteration 15301 / 76500) loss: 2.304651\n",
      "(Iteration 15401 / 76500) loss: 2.304659\n",
      "(Iteration 15501 / 76500) loss: 2.304654\n",
      "(Iteration 15601 / 76500) loss: 2.304653\n",
      "(Iteration 15701 / 76500) loss: 2.304667\n",
      "(Iteration 15801 / 76500) loss: 2.304643\n",
      "(Iteration 15901 / 76500) loss: 2.304643\n",
      "(Iteration 16001 / 76500) loss: 2.304651\n",
      "(Epoch 21 / 100) train acc: 0.087000; val_acc: 0.099000\n",
      "(Iteration 16101 / 76500) loss: 2.304660\n",
      "(Iteration 16201 / 76500) loss: 2.304651\n",
      "(Iteration 16301 / 76500) loss: 2.304656\n",
      "(Iteration 16401 / 76500) loss: 2.304661\n",
      "(Iteration 16501 / 76500) loss: 2.304656\n",
      "(Iteration 16601 / 76500) loss: 2.304650\n",
      "(Iteration 16701 / 76500) loss: 2.304658\n",
      "(Iteration 16801 / 76500) loss: 2.304649\n",
      "(Epoch 22 / 100) train acc: 0.097000; val_acc: 0.099000\n",
      "(Iteration 16901 / 76500) loss: 2.304657\n",
      "(Iteration 17001 / 76500) loss: 2.304650\n",
      "(Iteration 17101 / 76500) loss: 2.304656\n",
      "(Iteration 17201 / 76500) loss: 2.304657\n",
      "(Iteration 17301 / 76500) loss: 2.304650\n",
      "(Iteration 17401 / 76500) loss: 2.304661\n",
      "(Iteration 17501 / 76500) loss: 2.304656\n",
      "(Epoch 23 / 100) train acc: 0.095000; val_acc: 0.099000\n",
      "(Iteration 17601 / 76500) loss: 2.304644\n",
      "(Iteration 17701 / 76500) loss: 2.304641\n",
      "(Iteration 17801 / 76500) loss: 2.304659\n",
      "(Iteration 17901 / 76500) loss: 2.304658\n",
      "(Iteration 18001 / 76500) loss: 2.304648\n",
      "(Iteration 18101 / 76500) loss: 2.304649\n",
      "(Iteration 18201 / 76500) loss: 2.304650\n",
      "(Iteration 18301 / 76500) loss: 2.304645\n",
      "(Epoch 24 / 100) train acc: 0.090000; val_acc: 0.099000\n",
      "(Iteration 18401 / 76500) loss: 2.304654\n",
      "(Iteration 18501 / 76500) loss: 2.304652\n",
      "(Iteration 18601 / 76500) loss: 2.304656\n",
      "(Iteration 18701 / 76500) loss: 2.304662\n",
      "(Iteration 18801 / 76500) loss: 2.304664\n",
      "(Iteration 18901 / 76500) loss: 2.304649\n",
      "(Iteration 19001 / 76500) loss: 2.304660\n",
      "(Iteration 19101 / 76500) loss: 2.304652\n",
      "(Epoch 25 / 100) train acc: 0.077000; val_acc: 0.099000\n",
      "(Iteration 19201 / 76500) loss: 2.304652\n",
      "(Iteration 19301 / 76500) loss: 2.304653\n",
      "(Iteration 19401 / 76500) loss: 2.304660\n",
      "(Iteration 19501 / 76500) loss: 2.304654\n",
      "(Iteration 19601 / 76500) loss: 2.304649\n",
      "(Iteration 19701 / 76500) loss: 2.304641\n",
      "(Iteration 19801 / 76500) loss: 2.304656\n",
      "(Epoch 26 / 100) train acc: 0.104000; val_acc: 0.099000\n",
      "(Iteration 19901 / 76500) loss: 2.304663\n",
      "(Iteration 20001 / 76500) loss: 2.304669\n",
      "(Iteration 20101 / 76500) loss: 2.304646\n",
      "(Iteration 20201 / 76500) loss: 2.304654\n",
      "(Iteration 20301 / 76500) loss: 2.304645\n",
      "(Iteration 20401 / 76500) loss: 2.304660\n",
      "(Iteration 20501 / 76500) loss: 2.304644\n",
      "(Iteration 20601 / 76500) loss: 2.304650\n",
      "(Epoch 27 / 100) train acc: 0.098000; val_acc: 0.099000\n",
      "(Iteration 20701 / 76500) loss: 2.304656\n",
      "(Iteration 20801 / 76500) loss: 2.304655\n",
      "(Iteration 20901 / 76500) loss: 2.304653\n",
      "(Iteration 21001 / 76500) loss: 2.304661\n",
      "(Iteration 21101 / 76500) loss: 2.304653\n",
      "(Iteration 21201 / 76500) loss: 2.304658\n",
      "(Iteration 21301 / 76500) loss: 2.304666\n",
      "(Iteration 21401 / 76500) loss: 2.304647\n",
      "(Epoch 28 / 100) train acc: 0.087000; val_acc: 0.099000\n",
      "(Iteration 21501 / 76500) loss: 2.304652\n",
      "(Iteration 21601 / 76500) loss: 2.304650\n",
      "(Iteration 21701 / 76500) loss: 2.304655\n",
      "(Iteration 21801 / 76500) loss: 2.304664\n",
      "(Iteration 21901 / 76500) loss: 2.304658\n",
      "(Iteration 22001 / 76500) loss: 2.304652\n",
      "(Iteration 22101 / 76500) loss: 2.304661\n",
      "(Epoch 29 / 100) train acc: 0.097000; val_acc: 0.099000\n",
      "(Iteration 22201 / 76500) loss: 2.304654\n",
      "(Iteration 22301 / 76500) loss: 2.304663\n",
      "(Iteration 22401 / 76500) loss: 2.304651\n",
      "(Iteration 22501 / 76500) loss: 2.304651\n",
      "(Iteration 22601 / 76500) loss: 2.304653\n",
      "(Iteration 22701 / 76500) loss: 2.304654\n",
      "(Iteration 22801 / 76500) loss: 2.304657\n",
      "(Iteration 22901 / 76500) loss: 2.304650\n",
      "(Epoch 30 / 100) train acc: 0.093000; val_acc: 0.099000\n",
      "(Iteration 23001 / 76500) loss: 2.304663\n",
      "(Iteration 23101 / 76500) loss: 2.304644\n",
      "(Iteration 23201 / 76500) loss: 2.304662\n",
      "(Iteration 23301 / 76500) loss: 2.304642\n",
      "(Iteration 23401 / 76500) loss: 2.304654\n",
      "(Iteration 23501 / 76500) loss: 2.304648\n",
      "(Iteration 23601 / 76500) loss: 2.304666\n",
      "(Iteration 23701 / 76500) loss: 2.304659\n",
      "(Epoch 31 / 100) train acc: 0.102000; val_acc: 0.099000\n",
      "(Iteration 23801 / 76500) loss: 2.304655\n",
      "(Iteration 23901 / 76500) loss: 2.304650\n",
      "(Iteration 24001 / 76500) loss: 2.304658\n",
      "(Iteration 24101 / 76500) loss: 2.304658\n",
      "(Iteration 24201 / 76500) loss: 2.304645\n",
      "(Iteration 24301 / 76500) loss: 2.304644\n",
      "(Iteration 24401 / 76500) loss: 2.304657\n",
      "(Epoch 32 / 100) train acc: 0.102000; val_acc: 0.099000\n",
      "(Iteration 24501 / 76500) loss: 2.304654\n",
      "(Iteration 24601 / 76500) loss: 2.304657\n",
      "(Iteration 24701 / 76500) loss: 2.304664\n",
      "(Iteration 24801 / 76500) loss: 2.304649\n",
      "(Iteration 24901 / 76500) loss: 2.304655\n",
      "(Iteration 25001 / 76500) loss: 2.304666\n",
      "(Iteration 25101 / 76500) loss: 2.304645\n",
      "(Iteration 25201 / 76500) loss: 2.304659\n",
      "(Epoch 33 / 100) train acc: 0.108000; val_acc: 0.099000\n",
      "(Iteration 25301 / 76500) loss: 2.304660\n",
      "(Iteration 25401 / 76500) loss: 2.304659\n",
      "(Iteration 25501 / 76500) loss: 2.304669\n",
      "(Iteration 25601 / 76500) loss: 2.304633\n",
      "(Iteration 25701 / 76500) loss: 2.304669\n",
      "(Iteration 25801 / 76500) loss: 2.304656\n",
      "(Iteration 25901 / 76500) loss: 2.304662\n",
      "(Iteration 26001 / 76500) loss: 2.304651\n",
      "(Epoch 34 / 100) train acc: 0.076000; val_acc: 0.099000\n",
      "(Iteration 26101 / 76500) loss: 2.304655\n",
      "(Iteration 26201 / 76500) loss: 2.304651\n",
      "(Iteration 26301 / 76500) loss: 2.304659\n",
      "(Iteration 26401 / 76500) loss: 2.304645\n",
      "(Iteration 26501 / 76500) loss: 2.304656\n",
      "(Iteration 26601 / 76500) loss: 2.304656\n",
      "(Iteration 26701 / 76500) loss: 2.304648\n",
      "(Epoch 35 / 100) train acc: 0.085000; val_acc: 0.099000\n",
      "(Iteration 26801 / 76500) loss: 2.304661\n",
      "(Iteration 26901 / 76500) loss: 2.304663\n",
      "(Iteration 27001 / 76500) loss: 2.304657\n",
      "(Iteration 27101 / 76500) loss: 2.304649\n",
      "(Iteration 27201 / 76500) loss: 2.304652\n",
      "(Iteration 27301 / 76500) loss: 2.304652\n",
      "(Iteration 27401 / 76500) loss: 2.304655\n",
      "(Iteration 27501 / 76500) loss: 2.304644\n",
      "(Epoch 36 / 100) train acc: 0.089000; val_acc: 0.099000\n",
      "(Iteration 27601 / 76500) loss: 2.304642\n",
      "(Iteration 27701 / 76500) loss: 2.304653\n",
      "(Iteration 27801 / 76500) loss: 2.304644\n",
      "(Iteration 27901 / 76500) loss: 2.304646\n",
      "(Iteration 28001 / 76500) loss: 2.304654\n",
      "(Iteration 28101 / 76500) loss: 2.304665\n",
      "(Iteration 28201 / 76500) loss: 2.304654\n",
      "(Iteration 28301 / 76500) loss: 2.304652\n",
      "(Epoch 37 / 100) train acc: 0.086000; val_acc: 0.099000\n",
      "(Iteration 28401 / 76500) loss: 2.304656\n",
      "(Iteration 28501 / 76500) loss: 2.304661\n",
      "(Iteration 28601 / 76500) loss: 2.304644\n",
      "(Iteration 28701 / 76500) loss: 2.304656\n",
      "(Iteration 28801 / 76500) loss: 2.304649\n",
      "(Iteration 28901 / 76500) loss: 2.304643\n",
      "(Iteration 29001 / 76500) loss: 2.304643\n",
      "(Epoch 38 / 100) train acc: 0.108000; val_acc: 0.099000\n",
      "(Iteration 29101 / 76500) loss: 2.304645\n",
      "(Iteration 29201 / 76500) loss: 2.304662\n",
      "(Iteration 29301 / 76500) loss: 2.304650\n",
      "(Iteration 29401 / 76500) loss: 2.304662\n",
      "(Iteration 29501 / 76500) loss: 2.304671\n",
      "(Iteration 29601 / 76500) loss: 2.304649\n",
      "(Iteration 29701 / 76500) loss: 2.304659\n",
      "(Iteration 29801 / 76500) loss: 2.304658\n",
      "(Epoch 39 / 100) train acc: 0.096000; val_acc: 0.099000\n",
      "(Iteration 29901 / 76500) loss: 2.304648\n",
      "(Iteration 30001 / 76500) loss: 2.304655\n",
      "(Iteration 30101 / 76500) loss: 2.304654\n",
      "(Iteration 30201 / 76500) loss: 2.304652\n",
      "(Iteration 30301 / 76500) loss: 2.304658\n",
      "(Iteration 30401 / 76500) loss: 2.304659\n",
      "(Iteration 30501 / 76500) loss: 2.304661\n",
      "(Epoch 40 / 100) train acc: 0.106000; val_acc: 0.099000\n",
      "(Iteration 30601 / 76500) loss: 2.304650\n",
      "(Iteration 30701 / 76500) loss: 2.304648\n",
      "(Iteration 30801 / 76500) loss: 2.304658\n",
      "(Iteration 30901 / 76500) loss: 2.304655\n",
      "(Iteration 31001 / 76500) loss: 2.304653\n",
      "(Iteration 31101 / 76500) loss: 2.304650\n",
      "(Iteration 31201 / 76500) loss: 2.304650\n",
      "(Iteration 31301 / 76500) loss: 2.304637\n",
      "(Epoch 41 / 100) train acc: 0.105000; val_acc: 0.099000\n",
      "(Iteration 31401 / 76500) loss: 2.304657\n",
      "(Iteration 31501 / 76500) loss: 2.304632\n",
      "(Iteration 31601 / 76500) loss: 2.304651\n",
      "(Iteration 31701 / 76500) loss: 2.304647\n",
      "(Iteration 31801 / 76500) loss: 2.304641\n",
      "(Iteration 31901 / 76500) loss: 2.304646\n",
      "(Iteration 32001 / 76500) loss: 2.304653\n",
      "(Iteration 32101 / 76500) loss: 2.304661\n",
      "(Epoch 42 / 100) train acc: 0.124000; val_acc: 0.099000\n",
      "(Iteration 32201 / 76500) loss: 2.304650\n",
      "(Iteration 32301 / 76500) loss: 2.304653\n",
      "(Iteration 32401 / 76500) loss: 2.304653\n",
      "(Iteration 32501 / 76500) loss: 2.304644\n",
      "(Iteration 32601 / 76500) loss: 2.304648\n",
      "(Iteration 32701 / 76500) loss: 2.304639\n",
      "(Iteration 32801 / 76500) loss: 2.304647\n",
      "(Epoch 43 / 100) train acc: 0.092000; val_acc: 0.099000\n",
      "(Iteration 32901 / 76500) loss: 2.304655\n",
      "(Iteration 33001 / 76500) loss: 2.304655\n",
      "(Iteration 33101 / 76500) loss: 2.304650\n",
      "(Iteration 33201 / 76500) loss: 2.304649\n",
      "(Iteration 33301 / 76500) loss: 2.304658\n",
      "(Iteration 33401 / 76500) loss: 2.304654\n",
      "(Iteration 33501 / 76500) loss: 2.304662\n",
      "(Iteration 33601 / 76500) loss: 2.304652\n",
      "(Epoch 44 / 100) train acc: 0.087000; val_acc: 0.099000\n",
      "(Iteration 33701 / 76500) loss: 2.304649\n",
      "(Iteration 33801 / 76500) loss: 2.304641\n",
      "(Iteration 33901 / 76500) loss: 2.304654\n",
      "(Iteration 34001 / 76500) loss: 2.304655\n",
      "(Iteration 34101 / 76500) loss: 2.304653\n",
      "(Iteration 34201 / 76500) loss: 2.304654\n",
      "(Iteration 34301 / 76500) loss: 2.304652\n",
      "(Iteration 34401 / 76500) loss: 2.304656\n",
      "(Epoch 45 / 100) train acc: 0.095000; val_acc: 0.099000\n",
      "(Iteration 34501 / 76500) loss: 2.304655\n",
      "(Iteration 34601 / 76500) loss: 2.304662\n",
      "(Iteration 34701 / 76500) loss: 2.304661\n",
      "(Iteration 34801 / 76500) loss: 2.304652\n",
      "(Iteration 34901 / 76500) loss: 2.304647\n",
      "(Iteration 35001 / 76500) loss: 2.304650\n",
      "(Iteration 35101 / 76500) loss: 2.304642\n",
      "(Epoch 46 / 100) train acc: 0.097000; val_acc: 0.099000\n",
      "(Iteration 35201 / 76500) loss: 2.304654\n",
      "(Iteration 35301 / 76500) loss: 2.304652\n",
      "(Iteration 35401 / 76500) loss: 2.304654\n",
      "(Iteration 35501 / 76500) loss: 2.304651\n",
      "(Iteration 35601 / 76500) loss: 2.304657\n",
      "(Iteration 35701 / 76500) loss: 2.304650\n",
      "(Iteration 35801 / 76500) loss: 2.304655\n",
      "(Iteration 35901 / 76500) loss: 2.304646\n",
      "(Epoch 47 / 100) train acc: 0.090000; val_acc: 0.099000\n",
      "(Iteration 36001 / 76500) loss: 2.304664\n",
      "(Iteration 36101 / 76500) loss: 2.304660\n",
      "(Iteration 36201 / 76500) loss: 2.304650\n",
      "(Iteration 36301 / 76500) loss: 2.304650\n",
      "(Iteration 36401 / 76500) loss: 2.304655\n",
      "(Iteration 36501 / 76500) loss: 2.304654\n",
      "(Iteration 36601 / 76500) loss: 2.304654\n",
      "(Iteration 36701 / 76500) loss: 2.304642\n",
      "(Epoch 48 / 100) train acc: 0.093000; val_acc: 0.099000\n",
      "(Iteration 36801 / 76500) loss: 2.304655\n",
      "(Iteration 36901 / 76500) loss: 2.304650\n",
      "(Iteration 37001 / 76500) loss: 2.304647\n",
      "(Iteration 37101 / 76500) loss: 2.304656\n",
      "(Iteration 37201 / 76500) loss: 2.304660\n",
      "(Iteration 37301 / 76500) loss: 2.304644\n",
      "(Iteration 37401 / 76500) loss: 2.304655\n",
      "(Epoch 49 / 100) train acc: 0.087000; val_acc: 0.099000\n",
      "(Iteration 37501 / 76500) loss: 2.304646\n",
      "(Iteration 37601 / 76500) loss: 2.304666\n",
      "(Iteration 37701 / 76500) loss: 2.304649\n",
      "(Iteration 37801 / 76500) loss: 2.304650\n",
      "(Iteration 37901 / 76500) loss: 2.304644\n",
      "(Iteration 38001 / 76500) loss: 2.304668\n",
      "(Iteration 38101 / 76500) loss: 2.304658\n",
      "(Iteration 38201 / 76500) loss: 2.304649\n",
      "(Epoch 50 / 100) train acc: 0.105000; val_acc: 0.099000\n",
      "(Iteration 38301 / 76500) loss: 2.304660\n",
      "(Iteration 38401 / 76500) loss: 2.304662\n",
      "(Iteration 38501 / 76500) loss: 2.304652\n",
      "(Iteration 38601 / 76500) loss: 2.304653\n",
      "(Iteration 38701 / 76500) loss: 2.304651\n",
      "(Iteration 38801 / 76500) loss: 2.304644\n",
      "(Iteration 38901 / 76500) loss: 2.304661\n",
      "(Iteration 39001 / 76500) loss: 2.304660\n",
      "(Epoch 51 / 100) train acc: 0.078000; val_acc: 0.099000\n",
      "(Iteration 39101 / 76500) loss: 2.304655\n",
      "(Iteration 39201 / 76500) loss: 2.304657\n",
      "(Iteration 39301 / 76500) loss: 2.304652\n",
      "(Iteration 39401 / 76500) loss: 2.304646\n",
      "(Iteration 39501 / 76500) loss: 2.304656\n",
      "(Iteration 39601 / 76500) loss: 2.304656\n",
      "(Iteration 39701 / 76500) loss: 2.304654\n",
      "(Epoch 52 / 100) train acc: 0.101000; val_acc: 0.099000\n",
      "(Iteration 39801 / 76500) loss: 2.304651\n",
      "(Iteration 39901 / 76500) loss: 2.304652\n",
      "(Iteration 40001 / 76500) loss: 2.304652\n",
      "(Iteration 40101 / 76500) loss: 2.304651\n",
      "(Iteration 40201 / 76500) loss: 2.304645\n",
      "(Iteration 40301 / 76500) loss: 2.304666\n",
      "(Iteration 40401 / 76500) loss: 2.304656\n",
      "(Iteration 40501 / 76500) loss: 2.304647\n",
      "(Epoch 53 / 100) train acc: 0.104000; val_acc: 0.099000\n",
      "(Iteration 40601 / 76500) loss: 2.304659\n",
      "(Iteration 40701 / 76500) loss: 2.304655\n",
      "(Iteration 40801 / 76500) loss: 2.304664\n",
      "(Iteration 40901 / 76500) loss: 2.304651\n",
      "(Iteration 41001 / 76500) loss: 2.304656\n",
      "(Iteration 41101 / 76500) loss: 2.304656\n",
      "(Iteration 41201 / 76500) loss: 2.304658\n",
      "(Iteration 41301 / 76500) loss: 2.304656\n",
      "(Epoch 54 / 100) train acc: 0.125000; val_acc: 0.099000\n",
      "(Iteration 41401 / 76500) loss: 2.304658\n",
      "(Iteration 41501 / 76500) loss: 2.304655\n",
      "(Iteration 41601 / 76500) loss: 2.304650\n",
      "(Iteration 41701 / 76500) loss: 2.304657\n",
      "(Iteration 41801 / 76500) loss: 2.304650\n",
      "(Iteration 41901 / 76500) loss: 2.304658\n",
      "(Iteration 42001 / 76500) loss: 2.304651\n",
      "(Epoch 55 / 100) train acc: 0.085000; val_acc: 0.099000\n",
      "(Iteration 42101 / 76500) loss: 2.304651\n",
      "(Iteration 42201 / 76500) loss: 2.304645\n",
      "(Iteration 42301 / 76500) loss: 2.304667\n",
      "(Iteration 42401 / 76500) loss: 2.304653\n",
      "(Iteration 42501 / 76500) loss: 2.304644\n",
      "(Iteration 42601 / 76500) loss: 2.304661\n",
      "(Iteration 42701 / 76500) loss: 2.304651\n",
      "(Iteration 42801 / 76500) loss: 2.304657\n",
      "(Epoch 56 / 100) train acc: 0.090000; val_acc: 0.099000\n",
      "(Iteration 42901 / 76500) loss: 2.304659\n",
      "(Iteration 43001 / 76500) loss: 2.304646\n",
      "(Iteration 43101 / 76500) loss: 2.304649\n",
      "(Iteration 43201 / 76500) loss: 2.304660\n",
      "(Iteration 43301 / 76500) loss: 2.304661\n",
      "(Iteration 43401 / 76500) loss: 2.304649\n",
      "(Iteration 43501 / 76500) loss: 2.304652\n",
      "(Iteration 43601 / 76500) loss: 2.304663\n",
      "(Epoch 57 / 100) train acc: 0.091000; val_acc: 0.099000\n",
      "(Iteration 43701 / 76500) loss: 2.304662\n",
      "(Iteration 43801 / 76500) loss: 2.304654\n",
      "(Iteration 43901 / 76500) loss: 2.304657\n",
      "(Iteration 44001 / 76500) loss: 2.304650\n",
      "(Iteration 44101 / 76500) loss: 2.304658\n",
      "(Iteration 44201 / 76500) loss: 2.304670\n",
      "(Iteration 44301 / 76500) loss: 2.304646\n",
      "(Epoch 58 / 100) train acc: 0.108000; val_acc: 0.099000\n",
      "(Iteration 44401 / 76500) loss: 2.304665\n",
      "(Iteration 44501 / 76500) loss: 2.304655\n",
      "(Iteration 44601 / 76500) loss: 2.304660\n",
      "(Iteration 44701 / 76500) loss: 2.304649\n",
      "(Iteration 44801 / 76500) loss: 2.304648\n",
      "(Iteration 44901 / 76500) loss: 2.304655\n",
      "(Iteration 45001 / 76500) loss: 2.304649\n",
      "(Iteration 45101 / 76500) loss: 2.304657\n",
      "(Epoch 59 / 100) train acc: 0.102000; val_acc: 0.099000\n",
      "(Iteration 45201 / 76500) loss: 2.304672\n",
      "(Iteration 45301 / 76500) loss: 2.304661\n",
      "(Iteration 45401 / 76500) loss: 2.304657\n",
      "(Iteration 45501 / 76500) loss: 2.304641\n",
      "(Iteration 45601 / 76500) loss: 2.304657\n",
      "(Iteration 45701 / 76500) loss: 2.304654\n",
      "(Iteration 45801 / 76500) loss: 2.304649\n",
      "(Epoch 60 / 100) train acc: 0.102000; val_acc: 0.099000\n",
      "(Iteration 45901 / 76500) loss: 2.304655\n",
      "(Iteration 46001 / 76500) loss: 2.304646\n",
      "(Iteration 46101 / 76500) loss: 2.304657\n",
      "(Iteration 46201 / 76500) loss: 2.304649\n",
      "(Iteration 46301 / 76500) loss: 2.304654\n",
      "(Iteration 46401 / 76500) loss: 2.304664\n",
      "(Iteration 46501 / 76500) loss: 2.304658\n",
      "(Iteration 46601 / 76500) loss: 2.304653\n",
      "(Epoch 61 / 100) train acc: 0.085000; val_acc: 0.099000\n",
      "(Iteration 46701 / 76500) loss: 2.304661\n",
      "(Iteration 46801 / 76500) loss: 2.304640\n",
      "(Iteration 46901 / 76500) loss: 2.304643\n",
      "(Iteration 47001 / 76500) loss: 2.304650\n",
      "(Iteration 47101 / 76500) loss: 2.304650\n",
      "(Iteration 47201 / 76500) loss: 2.304655\n",
      "(Iteration 47301 / 76500) loss: 2.304653\n",
      "(Iteration 47401 / 76500) loss: 2.304657\n",
      "(Epoch 62 / 100) train acc: 0.096000; val_acc: 0.099000\n",
      "(Iteration 47501 / 76500) loss: 2.304653\n",
      "(Iteration 47601 / 76500) loss: 2.304651\n",
      "(Iteration 47701 / 76500) loss: 2.304656\n",
      "(Iteration 47801 / 76500) loss: 2.304654\n",
      "(Iteration 47901 / 76500) loss: 2.304653\n",
      "(Iteration 48001 / 76500) loss: 2.304660\n",
      "(Iteration 48101 / 76500) loss: 2.304662\n",
      "(Epoch 63 / 100) train acc: 0.098000; val_acc: 0.099000\n",
      "(Iteration 48201 / 76500) loss: 2.304656\n",
      "(Iteration 48301 / 76500) loss: 2.304645\n",
      "(Iteration 48401 / 76500) loss: 2.304661\n",
      "(Iteration 48501 / 76500) loss: 2.304656\n",
      "(Iteration 48601 / 76500) loss: 2.304667\n",
      "(Iteration 48701 / 76500) loss: 2.304647\n",
      "(Iteration 48801 / 76500) loss: 2.304657\n",
      "(Iteration 48901 / 76500) loss: 2.304647\n",
      "(Epoch 64 / 100) train acc: 0.071000; val_acc: 0.099000\n",
      "(Iteration 49001 / 76500) loss: 2.304646\n",
      "(Iteration 49101 / 76500) loss: 2.304654\n",
      "(Iteration 49201 / 76500) loss: 2.304644\n",
      "(Iteration 49301 / 76500) loss: 2.304655\n",
      "(Iteration 49401 / 76500) loss: 2.304650\n",
      "(Iteration 49501 / 76500) loss: 2.304665\n",
      "(Iteration 49601 / 76500) loss: 2.304653\n",
      "(Iteration 49701 / 76500) loss: 2.304652\n",
      "(Epoch 65 / 100) train acc: 0.100000; val_acc: 0.099000\n",
      "(Iteration 49801 / 76500) loss: 2.304649\n",
      "(Iteration 49901 / 76500) loss: 2.304658\n",
      "(Iteration 50001 / 76500) loss: 2.304660\n",
      "(Iteration 50101 / 76500) loss: 2.304645\n",
      "(Iteration 50201 / 76500) loss: 2.304651\n",
      "(Iteration 50301 / 76500) loss: 2.304662\n",
      "(Iteration 50401 / 76500) loss: 2.304665\n",
      "(Epoch 66 / 100) train acc: 0.096000; val_acc: 0.099000\n",
      "(Iteration 50501 / 76500) loss: 2.304657\n",
      "(Iteration 50601 / 76500) loss: 2.304653\n",
      "(Iteration 50701 / 76500) loss: 2.304661\n",
      "(Iteration 50801 / 76500) loss: 2.304659\n",
      "(Iteration 50901 / 76500) loss: 2.304656\n",
      "(Iteration 51001 / 76500) loss: 2.304641\n",
      "(Iteration 51101 / 76500) loss: 2.304655\n",
      "(Iteration 51201 / 76500) loss: 2.304669\n",
      "(Epoch 67 / 100) train acc: 0.105000; val_acc: 0.099000\n",
      "(Iteration 51301 / 76500) loss: 2.304655\n",
      "(Iteration 51401 / 76500) loss: 2.304652\n",
      "(Iteration 51501 / 76500) loss: 2.304654\n",
      "(Iteration 51601 / 76500) loss: 2.304656\n",
      "(Iteration 51701 / 76500) loss: 2.304667\n",
      "(Iteration 51801 / 76500) loss: 2.304650\n",
      "(Iteration 51901 / 76500) loss: 2.304677\n",
      "(Iteration 52001 / 76500) loss: 2.304651\n",
      "(Epoch 68 / 100) train acc: 0.100000; val_acc: 0.099000\n",
      "(Iteration 52101 / 76500) loss: 2.304644\n",
      "(Iteration 52201 / 76500) loss: 2.304659\n",
      "(Iteration 52301 / 76500) loss: 2.304659\n",
      "(Iteration 52401 / 76500) loss: 2.304655\n",
      "(Iteration 52501 / 76500) loss: 2.304646\n",
      "(Iteration 52601 / 76500) loss: 2.304653\n",
      "(Iteration 52701 / 76500) loss: 2.304654\n",
      "(Epoch 69 / 100) train acc: 0.090000; val_acc: 0.099000\n",
      "(Iteration 52801 / 76500) loss: 2.304652\n",
      "(Iteration 52901 / 76500) loss: 2.304656\n",
      "(Iteration 53001 / 76500) loss: 2.304658\n",
      "(Iteration 53101 / 76500) loss: 2.304662\n",
      "(Iteration 53201 / 76500) loss: 2.304660\n",
      "(Iteration 53301 / 76500) loss: 2.304651\n",
      "(Iteration 53401 / 76500) loss: 2.304640\n",
      "(Iteration 53501 / 76500) loss: 2.304655\n",
      "(Epoch 70 / 100) train acc: 0.076000; val_acc: 0.099000\n",
      "(Iteration 53601 / 76500) loss: 2.304650\n",
      "(Iteration 53701 / 76500) loss: 2.304658\n",
      "(Iteration 53801 / 76500) loss: 2.304645\n",
      "(Iteration 53901 / 76500) loss: 2.304654\n",
      "(Iteration 54001 / 76500) loss: 2.304651\n",
      "(Iteration 54101 / 76500) loss: 2.304654\n",
      "(Iteration 54201 / 76500) loss: 2.304656\n",
      "(Iteration 54301 / 76500) loss: 2.304655\n",
      "(Epoch 71 / 100) train acc: 0.098000; val_acc: 0.099000\n",
      "(Iteration 54401 / 76500) loss: 2.304651\n",
      "(Iteration 54501 / 76500) loss: 2.304657\n",
      "(Iteration 54601 / 76500) loss: 2.304654\n",
      "(Iteration 54701 / 76500) loss: 2.304663\n",
      "(Iteration 54801 / 76500) loss: 2.304659\n",
      "(Iteration 54901 / 76500) loss: 2.304660\n",
      "(Iteration 55001 / 76500) loss: 2.304662\n",
      "(Epoch 72 / 100) train acc: 0.087000; val_acc: 0.099000\n",
      "(Iteration 55101 / 76500) loss: 2.304652\n",
      "(Iteration 55201 / 76500) loss: 2.304663\n",
      "(Iteration 55301 / 76500) loss: 2.304654\n",
      "(Iteration 55401 / 76500) loss: 2.304657\n",
      "(Iteration 55501 / 76500) loss: 2.304657\n",
      "(Iteration 55601 / 76500) loss: 2.304646\n",
      "(Iteration 55701 / 76500) loss: 2.304657\n",
      "(Iteration 55801 / 76500) loss: 2.304658\n",
      "(Epoch 73 / 100) train acc: 0.078000; val_acc: 0.099000\n",
      "(Iteration 55901 / 76500) loss: 2.304647\n",
      "(Iteration 56001 / 76500) loss: 2.304646\n",
      "(Iteration 56101 / 76500) loss: 2.304650\n",
      "(Iteration 56201 / 76500) loss: 2.304653\n",
      "(Iteration 56301 / 76500) loss: 2.304656\n",
      "(Iteration 56401 / 76500) loss: 2.304665\n",
      "(Iteration 56501 / 76500) loss: 2.304655\n",
      "(Iteration 56601 / 76500) loss: 2.304650\n",
      "(Epoch 74 / 100) train acc: 0.107000; val_acc: 0.099000\n",
      "(Iteration 56701 / 76500) loss: 2.304662\n",
      "(Iteration 56801 / 76500) loss: 2.304657\n",
      "(Iteration 56901 / 76500) loss: 2.304654\n",
      "(Iteration 57001 / 76500) loss: 2.304654\n",
      "(Iteration 57101 / 76500) loss: 2.304643\n",
      "(Iteration 57201 / 76500) loss: 2.304659\n",
      "(Iteration 57301 / 76500) loss: 2.304648\n",
      "(Epoch 75 / 100) train acc: 0.112000; val_acc: 0.099000\n",
      "(Iteration 57401 / 76500) loss: 2.304656\n",
      "(Iteration 57501 / 76500) loss: 2.304662\n",
      "(Iteration 57601 / 76500) loss: 2.304657\n",
      "(Iteration 57701 / 76500) loss: 2.304653\n",
      "(Iteration 57801 / 76500) loss: 2.304663\n",
      "(Iteration 57901 / 76500) loss: 2.304656\n",
      "(Iteration 58001 / 76500) loss: 2.304651\n",
      "(Iteration 58101 / 76500) loss: 2.304654\n",
      "(Epoch 76 / 100) train acc: 0.094000; val_acc: 0.099000\n",
      "(Iteration 58201 / 76500) loss: 2.304641\n",
      "(Iteration 58301 / 76500) loss: 2.304663\n",
      "(Iteration 58401 / 76500) loss: 2.304642\n",
      "(Iteration 58501 / 76500) loss: 2.304639\n",
      "(Iteration 58601 / 76500) loss: 2.304645\n",
      "(Iteration 58701 / 76500) loss: 2.304654\n",
      "(Iteration 58801 / 76500) loss: 2.304655\n",
      "(Iteration 58901 / 76500) loss: 2.304656\n",
      "(Epoch 77 / 100) train acc: 0.104000; val_acc: 0.099000\n",
      "(Iteration 59001 / 76500) loss: 2.304662\n",
      "(Iteration 59101 / 76500) loss: 2.304655\n",
      "(Iteration 59201 / 76500) loss: 2.304636\n",
      "(Iteration 59301 / 76500) loss: 2.304646\n",
      "(Iteration 59401 / 76500) loss: 2.304655\n",
      "(Iteration 59501 / 76500) loss: 2.304651\n",
      "(Iteration 59601 / 76500) loss: 2.304663\n",
      "(Epoch 78 / 100) train acc: 0.089000; val_acc: 0.099000\n",
      "(Iteration 59701 / 76500) loss: 2.304646\n",
      "(Iteration 59801 / 76500) loss: 2.304650\n",
      "(Iteration 59901 / 76500) loss: 2.304659\n",
      "(Iteration 60001 / 76500) loss: 2.304648\n",
      "(Iteration 60101 / 76500) loss: 2.304661\n",
      "(Iteration 60201 / 76500) loss: 2.304641\n",
      "(Iteration 60301 / 76500) loss: 2.304649\n",
      "(Iteration 60401 / 76500) loss: 2.304658\n",
      "(Epoch 79 / 100) train acc: 0.095000; val_acc: 0.099000\n",
      "(Iteration 60501 / 76500) loss: 2.304658\n",
      "(Iteration 60601 / 76500) loss: 2.304649\n",
      "(Iteration 60701 / 76500) loss: 2.304654\n",
      "(Iteration 60801 / 76500) loss: 2.304669\n",
      "(Iteration 60901 / 76500) loss: 2.304650\n",
      "(Iteration 61001 / 76500) loss: 2.304655\n",
      "(Iteration 61101 / 76500) loss: 2.304659\n",
      "(Epoch 80 / 100) train acc: 0.116000; val_acc: 0.099000\n",
      "(Iteration 61201 / 76500) loss: 2.304644\n",
      "(Iteration 61301 / 76500) loss: 2.304658\n",
      "(Iteration 61401 / 76500) loss: 2.304666\n",
      "(Iteration 61501 / 76500) loss: 2.304661\n",
      "(Iteration 61601 / 76500) loss: 2.304643\n",
      "(Iteration 61701 / 76500) loss: 2.304655\n",
      "(Iteration 61801 / 76500) loss: 2.304661\n",
      "(Iteration 61901 / 76500) loss: 2.304657\n",
      "(Epoch 81 / 100) train acc: 0.081000; val_acc: 0.099000\n",
      "(Iteration 62001 / 76500) loss: 2.304654\n",
      "(Iteration 62101 / 76500) loss: 2.304657\n",
      "(Iteration 62201 / 76500) loss: 2.304644\n",
      "(Iteration 62301 / 76500) loss: 2.304660\n",
      "(Iteration 62401 / 76500) loss: 2.304654\n",
      "(Iteration 62501 / 76500) loss: 2.304665\n",
      "(Iteration 62601 / 76500) loss: 2.304659\n",
      "(Iteration 62701 / 76500) loss: 2.304666\n",
      "(Epoch 82 / 100) train acc: 0.093000; val_acc: 0.099000\n",
      "(Iteration 62801 / 76500) loss: 2.304672\n",
      "(Iteration 62901 / 76500) loss: 2.304664\n",
      "(Iteration 63001 / 76500) loss: 2.304646\n",
      "(Iteration 63101 / 76500) loss: 2.304648\n",
      "(Iteration 63201 / 76500) loss: 2.304654\n",
      "(Iteration 63301 / 76500) loss: 2.304664\n",
      "(Iteration 63401 / 76500) loss: 2.304663\n",
      "(Epoch 83 / 100) train acc: 0.098000; val_acc: 0.099000\n",
      "(Iteration 63501 / 76500) loss: 2.304660\n",
      "(Iteration 63601 / 76500) loss: 2.304669\n",
      "(Iteration 63701 / 76500) loss: 2.304651\n",
      "(Iteration 63801 / 76500) loss: 2.304658\n",
      "(Iteration 63901 / 76500) loss: 2.304662\n",
      "(Iteration 64001 / 76500) loss: 2.304649\n",
      "(Iteration 64101 / 76500) loss: 2.304659\n",
      "(Iteration 64201 / 76500) loss: 2.304655\n",
      "(Epoch 84 / 100) train acc: 0.087000; val_acc: 0.099000\n",
      "(Iteration 64301 / 76500) loss: 2.304658\n",
      "(Iteration 64401 / 76500) loss: 2.304653\n",
      "(Iteration 64501 / 76500) loss: 2.304651\n",
      "(Iteration 64601 / 76500) loss: 2.304654\n",
      "(Iteration 64701 / 76500) loss: 2.304663\n",
      "(Iteration 64801 / 76500) loss: 2.304646\n",
      "(Iteration 64901 / 76500) loss: 2.304653\n",
      "(Iteration 65001 / 76500) loss: 2.304657\n",
      "(Epoch 85 / 100) train acc: 0.093000; val_acc: 0.099000\n",
      "(Iteration 65101 / 76500) loss: 2.304655\n",
      "(Iteration 65201 / 76500) loss: 2.304648\n",
      "(Iteration 65301 / 76500) loss: 2.304660\n",
      "(Iteration 65401 / 76500) loss: 2.304657\n",
      "(Iteration 65501 / 76500) loss: 2.304669\n",
      "(Iteration 65601 / 76500) loss: 2.304644\n",
      "(Iteration 65701 / 76500) loss: 2.304661\n",
      "(Epoch 86 / 100) train acc: 0.106000; val_acc: 0.099000\n",
      "(Iteration 65801 / 76500) loss: 2.304649\n",
      "(Iteration 65901 / 76500) loss: 2.304658\n",
      "(Iteration 66001 / 76500) loss: 2.304652\n",
      "(Iteration 66101 / 76500) loss: 2.304653\n",
      "(Iteration 66201 / 76500) loss: 2.304651\n",
      "(Iteration 66301 / 76500) loss: 2.304658\n",
      "(Iteration 66401 / 76500) loss: 2.304665\n",
      "(Iteration 66501 / 76500) loss: 2.304659\n",
      "(Epoch 87 / 100) train acc: 0.105000; val_acc: 0.099000\n",
      "(Iteration 66601 / 76500) loss: 2.304650\n",
      "(Iteration 66701 / 76500) loss: 2.304655\n",
      "(Iteration 66801 / 76500) loss: 2.304661\n",
      "(Iteration 66901 / 76500) loss: 2.304647\n",
      "(Iteration 67001 / 76500) loss: 2.304649\n",
      "(Iteration 67101 / 76500) loss: 2.304647\n",
      "(Iteration 67201 / 76500) loss: 2.304648\n",
      "(Iteration 67301 / 76500) loss: 2.304664\n",
      "(Epoch 88 / 100) train acc: 0.093000; val_acc: 0.099000\n",
      "(Iteration 67401 / 76500) loss: 2.304639\n",
      "(Iteration 67501 / 76500) loss: 2.304664\n",
      "(Iteration 67601 / 76500) loss: 2.304648\n",
      "(Iteration 67701 / 76500) loss: 2.304663\n",
      "(Iteration 67801 / 76500) loss: 2.304658\n",
      "(Iteration 67901 / 76500) loss: 2.304656\n",
      "(Iteration 68001 / 76500) loss: 2.304656\n",
      "(Epoch 89 / 100) train acc: 0.084000; val_acc: 0.099000\n",
      "(Iteration 68101 / 76500) loss: 2.304658\n",
      "(Iteration 68201 / 76500) loss: 2.304650\n",
      "(Iteration 68301 / 76500) loss: 2.304660\n",
      "(Iteration 68401 / 76500) loss: 2.304648\n",
      "(Iteration 68501 / 76500) loss: 2.304659\n",
      "(Iteration 68601 / 76500) loss: 2.304655\n",
      "(Iteration 68701 / 76500) loss: 2.304652\n",
      "(Iteration 68801 / 76500) loss: 2.304662\n",
      "(Epoch 90 / 100) train acc: 0.107000; val_acc: 0.099000\n",
      "(Iteration 68901 / 76500) loss: 2.304653\n",
      "(Iteration 69001 / 76500) loss: 2.304653\n",
      "(Iteration 69101 / 76500) loss: 2.304648\n",
      "(Iteration 69201 / 76500) loss: 2.304654\n",
      "(Iteration 69301 / 76500) loss: 2.304655\n",
      "(Iteration 69401 / 76500) loss: 2.304651\n",
      "(Iteration 69501 / 76500) loss: 2.304656\n",
      "(Iteration 69601 / 76500) loss: 2.304658\n",
      "(Epoch 91 / 100) train acc: 0.090000; val_acc: 0.099000\n",
      "(Iteration 69701 / 76500) loss: 2.304654\n",
      "(Iteration 69801 / 76500) loss: 2.304671\n",
      "(Iteration 69901 / 76500) loss: 2.304657\n",
      "(Iteration 70001 / 76500) loss: 2.304645\n",
      "(Iteration 70101 / 76500) loss: 2.304654\n",
      "(Iteration 70201 / 76500) loss: 2.304641\n",
      "(Iteration 70301 / 76500) loss: 2.304658\n",
      "(Epoch 92 / 100) train acc: 0.105000; val_acc: 0.099000\n",
      "(Iteration 70401 / 76500) loss: 2.304651\n",
      "(Iteration 70501 / 76500) loss: 2.304652\n",
      "(Iteration 70601 / 76500) loss: 2.304657\n",
      "(Iteration 70701 / 76500) loss: 2.304650\n",
      "(Iteration 70801 / 76500) loss: 2.304646\n",
      "(Iteration 70901 / 76500) loss: 2.304650\n",
      "(Iteration 71001 / 76500) loss: 2.304654\n",
      "(Iteration 71101 / 76500) loss: 2.304649\n",
      "(Epoch 93 / 100) train acc: 0.087000; val_acc: 0.099000\n",
      "(Iteration 71201 / 76500) loss: 2.304670\n",
      "(Iteration 71301 / 76500) loss: 2.304655\n",
      "(Iteration 71401 / 76500) loss: 2.304657\n",
      "(Iteration 71501 / 76500) loss: 2.304670\n",
      "(Iteration 71601 / 76500) loss: 2.304652\n",
      "(Iteration 71701 / 76500) loss: 2.304653\n",
      "(Iteration 71801 / 76500) loss: 2.304656\n",
      "(Iteration 71901 / 76500) loss: 2.304647\n",
      "(Epoch 94 / 100) train acc: 0.085000; val_acc: 0.099000\n",
      "(Iteration 72001 / 76500) loss: 2.304658\n",
      "(Iteration 72101 / 76500) loss: 2.304662\n",
      "(Iteration 72201 / 76500) loss: 2.304640\n",
      "(Iteration 72301 / 76500) loss: 2.304660\n",
      "(Iteration 72401 / 76500) loss: 2.304652\n",
      "(Iteration 72501 / 76500) loss: 2.304652\n",
      "(Iteration 72601 / 76500) loss: 2.304649\n",
      "(Epoch 95 / 100) train acc: 0.096000; val_acc: 0.099000\n",
      "(Iteration 72701 / 76500) loss: 2.304652\n",
      "(Iteration 72801 / 76500) loss: 2.304653\n",
      "(Iteration 72901 / 76500) loss: 2.304660\n",
      "(Iteration 73001 / 76500) loss: 2.304655\n",
      "(Iteration 73101 / 76500) loss: 2.304650\n",
      "(Iteration 73201 / 76500) loss: 2.304649\n",
      "(Iteration 73301 / 76500) loss: 2.304656\n",
      "(Iteration 73401 / 76500) loss: 2.304665\n",
      "(Epoch 96 / 100) train acc: 0.104000; val_acc: 0.099000\n",
      "(Iteration 73501 / 76500) loss: 2.304654\n",
      "(Iteration 73601 / 76500) loss: 2.304644\n",
      "(Iteration 73701 / 76500) loss: 2.304670\n",
      "(Iteration 73801 / 76500) loss: 2.304642\n",
      "(Iteration 73901 / 76500) loss: 2.304661\n",
      "(Iteration 74001 / 76500) loss: 2.304652\n",
      "(Iteration 74101 / 76500) loss: 2.304654\n",
      "(Iteration 74201 / 76500) loss: 2.304656\n",
      "(Epoch 97 / 100) train acc: 0.101000; val_acc: 0.099000\n",
      "(Iteration 74301 / 76500) loss: 2.304649\n",
      "(Iteration 74401 / 76500) loss: 2.304661\n",
      "(Iteration 74501 / 76500) loss: 2.304659\n",
      "(Iteration 74601 / 76500) loss: 2.304659\n",
      "(Iteration 74701 / 76500) loss: 2.304655\n",
      "(Iteration 74801 / 76500) loss: 2.304662\n",
      "(Iteration 74901 / 76500) loss: 2.304660\n",
      "(Epoch 98 / 100) train acc: 0.087000; val_acc: 0.099000\n",
      "(Iteration 75001 / 76500) loss: 2.304658\n",
      "(Iteration 75101 / 76500) loss: 2.304654\n",
      "(Iteration 75201 / 76500) loss: 2.304654\n",
      "(Iteration 75301 / 76500) loss: 2.304651\n",
      "(Iteration 75401 / 76500) loss: 2.304653\n",
      "(Iteration 75501 / 76500) loss: 2.304646\n",
      "(Iteration 75601 / 76500) loss: 2.304659\n",
      "(Iteration 75701 / 76500) loss: 2.304653\n",
      "(Epoch 99 / 100) train acc: 0.091000; val_acc: 0.099000\n",
      "(Iteration 75801 / 76500) loss: 2.304657\n",
      "(Iteration 75901 / 76500) loss: 2.304655\n",
      "(Iteration 76001 / 76500) loss: 2.304654\n",
      "(Iteration 76101 / 76500) loss: 2.304662\n",
      "(Iteration 76201 / 76500) loss: 2.304667\n",
      "(Iteration 76301 / 76500) loss: 2.304662\n",
      "(Iteration 76401 / 76500) loss: 2.304646\n",
      "(Epoch 100 / 100) train acc: 0.097000; val_acc: 0.099000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.304630\n",
      "(Epoch 0 / 100) train acc: 0.086000; val_acc: 0.102000\n",
      "(Iteration 101 / 38200) loss: 2.304637\n",
      "(Iteration 201 / 38200) loss: 2.304639\n",
      "(Iteration 301 / 38200) loss: 2.304633\n",
      "(Epoch 1 / 100) train acc: 0.111000; val_acc: 0.102000\n",
      "(Iteration 401 / 38200) loss: 2.304626\n",
      "(Iteration 501 / 38200) loss: 2.304639\n",
      "(Iteration 601 / 38200) loss: 2.304638\n",
      "(Iteration 701 / 38200) loss: 2.304636\n",
      "(Epoch 2 / 100) train acc: 0.101000; val_acc: 0.103000\n",
      "(Iteration 801 / 38200) loss: 2.304631\n",
      "(Iteration 901 / 38200) loss: 2.304630\n",
      "(Iteration 1001 / 38200) loss: 2.304642\n",
      "(Iteration 1101 / 38200) loss: 2.304632\n",
      "(Epoch 3 / 100) train acc: 0.079000; val_acc: 0.103000\n",
      "(Iteration 1201 / 38200) loss: 2.304631\n",
      "(Iteration 1301 / 38200) loss: 2.304635\n",
      "(Iteration 1401 / 38200) loss: 2.304635\n",
      "(Iteration 1501 / 38200) loss: 2.304624\n",
      "(Epoch 4 / 100) train acc: 0.082000; val_acc: 0.103000\n",
      "(Iteration 1601 / 38200) loss: 2.304639\n",
      "(Iteration 1701 / 38200) loss: 2.304633\n",
      "(Iteration 1801 / 38200) loss: 2.304644\n",
      "(Iteration 1901 / 38200) loss: 2.304637\n",
      "(Epoch 5 / 100) train acc: 0.095000; val_acc: 0.103000\n",
      "(Iteration 2001 / 38200) loss: 2.304633\n",
      "(Iteration 2101 / 38200) loss: 2.304642\n",
      "(Iteration 2201 / 38200) loss: 2.304627\n",
      "(Epoch 6 / 100) train acc: 0.108000; val_acc: 0.103000\n",
      "(Iteration 2301 / 38200) loss: 2.304633\n",
      "(Iteration 2401 / 38200) loss: 2.304641\n",
      "(Iteration 2501 / 38200) loss: 2.304627\n",
      "(Iteration 2601 / 38200) loss: 2.304641\n",
      "(Epoch 7 / 100) train acc: 0.091000; val_acc: 0.103000\n",
      "(Iteration 2701 / 38200) loss: 2.304631\n",
      "(Iteration 2801 / 38200) loss: 2.304631\n",
      "(Iteration 2901 / 38200) loss: 2.304634\n",
      "(Iteration 3001 / 38200) loss: 2.304637\n",
      "(Epoch 8 / 100) train acc: 0.102000; val_acc: 0.103000\n",
      "(Iteration 3101 / 38200) loss: 2.304638\n",
      "(Iteration 3201 / 38200) loss: 2.304644\n",
      "(Iteration 3301 / 38200) loss: 2.304631\n",
      "(Iteration 3401 / 38200) loss: 2.304620\n",
      "(Epoch 9 / 100) train acc: 0.105000; val_acc: 0.103000\n",
      "(Iteration 3501 / 38200) loss: 2.304633\n",
      "(Iteration 3601 / 38200) loss: 2.304634\n",
      "(Iteration 3701 / 38200) loss: 2.304637\n",
      "(Iteration 3801 / 38200) loss: 2.304627\n",
      "(Epoch 10 / 100) train acc: 0.105000; val_acc: 0.103000\n",
      "(Iteration 3901 / 38200) loss: 2.304632\n",
      "(Iteration 4001 / 38200) loss: 2.304622\n",
      "(Iteration 4101 / 38200) loss: 2.304639\n",
      "(Iteration 4201 / 38200) loss: 2.304632\n",
      "(Epoch 11 / 100) train acc: 0.093000; val_acc: 0.103000\n",
      "(Iteration 4301 / 38200) loss: 2.304625\n",
      "(Iteration 4401 / 38200) loss: 2.304634\n",
      "(Iteration 4501 / 38200) loss: 2.304638\n",
      "(Epoch 12 / 100) train acc: 0.109000; val_acc: 0.102000\n",
      "(Iteration 4601 / 38200) loss: 2.304644\n",
      "(Iteration 4701 / 38200) loss: 2.304637\n",
      "(Iteration 4801 / 38200) loss: 2.304628\n",
      "(Iteration 4901 / 38200) loss: 2.304636\n",
      "(Epoch 13 / 100) train acc: 0.106000; val_acc: 0.102000\n",
      "(Iteration 5001 / 38200) loss: 2.304642\n",
      "(Iteration 5101 / 38200) loss: 2.304633\n",
      "(Iteration 5201 / 38200) loss: 2.304628\n",
      "(Iteration 5301 / 38200) loss: 2.304640\n",
      "(Epoch 14 / 100) train acc: 0.083000; val_acc: 0.102000\n",
      "(Iteration 5401 / 38200) loss: 2.304633\n",
      "(Iteration 5501 / 38200) loss: 2.304632\n",
      "(Iteration 5601 / 38200) loss: 2.304639\n",
      "(Iteration 5701 / 38200) loss: 2.304644\n",
      "(Epoch 15 / 100) train acc: 0.086000; val_acc: 0.102000\n",
      "(Iteration 5801 / 38200) loss: 2.304635\n",
      "(Iteration 5901 / 38200) loss: 2.304634\n",
      "(Iteration 6001 / 38200) loss: 2.304636\n",
      "(Iteration 6101 / 38200) loss: 2.304633\n",
      "(Epoch 16 / 100) train acc: 0.091000; val_acc: 0.102000\n",
      "(Iteration 6201 / 38200) loss: 2.304640\n",
      "(Iteration 6301 / 38200) loss: 2.304635\n",
      "(Iteration 6401 / 38200) loss: 2.304633\n",
      "(Epoch 17 / 100) train acc: 0.075000; val_acc: 0.102000\n",
      "(Iteration 6501 / 38200) loss: 2.304632\n",
      "(Iteration 6601 / 38200) loss: 2.304636\n",
      "(Iteration 6701 / 38200) loss: 2.304640\n",
      "(Iteration 6801 / 38200) loss: 2.304642\n",
      "(Epoch 18 / 100) train acc: 0.095000; val_acc: 0.102000\n",
      "(Iteration 6901 / 38200) loss: 2.304643\n",
      "(Iteration 7001 / 38200) loss: 2.304639\n",
      "(Iteration 7101 / 38200) loss: 2.304642\n",
      "(Iteration 7201 / 38200) loss: 2.304625\n",
      "(Epoch 19 / 100) train acc: 0.083000; val_acc: 0.102000\n",
      "(Iteration 7301 / 38200) loss: 2.304633\n",
      "(Iteration 7401 / 38200) loss: 2.304641\n",
      "(Iteration 7501 / 38200) loss: 2.304634\n",
      "(Iteration 7601 / 38200) loss: 2.304632\n",
      "(Epoch 20 / 100) train acc: 0.101000; val_acc: 0.102000\n",
      "(Iteration 7701 / 38200) loss: 2.304632\n",
      "(Iteration 7801 / 38200) loss: 2.304647\n",
      "(Iteration 7901 / 38200) loss: 2.304627\n",
      "(Iteration 8001 / 38200) loss: 2.304635\n",
      "(Epoch 21 / 100) train acc: 0.107000; val_acc: 0.102000\n",
      "(Iteration 8101 / 38200) loss: 2.304640\n",
      "(Iteration 8201 / 38200) loss: 2.304646\n",
      "(Iteration 8301 / 38200) loss: 2.304630\n",
      "(Iteration 8401 / 38200) loss: 2.304637\n",
      "(Epoch 22 / 100) train acc: 0.102000; val_acc: 0.102000\n",
      "(Iteration 8501 / 38200) loss: 2.304641\n",
      "(Iteration 8601 / 38200) loss: 2.304640\n",
      "(Iteration 8701 / 38200) loss: 2.304637\n",
      "(Epoch 23 / 100) train acc: 0.097000; val_acc: 0.102000\n",
      "(Iteration 8801 / 38200) loss: 2.304634\n",
      "(Iteration 8901 / 38200) loss: 2.304633\n",
      "(Iteration 9001 / 38200) loss: 2.304625\n",
      "(Iteration 9101 / 38200) loss: 2.304631\n",
      "(Epoch 24 / 100) train acc: 0.116000; val_acc: 0.102000\n",
      "(Iteration 9201 / 38200) loss: 2.304641\n",
      "(Iteration 9301 / 38200) loss: 2.304638\n",
      "(Iteration 9401 / 38200) loss: 2.304642\n",
      "(Iteration 9501 / 38200) loss: 2.304637\n",
      "(Epoch 25 / 100) train acc: 0.094000; val_acc: 0.102000\n",
      "(Iteration 9601 / 38200) loss: 2.304644\n",
      "(Iteration 9701 / 38200) loss: 2.304635\n",
      "(Iteration 9801 / 38200) loss: 2.304634\n",
      "(Iteration 9901 / 38200) loss: 2.304630\n",
      "(Epoch 26 / 100) train acc: 0.092000; val_acc: 0.102000\n",
      "(Iteration 10001 / 38200) loss: 2.304630\n",
      "(Iteration 10101 / 38200) loss: 2.304637\n",
      "(Iteration 10201 / 38200) loss: 2.304635\n",
      "(Iteration 10301 / 38200) loss: 2.304639\n",
      "(Epoch 27 / 100) train acc: 0.114000; val_acc: 0.102000\n",
      "(Iteration 10401 / 38200) loss: 2.304634\n",
      "(Iteration 10501 / 38200) loss: 2.304629\n",
      "(Iteration 10601 / 38200) loss: 2.304637\n",
      "(Epoch 28 / 100) train acc: 0.091000; val_acc: 0.102000\n",
      "(Iteration 10701 / 38200) loss: 2.304643\n",
      "(Iteration 10801 / 38200) loss: 2.304624\n",
      "(Iteration 10901 / 38200) loss: 2.304635\n",
      "(Iteration 11001 / 38200) loss: 2.304627\n",
      "(Epoch 29 / 100) train acc: 0.092000; val_acc: 0.102000\n",
      "(Iteration 11101 / 38200) loss: 2.304637\n",
      "(Iteration 11201 / 38200) loss: 2.304634\n",
      "(Iteration 11301 / 38200) loss: 2.304633\n",
      "(Iteration 11401 / 38200) loss: 2.304636\n",
      "(Epoch 30 / 100) train acc: 0.090000; val_acc: 0.102000\n",
      "(Iteration 11501 / 38200) loss: 2.304624\n",
      "(Iteration 11601 / 38200) loss: 2.304629\n",
      "(Iteration 11701 / 38200) loss: 2.304635\n",
      "(Iteration 11801 / 38200) loss: 2.304634\n",
      "(Epoch 31 / 100) train acc: 0.099000; val_acc: 0.102000\n",
      "(Iteration 11901 / 38200) loss: 2.304633\n",
      "(Iteration 12001 / 38200) loss: 2.304636\n",
      "(Iteration 12101 / 38200) loss: 2.304627\n",
      "(Iteration 12201 / 38200) loss: 2.304634\n",
      "(Epoch 32 / 100) train acc: 0.109000; val_acc: 0.102000\n",
      "(Iteration 12301 / 38200) loss: 2.304629\n",
      "(Iteration 12401 / 38200) loss: 2.304636\n",
      "(Iteration 12501 / 38200) loss: 2.304634\n",
      "(Iteration 12601 / 38200) loss: 2.304647\n",
      "(Epoch 33 / 100) train acc: 0.094000; val_acc: 0.102000\n",
      "(Iteration 12701 / 38200) loss: 2.304627\n",
      "(Iteration 12801 / 38200) loss: 2.304633\n",
      "(Iteration 12901 / 38200) loss: 2.304639\n",
      "(Epoch 34 / 100) train acc: 0.110000; val_acc: 0.102000\n",
      "(Iteration 13001 / 38200) loss: 2.304651\n",
      "(Iteration 13101 / 38200) loss: 2.304642\n",
      "(Iteration 13201 / 38200) loss: 2.304640\n",
      "(Iteration 13301 / 38200) loss: 2.304643\n",
      "(Epoch 35 / 100) train acc: 0.102000; val_acc: 0.102000\n",
      "(Iteration 13401 / 38200) loss: 2.304631\n",
      "(Iteration 13501 / 38200) loss: 2.304628\n",
      "(Iteration 13601 / 38200) loss: 2.304639\n",
      "(Iteration 13701 / 38200) loss: 2.304634\n",
      "(Epoch 36 / 100) train acc: 0.110000; val_acc: 0.102000\n",
      "(Iteration 13801 / 38200) loss: 2.304645\n",
      "(Iteration 13901 / 38200) loss: 2.304633\n",
      "(Iteration 14001 / 38200) loss: 2.304642\n",
      "(Iteration 14101 / 38200) loss: 2.304626\n",
      "(Epoch 37 / 100) train acc: 0.098000; val_acc: 0.102000\n",
      "(Iteration 14201 / 38200) loss: 2.304634\n",
      "(Iteration 14301 / 38200) loss: 2.304633\n",
      "(Iteration 14401 / 38200) loss: 2.304637\n",
      "(Iteration 14501 / 38200) loss: 2.304637\n",
      "(Epoch 38 / 100) train acc: 0.097000; val_acc: 0.102000\n",
      "(Iteration 14601 / 38200) loss: 2.304645\n",
      "(Iteration 14701 / 38200) loss: 2.304627\n",
      "(Iteration 14801 / 38200) loss: 2.304628\n",
      "(Epoch 39 / 100) train acc: 0.093000; val_acc: 0.102000\n",
      "(Iteration 14901 / 38200) loss: 2.304620\n",
      "(Iteration 15001 / 38200) loss: 2.304637\n",
      "(Iteration 15101 / 38200) loss: 2.304631\n",
      "(Iteration 15201 / 38200) loss: 2.304639\n",
      "(Epoch 40 / 100) train acc: 0.095000; val_acc: 0.102000\n",
      "(Iteration 15301 / 38200) loss: 2.304632\n",
      "(Iteration 15401 / 38200) loss: 2.304625\n",
      "(Iteration 15501 / 38200) loss: 2.304640\n",
      "(Iteration 15601 / 38200) loss: 2.304637\n",
      "(Epoch 41 / 100) train acc: 0.104000; val_acc: 0.102000\n",
      "(Iteration 15701 / 38200) loss: 2.304640\n",
      "(Iteration 15801 / 38200) loss: 2.304630\n",
      "(Iteration 15901 / 38200) loss: 2.304640\n",
      "(Iteration 16001 / 38200) loss: 2.304629\n",
      "(Epoch 42 / 100) train acc: 0.070000; val_acc: 0.102000\n",
      "(Iteration 16101 / 38200) loss: 2.304639\n",
      "(Iteration 16201 / 38200) loss: 2.304628\n",
      "(Iteration 16301 / 38200) loss: 2.304630\n",
      "(Iteration 16401 / 38200) loss: 2.304640\n",
      "(Epoch 43 / 100) train acc: 0.080000; val_acc: 0.102000\n",
      "(Iteration 16501 / 38200) loss: 2.304630\n",
      "(Iteration 16601 / 38200) loss: 2.304637\n",
      "(Iteration 16701 / 38200) loss: 2.304643\n",
      "(Iteration 16801 / 38200) loss: 2.304629\n",
      "(Epoch 44 / 100) train acc: 0.100000; val_acc: 0.102000\n",
      "(Iteration 16901 / 38200) loss: 2.304630\n",
      "(Iteration 17001 / 38200) loss: 2.304630\n",
      "(Iteration 17101 / 38200) loss: 2.304643\n",
      "(Epoch 45 / 100) train acc: 0.106000; val_acc: 0.102000\n",
      "(Iteration 17201 / 38200) loss: 2.304632\n",
      "(Iteration 17301 / 38200) loss: 2.304641\n",
      "(Iteration 17401 / 38200) loss: 2.304634\n",
      "(Iteration 17501 / 38200) loss: 2.304643\n",
      "(Epoch 46 / 100) train acc: 0.099000; val_acc: 0.102000\n",
      "(Iteration 17601 / 38200) loss: 2.304626\n",
      "(Iteration 17701 / 38200) loss: 2.304638\n",
      "(Iteration 17801 / 38200) loss: 2.304630\n",
      "(Iteration 17901 / 38200) loss: 2.304620\n",
      "(Epoch 47 / 100) train acc: 0.101000; val_acc: 0.102000\n",
      "(Iteration 18001 / 38200) loss: 2.304631\n",
      "(Iteration 18101 / 38200) loss: 2.304640\n",
      "(Iteration 18201 / 38200) loss: 2.304628\n",
      "(Iteration 18301 / 38200) loss: 2.304629\n",
      "(Epoch 48 / 100) train acc: 0.095000; val_acc: 0.102000\n",
      "(Iteration 18401 / 38200) loss: 2.304632\n",
      "(Iteration 18501 / 38200) loss: 2.304643\n",
      "(Iteration 18601 / 38200) loss: 2.304624\n",
      "(Iteration 18701 / 38200) loss: 2.304635\n",
      "(Epoch 49 / 100) train acc: 0.090000; val_acc: 0.102000\n",
      "(Iteration 18801 / 38200) loss: 2.304626\n",
      "(Iteration 18901 / 38200) loss: 2.304643\n",
      "(Iteration 19001 / 38200) loss: 2.304634\n",
      "(Epoch 50 / 100) train acc: 0.102000; val_acc: 0.102000\n",
      "(Iteration 19101 / 38200) loss: 2.304637\n",
      "(Iteration 19201 / 38200) loss: 2.304632\n",
      "(Iteration 19301 / 38200) loss: 2.304623\n",
      "(Iteration 19401 / 38200) loss: 2.304632\n",
      "(Epoch 51 / 100) train acc: 0.101000; val_acc: 0.102000\n",
      "(Iteration 19501 / 38200) loss: 2.304631\n",
      "(Iteration 19601 / 38200) loss: 2.304632\n",
      "(Iteration 19701 / 38200) loss: 2.304632\n",
      "(Iteration 19801 / 38200) loss: 2.304643\n",
      "(Epoch 52 / 100) train acc: 0.094000; val_acc: 0.102000\n",
      "(Iteration 19901 / 38200) loss: 2.304642\n",
      "(Iteration 20001 / 38200) loss: 2.304632\n",
      "(Iteration 20101 / 38200) loss: 2.304631\n",
      "(Iteration 20201 / 38200) loss: 2.304630\n",
      "(Epoch 53 / 100) train acc: 0.104000; val_acc: 0.102000\n",
      "(Iteration 20301 / 38200) loss: 2.304641\n",
      "(Iteration 20401 / 38200) loss: 2.304628\n",
      "(Iteration 20501 / 38200) loss: 2.304640\n",
      "(Iteration 20601 / 38200) loss: 2.304635\n",
      "(Epoch 54 / 100) train acc: 0.092000; val_acc: 0.102000\n",
      "(Iteration 20701 / 38200) loss: 2.304638\n",
      "(Iteration 20801 / 38200) loss: 2.304641\n",
      "(Iteration 20901 / 38200) loss: 2.304634\n",
      "(Iteration 21001 / 38200) loss: 2.304635\n",
      "(Epoch 55 / 100) train acc: 0.112000; val_acc: 0.102000\n",
      "(Iteration 21101 / 38200) loss: 2.304648\n",
      "(Iteration 21201 / 38200) loss: 2.304636\n",
      "(Iteration 21301 / 38200) loss: 2.304635\n",
      "(Epoch 56 / 100) train acc: 0.094000; val_acc: 0.102000\n",
      "(Iteration 21401 / 38200) loss: 2.304638\n",
      "(Iteration 21501 / 38200) loss: 2.304638\n",
      "(Iteration 21601 / 38200) loss: 2.304644\n",
      "(Iteration 21701 / 38200) loss: 2.304621\n",
      "(Epoch 57 / 100) train acc: 0.088000; val_acc: 0.102000\n",
      "(Iteration 21801 / 38200) loss: 2.304635\n",
      "(Iteration 21901 / 38200) loss: 2.304629\n",
      "(Iteration 22001 / 38200) loss: 2.304635\n",
      "(Iteration 22101 / 38200) loss: 2.304628\n",
      "(Epoch 58 / 100) train acc: 0.092000; val_acc: 0.102000\n",
      "(Iteration 22201 / 38200) loss: 2.304637\n",
      "(Iteration 22301 / 38200) loss: 2.304635\n",
      "(Iteration 22401 / 38200) loss: 2.304634\n",
      "(Iteration 22501 / 38200) loss: 2.304627\n",
      "(Epoch 59 / 100) train acc: 0.081000; val_acc: 0.102000\n",
      "(Iteration 22601 / 38200) loss: 2.304636\n",
      "(Iteration 22701 / 38200) loss: 2.304638\n",
      "(Iteration 22801 / 38200) loss: 2.304635\n",
      "(Iteration 22901 / 38200) loss: 2.304636\n",
      "(Epoch 60 / 100) train acc: 0.107000; val_acc: 0.102000\n",
      "(Iteration 23001 / 38200) loss: 2.304635\n",
      "(Iteration 23101 / 38200) loss: 2.304632\n",
      "(Iteration 23201 / 38200) loss: 2.304638\n",
      "(Iteration 23301 / 38200) loss: 2.304637\n",
      "(Epoch 61 / 100) train acc: 0.104000; val_acc: 0.102000\n",
      "(Iteration 23401 / 38200) loss: 2.304633\n",
      "(Iteration 23501 / 38200) loss: 2.304636\n",
      "(Iteration 23601 / 38200) loss: 2.304634\n",
      "(Epoch 62 / 100) train acc: 0.107000; val_acc: 0.102000\n",
      "(Iteration 23701 / 38200) loss: 2.304630\n",
      "(Iteration 23801 / 38200) loss: 2.304636\n",
      "(Iteration 23901 / 38200) loss: 2.304630\n",
      "(Iteration 24001 / 38200) loss: 2.304645\n",
      "(Epoch 63 / 100) train acc: 0.106000; val_acc: 0.102000\n",
      "(Iteration 24101 / 38200) loss: 2.304634\n",
      "(Iteration 24201 / 38200) loss: 2.304627\n",
      "(Iteration 24301 / 38200) loss: 2.304642\n",
      "(Iteration 24401 / 38200) loss: 2.304633\n",
      "(Epoch 64 / 100) train acc: 0.109000; val_acc: 0.102000\n",
      "(Iteration 24501 / 38200) loss: 2.304625\n",
      "(Iteration 24601 / 38200) loss: 2.304637\n",
      "(Iteration 24701 / 38200) loss: 2.304628\n",
      "(Iteration 24801 / 38200) loss: 2.304635\n",
      "(Epoch 65 / 100) train acc: 0.095000; val_acc: 0.102000\n",
      "(Iteration 24901 / 38200) loss: 2.304643\n",
      "(Iteration 25001 / 38200) loss: 2.304633\n",
      "(Iteration 25101 / 38200) loss: 2.304639\n",
      "(Iteration 25201 / 38200) loss: 2.304634\n",
      "(Epoch 66 / 100) train acc: 0.106000; val_acc: 0.102000\n",
      "(Iteration 25301 / 38200) loss: 2.304647\n",
      "(Iteration 25401 / 38200) loss: 2.304636\n",
      "(Iteration 25501 / 38200) loss: 2.304638\n",
      "(Epoch 67 / 100) train acc: 0.115000; val_acc: 0.102000\n",
      "(Iteration 25601 / 38200) loss: 2.304642\n",
      "(Iteration 25701 / 38200) loss: 2.304631\n",
      "(Iteration 25801 / 38200) loss: 2.304638\n",
      "(Iteration 25901 / 38200) loss: 2.304625\n",
      "(Epoch 68 / 100) train acc: 0.108000; val_acc: 0.102000\n",
      "(Iteration 26001 / 38200) loss: 2.304628\n",
      "(Iteration 26101 / 38200) loss: 2.304632\n",
      "(Iteration 26201 / 38200) loss: 2.304640\n",
      "(Iteration 26301 / 38200) loss: 2.304644\n",
      "(Epoch 69 / 100) train acc: 0.099000; val_acc: 0.102000\n",
      "(Iteration 26401 / 38200) loss: 2.304636\n",
      "(Iteration 26501 / 38200) loss: 2.304637\n",
      "(Iteration 26601 / 38200) loss: 2.304624\n",
      "(Iteration 26701 / 38200) loss: 2.304643\n",
      "(Epoch 70 / 100) train acc: 0.095000; val_acc: 0.102000\n",
      "(Iteration 26801 / 38200) loss: 2.304628\n",
      "(Iteration 26901 / 38200) loss: 2.304628\n",
      "(Iteration 27001 / 38200) loss: 2.304627\n",
      "(Iteration 27101 / 38200) loss: 2.304639\n",
      "(Epoch 71 / 100) train acc: 0.075000; val_acc: 0.102000\n",
      "(Iteration 27201 / 38200) loss: 2.304627\n",
      "(Iteration 27301 / 38200) loss: 2.304640\n",
      "(Iteration 27401 / 38200) loss: 2.304623\n",
      "(Iteration 27501 / 38200) loss: 2.304633\n",
      "(Epoch 72 / 100) train acc: 0.106000; val_acc: 0.102000\n",
      "(Iteration 27601 / 38200) loss: 2.304626\n",
      "(Iteration 27701 / 38200) loss: 2.304638\n",
      "(Iteration 27801 / 38200) loss: 2.304629\n",
      "(Epoch 73 / 100) train acc: 0.114000; val_acc: 0.102000\n",
      "(Iteration 27901 / 38200) loss: 2.304636\n",
      "(Iteration 28001 / 38200) loss: 2.304633\n",
      "(Iteration 28101 / 38200) loss: 2.304637\n",
      "(Iteration 28201 / 38200) loss: 2.304632\n",
      "(Epoch 74 / 100) train acc: 0.106000; val_acc: 0.102000\n",
      "(Iteration 28301 / 38200) loss: 2.304628\n",
      "(Iteration 28401 / 38200) loss: 2.304629\n",
      "(Iteration 28501 / 38200) loss: 2.304634\n",
      "(Iteration 28601 / 38200) loss: 2.304632\n",
      "(Epoch 75 / 100) train acc: 0.098000; val_acc: 0.102000\n",
      "(Iteration 28701 / 38200) loss: 2.304641\n",
      "(Iteration 28801 / 38200) loss: 2.304634\n",
      "(Iteration 28901 / 38200) loss: 2.304637\n",
      "(Iteration 29001 / 38200) loss: 2.304640\n",
      "(Epoch 76 / 100) train acc: 0.106000; val_acc: 0.102000\n",
      "(Iteration 29101 / 38200) loss: 2.304638\n",
      "(Iteration 29201 / 38200) loss: 2.304637\n",
      "(Iteration 29301 / 38200) loss: 2.304641\n",
      "(Iteration 29401 / 38200) loss: 2.304626\n",
      "(Epoch 77 / 100) train acc: 0.099000; val_acc: 0.102000\n",
      "(Iteration 29501 / 38200) loss: 2.304631\n",
      "(Iteration 29601 / 38200) loss: 2.304642\n",
      "(Iteration 29701 / 38200) loss: 2.304632\n",
      "(Epoch 78 / 100) train acc: 0.075000; val_acc: 0.102000\n",
      "(Iteration 29801 / 38200) loss: 2.304630\n",
      "(Iteration 29901 / 38200) loss: 2.304641\n",
      "(Iteration 30001 / 38200) loss: 2.304635\n",
      "(Iteration 30101 / 38200) loss: 2.304640\n",
      "(Epoch 79 / 100) train acc: 0.087000; val_acc: 0.102000\n",
      "(Iteration 30201 / 38200) loss: 2.304637\n",
      "(Iteration 30301 / 38200) loss: 2.304633\n",
      "(Iteration 30401 / 38200) loss: 2.304628\n",
      "(Iteration 30501 / 38200) loss: 2.304640\n",
      "(Epoch 80 / 100) train acc: 0.098000; val_acc: 0.102000\n",
      "(Iteration 30601 / 38200) loss: 2.304632\n",
      "(Iteration 30701 / 38200) loss: 2.304627\n",
      "(Iteration 30801 / 38200) loss: 2.304635\n",
      "(Iteration 30901 / 38200) loss: 2.304633\n",
      "(Epoch 81 / 100) train acc: 0.081000; val_acc: 0.102000\n",
      "(Iteration 31001 / 38200) loss: 2.304635\n",
      "(Iteration 31101 / 38200) loss: 2.304636\n",
      "(Iteration 31201 / 38200) loss: 2.304639\n",
      "(Iteration 31301 / 38200) loss: 2.304642\n",
      "(Epoch 82 / 100) train acc: 0.104000; val_acc: 0.102000\n",
      "(Iteration 31401 / 38200) loss: 2.304636\n",
      "(Iteration 31501 / 38200) loss: 2.304637\n",
      "(Iteration 31601 / 38200) loss: 2.304633\n",
      "(Iteration 31701 / 38200) loss: 2.304632\n",
      "(Epoch 83 / 100) train acc: 0.089000; val_acc: 0.102000\n",
      "(Iteration 31801 / 38200) loss: 2.304639\n",
      "(Iteration 31901 / 38200) loss: 2.304625\n",
      "(Iteration 32001 / 38200) loss: 2.304635\n",
      "(Epoch 84 / 100) train acc: 0.103000; val_acc: 0.102000\n",
      "(Iteration 32101 / 38200) loss: 2.304640\n",
      "(Iteration 32201 / 38200) loss: 2.304634\n",
      "(Iteration 32301 / 38200) loss: 2.304629\n",
      "(Iteration 32401 / 38200) loss: 2.304618\n",
      "(Epoch 85 / 100) train acc: 0.117000; val_acc: 0.102000\n",
      "(Iteration 32501 / 38200) loss: 2.304628\n",
      "(Iteration 32601 / 38200) loss: 2.304643\n",
      "(Iteration 32701 / 38200) loss: 2.304643\n",
      "(Iteration 32801 / 38200) loss: 2.304628\n",
      "(Epoch 86 / 100) train acc: 0.097000; val_acc: 0.102000\n",
      "(Iteration 32901 / 38200) loss: 2.304638\n",
      "(Iteration 33001 / 38200) loss: 2.304643\n",
      "(Iteration 33101 / 38200) loss: 2.304631\n",
      "(Iteration 33201 / 38200) loss: 2.304629\n",
      "(Epoch 87 / 100) train acc: 0.095000; val_acc: 0.102000\n",
      "(Iteration 33301 / 38200) loss: 2.304640\n",
      "(Iteration 33401 / 38200) loss: 2.304640\n",
      "(Iteration 33501 / 38200) loss: 2.304633\n",
      "(Iteration 33601 / 38200) loss: 2.304633\n",
      "(Epoch 88 / 100) train acc: 0.109000; val_acc: 0.102000\n",
      "(Iteration 33701 / 38200) loss: 2.304625\n",
      "(Iteration 33801 / 38200) loss: 2.304639\n",
      "(Iteration 33901 / 38200) loss: 2.304636\n",
      "(Epoch 89 / 100) train acc: 0.092000; val_acc: 0.102000\n",
      "(Iteration 34001 / 38200) loss: 2.304639\n",
      "(Iteration 34101 / 38200) loss: 2.304637\n",
      "(Iteration 34201 / 38200) loss: 2.304633\n",
      "(Iteration 34301 / 38200) loss: 2.304635\n",
      "(Epoch 90 / 100) train acc: 0.090000; val_acc: 0.102000\n",
      "(Iteration 34401 / 38200) loss: 2.304628\n",
      "(Iteration 34501 / 38200) loss: 2.304633\n",
      "(Iteration 34601 / 38200) loss: 2.304631\n",
      "(Iteration 34701 / 38200) loss: 2.304638\n",
      "(Epoch 91 / 100) train acc: 0.101000; val_acc: 0.102000\n",
      "(Iteration 34801 / 38200) loss: 2.304630\n",
      "(Iteration 34901 / 38200) loss: 2.304638\n",
      "(Iteration 35001 / 38200) loss: 2.304635\n",
      "(Iteration 35101 / 38200) loss: 2.304639\n",
      "(Epoch 92 / 100) train acc: 0.110000; val_acc: 0.102000\n",
      "(Iteration 35201 / 38200) loss: 2.304636\n",
      "(Iteration 35301 / 38200) loss: 2.304636\n",
      "(Iteration 35401 / 38200) loss: 2.304627\n",
      "(Iteration 35501 / 38200) loss: 2.304632\n",
      "(Epoch 93 / 100) train acc: 0.091000; val_acc: 0.102000\n",
      "(Iteration 35601 / 38200) loss: 2.304640\n",
      "(Iteration 35701 / 38200) loss: 2.304636\n",
      "(Iteration 35801 / 38200) loss: 2.304634\n",
      "(Iteration 35901 / 38200) loss: 2.304635\n",
      "(Epoch 94 / 100) train acc: 0.111000; val_acc: 0.102000\n",
      "(Iteration 36001 / 38200) loss: 2.304637\n",
      "(Iteration 36101 / 38200) loss: 2.304638\n",
      "(Iteration 36201 / 38200) loss: 2.304636\n",
      "(Epoch 95 / 100) train acc: 0.086000; val_acc: 0.102000\n",
      "(Iteration 36301 / 38200) loss: 2.304643\n",
      "(Iteration 36401 / 38200) loss: 2.304633\n",
      "(Iteration 36501 / 38200) loss: 2.304624\n",
      "(Iteration 36601 / 38200) loss: 2.304636\n",
      "(Epoch 96 / 100) train acc: 0.096000; val_acc: 0.102000\n",
      "(Iteration 36701 / 38200) loss: 2.304634\n",
      "(Iteration 36801 / 38200) loss: 2.304632\n",
      "(Iteration 36901 / 38200) loss: 2.304631\n",
      "(Iteration 37001 / 38200) loss: 2.304633\n",
      "(Epoch 97 / 100) train acc: 0.090000; val_acc: 0.102000\n",
      "(Iteration 37101 / 38200) loss: 2.304638\n",
      "(Iteration 37201 / 38200) loss: 2.304636\n",
      "(Iteration 37301 / 38200) loss: 2.304629\n",
      "(Iteration 37401 / 38200) loss: 2.304637\n",
      "(Epoch 98 / 100) train acc: 0.098000; val_acc: 0.102000\n",
      "(Iteration 37501 / 38200) loss: 2.304648\n",
      "(Iteration 37601 / 38200) loss: 2.304641\n",
      "(Iteration 37701 / 38200) loss: 2.304641\n",
      "(Iteration 37801 / 38200) loss: 2.304638\n",
      "(Epoch 99 / 100) train acc: 0.076000; val_acc: 0.102000\n",
      "(Iteration 37901 / 38200) loss: 2.304642\n",
      "(Iteration 38001 / 38200) loss: 2.304632\n",
      "(Iteration 38101 / 38200) loss: 2.304636\n",
      "(Epoch 100 / 100) train acc: 0.108000; val_acc: 0.102000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.304685\n",
      "(Epoch 0 / 100) train acc: 0.093000; val_acc: 0.077000\n",
      "(Iteration 101 / 76500) loss: 2.304687\n",
      "(Iteration 201 / 76500) loss: 2.304689\n",
      "(Iteration 301 / 76500) loss: 2.304677\n",
      "(Iteration 401 / 76500) loss: 2.304676\n",
      "(Iteration 501 / 76500) loss: 2.304680\n",
      "(Iteration 601 / 76500) loss: 2.304684\n",
      "(Iteration 701 / 76500) loss: 2.304675\n",
      "(Epoch 1 / 100) train acc: 0.074000; val_acc: 0.077000\n",
      "(Iteration 801 / 76500) loss: 2.304688\n",
      "(Iteration 901 / 76500) loss: 2.304680\n",
      "(Iteration 1001 / 76500) loss: 2.304682\n",
      "(Iteration 1101 / 76500) loss: 2.304678\n",
      "(Iteration 1201 / 76500) loss: 2.304668\n",
      "(Iteration 1301 / 76500) loss: 2.304683\n",
      "(Iteration 1401 / 76500) loss: 2.304681\n",
      "(Iteration 1501 / 76500) loss: 2.304678\n",
      "(Epoch 2 / 100) train acc: 0.084000; val_acc: 0.077000\n",
      "(Iteration 1601 / 76500) loss: 2.304671\n",
      "(Iteration 1701 / 76500) loss: 2.304667\n",
      "(Iteration 1801 / 76500) loss: 2.304679\n",
      "(Iteration 1901 / 76500) loss: 2.304678\n",
      "(Iteration 2001 / 76500) loss: 2.304668\n",
      "(Iteration 2101 / 76500) loss: 2.304685\n",
      "(Iteration 2201 / 76500) loss: 2.304687\n",
      "(Epoch 3 / 100) train acc: 0.084000; val_acc: 0.077000\n",
      "(Iteration 2301 / 76500) loss: 2.304676\n",
      "(Iteration 2401 / 76500) loss: 2.304664\n",
      "(Iteration 2501 / 76500) loss: 2.304691\n",
      "(Iteration 2601 / 76500) loss: 2.304670\n",
      "(Iteration 2701 / 76500) loss: 2.304689\n",
      "(Iteration 2801 / 76500) loss: 2.304672\n",
      "(Iteration 2901 / 76500) loss: 2.304690\n",
      "(Iteration 3001 / 76500) loss: 2.304677\n",
      "(Epoch 4 / 100) train acc: 0.080000; val_acc: 0.077000\n",
      "(Iteration 3101 / 76500) loss: 2.304669\n",
      "(Iteration 3201 / 76500) loss: 2.304672\n",
      "(Iteration 3301 / 76500) loss: 2.304675\n",
      "(Iteration 3401 / 76500) loss: 2.304689\n",
      "(Iteration 3501 / 76500) loss: 2.304674\n",
      "(Iteration 3601 / 76500) loss: 2.304677\n",
      "(Iteration 3701 / 76500) loss: 2.304689\n",
      "(Iteration 3801 / 76500) loss: 2.304674\n",
      "(Epoch 5 / 100) train acc: 0.076000; val_acc: 0.077000\n",
      "(Iteration 3901 / 76500) loss: 2.304673\n",
      "(Iteration 4001 / 76500) loss: 2.304682\n",
      "(Iteration 4101 / 76500) loss: 2.304682\n",
      "(Iteration 4201 / 76500) loss: 2.304671\n",
      "(Iteration 4301 / 76500) loss: 2.304670\n",
      "(Iteration 4401 / 76500) loss: 2.304668\n",
      "(Iteration 4501 / 76500) loss: 2.304680\n",
      "(Epoch 6 / 100) train acc: 0.075000; val_acc: 0.077000\n",
      "(Iteration 4601 / 76500) loss: 2.304687\n",
      "(Iteration 4701 / 76500) loss: 2.304672\n",
      "(Iteration 4801 / 76500) loss: 2.304686\n",
      "(Iteration 4901 / 76500) loss: 2.304681\n",
      "(Iteration 5001 / 76500) loss: 2.304677\n",
      "(Iteration 5101 / 76500) loss: 2.304678\n",
      "(Iteration 5201 / 76500) loss: 2.304678\n",
      "(Iteration 5301 / 76500) loss: 2.304681\n",
      "(Epoch 7 / 100) train acc: 0.081000; val_acc: 0.077000\n",
      "(Iteration 5401 / 76500) loss: 2.304672\n",
      "(Iteration 5501 / 76500) loss: 2.304689\n",
      "(Iteration 5601 / 76500) loss: 2.304686\n",
      "(Iteration 5701 / 76500) loss: 2.304686\n",
      "(Iteration 5801 / 76500) loss: 2.304683\n",
      "(Iteration 5901 / 76500) loss: 2.304680\n",
      "(Iteration 6001 / 76500) loss: 2.304689\n",
      "(Iteration 6101 / 76500) loss: 2.304685\n",
      "(Epoch 8 / 100) train acc: 0.074000; val_acc: 0.077000\n",
      "(Iteration 6201 / 76500) loss: 2.304679\n",
      "(Iteration 6301 / 76500) loss: 2.304681\n",
      "(Iteration 6401 / 76500) loss: 2.304682\n",
      "(Iteration 6501 / 76500) loss: 2.304683\n",
      "(Iteration 6601 / 76500) loss: 2.304670\n",
      "(Iteration 6701 / 76500) loss: 2.304681\n",
      "(Iteration 6801 / 76500) loss: 2.304681\n",
      "(Epoch 9 / 100) train acc: 0.106000; val_acc: 0.077000\n",
      "(Iteration 6901 / 76500) loss: 2.304650\n",
      "(Iteration 7001 / 76500) loss: 2.304692\n",
      "(Iteration 7101 / 76500) loss: 2.304665\n",
      "(Iteration 7201 / 76500) loss: 2.304677\n",
      "(Iteration 7301 / 76500) loss: 2.304684\n",
      "(Iteration 7401 / 76500) loss: 2.304673\n",
      "(Iteration 7501 / 76500) loss: 2.304677\n",
      "(Iteration 7601 / 76500) loss: 2.304684\n",
      "(Epoch 10 / 100) train acc: 0.071000; val_acc: 0.077000\n",
      "(Iteration 7701 / 76500) loss: 2.304681\n",
      "(Iteration 7801 / 76500) loss: 2.304673\n",
      "(Iteration 7901 / 76500) loss: 2.304687\n",
      "(Iteration 8001 / 76500) loss: 2.304673\n",
      "(Iteration 8101 / 76500) loss: 2.304675\n",
      "(Iteration 8201 / 76500) loss: 2.304675\n",
      "(Iteration 8301 / 76500) loss: 2.304680\n",
      "(Iteration 8401 / 76500) loss: 2.304677\n",
      "(Epoch 11 / 100) train acc: 0.087000; val_acc: 0.077000\n",
      "(Iteration 8501 / 76500) loss: 2.304678\n",
      "(Iteration 8601 / 76500) loss: 2.304675\n",
      "(Iteration 8701 / 76500) loss: 2.304675\n",
      "(Iteration 8801 / 76500) loss: 2.304688\n",
      "(Iteration 8901 / 76500) loss: 2.304686\n",
      "(Iteration 9001 / 76500) loss: 2.304680\n",
      "(Iteration 9101 / 76500) loss: 2.304671\n",
      "(Epoch 12 / 100) train acc: 0.072000; val_acc: 0.077000\n",
      "(Iteration 9201 / 76500) loss: 2.304671\n",
      "(Iteration 9301 / 76500) loss: 2.304670\n",
      "(Iteration 9401 / 76500) loss: 2.304686\n",
      "(Iteration 9501 / 76500) loss: 2.304661\n",
      "(Iteration 9601 / 76500) loss: 2.304678\n",
      "(Iteration 9701 / 76500) loss: 2.304673\n",
      "(Iteration 9801 / 76500) loss: 2.304686\n",
      "(Iteration 9901 / 76500) loss: 2.304669\n",
      "(Epoch 13 / 100) train acc: 0.100000; val_acc: 0.077000\n",
      "(Iteration 10001 / 76500) loss: 2.304672\n",
      "(Iteration 10101 / 76500) loss: 2.304673\n",
      "(Iteration 10201 / 76500) loss: 2.304677\n",
      "(Iteration 10301 / 76500) loss: 2.304682\n",
      "(Iteration 10401 / 76500) loss: 2.304672\n",
      "(Iteration 10501 / 76500) loss: 2.304678\n",
      "(Iteration 10601 / 76500) loss: 2.304681\n",
      "(Iteration 10701 / 76500) loss: 2.304677\n",
      "(Epoch 14 / 100) train acc: 0.086000; val_acc: 0.077000\n",
      "(Iteration 10801 / 76500) loss: 2.304672\n",
      "(Iteration 10901 / 76500) loss: 2.304688\n",
      "(Iteration 11001 / 76500) loss: 2.304664\n",
      "(Iteration 11101 / 76500) loss: 2.304667\n",
      "(Iteration 11201 / 76500) loss: 2.304680\n",
      "(Iteration 11301 / 76500) loss: 2.304690\n",
      "(Iteration 11401 / 76500) loss: 2.304682\n",
      "(Epoch 15 / 100) train acc: 0.079000; val_acc: 0.077000\n",
      "(Iteration 11501 / 76500) loss: 2.304677\n",
      "(Iteration 11601 / 76500) loss: 2.304677\n",
      "(Iteration 11701 / 76500) loss: 2.304681\n",
      "(Iteration 11801 / 76500) loss: 2.304688\n",
      "(Iteration 11901 / 76500) loss: 2.304664\n",
      "(Iteration 12001 / 76500) loss: 2.304680\n",
      "(Iteration 12101 / 76500) loss: 2.304667\n",
      "(Iteration 12201 / 76500) loss: 2.304678\n",
      "(Epoch 16 / 100) train acc: 0.092000; val_acc: 0.077000\n",
      "(Iteration 12301 / 76500) loss: 2.304666\n",
      "(Iteration 12401 / 76500) loss: 2.304679\n",
      "(Iteration 12501 / 76500) loss: 2.304672\n",
      "(Iteration 12601 / 76500) loss: 2.304674\n",
      "(Iteration 12701 / 76500) loss: 2.304680\n",
      "(Iteration 12801 / 76500) loss: 2.304666\n",
      "(Iteration 12901 / 76500) loss: 2.304665\n",
      "(Iteration 13001 / 76500) loss: 2.304690\n",
      "(Epoch 17 / 100) train acc: 0.089000; val_acc: 0.077000\n",
      "(Iteration 13101 / 76500) loss: 2.304685\n",
      "(Iteration 13201 / 76500) loss: 2.304676\n",
      "(Iteration 13301 / 76500) loss: 2.304673\n",
      "(Iteration 13401 / 76500) loss: 2.304665\n",
      "(Iteration 13501 / 76500) loss: 2.304679\n",
      "(Iteration 13601 / 76500) loss: 2.304679\n",
      "(Iteration 13701 / 76500) loss: 2.304692\n",
      "(Epoch 18 / 100) train acc: 0.081000; val_acc: 0.077000\n",
      "(Iteration 13801 / 76500) loss: 2.304681\n",
      "(Iteration 13901 / 76500) loss: 2.304691\n",
      "(Iteration 14001 / 76500) loss: 2.304687\n",
      "(Iteration 14101 / 76500) loss: 2.304681\n",
      "(Iteration 14201 / 76500) loss: 2.304672\n",
      "(Iteration 14301 / 76500) loss: 2.304681\n",
      "(Iteration 14401 / 76500) loss: 2.304679\n",
      "(Iteration 14501 / 76500) loss: 2.304668\n",
      "(Epoch 19 / 100) train acc: 0.066000; val_acc: 0.077000\n",
      "(Iteration 14601 / 76500) loss: 2.304681\n",
      "(Iteration 14701 / 76500) loss: 2.304658\n",
      "(Iteration 14801 / 76500) loss: 2.304689\n",
      "(Iteration 14901 / 76500) loss: 2.304683\n",
      "(Iteration 15001 / 76500) loss: 2.304680\n",
      "(Iteration 15101 / 76500) loss: 2.304683\n",
      "(Iteration 15201 / 76500) loss: 2.304679\n",
      "(Epoch 20 / 100) train acc: 0.089000; val_acc: 0.077000\n",
      "(Iteration 15301 / 76500) loss: 2.304658\n",
      "(Iteration 15401 / 76500) loss: 2.304686\n",
      "(Iteration 15501 / 76500) loss: 2.304681\n",
      "(Iteration 15601 / 76500) loss: 2.304684\n",
      "(Iteration 15701 / 76500) loss: 2.304673\n",
      "(Iteration 15801 / 76500) loss: 2.304665\n",
      "(Iteration 15901 / 76500) loss: 2.304667\n",
      "(Iteration 16001 / 76500) loss: 2.304676\n",
      "(Epoch 21 / 100) train acc: 0.074000; val_acc: 0.078000\n",
      "(Iteration 16101 / 76500) loss: 2.304691\n",
      "(Iteration 16201 / 76500) loss: 2.304669\n",
      "(Iteration 16301 / 76500) loss: 2.304675\n",
      "(Iteration 16401 / 76500) loss: 2.304675\n",
      "(Iteration 16501 / 76500) loss: 2.304684\n",
      "(Iteration 16601 / 76500) loss: 2.304677\n",
      "(Iteration 16701 / 76500) loss: 2.304683\n",
      "(Iteration 16801 / 76500) loss: 2.304677\n",
      "(Epoch 22 / 100) train acc: 0.085000; val_acc: 0.078000\n",
      "(Iteration 16901 / 76500) loss: 2.304681\n",
      "(Iteration 17001 / 76500) loss: 2.304680\n",
      "(Iteration 17101 / 76500) loss: 2.304675\n",
      "(Iteration 17201 / 76500) loss: 2.304669\n",
      "(Iteration 17301 / 76500) loss: 2.304680\n",
      "(Iteration 17401 / 76500) loss: 2.304665\n",
      "(Iteration 17501 / 76500) loss: 2.304672\n",
      "(Epoch 23 / 100) train acc: 0.082000; val_acc: 0.078000\n",
      "(Iteration 17601 / 76500) loss: 2.304678\n",
      "(Iteration 17701 / 76500) loss: 2.304671\n",
      "(Iteration 17801 / 76500) loss: 2.304675\n",
      "(Iteration 17901 / 76500) loss: 2.304685\n",
      "(Iteration 18001 / 76500) loss: 2.304680\n",
      "(Iteration 18101 / 76500) loss: 2.304681\n",
      "(Iteration 18201 / 76500) loss: 2.304681\n",
      "(Iteration 18301 / 76500) loss: 2.304657\n",
      "(Epoch 24 / 100) train acc: 0.082000; val_acc: 0.078000\n",
      "(Iteration 18401 / 76500) loss: 2.304683\n",
      "(Iteration 18501 / 76500) loss: 2.304700\n",
      "(Iteration 18601 / 76500) loss: 2.304671\n",
      "(Iteration 18701 / 76500) loss: 2.304679\n",
      "(Iteration 18801 / 76500) loss: 2.304672\n",
      "(Iteration 18901 / 76500) loss: 2.304683\n",
      "(Iteration 19001 / 76500) loss: 2.304671\n",
      "(Iteration 19101 / 76500) loss: 2.304674\n",
      "(Epoch 25 / 100) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 19201 / 76500) loss: 2.304669\n",
      "(Iteration 19301 / 76500) loss: 2.304670\n",
      "(Iteration 19401 / 76500) loss: 2.304670\n",
      "(Iteration 19501 / 76500) loss: 2.304676\n",
      "(Iteration 19601 / 76500) loss: 2.304676\n",
      "(Iteration 19701 / 76500) loss: 2.304687\n",
      "(Iteration 19801 / 76500) loss: 2.304678\n",
      "(Epoch 26 / 100) train acc: 0.081000; val_acc: 0.078000\n",
      "(Iteration 19901 / 76500) loss: 2.304683\n",
      "(Iteration 20001 / 76500) loss: 2.304672\n",
      "(Iteration 20101 / 76500) loss: 2.304678\n",
      "(Iteration 20201 / 76500) loss: 2.304670\n",
      "(Iteration 20301 / 76500) loss: 2.304679\n",
      "(Iteration 20401 / 76500) loss: 2.304676\n",
      "(Iteration 20501 / 76500) loss: 2.304670\n",
      "(Iteration 20601 / 76500) loss: 2.304674\n",
      "(Epoch 27 / 100) train acc: 0.086000; val_acc: 0.078000\n",
      "(Iteration 20701 / 76500) loss: 2.304685\n",
      "(Iteration 20801 / 76500) loss: 2.304677\n",
      "(Iteration 20901 / 76500) loss: 2.304681\n",
      "(Iteration 21001 / 76500) loss: 2.304674\n",
      "(Iteration 21101 / 76500) loss: 2.304677\n",
      "(Iteration 21201 / 76500) loss: 2.304697\n",
      "(Iteration 21301 / 76500) loss: 2.304667\n",
      "(Iteration 21401 / 76500) loss: 2.304671\n",
      "(Epoch 28 / 100) train acc: 0.085000; val_acc: 0.078000\n",
      "(Iteration 21501 / 76500) loss: 2.304668\n",
      "(Iteration 21601 / 76500) loss: 2.304681\n",
      "(Iteration 21701 / 76500) loss: 2.304679\n",
      "(Iteration 21801 / 76500) loss: 2.304672\n",
      "(Iteration 21901 / 76500) loss: 2.304680\n",
      "(Iteration 22001 / 76500) loss: 2.304675\n",
      "(Iteration 22101 / 76500) loss: 2.304661\n",
      "(Epoch 29 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 22201 / 76500) loss: 2.304665\n",
      "(Iteration 22301 / 76500) loss: 2.304695\n",
      "(Iteration 22401 / 76500) loss: 2.304675\n",
      "(Iteration 22501 / 76500) loss: 2.304680\n",
      "(Iteration 22601 / 76500) loss: 2.304673\n",
      "(Iteration 22701 / 76500) loss: 2.304668\n",
      "(Iteration 22801 / 76500) loss: 2.304682\n",
      "(Iteration 22901 / 76500) loss: 2.304670\n",
      "(Epoch 30 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 23001 / 76500) loss: 2.304669\n",
      "(Iteration 23101 / 76500) loss: 2.304689\n",
      "(Iteration 23201 / 76500) loss: 2.304677\n",
      "(Iteration 23301 / 76500) loss: 2.304677\n",
      "(Iteration 23401 / 76500) loss: 2.304680\n",
      "(Iteration 23501 / 76500) loss: 2.304686\n",
      "(Iteration 23601 / 76500) loss: 2.304682\n",
      "(Iteration 23701 / 76500) loss: 2.304682\n",
      "(Epoch 31 / 100) train acc: 0.086000; val_acc: 0.078000\n",
      "(Iteration 23801 / 76500) loss: 2.304681\n",
      "(Iteration 23901 / 76500) loss: 2.304668\n",
      "(Iteration 24001 / 76500) loss: 2.304678\n",
      "(Iteration 24101 / 76500) loss: 2.304669\n",
      "(Iteration 24201 / 76500) loss: 2.304683\n",
      "(Iteration 24301 / 76500) loss: 2.304670\n",
      "(Iteration 24401 / 76500) loss: 2.304673\n",
      "(Epoch 32 / 100) train acc: 0.076000; val_acc: 0.078000\n",
      "(Iteration 24501 / 76500) loss: 2.304682\n",
      "(Iteration 24601 / 76500) loss: 2.304689\n",
      "(Iteration 24701 / 76500) loss: 2.304674\n",
      "(Iteration 24801 / 76500) loss: 2.304681\n",
      "(Iteration 24901 / 76500) loss: 2.304673\n",
      "(Iteration 25001 / 76500) loss: 2.304662\n",
      "(Iteration 25101 / 76500) loss: 2.304661\n",
      "(Iteration 25201 / 76500) loss: 2.304680\n",
      "(Epoch 33 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 25301 / 76500) loss: 2.304683\n",
      "(Iteration 25401 / 76500) loss: 2.304664\n",
      "(Iteration 25501 / 76500) loss: 2.304676\n",
      "(Iteration 25601 / 76500) loss: 2.304678\n",
      "(Iteration 25701 / 76500) loss: 2.304661\n",
      "(Iteration 25801 / 76500) loss: 2.304673\n",
      "(Iteration 25901 / 76500) loss: 2.304678\n",
      "(Iteration 26001 / 76500) loss: 2.304680\n",
      "(Epoch 34 / 100) train acc: 0.086000; val_acc: 0.078000\n",
      "(Iteration 26101 / 76500) loss: 2.304668\n",
      "(Iteration 26201 / 76500) loss: 2.304678\n",
      "(Iteration 26301 / 76500) loss: 2.304676\n",
      "(Iteration 26401 / 76500) loss: 2.304686\n",
      "(Iteration 26501 / 76500) loss: 2.304667\n",
      "(Iteration 26601 / 76500) loss: 2.304685\n",
      "(Iteration 26701 / 76500) loss: 2.304689\n",
      "(Epoch 35 / 100) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 26801 / 76500) loss: 2.304676\n",
      "(Iteration 26901 / 76500) loss: 2.304681\n",
      "(Iteration 27001 / 76500) loss: 2.304681\n",
      "(Iteration 27101 / 76500) loss: 2.304671\n",
      "(Iteration 27201 / 76500) loss: 2.304668\n",
      "(Iteration 27301 / 76500) loss: 2.304673\n",
      "(Iteration 27401 / 76500) loss: 2.304664\n",
      "(Iteration 27501 / 76500) loss: 2.304690\n",
      "(Epoch 36 / 100) train acc: 0.078000; val_acc: 0.078000\n",
      "(Iteration 27601 / 76500) loss: 2.304659\n",
      "(Iteration 27701 / 76500) loss: 2.304685\n",
      "(Iteration 27801 / 76500) loss: 2.304667\n",
      "(Iteration 27901 / 76500) loss: 2.304683\n",
      "(Iteration 28001 / 76500) loss: 2.304669\n",
      "(Iteration 28101 / 76500) loss: 2.304679\n",
      "(Iteration 28201 / 76500) loss: 2.304661\n",
      "(Iteration 28301 / 76500) loss: 2.304661\n",
      "(Epoch 37 / 100) train acc: 0.081000; val_acc: 0.078000\n",
      "(Iteration 28401 / 76500) loss: 2.304667\n",
      "(Iteration 28501 / 76500) loss: 2.304668\n",
      "(Iteration 28601 / 76500) loss: 2.304681\n",
      "(Iteration 28701 / 76500) loss: 2.304677\n",
      "(Iteration 28801 / 76500) loss: 2.304672\n",
      "(Iteration 28901 / 76500) loss: 2.304663\n",
      "(Iteration 29001 / 76500) loss: 2.304672\n",
      "(Epoch 38 / 100) train acc: 0.074000; val_acc: 0.078000\n",
      "(Iteration 29101 / 76500) loss: 2.304681\n",
      "(Iteration 29201 / 76500) loss: 2.304672\n",
      "(Iteration 29301 / 76500) loss: 2.304682\n",
      "(Iteration 29401 / 76500) loss: 2.304664\n",
      "(Iteration 29501 / 76500) loss: 2.304663\n",
      "(Iteration 29601 / 76500) loss: 2.304667\n",
      "(Iteration 29701 / 76500) loss: 2.304679\n",
      "(Iteration 29801 / 76500) loss: 2.304680\n",
      "(Epoch 39 / 100) train acc: 0.082000; val_acc: 0.078000\n",
      "(Iteration 29901 / 76500) loss: 2.304673\n",
      "(Iteration 30001 / 76500) loss: 2.304683\n",
      "(Iteration 30101 / 76500) loss: 2.304669\n",
      "(Iteration 30201 / 76500) loss: 2.304687\n",
      "(Iteration 30301 / 76500) loss: 2.304688\n",
      "(Iteration 30401 / 76500) loss: 2.304676\n",
      "(Iteration 30501 / 76500) loss: 2.304685\n",
      "(Epoch 40 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 30601 / 76500) loss: 2.304683\n",
      "(Iteration 30701 / 76500) loss: 2.304684\n",
      "(Iteration 30801 / 76500) loss: 2.304688\n",
      "(Iteration 30901 / 76500) loss: 2.304682\n",
      "(Iteration 31001 / 76500) loss: 2.304696\n",
      "(Iteration 31101 / 76500) loss: 2.304683\n",
      "(Iteration 31201 / 76500) loss: 2.304659\n",
      "(Iteration 31301 / 76500) loss: 2.304680\n",
      "(Epoch 41 / 100) train acc: 0.077000; val_acc: 0.078000\n",
      "(Iteration 31401 / 76500) loss: 2.304682\n",
      "(Iteration 31501 / 76500) loss: 2.304684\n",
      "(Iteration 31601 / 76500) loss: 2.304673\n",
      "(Iteration 31701 / 76500) loss: 2.304680\n",
      "(Iteration 31801 / 76500) loss: 2.304656\n",
      "(Iteration 31901 / 76500) loss: 2.304689\n",
      "(Iteration 32001 / 76500) loss: 2.304682\n",
      "(Iteration 32101 / 76500) loss: 2.304696\n",
      "(Epoch 42 / 100) train acc: 0.082000; val_acc: 0.078000\n",
      "(Iteration 32201 / 76500) loss: 2.304680\n",
      "(Iteration 32301 / 76500) loss: 2.304690\n",
      "(Iteration 32401 / 76500) loss: 2.304664\n",
      "(Iteration 32501 / 76500) loss: 2.304675\n",
      "(Iteration 32601 / 76500) loss: 2.304673\n",
      "(Iteration 32701 / 76500) loss: 2.304678\n",
      "(Iteration 32801 / 76500) loss: 2.304679\n",
      "(Epoch 43 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 32901 / 76500) loss: 2.304668\n",
      "(Iteration 33001 / 76500) loss: 2.304683\n",
      "(Iteration 33101 / 76500) loss: 2.304682\n",
      "(Iteration 33201 / 76500) loss: 2.304683\n",
      "(Iteration 33301 / 76500) loss: 2.304660\n",
      "(Iteration 33401 / 76500) loss: 2.304682\n",
      "(Iteration 33501 / 76500) loss: 2.304676\n",
      "(Iteration 33601 / 76500) loss: 2.304663\n",
      "(Epoch 44 / 100) train acc: 0.079000; val_acc: 0.078000\n",
      "(Iteration 33701 / 76500) loss: 2.304686\n",
      "(Iteration 33801 / 76500) loss: 2.304681\n",
      "(Iteration 33901 / 76500) loss: 2.304670\n",
      "(Iteration 34001 / 76500) loss: 2.304674\n",
      "(Iteration 34101 / 76500) loss: 2.304666\n",
      "(Iteration 34201 / 76500) loss: 2.304668\n",
      "(Iteration 34301 / 76500) loss: 2.304675\n",
      "(Iteration 34401 / 76500) loss: 2.304672\n",
      "(Epoch 45 / 100) train acc: 0.075000; val_acc: 0.078000\n",
      "(Iteration 34501 / 76500) loss: 2.304680\n",
      "(Iteration 34601 / 76500) loss: 2.304686\n",
      "(Iteration 34701 / 76500) loss: 2.304687\n",
      "(Iteration 34801 / 76500) loss: 2.304680\n",
      "(Iteration 34901 / 76500) loss: 2.304671\n",
      "(Iteration 35001 / 76500) loss: 2.304676\n",
      "(Iteration 35101 / 76500) loss: 2.304665\n",
      "(Epoch 46 / 100) train acc: 0.082000; val_acc: 0.078000\n",
      "(Iteration 35201 / 76500) loss: 2.304674\n",
      "(Iteration 35301 / 76500) loss: 2.304676\n",
      "(Iteration 35401 / 76500) loss: 2.304675\n",
      "(Iteration 35501 / 76500) loss: 2.304677\n",
      "(Iteration 35601 / 76500) loss: 2.304682\n",
      "(Iteration 35701 / 76500) loss: 2.304682\n",
      "(Iteration 35801 / 76500) loss: 2.304678\n",
      "(Iteration 35901 / 76500) loss: 2.304667\n",
      "(Epoch 47 / 100) train acc: 0.072000; val_acc: 0.078000\n",
      "(Iteration 36001 / 76500) loss: 2.304666\n",
      "(Iteration 36101 / 76500) loss: 2.304669\n",
      "(Iteration 36201 / 76500) loss: 2.304675\n",
      "(Iteration 36301 / 76500) loss: 2.304683\n",
      "(Iteration 36401 / 76500) loss: 2.304677\n",
      "(Iteration 36501 / 76500) loss: 2.304683\n",
      "(Iteration 36601 / 76500) loss: 2.304673\n",
      "(Iteration 36701 / 76500) loss: 2.304662\n",
      "(Epoch 48 / 100) train acc: 0.084000; val_acc: 0.078000\n",
      "(Iteration 36801 / 76500) loss: 2.304669\n",
      "(Iteration 36901 / 76500) loss: 2.304666\n",
      "(Iteration 37001 / 76500) loss: 2.304679\n",
      "(Iteration 37101 / 76500) loss: 2.304663\n",
      "(Iteration 37201 / 76500) loss: 2.304690\n",
      "(Iteration 37301 / 76500) loss: 2.304685\n",
      "(Iteration 37401 / 76500) loss: 2.304677\n",
      "(Epoch 49 / 100) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 37501 / 76500) loss: 2.304685\n",
      "(Iteration 37601 / 76500) loss: 2.304667\n",
      "(Iteration 37701 / 76500) loss: 2.304664\n",
      "(Iteration 37801 / 76500) loss: 2.304670\n",
      "(Iteration 37901 / 76500) loss: 2.304660\n",
      "(Iteration 38001 / 76500) loss: 2.304684\n",
      "(Iteration 38101 / 76500) loss: 2.304665\n",
      "(Iteration 38201 / 76500) loss: 2.304682\n",
      "(Epoch 50 / 100) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 38301 / 76500) loss: 2.304674\n",
      "(Iteration 38401 / 76500) loss: 2.304679\n",
      "(Iteration 38501 / 76500) loss: 2.304678\n",
      "(Iteration 38601 / 76500) loss: 2.304667\n",
      "(Iteration 38701 / 76500) loss: 2.304673\n",
      "(Iteration 38801 / 76500) loss: 2.304670\n",
      "(Iteration 38901 / 76500) loss: 2.304669\n",
      "(Iteration 39001 / 76500) loss: 2.304691\n",
      "(Epoch 51 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 39101 / 76500) loss: 2.304669\n",
      "(Iteration 39201 / 76500) loss: 2.304672\n",
      "(Iteration 39301 / 76500) loss: 2.304679\n",
      "(Iteration 39401 / 76500) loss: 2.304682\n",
      "(Iteration 39501 / 76500) loss: 2.304678\n",
      "(Iteration 39601 / 76500) loss: 2.304669\n",
      "(Iteration 39701 / 76500) loss: 2.304665\n",
      "(Epoch 52 / 100) train acc: 0.081000; val_acc: 0.078000\n",
      "(Iteration 39801 / 76500) loss: 2.304673\n",
      "(Iteration 39901 / 76500) loss: 2.304682\n",
      "(Iteration 40001 / 76500) loss: 2.304664\n",
      "(Iteration 40101 / 76500) loss: 2.304678\n",
      "(Iteration 40201 / 76500) loss: 2.304672\n",
      "(Iteration 40301 / 76500) loss: 2.304672\n",
      "(Iteration 40401 / 76500) loss: 2.304669\n",
      "(Iteration 40501 / 76500) loss: 2.304690\n",
      "(Epoch 53 / 100) train acc: 0.080000; val_acc: 0.078000\n",
      "(Iteration 40601 / 76500) loss: 2.304672\n",
      "(Iteration 40701 / 76500) loss: 2.304674\n",
      "(Iteration 40801 / 76500) loss: 2.304677\n",
      "(Iteration 40901 / 76500) loss: 2.304688\n",
      "(Iteration 41001 / 76500) loss: 2.304677\n",
      "(Iteration 41101 / 76500) loss: 2.304662\n",
      "(Iteration 41201 / 76500) loss: 2.304679\n",
      "(Iteration 41301 / 76500) loss: 2.304678\n",
      "(Epoch 54 / 100) train acc: 0.081000; val_acc: 0.078000\n",
      "(Iteration 41401 / 76500) loss: 2.304672\n",
      "(Iteration 41501 / 76500) loss: 2.304668\n",
      "(Iteration 41601 / 76500) loss: 2.304672\n",
      "(Iteration 41701 / 76500) loss: 2.304682\n",
      "(Iteration 41801 / 76500) loss: 2.304673\n",
      "(Iteration 41901 / 76500) loss: 2.304679\n",
      "(Iteration 42001 / 76500) loss: 2.304668\n",
      "(Epoch 55 / 100) train acc: 0.078000; val_acc: 0.078000\n",
      "(Iteration 42101 / 76500) loss: 2.304674\n",
      "(Iteration 42201 / 76500) loss: 2.304667\n",
      "(Iteration 42301 / 76500) loss: 2.304676\n",
      "(Iteration 42401 / 76500) loss: 2.304668\n",
      "(Iteration 42501 / 76500) loss: 2.304667\n",
      "(Iteration 42601 / 76500) loss: 2.304687\n",
      "(Iteration 42701 / 76500) loss: 2.304673\n",
      "(Iteration 42801 / 76500) loss: 2.304678\n",
      "(Epoch 56 / 100) train acc: 0.080000; val_acc: 0.078000\n",
      "(Iteration 42901 / 76500) loss: 2.304678\n",
      "(Iteration 43001 / 76500) loss: 2.304680\n",
      "(Iteration 43101 / 76500) loss: 2.304667\n",
      "(Iteration 43201 / 76500) loss: 2.304673\n",
      "(Iteration 43301 / 76500) loss: 2.304681\n",
      "(Iteration 43401 / 76500) loss: 2.304675\n",
      "(Iteration 43501 / 76500) loss: 2.304681\n",
      "(Iteration 43601 / 76500) loss: 2.304680\n",
      "(Epoch 57 / 100) train acc: 0.073000; val_acc: 0.078000\n",
      "(Iteration 43701 / 76500) loss: 2.304688\n",
      "(Iteration 43801 / 76500) loss: 2.304673\n",
      "(Iteration 43901 / 76500) loss: 2.304685\n",
      "(Iteration 44001 / 76500) loss: 2.304677\n",
      "(Iteration 44101 / 76500) loss: 2.304691\n",
      "(Iteration 44201 / 76500) loss: 2.304676\n",
      "(Iteration 44301 / 76500) loss: 2.304664\n",
      "(Epoch 58 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 44401 / 76500) loss: 2.304675\n",
      "(Iteration 44501 / 76500) loss: 2.304681\n",
      "(Iteration 44601 / 76500) loss: 2.304683\n",
      "(Iteration 44701 / 76500) loss: 2.304670\n",
      "(Iteration 44801 / 76500) loss: 2.304681\n",
      "(Iteration 44901 / 76500) loss: 2.304693\n",
      "(Iteration 45001 / 76500) loss: 2.304677\n",
      "(Iteration 45101 / 76500) loss: 2.304686\n",
      "(Epoch 59 / 100) train acc: 0.074000; val_acc: 0.078000\n",
      "(Iteration 45201 / 76500) loss: 2.304685\n",
      "(Iteration 45301 / 76500) loss: 2.304681\n",
      "(Iteration 45401 / 76500) loss: 2.304689\n",
      "(Iteration 45501 / 76500) loss: 2.304675\n",
      "(Iteration 45601 / 76500) loss: 2.304683\n",
      "(Iteration 45701 / 76500) loss: 2.304678\n",
      "(Iteration 45801 / 76500) loss: 2.304670\n",
      "(Epoch 60 / 100) train acc: 0.084000; val_acc: 0.078000\n",
      "(Iteration 45901 / 76500) loss: 2.304664\n",
      "(Iteration 46001 / 76500) loss: 2.304687\n",
      "(Iteration 46101 / 76500) loss: 2.304679\n",
      "(Iteration 46201 / 76500) loss: 2.304678\n",
      "(Iteration 46301 / 76500) loss: 2.304678\n",
      "(Iteration 46401 / 76500) loss: 2.304674\n",
      "(Iteration 46501 / 76500) loss: 2.304672\n",
      "(Iteration 46601 / 76500) loss: 2.304669\n",
      "(Epoch 61 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 46701 / 76500) loss: 2.304667\n",
      "(Iteration 46801 / 76500) loss: 2.304679\n",
      "(Iteration 46901 / 76500) loss: 2.304663\n",
      "(Iteration 47001 / 76500) loss: 2.304670\n",
      "(Iteration 47101 / 76500) loss: 2.304665\n",
      "(Iteration 47201 / 76500) loss: 2.304682\n",
      "(Iteration 47301 / 76500) loss: 2.304679\n",
      "(Iteration 47401 / 76500) loss: 2.304680\n",
      "(Epoch 62 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 47501 / 76500) loss: 2.304670\n",
      "(Iteration 47601 / 76500) loss: 2.304667\n",
      "(Iteration 47701 / 76500) loss: 2.304681\n",
      "(Iteration 47801 / 76500) loss: 2.304684\n",
      "(Iteration 47901 / 76500) loss: 2.304667\n",
      "(Iteration 48001 / 76500) loss: 2.304688\n",
      "(Iteration 48101 / 76500) loss: 2.304680\n",
      "(Epoch 63 / 100) train acc: 0.074000; val_acc: 0.078000\n",
      "(Iteration 48201 / 76500) loss: 2.304682\n",
      "(Iteration 48301 / 76500) loss: 2.304679\n",
      "(Iteration 48401 / 76500) loss: 2.304679\n",
      "(Iteration 48501 / 76500) loss: 2.304683\n",
      "(Iteration 48601 / 76500) loss: 2.304673\n",
      "(Iteration 48701 / 76500) loss: 2.304680\n",
      "(Iteration 48801 / 76500) loss: 2.304691\n",
      "(Iteration 48901 / 76500) loss: 2.304682\n",
      "(Epoch 64 / 100) train acc: 0.085000; val_acc: 0.078000\n",
      "(Iteration 49001 / 76500) loss: 2.304683\n",
      "(Iteration 49101 / 76500) loss: 2.304670\n",
      "(Iteration 49201 / 76500) loss: 2.304664\n",
      "(Iteration 49301 / 76500) loss: 2.304671\n",
      "(Iteration 49401 / 76500) loss: 2.304683\n",
      "(Iteration 49501 / 76500) loss: 2.304681\n",
      "(Iteration 49601 / 76500) loss: 2.304667\n",
      "(Iteration 49701 / 76500) loss: 2.304675\n",
      "(Epoch 65 / 100) train acc: 0.082000; val_acc: 0.078000\n",
      "(Iteration 49801 / 76500) loss: 2.304664\n",
      "(Iteration 49901 / 76500) loss: 2.304664\n",
      "(Iteration 50001 / 76500) loss: 2.304667\n",
      "(Iteration 50101 / 76500) loss: 2.304683\n",
      "(Iteration 50201 / 76500) loss: 2.304682\n",
      "(Iteration 50301 / 76500) loss: 2.304688\n",
      "(Iteration 50401 / 76500) loss: 2.304676\n",
      "(Epoch 66 / 100) train acc: 0.070000; val_acc: 0.078000\n",
      "(Iteration 50501 / 76500) loss: 2.304672\n",
      "(Iteration 50601 / 76500) loss: 2.304681\n",
      "(Iteration 50701 / 76500) loss: 2.304686\n",
      "(Iteration 50801 / 76500) loss: 2.304660\n",
      "(Iteration 50901 / 76500) loss: 2.304670\n",
      "(Iteration 51001 / 76500) loss: 2.304673\n",
      "(Iteration 51101 / 76500) loss: 2.304685\n",
      "(Iteration 51201 / 76500) loss: 2.304679\n",
      "(Epoch 67 / 100) train acc: 0.079000; val_acc: 0.078000\n",
      "(Iteration 51301 / 76500) loss: 2.304678\n",
      "(Iteration 51401 / 76500) loss: 2.304673\n",
      "(Iteration 51501 / 76500) loss: 2.304678\n",
      "(Iteration 51601 / 76500) loss: 2.304678\n",
      "(Iteration 51701 / 76500) loss: 2.304686\n",
      "(Iteration 51801 / 76500) loss: 2.304665\n",
      "(Iteration 51901 / 76500) loss: 2.304680\n",
      "(Iteration 52001 / 76500) loss: 2.304669\n",
      "(Epoch 68 / 100) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 52101 / 76500) loss: 2.304673\n",
      "(Iteration 52201 / 76500) loss: 2.304666\n",
      "(Iteration 52301 / 76500) loss: 2.304672\n",
      "(Iteration 52401 / 76500) loss: 2.304671\n",
      "(Iteration 52501 / 76500) loss: 2.304677\n",
      "(Iteration 52601 / 76500) loss: 2.304680\n",
      "(Iteration 52701 / 76500) loss: 2.304682\n",
      "(Epoch 69 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 52801 / 76500) loss: 2.304673\n",
      "(Iteration 52901 / 76500) loss: 2.304677\n",
      "(Iteration 53001 / 76500) loss: 2.304679\n",
      "(Iteration 53101 / 76500) loss: 2.304683\n",
      "(Iteration 53201 / 76500) loss: 2.304674\n",
      "(Iteration 53301 / 76500) loss: 2.304691\n",
      "(Iteration 53401 / 76500) loss: 2.304673\n",
      "(Iteration 53501 / 76500) loss: 2.304673\n",
      "(Epoch 70 / 100) train acc: 0.081000; val_acc: 0.078000\n",
      "(Iteration 53601 / 76500) loss: 2.304677\n",
      "(Iteration 53701 / 76500) loss: 2.304676\n",
      "(Iteration 53801 / 76500) loss: 2.304680\n",
      "(Iteration 53901 / 76500) loss: 2.304672\n",
      "(Iteration 54001 / 76500) loss: 2.304677\n",
      "(Iteration 54101 / 76500) loss: 2.304673\n",
      "(Iteration 54201 / 76500) loss: 2.304684\n",
      "(Iteration 54301 / 76500) loss: 2.304659\n",
      "(Epoch 71 / 100) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 54401 / 76500) loss: 2.304675\n",
      "(Iteration 54501 / 76500) loss: 2.304683\n",
      "(Iteration 54601 / 76500) loss: 2.304673\n",
      "(Iteration 54701 / 76500) loss: 2.304673\n",
      "(Iteration 54801 / 76500) loss: 2.304677\n",
      "(Iteration 54901 / 76500) loss: 2.304674\n",
      "(Iteration 55001 / 76500) loss: 2.304676\n",
      "(Epoch 72 / 100) train acc: 0.071000; val_acc: 0.078000\n",
      "(Iteration 55101 / 76500) loss: 2.304672\n",
      "(Iteration 55201 / 76500) loss: 2.304670\n",
      "(Iteration 55301 / 76500) loss: 2.304678\n",
      "(Iteration 55401 / 76500) loss: 2.304678\n",
      "(Iteration 55501 / 76500) loss: 2.304665\n",
      "(Iteration 55601 / 76500) loss: 2.304677\n",
      "(Iteration 55701 / 76500) loss: 2.304686\n",
      "(Iteration 55801 / 76500) loss: 2.304675\n",
      "(Epoch 73 / 100) train acc: 0.086000; val_acc: 0.078000\n",
      "(Iteration 55901 / 76500) loss: 2.304671\n",
      "(Iteration 56001 / 76500) loss: 2.304689\n",
      "(Iteration 56101 / 76500) loss: 2.304680\n",
      "(Iteration 56201 / 76500) loss: 2.304693\n",
      "(Iteration 56301 / 76500) loss: 2.304665\n",
      "(Iteration 56401 / 76500) loss: 2.304666\n",
      "(Iteration 56501 / 76500) loss: 2.304690\n",
      "(Iteration 56601 / 76500) loss: 2.304677\n",
      "(Epoch 74 / 100) train acc: 0.083000; val_acc: 0.078000\n",
      "(Iteration 56701 / 76500) loss: 2.304685\n",
      "(Iteration 56801 / 76500) loss: 2.304667\n",
      "(Iteration 56901 / 76500) loss: 2.304674\n",
      "(Iteration 57001 / 76500) loss: 2.304670\n",
      "(Iteration 57101 / 76500) loss: 2.304673\n",
      "(Iteration 57201 / 76500) loss: 2.304669\n",
      "(Iteration 57301 / 76500) loss: 2.304670\n",
      "(Epoch 75 / 100) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 57401 / 76500) loss: 2.304678\n",
      "(Iteration 57501 / 76500) loss: 2.304681\n",
      "(Iteration 57601 / 76500) loss: 2.304679\n",
      "(Iteration 57701 / 76500) loss: 2.304673\n",
      "(Iteration 57801 / 76500) loss: 2.304680\n",
      "(Iteration 57901 / 76500) loss: 2.304679\n",
      "(Iteration 58001 / 76500) loss: 2.304666\n",
      "(Iteration 58101 / 76500) loss: 2.304674\n",
      "(Epoch 76 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 58201 / 76500) loss: 2.304679\n",
      "(Iteration 58301 / 76500) loss: 2.304673\n",
      "(Iteration 58401 / 76500) loss: 2.304682\n",
      "(Iteration 58501 / 76500) loss: 2.304671\n",
      "(Iteration 58601 / 76500) loss: 2.304679\n",
      "(Iteration 58701 / 76500) loss: 2.304672\n",
      "(Iteration 58801 / 76500) loss: 2.304675\n",
      "(Iteration 58901 / 76500) loss: 2.304681\n",
      "(Epoch 77 / 100) train acc: 0.085000; val_acc: 0.078000\n",
      "(Iteration 59001 / 76500) loss: 2.304676\n",
      "(Iteration 59101 / 76500) loss: 2.304675\n",
      "(Iteration 59201 / 76500) loss: 2.304676\n",
      "(Iteration 59301 / 76500) loss: 2.304674\n",
      "(Iteration 59401 / 76500) loss: 2.304674\n",
      "(Iteration 59501 / 76500) loss: 2.304685\n",
      "(Iteration 59601 / 76500) loss: 2.304681\n",
      "(Epoch 78 / 100) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 59701 / 76500) loss: 2.304687\n",
      "(Iteration 59801 / 76500) loss: 2.304664\n",
      "(Iteration 59901 / 76500) loss: 2.304675\n",
      "(Iteration 60001 / 76500) loss: 2.304666\n",
      "(Iteration 60101 / 76500) loss: 2.304676\n",
      "(Iteration 60201 / 76500) loss: 2.304681\n",
      "(Iteration 60301 / 76500) loss: 2.304669\n",
      "(Iteration 60401 / 76500) loss: 2.304673\n",
      "(Epoch 79 / 100) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 60501 / 76500) loss: 2.304674\n",
      "(Iteration 60601 / 76500) loss: 2.304692\n",
      "(Iteration 60701 / 76500) loss: 2.304677\n",
      "(Iteration 60801 / 76500) loss: 2.304677\n",
      "(Iteration 60901 / 76500) loss: 2.304671\n",
      "(Iteration 61001 / 76500) loss: 2.304675\n",
      "(Iteration 61101 / 76500) loss: 2.304674\n",
      "(Epoch 80 / 100) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 61201 / 76500) loss: 2.304678\n",
      "(Iteration 61301 / 76500) loss: 2.304680\n",
      "(Iteration 61401 / 76500) loss: 2.304680\n",
      "(Iteration 61501 / 76500) loss: 2.304668\n",
      "(Iteration 61601 / 76500) loss: 2.304681\n",
      "(Iteration 61701 / 76500) loss: 2.304677\n",
      "(Iteration 61801 / 76500) loss: 2.304676\n",
      "(Iteration 61901 / 76500) loss: 2.304674\n",
      "(Epoch 81 / 100) train acc: 0.076000; val_acc: 0.078000\n",
      "(Iteration 62001 / 76500) loss: 2.304688\n",
      "(Iteration 62101 / 76500) loss: 2.304677\n",
      "(Iteration 62201 / 76500) loss: 2.304659\n",
      "(Iteration 62301 / 76500) loss: 2.304676\n",
      "(Iteration 62401 / 76500) loss: 2.304666\n",
      "(Iteration 62501 / 76500) loss: 2.304679\n",
      "(Iteration 62601 / 76500) loss: 2.304687\n",
      "(Iteration 62701 / 76500) loss: 2.304681\n",
      "(Epoch 82 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 62801 / 76500) loss: 2.304674\n",
      "(Iteration 62901 / 76500) loss: 2.304671\n",
      "(Iteration 63001 / 76500) loss: 2.304663\n",
      "(Iteration 63101 / 76500) loss: 2.304676\n",
      "(Iteration 63201 / 76500) loss: 2.304681\n",
      "(Iteration 63301 / 76500) loss: 2.304681\n",
      "(Iteration 63401 / 76500) loss: 2.304685\n",
      "(Epoch 83 / 100) train acc: 0.083000; val_acc: 0.078000\n",
      "(Iteration 63501 / 76500) loss: 2.304675\n",
      "(Iteration 63601 / 76500) loss: 2.304674\n",
      "(Iteration 63701 / 76500) loss: 2.304682\n",
      "(Iteration 63801 / 76500) loss: 2.304668\n",
      "(Iteration 63901 / 76500) loss: 2.304673\n",
      "(Iteration 64001 / 76500) loss: 2.304684\n",
      "(Iteration 64101 / 76500) loss: 2.304678\n",
      "(Iteration 64201 / 76500) loss: 2.304683\n",
      "(Epoch 84 / 100) train acc: 0.076000; val_acc: 0.078000\n",
      "(Iteration 64301 / 76500) loss: 2.304667\n",
      "(Iteration 64401 / 76500) loss: 2.304659\n",
      "(Iteration 64501 / 76500) loss: 2.304675\n",
      "(Iteration 64601 / 76500) loss: 2.304673\n",
      "(Iteration 64701 / 76500) loss: 2.304679\n",
      "(Iteration 64801 / 76500) loss: 2.304678\n",
      "(Iteration 64901 / 76500) loss: 2.304673\n",
      "(Iteration 65001 / 76500) loss: 2.304674\n",
      "(Epoch 85 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 65101 / 76500) loss: 2.304673\n",
      "(Iteration 65201 / 76500) loss: 2.304675\n",
      "(Iteration 65301 / 76500) loss: 2.304688\n",
      "(Iteration 65401 / 76500) loss: 2.304680\n",
      "(Iteration 65501 / 76500) loss: 2.304669\n",
      "(Iteration 65601 / 76500) loss: 2.304673\n",
      "(Iteration 65701 / 76500) loss: 2.304681\n",
      "(Epoch 86 / 100) train acc: 0.083000; val_acc: 0.078000\n",
      "(Iteration 65801 / 76500) loss: 2.304675\n",
      "(Iteration 65901 / 76500) loss: 2.304667\n",
      "(Iteration 66001 / 76500) loss: 2.304683\n",
      "(Iteration 66101 / 76500) loss: 2.304669\n",
      "(Iteration 66201 / 76500) loss: 2.304673\n",
      "(Iteration 66301 / 76500) loss: 2.304690\n",
      "(Iteration 66401 / 76500) loss: 2.304678\n",
      "(Iteration 66501 / 76500) loss: 2.304678\n",
      "(Epoch 87 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 66601 / 76500) loss: 2.304684\n",
      "(Iteration 66701 / 76500) loss: 2.304685\n",
      "(Iteration 66801 / 76500) loss: 2.304680\n",
      "(Iteration 66901 / 76500) loss: 2.304662\n",
      "(Iteration 67001 / 76500) loss: 2.304670\n",
      "(Iteration 67101 / 76500) loss: 2.304672\n",
      "(Iteration 67201 / 76500) loss: 2.304681\n",
      "(Iteration 67301 / 76500) loss: 2.304674\n",
      "(Epoch 88 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 67401 / 76500) loss: 2.304679\n",
      "(Iteration 67501 / 76500) loss: 2.304675\n",
      "(Iteration 67601 / 76500) loss: 2.304665\n",
      "(Iteration 67701 / 76500) loss: 2.304674\n",
      "(Iteration 67801 / 76500) loss: 2.304667\n",
      "(Iteration 67901 / 76500) loss: 2.304685\n",
      "(Iteration 68001 / 76500) loss: 2.304682\n",
      "(Epoch 89 / 100) train acc: 0.085000; val_acc: 0.078000\n",
      "(Iteration 68101 / 76500) loss: 2.304683\n",
      "(Iteration 68201 / 76500) loss: 2.304678\n",
      "(Iteration 68301 / 76500) loss: 2.304665\n",
      "(Iteration 68401 / 76500) loss: 2.304676\n",
      "(Iteration 68501 / 76500) loss: 2.304676\n",
      "(Iteration 68601 / 76500) loss: 2.304671\n",
      "(Iteration 68701 / 76500) loss: 2.304668\n",
      "(Iteration 68801 / 76500) loss: 2.304664\n",
      "(Epoch 90 / 100) train acc: 0.075000; val_acc: 0.078000\n",
      "(Iteration 68901 / 76500) loss: 2.304665\n",
      "(Iteration 69001 / 76500) loss: 2.304672\n",
      "(Iteration 69101 / 76500) loss: 2.304677\n",
      "(Iteration 69201 / 76500) loss: 2.304682\n",
      "(Iteration 69301 / 76500) loss: 2.304670\n",
      "(Iteration 69401 / 76500) loss: 2.304666\n",
      "(Iteration 69501 / 76500) loss: 2.304661\n",
      "(Iteration 69601 / 76500) loss: 2.304673\n",
      "(Epoch 91 / 100) train acc: 0.085000; val_acc: 0.078000\n",
      "(Iteration 69701 / 76500) loss: 2.304661\n",
      "(Iteration 69801 / 76500) loss: 2.304684\n",
      "(Iteration 69901 / 76500) loss: 2.304675\n",
      "(Iteration 70001 / 76500) loss: 2.304692\n",
      "(Iteration 70101 / 76500) loss: 2.304683\n",
      "(Iteration 70201 / 76500) loss: 2.304678\n",
      "(Iteration 70301 / 76500) loss: 2.304688\n",
      "(Epoch 92 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 70401 / 76500) loss: 2.304658\n",
      "(Iteration 70501 / 76500) loss: 2.304685\n",
      "(Iteration 70601 / 76500) loss: 2.304684\n",
      "(Iteration 70701 / 76500) loss: 2.304680\n",
      "(Iteration 70801 / 76500) loss: 2.304679\n",
      "(Iteration 70901 / 76500) loss: 2.304669\n",
      "(Iteration 71001 / 76500) loss: 2.304682\n",
      "(Iteration 71101 / 76500) loss: 2.304678\n",
      "(Epoch 93 / 100) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 71201 / 76500) loss: 2.304686\n",
      "(Iteration 71301 / 76500) loss: 2.304677\n",
      "(Iteration 71401 / 76500) loss: 2.304688\n",
      "(Iteration 71501 / 76500) loss: 2.304694\n",
      "(Iteration 71601 / 76500) loss: 2.304667\n",
      "(Iteration 71701 / 76500) loss: 2.304679\n",
      "(Iteration 71801 / 76500) loss: 2.304686\n",
      "(Iteration 71901 / 76500) loss: 2.304692\n",
      "(Epoch 94 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 72001 / 76500) loss: 2.304679\n",
      "(Iteration 72101 / 76500) loss: 2.304666\n",
      "(Iteration 72201 / 76500) loss: 2.304658\n",
      "(Iteration 72301 / 76500) loss: 2.304688\n",
      "(Iteration 72401 / 76500) loss: 2.304679\n",
      "(Iteration 72501 / 76500) loss: 2.304681\n",
      "(Iteration 72601 / 76500) loss: 2.304678\n",
      "(Epoch 95 / 100) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 72701 / 76500) loss: 2.304676\n",
      "(Iteration 72801 / 76500) loss: 2.304680\n",
      "(Iteration 72901 / 76500) loss: 2.304682\n",
      "(Iteration 73001 / 76500) loss: 2.304671\n",
      "(Iteration 73101 / 76500) loss: 2.304682\n",
      "(Iteration 73201 / 76500) loss: 2.304680\n",
      "(Iteration 73301 / 76500) loss: 2.304671\n",
      "(Iteration 73401 / 76500) loss: 2.304677\n",
      "(Epoch 96 / 100) train acc: 0.075000; val_acc: 0.078000\n",
      "(Iteration 73501 / 76500) loss: 2.304666\n",
      "(Iteration 73601 / 76500) loss: 2.304674\n",
      "(Iteration 73701 / 76500) loss: 2.304673\n",
      "(Iteration 73801 / 76500) loss: 2.304679\n",
      "(Iteration 73901 / 76500) loss: 2.304662\n",
      "(Iteration 74001 / 76500) loss: 2.304671\n",
      "(Iteration 74101 / 76500) loss: 2.304676\n",
      "(Iteration 74201 / 76500) loss: 2.304681\n",
      "(Epoch 97 / 100) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 74301 / 76500) loss: 2.304649\n",
      "(Iteration 74401 / 76500) loss: 2.304684\n",
      "(Iteration 74501 / 76500) loss: 2.304685\n",
      "(Iteration 74601 / 76500) loss: 2.304683\n",
      "(Iteration 74701 / 76500) loss: 2.304679\n",
      "(Iteration 74801 / 76500) loss: 2.304694\n",
      "(Iteration 74901 / 76500) loss: 2.304673\n",
      "(Epoch 98 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 75001 / 76500) loss: 2.304672\n",
      "(Iteration 75101 / 76500) loss: 2.304668\n",
      "(Iteration 75201 / 76500) loss: 2.304674\n",
      "(Iteration 75301 / 76500) loss: 2.304676\n",
      "(Iteration 75401 / 76500) loss: 2.304674\n",
      "(Iteration 75501 / 76500) loss: 2.304676\n",
      "(Iteration 75601 / 76500) loss: 2.304661\n",
      "(Iteration 75701 / 76500) loss: 2.304676\n",
      "(Epoch 99 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 75801 / 76500) loss: 2.304670\n",
      "(Iteration 75901 / 76500) loss: 2.304680\n",
      "(Iteration 76001 / 76500) loss: 2.304673\n",
      "(Iteration 76101 / 76500) loss: 2.304675\n",
      "(Iteration 76201 / 76500) loss: 2.304670\n",
      "(Iteration 76301 / 76500) loss: 2.304688\n",
      "(Iteration 76401 / 76500) loss: 2.304678\n",
      "(Epoch 100 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.304678\n",
      "(Epoch 0 / 100) train acc: 0.090000; val_acc: 0.112000\n",
      "(Iteration 101 / 38200) loss: 2.304676\n",
      "(Iteration 201 / 38200) loss: 2.304686\n",
      "(Iteration 301 / 38200) loss: 2.304679\n",
      "(Epoch 1 / 100) train acc: 0.095000; val_acc: 0.112000\n",
      "(Iteration 401 / 38200) loss: 2.304683\n",
      "(Iteration 501 / 38200) loss: 2.304677\n",
      "(Iteration 601 / 38200) loss: 2.304682\n",
      "(Iteration 701 / 38200) loss: 2.304681\n",
      "(Epoch 2 / 100) train acc: 0.079000; val_acc: 0.112000\n",
      "(Iteration 801 / 38200) loss: 2.304682\n",
      "(Iteration 901 / 38200) loss: 2.304672\n",
      "(Iteration 1001 / 38200) loss: 2.304687\n",
      "(Iteration 1101 / 38200) loss: 2.304683\n",
      "(Epoch 3 / 100) train acc: 0.085000; val_acc: 0.112000\n",
      "(Iteration 1201 / 38200) loss: 2.304682\n",
      "(Iteration 1301 / 38200) loss: 2.304690\n",
      "(Iteration 1401 / 38200) loss: 2.304693\n",
      "(Iteration 1501 / 38200) loss: 2.304681\n",
      "(Epoch 4 / 100) train acc: 0.098000; val_acc: 0.112000\n",
      "(Iteration 1601 / 38200) loss: 2.304685\n",
      "(Iteration 1701 / 38200) loss: 2.304683\n",
      "(Iteration 1801 / 38200) loss: 2.304677\n",
      "(Iteration 1901 / 38200) loss: 2.304681\n",
      "(Epoch 5 / 100) train acc: 0.087000; val_acc: 0.112000\n",
      "(Iteration 2001 / 38200) loss: 2.304682\n",
      "(Iteration 2101 / 38200) loss: 2.304676\n",
      "(Iteration 2201 / 38200) loss: 2.304681\n",
      "(Epoch 6 / 100) train acc: 0.110000; val_acc: 0.112000\n",
      "(Iteration 2301 / 38200) loss: 2.304676\n",
      "(Iteration 2401 / 38200) loss: 2.304687\n",
      "(Iteration 2501 / 38200) loss: 2.304682\n",
      "(Iteration 2601 / 38200) loss: 2.304686\n",
      "(Epoch 7 / 100) train acc: 0.107000; val_acc: 0.112000\n",
      "(Iteration 2701 / 38200) loss: 2.304682\n",
      "(Iteration 2801 / 38200) loss: 2.304677\n",
      "(Iteration 2901 / 38200) loss: 2.304685\n",
      "(Iteration 3001 / 38200) loss: 2.304685\n",
      "(Epoch 8 / 100) train acc: 0.096000; val_acc: 0.112000\n",
      "(Iteration 3101 / 38200) loss: 2.304685\n",
      "(Iteration 3201 / 38200) loss: 2.304677\n",
      "(Iteration 3301 / 38200) loss: 2.304680\n",
      "(Iteration 3401 / 38200) loss: 2.304675\n",
      "(Epoch 9 / 100) train acc: 0.111000; val_acc: 0.112000\n",
      "(Iteration 3501 / 38200) loss: 2.304690\n",
      "(Iteration 3601 / 38200) loss: 2.304678\n",
      "(Iteration 3701 / 38200) loss: 2.304675\n",
      "(Iteration 3801 / 38200) loss: 2.304690\n",
      "(Epoch 10 / 100) train acc: 0.085000; val_acc: 0.112000\n",
      "(Iteration 3901 / 38200) loss: 2.304681\n",
      "(Iteration 4001 / 38200) loss: 2.304686\n",
      "(Iteration 4101 / 38200) loss: 2.304684\n",
      "(Iteration 4201 / 38200) loss: 2.304688\n",
      "(Epoch 11 / 100) train acc: 0.098000; val_acc: 0.112000\n",
      "(Iteration 4301 / 38200) loss: 2.304674\n",
      "(Iteration 4401 / 38200) loss: 2.304685\n",
      "(Iteration 4501 / 38200) loss: 2.304691\n",
      "(Epoch 12 / 100) train acc: 0.116000; val_acc: 0.112000\n",
      "(Iteration 4601 / 38200) loss: 2.304678\n",
      "(Iteration 4701 / 38200) loss: 2.304691\n",
      "(Iteration 4801 / 38200) loss: 2.304685\n",
      "(Iteration 4901 / 38200) loss: 2.304673\n",
      "(Epoch 13 / 100) train acc: 0.097000; val_acc: 0.112000\n",
      "(Iteration 5001 / 38200) loss: 2.304681\n",
      "(Iteration 5101 / 38200) loss: 2.304695\n",
      "(Iteration 5201 / 38200) loss: 2.304687\n",
      "(Iteration 5301 / 38200) loss: 2.304679\n",
      "(Epoch 14 / 100) train acc: 0.092000; val_acc: 0.112000\n",
      "(Iteration 5401 / 38200) loss: 2.304674\n",
      "(Iteration 5501 / 38200) loss: 2.304681\n",
      "(Iteration 5601 / 38200) loss: 2.304675\n",
      "(Iteration 5701 / 38200) loss: 2.304685\n",
      "(Epoch 15 / 100) train acc: 0.101000; val_acc: 0.112000\n",
      "(Iteration 5801 / 38200) loss: 2.304678\n",
      "(Iteration 5901 / 38200) loss: 2.304679\n",
      "(Iteration 6001 / 38200) loss: 2.304681\n",
      "(Iteration 6101 / 38200) loss: 2.304681\n",
      "(Epoch 16 / 100) train acc: 0.092000; val_acc: 0.112000\n",
      "(Iteration 6201 / 38200) loss: 2.304682\n",
      "(Iteration 6301 / 38200) loss: 2.304683\n",
      "(Iteration 6401 / 38200) loss: 2.304679\n",
      "(Epoch 17 / 100) train acc: 0.080000; val_acc: 0.112000\n",
      "(Iteration 6501 / 38200) loss: 2.304672\n",
      "(Iteration 6601 / 38200) loss: 2.304684\n",
      "(Iteration 6701 / 38200) loss: 2.304682\n",
      "(Iteration 6801 / 38200) loss: 2.304682\n",
      "(Epoch 18 / 100) train acc: 0.088000; val_acc: 0.112000\n",
      "(Iteration 6901 / 38200) loss: 2.304683\n",
      "(Iteration 7001 / 38200) loss: 2.304678\n",
      "(Iteration 7101 / 38200) loss: 2.304687\n",
      "(Iteration 7201 / 38200) loss: 2.304688\n",
      "(Epoch 19 / 100) train acc: 0.089000; val_acc: 0.112000\n",
      "(Iteration 7301 / 38200) loss: 2.304685\n",
      "(Iteration 7401 / 38200) loss: 2.304679\n",
      "(Iteration 7501 / 38200) loss: 2.304680\n",
      "(Iteration 7601 / 38200) loss: 2.304681\n",
      "(Epoch 20 / 100) train acc: 0.097000; val_acc: 0.112000\n",
      "(Iteration 7701 / 38200) loss: 2.304685\n",
      "(Iteration 7801 / 38200) loss: 2.304689\n",
      "(Iteration 7901 / 38200) loss: 2.304679\n",
      "(Iteration 8001 / 38200) loss: 2.304681\n",
      "(Epoch 21 / 100) train acc: 0.090000; val_acc: 0.112000\n",
      "(Iteration 8101 / 38200) loss: 2.304681\n",
      "(Iteration 8201 / 38200) loss: 2.304682\n",
      "(Iteration 8301 / 38200) loss: 2.304682\n",
      "(Iteration 8401 / 38200) loss: 2.304681\n",
      "(Epoch 22 / 100) train acc: 0.091000; val_acc: 0.112000\n",
      "(Iteration 8501 / 38200) loss: 2.304681\n",
      "(Iteration 8601 / 38200) loss: 2.304680\n",
      "(Iteration 8701 / 38200) loss: 2.304677\n",
      "(Epoch 23 / 100) train acc: 0.101000; val_acc: 0.112000\n",
      "(Iteration 8801 / 38200) loss: 2.304683\n",
      "(Iteration 8901 / 38200) loss: 2.304678\n",
      "(Iteration 9001 / 38200) loss: 2.304684\n",
      "(Iteration 9101 / 38200) loss: 2.304676\n",
      "(Epoch 24 / 100) train acc: 0.091000; val_acc: 0.112000\n",
      "(Iteration 9201 / 38200) loss: 2.304687\n",
      "(Iteration 9301 / 38200) loss: 2.304676\n",
      "(Iteration 9401 / 38200) loss: 2.304673\n",
      "(Iteration 9501 / 38200) loss: 2.304684\n",
      "(Epoch 25 / 100) train acc: 0.102000; val_acc: 0.112000\n",
      "(Iteration 9601 / 38200) loss: 2.304680\n",
      "(Iteration 9701 / 38200) loss: 2.304687\n",
      "(Iteration 9801 / 38200) loss: 2.304678\n",
      "(Iteration 9901 / 38200) loss: 2.304687\n",
      "(Epoch 26 / 100) train acc: 0.098000; val_acc: 0.112000\n",
      "(Iteration 10001 / 38200) loss: 2.304682\n",
      "(Iteration 10101 / 38200) loss: 2.304687\n",
      "(Iteration 10201 / 38200) loss: 2.304674\n",
      "(Iteration 10301 / 38200) loss: 2.304684\n",
      "(Epoch 27 / 100) train acc: 0.108000; val_acc: 0.112000\n",
      "(Iteration 10401 / 38200) loss: 2.304684\n",
      "(Iteration 10501 / 38200) loss: 2.304675\n",
      "(Iteration 10601 / 38200) loss: 2.304677\n",
      "(Epoch 28 / 100) train acc: 0.114000; val_acc: 0.112000\n",
      "(Iteration 10701 / 38200) loss: 2.304675\n",
      "(Iteration 10801 / 38200) loss: 2.304682\n",
      "(Iteration 10901 / 38200) loss: 2.304693\n",
      "(Iteration 11001 / 38200) loss: 2.304684\n",
      "(Epoch 29 / 100) train acc: 0.095000; val_acc: 0.112000\n",
      "(Iteration 11101 / 38200) loss: 2.304667\n",
      "(Iteration 11201 / 38200) loss: 2.304679\n",
      "(Iteration 11301 / 38200) loss: 2.304688\n",
      "(Iteration 11401 / 38200) loss: 2.304675\n",
      "(Epoch 30 / 100) train acc: 0.089000; val_acc: 0.112000\n",
      "(Iteration 11501 / 38200) loss: 2.304679\n",
      "(Iteration 11601 / 38200) loss: 2.304678\n",
      "(Iteration 11701 / 38200) loss: 2.304675\n",
      "(Iteration 11801 / 38200) loss: 2.304681\n",
      "(Epoch 31 / 100) train acc: 0.110000; val_acc: 0.112000\n",
      "(Iteration 11901 / 38200) loss: 2.304683\n",
      "(Iteration 12001 / 38200) loss: 2.304676\n",
      "(Iteration 12101 / 38200) loss: 2.304676\n",
      "(Iteration 12201 / 38200) loss: 2.304683\n",
      "(Epoch 32 / 100) train acc: 0.100000; val_acc: 0.112000\n",
      "(Iteration 12301 / 38200) loss: 2.304678\n",
      "(Iteration 12401 / 38200) loss: 2.304679\n",
      "(Iteration 12501 / 38200) loss: 2.304684\n",
      "(Iteration 12601 / 38200) loss: 2.304672\n",
      "(Epoch 33 / 100) train acc: 0.089000; val_acc: 0.112000\n",
      "(Iteration 12701 / 38200) loss: 2.304684\n",
      "(Iteration 12801 / 38200) loss: 2.304683\n",
      "(Iteration 12901 / 38200) loss: 2.304684\n",
      "(Epoch 34 / 100) train acc: 0.092000; val_acc: 0.112000\n",
      "(Iteration 13001 / 38200) loss: 2.304676\n",
      "(Iteration 13101 / 38200) loss: 2.304695\n",
      "(Iteration 13201 / 38200) loss: 2.304678\n",
      "(Iteration 13301 / 38200) loss: 2.304675\n",
      "(Epoch 35 / 100) train acc: 0.099000; val_acc: 0.112000\n",
      "(Iteration 13401 / 38200) loss: 2.304677\n",
      "(Iteration 13501 / 38200) loss: 2.304679\n",
      "(Iteration 13601 / 38200) loss: 2.304682\n",
      "(Iteration 13701 / 38200) loss: 2.304686\n",
      "(Epoch 36 / 100) train acc: 0.079000; val_acc: 0.112000\n",
      "(Iteration 13801 / 38200) loss: 2.304683\n",
      "(Iteration 13901 / 38200) loss: 2.304693\n",
      "(Iteration 14001 / 38200) loss: 2.304675\n",
      "(Iteration 14101 / 38200) loss: 2.304675\n",
      "(Epoch 37 / 100) train acc: 0.077000; val_acc: 0.112000\n",
      "(Iteration 14201 / 38200) loss: 2.304686\n",
      "(Iteration 14301 / 38200) loss: 2.304674\n",
      "(Iteration 14401 / 38200) loss: 2.304685\n",
      "(Iteration 14501 / 38200) loss: 2.304682\n",
      "(Epoch 38 / 100) train acc: 0.105000; val_acc: 0.112000\n",
      "(Iteration 14601 / 38200) loss: 2.304680\n",
      "(Iteration 14701 / 38200) loss: 2.304680\n",
      "(Iteration 14801 / 38200) loss: 2.304675\n",
      "(Epoch 39 / 100) train acc: 0.106000; val_acc: 0.112000\n",
      "(Iteration 14901 / 38200) loss: 2.304681\n",
      "(Iteration 15001 / 38200) loss: 2.304681\n",
      "(Iteration 15101 / 38200) loss: 2.304676\n",
      "(Iteration 15201 / 38200) loss: 2.304687\n",
      "(Epoch 40 / 100) train acc: 0.101000; val_acc: 0.112000\n",
      "(Iteration 15301 / 38200) loss: 2.304677\n",
      "(Iteration 15401 / 38200) loss: 2.304683\n",
      "(Iteration 15501 / 38200) loss: 2.304682\n",
      "(Iteration 15601 / 38200) loss: 2.304694\n",
      "(Epoch 41 / 100) train acc: 0.098000; val_acc: 0.112000\n",
      "(Iteration 15701 / 38200) loss: 2.304689\n",
      "(Iteration 15801 / 38200) loss: 2.304677\n",
      "(Iteration 15901 / 38200) loss: 2.304681\n",
      "(Iteration 16001 / 38200) loss: 2.304683\n",
      "(Epoch 42 / 100) train acc: 0.094000; val_acc: 0.112000\n",
      "(Iteration 16101 / 38200) loss: 2.304684\n",
      "(Iteration 16201 / 38200) loss: 2.304681\n",
      "(Iteration 16301 / 38200) loss: 2.304686\n",
      "(Iteration 16401 / 38200) loss: 2.304685\n",
      "(Epoch 43 / 100) train acc: 0.100000; val_acc: 0.112000\n",
      "(Iteration 16501 / 38200) loss: 2.304673\n",
      "(Iteration 16601 / 38200) loss: 2.304682\n",
      "(Iteration 16701 / 38200) loss: 2.304686\n",
      "(Iteration 16801 / 38200) loss: 2.304675\n",
      "(Epoch 44 / 100) train acc: 0.080000; val_acc: 0.112000\n",
      "(Iteration 16901 / 38200) loss: 2.304684\n",
      "(Iteration 17001 / 38200) loss: 2.304686\n",
      "(Iteration 17101 / 38200) loss: 2.304680\n",
      "(Epoch 45 / 100) train acc: 0.100000; val_acc: 0.112000\n",
      "(Iteration 17201 / 38200) loss: 2.304683\n",
      "(Iteration 17301 / 38200) loss: 2.304677\n",
      "(Iteration 17401 / 38200) loss: 2.304678\n",
      "(Iteration 17501 / 38200) loss: 2.304690\n",
      "(Epoch 46 / 100) train acc: 0.104000; val_acc: 0.112000\n",
      "(Iteration 17601 / 38200) loss: 2.304681\n",
      "(Iteration 17701 / 38200) loss: 2.304681\n",
      "(Iteration 17801 / 38200) loss: 2.304685\n",
      "(Iteration 17901 / 38200) loss: 2.304683\n",
      "(Epoch 47 / 100) train acc: 0.090000; val_acc: 0.112000\n",
      "(Iteration 18001 / 38200) loss: 2.304685\n",
      "(Iteration 18101 / 38200) loss: 2.304685\n",
      "(Iteration 18201 / 38200) loss: 2.304688\n",
      "(Iteration 18301 / 38200) loss: 2.304678\n",
      "(Epoch 48 / 100) train acc: 0.106000; val_acc: 0.112000\n",
      "(Iteration 18401 / 38200) loss: 2.304685\n",
      "(Iteration 18501 / 38200) loss: 2.304679\n",
      "(Iteration 18601 / 38200) loss: 2.304677\n",
      "(Iteration 18701 / 38200) loss: 2.304686\n",
      "(Epoch 49 / 100) train acc: 0.092000; val_acc: 0.112000\n",
      "(Iteration 18801 / 38200) loss: 2.304691\n",
      "(Iteration 18901 / 38200) loss: 2.304684\n",
      "(Iteration 19001 / 38200) loss: 2.304681\n",
      "(Epoch 50 / 100) train acc: 0.115000; val_acc: 0.112000\n",
      "(Iteration 19101 / 38200) loss: 2.304682\n",
      "(Iteration 19201 / 38200) loss: 2.304682\n",
      "(Iteration 19301 / 38200) loss: 2.304674\n",
      "(Iteration 19401 / 38200) loss: 2.304686\n",
      "(Epoch 51 / 100) train acc: 0.096000; val_acc: 0.112000\n",
      "(Iteration 19501 / 38200) loss: 2.304682\n",
      "(Iteration 19601 / 38200) loss: 2.304677\n",
      "(Iteration 19701 / 38200) loss: 2.304678\n",
      "(Iteration 19801 / 38200) loss: 2.304675\n",
      "(Epoch 52 / 100) train acc: 0.103000; val_acc: 0.112000\n",
      "(Iteration 19901 / 38200) loss: 2.304686\n",
      "(Iteration 20001 / 38200) loss: 2.304682\n",
      "(Iteration 20101 / 38200) loss: 2.304683\n",
      "(Iteration 20201 / 38200) loss: 2.304676\n",
      "(Epoch 53 / 100) train acc: 0.102000; val_acc: 0.112000\n",
      "(Iteration 20301 / 38200) loss: 2.304684\n",
      "(Iteration 20401 / 38200) loss: 2.304681\n",
      "(Iteration 20501 / 38200) loss: 2.304692\n",
      "(Iteration 20601 / 38200) loss: 2.304669\n",
      "(Epoch 54 / 100) train acc: 0.097000; val_acc: 0.112000\n",
      "(Iteration 20701 / 38200) loss: 2.304676\n",
      "(Iteration 20801 / 38200) loss: 2.304681\n",
      "(Iteration 20901 / 38200) loss: 2.304684\n",
      "(Iteration 21001 / 38200) loss: 2.304679\n",
      "(Epoch 55 / 100) train acc: 0.090000; val_acc: 0.112000\n",
      "(Iteration 21101 / 38200) loss: 2.304678\n",
      "(Iteration 21201 / 38200) loss: 2.304678\n",
      "(Iteration 21301 / 38200) loss: 2.304681\n",
      "(Epoch 56 / 100) train acc: 0.095000; val_acc: 0.112000\n",
      "(Iteration 21401 / 38200) loss: 2.304682\n",
      "(Iteration 21501 / 38200) loss: 2.304672\n",
      "(Iteration 21601 / 38200) loss: 2.304685\n",
      "(Iteration 21701 / 38200) loss: 2.304680\n",
      "(Epoch 57 / 100) train acc: 0.101000; val_acc: 0.112000\n",
      "(Iteration 21801 / 38200) loss: 2.304681\n",
      "(Iteration 21901 / 38200) loss: 2.304687\n",
      "(Iteration 22001 / 38200) loss: 2.304681\n",
      "(Iteration 22101 / 38200) loss: 2.304671\n",
      "(Epoch 58 / 100) train acc: 0.096000; val_acc: 0.112000\n",
      "(Iteration 22201 / 38200) loss: 2.304681\n",
      "(Iteration 22301 / 38200) loss: 2.304684\n",
      "(Iteration 22401 / 38200) loss: 2.304683\n",
      "(Iteration 22501 / 38200) loss: 2.304689\n",
      "(Epoch 59 / 100) train acc: 0.081000; val_acc: 0.112000\n",
      "(Iteration 22601 / 38200) loss: 2.304684\n",
      "(Iteration 22701 / 38200) loss: 2.304681\n",
      "(Iteration 22801 / 38200) loss: 2.304678\n",
      "(Iteration 22901 / 38200) loss: 2.304680\n",
      "(Epoch 60 / 100) train acc: 0.105000; val_acc: 0.112000\n",
      "(Iteration 23001 / 38200) loss: 2.304687\n",
      "(Iteration 23101 / 38200) loss: 2.304686\n",
      "(Iteration 23201 / 38200) loss: 2.304677\n",
      "(Iteration 23301 / 38200) loss: 2.304676\n",
      "(Epoch 61 / 100) train acc: 0.098000; val_acc: 0.112000\n",
      "(Iteration 23401 / 38200) loss: 2.304675\n",
      "(Iteration 23501 / 38200) loss: 2.304682\n",
      "(Iteration 23601 / 38200) loss: 2.304679\n",
      "(Epoch 62 / 100) train acc: 0.092000; val_acc: 0.112000\n",
      "(Iteration 23701 / 38200) loss: 2.304689\n",
      "(Iteration 23801 / 38200) loss: 2.304678\n",
      "(Iteration 23901 / 38200) loss: 2.304669\n",
      "(Iteration 24001 / 38200) loss: 2.304678\n",
      "(Epoch 63 / 100) train acc: 0.088000; val_acc: 0.112000\n",
      "(Iteration 24101 / 38200) loss: 2.304675\n",
      "(Iteration 24201 / 38200) loss: 2.304685\n",
      "(Iteration 24301 / 38200) loss: 2.304679\n",
      "(Iteration 24401 / 38200) loss: 2.304681\n",
      "(Epoch 64 / 100) train acc: 0.086000; val_acc: 0.112000\n",
      "(Iteration 24501 / 38200) loss: 2.304685\n",
      "(Iteration 24601 / 38200) loss: 2.304675\n",
      "(Iteration 24701 / 38200) loss: 2.304679\n",
      "(Iteration 24801 / 38200) loss: 2.304673\n",
      "(Epoch 65 / 100) train acc: 0.096000; val_acc: 0.112000\n",
      "(Iteration 24901 / 38200) loss: 2.304683\n",
      "(Iteration 25001 / 38200) loss: 2.304682\n",
      "(Iteration 25101 / 38200) loss: 2.304677\n",
      "(Iteration 25201 / 38200) loss: 2.304675\n",
      "(Epoch 66 / 100) train acc: 0.116000; val_acc: 0.112000\n",
      "(Iteration 25301 / 38200) loss: 2.304679\n",
      "(Iteration 25401 / 38200) loss: 2.304690\n",
      "(Iteration 25501 / 38200) loss: 2.304680\n",
      "(Epoch 67 / 100) train acc: 0.079000; val_acc: 0.112000\n",
      "(Iteration 25601 / 38200) loss: 2.304680\n",
      "(Iteration 25701 / 38200) loss: 2.304683\n",
      "(Iteration 25801 / 38200) loss: 2.304681\n",
      "(Iteration 25901 / 38200) loss: 2.304683\n",
      "(Epoch 68 / 100) train acc: 0.086000; val_acc: 0.112000\n",
      "(Iteration 26001 / 38200) loss: 2.304680\n",
      "(Iteration 26101 / 38200) loss: 2.304680\n",
      "(Iteration 26201 / 38200) loss: 2.304688\n",
      "(Iteration 26301 / 38200) loss: 2.304677\n",
      "(Epoch 69 / 100) train acc: 0.109000; val_acc: 0.112000\n",
      "(Iteration 26401 / 38200) loss: 2.304686\n",
      "(Iteration 26501 / 38200) loss: 2.304680\n",
      "(Iteration 26601 / 38200) loss: 2.304680\n",
      "(Iteration 26701 / 38200) loss: 2.304689\n",
      "(Epoch 70 / 100) train acc: 0.109000; val_acc: 0.112000\n",
      "(Iteration 26801 / 38200) loss: 2.304681\n",
      "(Iteration 26901 / 38200) loss: 2.304685\n",
      "(Iteration 27001 / 38200) loss: 2.304686\n",
      "(Iteration 27101 / 38200) loss: 2.304687\n",
      "(Epoch 71 / 100) train acc: 0.092000; val_acc: 0.112000\n",
      "(Iteration 27201 / 38200) loss: 2.304690\n",
      "(Iteration 27301 / 38200) loss: 2.304686\n",
      "(Iteration 27401 / 38200) loss: 2.304682\n",
      "(Iteration 27501 / 38200) loss: 2.304685\n",
      "(Epoch 72 / 100) train acc: 0.097000; val_acc: 0.112000\n",
      "(Iteration 27601 / 38200) loss: 2.304682\n",
      "(Iteration 27701 / 38200) loss: 2.304677\n",
      "(Iteration 27801 / 38200) loss: 2.304687\n",
      "(Epoch 73 / 100) train acc: 0.089000; val_acc: 0.112000\n",
      "(Iteration 27901 / 38200) loss: 2.304677\n",
      "(Iteration 28001 / 38200) loss: 2.304675\n",
      "(Iteration 28101 / 38200) loss: 2.304686\n",
      "(Iteration 28201 / 38200) loss: 2.304674\n",
      "(Epoch 74 / 100) train acc: 0.075000; val_acc: 0.112000\n",
      "(Iteration 28301 / 38200) loss: 2.304672\n",
      "(Iteration 28401 / 38200) loss: 2.304682\n",
      "(Iteration 28501 / 38200) loss: 2.304686\n",
      "(Iteration 28601 / 38200) loss: 2.304676\n",
      "(Epoch 75 / 100) train acc: 0.102000; val_acc: 0.112000\n",
      "(Iteration 28701 / 38200) loss: 2.304683\n",
      "(Iteration 28801 / 38200) loss: 2.304683\n",
      "(Iteration 28901 / 38200) loss: 2.304683\n",
      "(Iteration 29001 / 38200) loss: 2.304682\n",
      "(Epoch 76 / 100) train acc: 0.087000; val_acc: 0.112000\n",
      "(Iteration 29101 / 38200) loss: 2.304681\n",
      "(Iteration 29201 / 38200) loss: 2.304685\n",
      "(Iteration 29301 / 38200) loss: 2.304673\n",
      "(Iteration 29401 / 38200) loss: 2.304680\n",
      "(Epoch 77 / 100) train acc: 0.093000; val_acc: 0.112000\n",
      "(Iteration 29501 / 38200) loss: 2.304671\n",
      "(Iteration 29601 / 38200) loss: 2.304676\n",
      "(Iteration 29701 / 38200) loss: 2.304675\n",
      "(Epoch 78 / 100) train acc: 0.093000; val_acc: 0.112000\n",
      "(Iteration 29801 / 38200) loss: 2.304685\n",
      "(Iteration 29901 / 38200) loss: 2.304684\n",
      "(Iteration 30001 / 38200) loss: 2.304680\n",
      "(Iteration 30101 / 38200) loss: 2.304677\n",
      "(Epoch 79 / 100) train acc: 0.109000; val_acc: 0.112000\n",
      "(Iteration 30201 / 38200) loss: 2.304677\n",
      "(Iteration 30301 / 38200) loss: 2.304679\n",
      "(Iteration 30401 / 38200) loss: 2.304679\n",
      "(Iteration 30501 / 38200) loss: 2.304678\n",
      "(Epoch 80 / 100) train acc: 0.101000; val_acc: 0.112000\n",
      "(Iteration 30601 / 38200) loss: 2.304679\n",
      "(Iteration 30701 / 38200) loss: 2.304687\n",
      "(Iteration 30801 / 38200) loss: 2.304690\n",
      "(Iteration 30901 / 38200) loss: 2.304677\n",
      "(Epoch 81 / 100) train acc: 0.096000; val_acc: 0.112000\n",
      "(Iteration 31001 / 38200) loss: 2.304676\n",
      "(Iteration 31101 / 38200) loss: 2.304685\n",
      "(Iteration 31201 / 38200) loss: 2.304677\n",
      "(Iteration 31301 / 38200) loss: 2.304685\n",
      "(Epoch 82 / 100) train acc: 0.095000; val_acc: 0.112000\n",
      "(Iteration 31401 / 38200) loss: 2.304678\n",
      "(Iteration 31501 / 38200) loss: 2.304678\n",
      "(Iteration 31601 / 38200) loss: 2.304686\n",
      "(Iteration 31701 / 38200) loss: 2.304688\n",
      "(Epoch 83 / 100) train acc: 0.089000; val_acc: 0.112000\n",
      "(Iteration 31801 / 38200) loss: 2.304678\n",
      "(Iteration 31901 / 38200) loss: 2.304684\n",
      "(Iteration 32001 / 38200) loss: 2.304682\n",
      "(Epoch 84 / 100) train acc: 0.100000; val_acc: 0.112000\n",
      "(Iteration 32101 / 38200) loss: 2.304683\n",
      "(Iteration 32201 / 38200) loss: 2.304681\n",
      "(Iteration 32301 / 38200) loss: 2.304684\n",
      "(Iteration 32401 / 38200) loss: 2.304680\n",
      "(Epoch 85 / 100) train acc: 0.101000; val_acc: 0.112000\n",
      "(Iteration 32501 / 38200) loss: 2.304677\n",
      "(Iteration 32601 / 38200) loss: 2.304674\n",
      "(Iteration 32701 / 38200) loss: 2.304676\n",
      "(Iteration 32801 / 38200) loss: 2.304677\n",
      "(Epoch 86 / 100) train acc: 0.107000; val_acc: 0.112000\n",
      "(Iteration 32901 / 38200) loss: 2.304679\n",
      "(Iteration 33001 / 38200) loss: 2.304677\n",
      "(Iteration 33101 / 38200) loss: 2.304690\n",
      "(Iteration 33201 / 38200) loss: 2.304678\n",
      "(Epoch 87 / 100) train acc: 0.084000; val_acc: 0.112000\n",
      "(Iteration 33301 / 38200) loss: 2.304679\n",
      "(Iteration 33401 / 38200) loss: 2.304682\n",
      "(Iteration 33501 / 38200) loss: 2.304675\n",
      "(Iteration 33601 / 38200) loss: 2.304680\n",
      "(Epoch 88 / 100) train acc: 0.084000; val_acc: 0.112000\n",
      "(Iteration 33701 / 38200) loss: 2.304678\n",
      "(Iteration 33801 / 38200) loss: 2.304678\n",
      "(Iteration 33901 / 38200) loss: 2.304682\n",
      "(Epoch 89 / 100) train acc: 0.098000; val_acc: 0.112000\n",
      "(Iteration 34001 / 38200) loss: 2.304676\n",
      "(Iteration 34101 / 38200) loss: 2.304679\n",
      "(Iteration 34201 / 38200) loss: 2.304682\n",
      "(Iteration 34301 / 38200) loss: 2.304681\n",
      "(Epoch 90 / 100) train acc: 0.089000; val_acc: 0.112000\n",
      "(Iteration 34401 / 38200) loss: 2.304682\n",
      "(Iteration 34501 / 38200) loss: 2.304689\n",
      "(Iteration 34601 / 38200) loss: 2.304681\n",
      "(Iteration 34701 / 38200) loss: 2.304690\n",
      "(Epoch 91 / 100) train acc: 0.094000; val_acc: 0.112000\n",
      "(Iteration 34801 / 38200) loss: 2.304686\n",
      "(Iteration 34901 / 38200) loss: 2.304682\n",
      "(Iteration 35001 / 38200) loss: 2.304682\n",
      "(Iteration 35101 / 38200) loss: 2.304687\n",
      "(Epoch 92 / 100) train acc: 0.098000; val_acc: 0.112000\n",
      "(Iteration 35201 / 38200) loss: 2.304673\n",
      "(Iteration 35301 / 38200) loss: 2.304686\n",
      "(Iteration 35401 / 38200) loss: 2.304691\n",
      "(Iteration 35501 / 38200) loss: 2.304677\n",
      "(Epoch 93 / 100) train acc: 0.098000; val_acc: 0.112000\n",
      "(Iteration 35601 / 38200) loss: 2.304689\n",
      "(Iteration 35701 / 38200) loss: 2.304693\n",
      "(Iteration 35801 / 38200) loss: 2.304679\n",
      "(Iteration 35901 / 38200) loss: 2.304682\n",
      "(Epoch 94 / 100) train acc: 0.090000; val_acc: 0.112000\n",
      "(Iteration 36001 / 38200) loss: 2.304683\n",
      "(Iteration 36101 / 38200) loss: 2.304691\n",
      "(Iteration 36201 / 38200) loss: 2.304685\n",
      "(Epoch 95 / 100) train acc: 0.106000; val_acc: 0.112000\n",
      "(Iteration 36301 / 38200) loss: 2.304687\n",
      "(Iteration 36401 / 38200) loss: 2.304684\n",
      "(Iteration 36501 / 38200) loss: 2.304682\n",
      "(Iteration 36601 / 38200) loss: 2.304678\n",
      "(Epoch 96 / 100) train acc: 0.095000; val_acc: 0.112000\n",
      "(Iteration 36701 / 38200) loss: 2.304686\n",
      "(Iteration 36801 / 38200) loss: 2.304676\n",
      "(Iteration 36901 / 38200) loss: 2.304680\n",
      "(Iteration 37001 / 38200) loss: 2.304676\n",
      "(Epoch 97 / 100) train acc: 0.092000; val_acc: 0.112000\n",
      "(Iteration 37101 / 38200) loss: 2.304695\n",
      "(Iteration 37201 / 38200) loss: 2.304685\n",
      "(Iteration 37301 / 38200) loss: 2.304682\n",
      "(Iteration 37401 / 38200) loss: 2.304689\n",
      "(Epoch 98 / 100) train acc: 0.095000; val_acc: 0.112000\n",
      "(Iteration 37501 / 38200) loss: 2.304688\n",
      "(Iteration 37601 / 38200) loss: 2.304687\n",
      "(Iteration 37701 / 38200) loss: 2.304679\n",
      "(Iteration 37801 / 38200) loss: 2.304682\n",
      "(Epoch 99 / 100) train acc: 0.108000; val_acc: 0.112000\n",
      "(Iteration 37901 / 38200) loss: 2.304684\n",
      "(Iteration 38001 / 38200) loss: 2.304668\n",
      "(Iteration 38101 / 38200) loss: 2.304681\n",
      "(Epoch 100 / 100) train acc: 0.085000; val_acc: 0.112000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.305429\n",
      "(Epoch 0 / 100) train acc: 0.108000; val_acc: 0.110000\n",
      "(Iteration 101 / 76500) loss: 2.305412\n",
      "(Iteration 201 / 76500) loss: 2.305415\n",
      "(Iteration 301 / 76500) loss: 2.305426\n",
      "(Iteration 401 / 76500) loss: 2.305424\n",
      "(Iteration 501 / 76500) loss: 2.305416\n",
      "(Iteration 601 / 76500) loss: 2.305405\n",
      "(Iteration 701 / 76500) loss: 2.305416\n",
      "(Epoch 1 / 100) train acc: 0.130000; val_acc: 0.110000\n",
      "(Iteration 801 / 76500) loss: 2.305419\n",
      "(Iteration 901 / 76500) loss: 2.305416\n",
      "(Iteration 1001 / 76500) loss: 2.305418\n",
      "(Iteration 1101 / 76500) loss: 2.305424\n",
      "(Iteration 1201 / 76500) loss: 2.305416\n",
      "(Iteration 1301 / 76500) loss: 2.305421\n",
      "(Iteration 1401 / 76500) loss: 2.305413\n",
      "(Iteration 1501 / 76500) loss: 2.305423\n",
      "(Epoch 2 / 100) train acc: 0.116000; val_acc: 0.111000\n",
      "(Iteration 1601 / 76500) loss: 2.305405\n",
      "(Iteration 1701 / 76500) loss: 2.305413\n",
      "(Iteration 1801 / 76500) loss: 2.305418\n",
      "(Iteration 1901 / 76500) loss: 2.305417\n",
      "(Iteration 2001 / 76500) loss: 2.305405\n",
      "(Iteration 2101 / 76500) loss: 2.305417\n",
      "(Iteration 2201 / 76500) loss: 2.305414\n",
      "(Epoch 3 / 100) train acc: 0.105000; val_acc: 0.110000\n",
      "(Iteration 2301 / 76500) loss: 2.305402\n",
      "(Iteration 2401 / 76500) loss: 2.305408\n",
      "(Iteration 2501 / 76500) loss: 2.305411\n",
      "(Iteration 2601 / 76500) loss: 2.305413\n",
      "(Iteration 2701 / 76500) loss: 2.305420\n",
      "(Iteration 2801 / 76500) loss: 2.305410\n",
      "(Iteration 2901 / 76500) loss: 2.305418\n",
      "(Iteration 3001 / 76500) loss: 2.305397\n",
      "(Epoch 4 / 100) train acc: 0.125000; val_acc: 0.111000\n",
      "(Iteration 3101 / 76500) loss: 2.305415\n",
      "(Iteration 3201 / 76500) loss: 2.305414\n",
      "(Iteration 3301 / 76500) loss: 2.305431\n",
      "(Iteration 3401 / 76500) loss: 2.305428\n",
      "(Iteration 3501 / 76500) loss: 2.305410\n",
      "(Iteration 3601 / 76500) loss: 2.305425\n",
      "(Iteration 3701 / 76500) loss: 2.305414\n",
      "(Iteration 3801 / 76500) loss: 2.305415\n",
      "(Epoch 5 / 100) train acc: 0.113000; val_acc: 0.109000\n",
      "(Iteration 3901 / 76500) loss: 2.305416\n",
      "(Iteration 4001 / 76500) loss: 2.305410\n",
      "(Iteration 4101 / 76500) loss: 2.305406\n",
      "(Iteration 4201 / 76500) loss: 2.305415\n",
      "(Iteration 4301 / 76500) loss: 2.305408\n",
      "(Iteration 4401 / 76500) loss: 2.305426\n",
      "(Iteration 4501 / 76500) loss: 2.305415\n",
      "(Epoch 6 / 100) train acc: 0.110000; val_acc: 0.109000\n",
      "(Iteration 4601 / 76500) loss: 2.305420\n",
      "(Iteration 4701 / 76500) loss: 2.305415\n",
      "(Iteration 4801 / 76500) loss: 2.305420\n",
      "(Iteration 4901 / 76500) loss: 2.305420\n",
      "(Iteration 5001 / 76500) loss: 2.305410\n",
      "(Iteration 5101 / 76500) loss: 2.305416\n",
      "(Iteration 5201 / 76500) loss: 2.305412\n",
      "(Iteration 5301 / 76500) loss: 2.305424\n",
      "(Epoch 7 / 100) train acc: 0.113000; val_acc: 0.110000\n",
      "(Iteration 5401 / 76500) loss: 2.305415\n",
      "(Iteration 5501 / 76500) loss: 2.305426\n",
      "(Iteration 5601 / 76500) loss: 2.305418\n",
      "(Iteration 5701 / 76500) loss: 2.305431\n",
      "(Iteration 5801 / 76500) loss: 2.305416\n",
      "(Iteration 5901 / 76500) loss: 2.305425\n",
      "(Iteration 6001 / 76500) loss: 2.305415\n",
      "(Iteration 6101 / 76500) loss: 2.305419\n",
      "(Epoch 8 / 100) train acc: 0.111000; val_acc: 0.109000\n",
      "(Iteration 6201 / 76500) loss: 2.305421\n",
      "(Iteration 6301 / 76500) loss: 2.305418\n",
      "(Iteration 6401 / 76500) loss: 2.305416\n",
      "(Iteration 6501 / 76500) loss: 2.305407\n",
      "(Iteration 6601 / 76500) loss: 2.305426\n",
      "(Iteration 6701 / 76500) loss: 2.305424\n",
      "(Iteration 6801 / 76500) loss: 2.305419\n",
      "(Epoch 9 / 100) train acc: 0.103000; val_acc: 0.109000\n",
      "(Iteration 6901 / 76500) loss: 2.305415\n",
      "(Iteration 7001 / 76500) loss: 2.305426\n",
      "(Iteration 7101 / 76500) loss: 2.305412\n",
      "(Iteration 7201 / 76500) loss: 2.305418\n",
      "(Iteration 7301 / 76500) loss: 2.305411\n",
      "(Iteration 7401 / 76500) loss: 2.305425\n",
      "(Iteration 7501 / 76500) loss: 2.305420\n",
      "(Iteration 7601 / 76500) loss: 2.305413\n",
      "(Epoch 10 / 100) train acc: 0.107000; val_acc: 0.109000\n",
      "(Iteration 7701 / 76500) loss: 2.305406\n",
      "(Iteration 7801 / 76500) loss: 2.305428\n",
      "(Iteration 7901 / 76500) loss: 2.305411\n",
      "(Iteration 8001 / 76500) loss: 2.305408\n",
      "(Iteration 8101 / 76500) loss: 2.305430\n",
      "(Iteration 8201 / 76500) loss: 2.305414\n",
      "(Iteration 8301 / 76500) loss: 2.305428\n",
      "(Iteration 8401 / 76500) loss: 2.305424\n",
      "(Epoch 11 / 100) train acc: 0.120000; val_acc: 0.109000\n",
      "(Iteration 8501 / 76500) loss: 2.305403\n",
      "(Iteration 8601 / 76500) loss: 2.305432\n",
      "(Iteration 8701 / 76500) loss: 2.305427\n",
      "(Iteration 8801 / 76500) loss: 2.305418\n",
      "(Iteration 8901 / 76500) loss: 2.305403\n",
      "(Iteration 9001 / 76500) loss: 2.305417\n",
      "(Iteration 9101 / 76500) loss: 2.305409\n",
      "(Epoch 12 / 100) train acc: 0.133000; val_acc: 0.109000\n",
      "(Iteration 9201 / 76500) loss: 2.305424\n",
      "(Iteration 9301 / 76500) loss: 2.305417\n",
      "(Iteration 9401 / 76500) loss: 2.305422\n",
      "(Iteration 9501 / 76500) loss: 2.305426\n",
      "(Iteration 9601 / 76500) loss: 2.305417\n",
      "(Iteration 9701 / 76500) loss: 2.305413\n",
      "(Iteration 9801 / 76500) loss: 2.305420\n",
      "(Iteration 9901 / 76500) loss: 2.305415\n",
      "(Epoch 13 / 100) train acc: 0.128000; val_acc: 0.109000\n",
      "(Iteration 10001 / 76500) loss: 2.305410\n",
      "(Iteration 10101 / 76500) loss: 2.305414\n",
      "(Iteration 10201 / 76500) loss: 2.305423\n",
      "(Iteration 10301 / 76500) loss: 2.305414\n",
      "(Iteration 10401 / 76500) loss: 2.305419\n",
      "(Iteration 10501 / 76500) loss: 2.305414\n",
      "(Iteration 10601 / 76500) loss: 2.305416\n",
      "(Iteration 10701 / 76500) loss: 2.305424\n",
      "(Epoch 14 / 100) train acc: 0.091000; val_acc: 0.109000\n",
      "(Iteration 10801 / 76500) loss: 2.305419\n",
      "(Iteration 10901 / 76500) loss: 2.305405\n",
      "(Iteration 11001 / 76500) loss: 2.305419\n",
      "(Iteration 11101 / 76500) loss: 2.305414\n",
      "(Iteration 11201 / 76500) loss: 2.305416\n",
      "(Iteration 11301 / 76500) loss: 2.305422\n",
      "(Iteration 11401 / 76500) loss: 2.305409\n",
      "(Epoch 15 / 100) train acc: 0.106000; val_acc: 0.109000\n",
      "(Iteration 11501 / 76500) loss: 2.305416\n",
      "(Iteration 11601 / 76500) loss: 2.305408\n",
      "(Iteration 11701 / 76500) loss: 2.305418\n",
      "(Iteration 11801 / 76500) loss: 2.305415\n",
      "(Iteration 11901 / 76500) loss: 2.305415\n",
      "(Iteration 12001 / 76500) loss: 2.305400\n",
      "(Iteration 12101 / 76500) loss: 2.305407\n",
      "(Iteration 12201 / 76500) loss: 2.305410\n",
      "(Epoch 16 / 100) train acc: 0.104000; val_acc: 0.109000\n",
      "(Iteration 12301 / 76500) loss: 2.305411\n",
      "(Iteration 12401 / 76500) loss: 2.305428\n",
      "(Iteration 12501 / 76500) loss: 2.305419\n",
      "(Iteration 12601 / 76500) loss: 2.305410\n",
      "(Iteration 12701 / 76500) loss: 2.305414\n",
      "(Iteration 12801 / 76500) loss: 2.305407\n",
      "(Iteration 12901 / 76500) loss: 2.305421\n",
      "(Iteration 13001 / 76500) loss: 2.305417\n",
      "(Epoch 17 / 100) train acc: 0.094000; val_acc: 0.109000\n",
      "(Iteration 13101 / 76500) loss: 2.305424\n",
      "(Iteration 13201 / 76500) loss: 2.305421\n",
      "(Iteration 13301 / 76500) loss: 2.305418\n",
      "(Iteration 13401 / 76500) loss: 2.305415\n",
      "(Iteration 13501 / 76500) loss: 2.305410\n",
      "(Iteration 13601 / 76500) loss: 2.305407\n",
      "(Iteration 13701 / 76500) loss: 2.305407\n",
      "(Epoch 18 / 100) train acc: 0.125000; val_acc: 0.110000\n",
      "(Iteration 13801 / 76500) loss: 2.305410\n",
      "(Iteration 13901 / 76500) loss: 2.305417\n",
      "(Iteration 14001 / 76500) loss: 2.305424\n",
      "(Iteration 14101 / 76500) loss: 2.305413\n",
      "(Iteration 14201 / 76500) loss: 2.305419\n",
      "(Iteration 14301 / 76500) loss: 2.305418\n",
      "(Iteration 14401 / 76500) loss: 2.305424\n",
      "(Iteration 14501 / 76500) loss: 2.305428\n",
      "(Epoch 19 / 100) train acc: 0.108000; val_acc: 0.110000\n",
      "(Iteration 14601 / 76500) loss: 2.305418\n",
      "(Iteration 14701 / 76500) loss: 2.305416\n",
      "(Iteration 14801 / 76500) loss: 2.305417\n",
      "(Iteration 14901 / 76500) loss: 2.305415\n",
      "(Iteration 15001 / 76500) loss: 2.305420\n",
      "(Iteration 15101 / 76500) loss: 2.305408\n",
      "(Iteration 15201 / 76500) loss: 2.305410\n",
      "(Epoch 20 / 100) train acc: 0.106000; val_acc: 0.110000\n",
      "(Iteration 15301 / 76500) loss: 2.305419\n",
      "(Iteration 15401 / 76500) loss: 2.305423\n",
      "(Iteration 15501 / 76500) loss: 2.305407\n",
      "(Iteration 15601 / 76500) loss: 2.305404\n",
      "(Iteration 15701 / 76500) loss: 2.305410\n",
      "(Iteration 15801 / 76500) loss: 2.305413\n",
      "(Iteration 15901 / 76500) loss: 2.305419\n",
      "(Iteration 16001 / 76500) loss: 2.305423\n",
      "(Epoch 21 / 100) train acc: 0.097000; val_acc: 0.110000\n",
      "(Iteration 16101 / 76500) loss: 2.305420\n",
      "(Iteration 16201 / 76500) loss: 2.305414\n",
      "(Iteration 16301 / 76500) loss: 2.305421\n",
      "(Iteration 16401 / 76500) loss: 2.305414\n",
      "(Iteration 16501 / 76500) loss: 2.305415\n",
      "(Iteration 16601 / 76500) loss: 2.305419\n",
      "(Iteration 16701 / 76500) loss: 2.305417\n",
      "(Iteration 16801 / 76500) loss: 2.305415\n",
      "(Epoch 22 / 100) train acc: 0.116000; val_acc: 0.110000\n",
      "(Iteration 16901 / 76500) loss: 2.305433\n",
      "(Iteration 17001 / 76500) loss: 2.305418\n",
      "(Iteration 17101 / 76500) loss: 2.305423\n",
      "(Iteration 17201 / 76500) loss: 2.305421\n",
      "(Iteration 17301 / 76500) loss: 2.305417\n",
      "(Iteration 17401 / 76500) loss: 2.305419\n",
      "(Iteration 17501 / 76500) loss: 2.305416\n",
      "(Epoch 23 / 100) train acc: 0.137000; val_acc: 0.110000\n",
      "(Iteration 17601 / 76500) loss: 2.305418\n",
      "(Iteration 17701 / 76500) loss: 2.305409\n",
      "(Iteration 17801 / 76500) loss: 2.305420\n",
      "(Iteration 17901 / 76500) loss: 2.305413\n",
      "(Iteration 18001 / 76500) loss: 2.305402\n",
      "(Iteration 18101 / 76500) loss: 2.305408\n",
      "(Iteration 18201 / 76500) loss: 2.305428\n",
      "(Iteration 18301 / 76500) loss: 2.305413\n",
      "(Epoch 24 / 100) train acc: 0.106000; val_acc: 0.110000\n",
      "(Iteration 18401 / 76500) loss: 2.305422\n",
      "(Iteration 18501 / 76500) loss: 2.305417\n",
      "(Iteration 18601 / 76500) loss: 2.305428\n",
      "(Iteration 18701 / 76500) loss: 2.305400\n",
      "(Iteration 18801 / 76500) loss: 2.305420\n",
      "(Iteration 18901 / 76500) loss: 2.305408\n",
      "(Iteration 19001 / 76500) loss: 2.305407\n",
      "(Iteration 19101 / 76500) loss: 2.305414\n",
      "(Epoch 25 / 100) train acc: 0.107000; val_acc: 0.110000\n",
      "(Iteration 19201 / 76500) loss: 2.305418\n",
      "(Iteration 19301 / 76500) loss: 2.305422\n",
      "(Iteration 19401 / 76500) loss: 2.305420\n",
      "(Iteration 19501 / 76500) loss: 2.305418\n",
      "(Iteration 19601 / 76500) loss: 2.305415\n",
      "(Iteration 19701 / 76500) loss: 2.305410\n",
      "(Iteration 19801 / 76500) loss: 2.305414\n",
      "(Epoch 26 / 100) train acc: 0.098000; val_acc: 0.110000\n",
      "(Iteration 19901 / 76500) loss: 2.305422\n",
      "(Iteration 20001 / 76500) loss: 2.305405\n",
      "(Iteration 20101 / 76500) loss: 2.305413\n",
      "(Iteration 20201 / 76500) loss: 2.305426\n",
      "(Iteration 20301 / 76500) loss: 2.305411\n",
      "(Iteration 20401 / 76500) loss: 2.305406\n",
      "(Iteration 20501 / 76500) loss: 2.305410\n",
      "(Iteration 20601 / 76500) loss: 2.305421\n",
      "(Epoch 27 / 100) train acc: 0.095000; val_acc: 0.110000\n",
      "(Iteration 20701 / 76500) loss: 2.305415\n",
      "(Iteration 20801 / 76500) loss: 2.305414\n",
      "(Iteration 20901 / 76500) loss: 2.305416\n",
      "(Iteration 21001 / 76500) loss: 2.305427\n",
      "(Iteration 21101 / 76500) loss: 2.305414\n",
      "(Iteration 21201 / 76500) loss: 2.305419\n",
      "(Iteration 21301 / 76500) loss: 2.305419\n",
      "(Iteration 21401 / 76500) loss: 2.305427\n",
      "(Epoch 28 / 100) train acc: 0.114000; val_acc: 0.110000\n",
      "(Iteration 21501 / 76500) loss: 2.305417\n",
      "(Iteration 21601 / 76500) loss: 2.305424\n",
      "(Iteration 21701 / 76500) loss: 2.305420\n",
      "(Iteration 21801 / 76500) loss: 2.305405\n",
      "(Iteration 21901 / 76500) loss: 2.305407\n",
      "(Iteration 22001 / 76500) loss: 2.305404\n",
      "(Iteration 22101 / 76500) loss: 2.305402\n",
      "(Epoch 29 / 100) train acc: 0.112000; val_acc: 0.110000\n",
      "(Iteration 22201 / 76500) loss: 2.305416\n",
      "(Iteration 22301 / 76500) loss: 2.305407\n",
      "(Iteration 22401 / 76500) loss: 2.305426\n",
      "(Iteration 22501 / 76500) loss: 2.305418\n",
      "(Iteration 22601 / 76500) loss: 2.305414\n",
      "(Iteration 22701 / 76500) loss: 2.305413\n",
      "(Iteration 22801 / 76500) loss: 2.305409\n",
      "(Iteration 22901 / 76500) loss: 2.305417\n",
      "(Epoch 30 / 100) train acc: 0.110000; val_acc: 0.110000\n",
      "(Iteration 23001 / 76500) loss: 2.305408\n",
      "(Iteration 23101 / 76500) loss: 2.305419\n",
      "(Iteration 23201 / 76500) loss: 2.305410\n",
      "(Iteration 23301 / 76500) loss: 2.305406\n",
      "(Iteration 23401 / 76500) loss: 2.305416\n",
      "(Iteration 23501 / 76500) loss: 2.305407\n",
      "(Iteration 23601 / 76500) loss: 2.305422\n",
      "(Iteration 23701 / 76500) loss: 2.305419\n",
      "(Epoch 31 / 100) train acc: 0.129000; val_acc: 0.110000\n",
      "(Iteration 23801 / 76500) loss: 2.305401\n",
      "(Iteration 23901 / 76500) loss: 2.305408\n",
      "(Iteration 24001 / 76500) loss: 2.305420\n",
      "(Iteration 24101 / 76500) loss: 2.305423\n",
      "(Iteration 24201 / 76500) loss: 2.305414\n",
      "(Iteration 24301 / 76500) loss: 2.305423\n",
      "(Iteration 24401 / 76500) loss: 2.305409\n",
      "(Epoch 32 / 100) train acc: 0.119000; val_acc: 0.110000\n",
      "(Iteration 24501 / 76500) loss: 2.305416\n",
      "(Iteration 24601 / 76500) loss: 2.305418\n",
      "(Iteration 24701 / 76500) loss: 2.305417\n",
      "(Iteration 24801 / 76500) loss: 2.305418\n",
      "(Iteration 24901 / 76500) loss: 2.305423\n",
      "(Iteration 25001 / 76500) loss: 2.305407\n",
      "(Iteration 25101 / 76500) loss: 2.305419\n",
      "(Iteration 25201 / 76500) loss: 2.305417\n",
      "(Epoch 33 / 100) train acc: 0.105000; val_acc: 0.110000\n",
      "(Iteration 25301 / 76500) loss: 2.305420\n",
      "(Iteration 25401 / 76500) loss: 2.305416\n",
      "(Iteration 25501 / 76500) loss: 2.305416\n",
      "(Iteration 25601 / 76500) loss: 2.305412\n",
      "(Iteration 25701 / 76500) loss: 2.305421\n",
      "(Iteration 25801 / 76500) loss: 2.305412\n",
      "(Iteration 25901 / 76500) loss: 2.305417\n",
      "(Iteration 26001 / 76500) loss: 2.305423\n",
      "(Epoch 34 / 100) train acc: 0.099000; val_acc: 0.110000\n",
      "(Iteration 26101 / 76500) loss: 2.305407\n",
      "(Iteration 26201 / 76500) loss: 2.305412\n",
      "(Iteration 26301 / 76500) loss: 2.305422\n",
      "(Iteration 26401 / 76500) loss: 2.305416\n",
      "(Iteration 26501 / 76500) loss: 2.305415\n",
      "(Iteration 26601 / 76500) loss: 2.305401\n",
      "(Iteration 26701 / 76500) loss: 2.305422\n",
      "(Epoch 35 / 100) train acc: 0.147000; val_acc: 0.110000\n",
      "(Iteration 26801 / 76500) loss: 2.305425\n",
      "(Iteration 26901 / 76500) loss: 2.305407\n",
      "(Iteration 27001 / 76500) loss: 2.305416\n",
      "(Iteration 27101 / 76500) loss: 2.305411\n",
      "(Iteration 27201 / 76500) loss: 2.305419\n",
      "(Iteration 27301 / 76500) loss: 2.305415\n",
      "(Iteration 27401 / 76500) loss: 2.305410\n",
      "(Iteration 27501 / 76500) loss: 2.305419\n",
      "(Epoch 36 / 100) train acc: 0.122000; val_acc: 0.110000\n",
      "(Iteration 27601 / 76500) loss: 2.305407\n",
      "(Iteration 27701 / 76500) loss: 2.305417\n",
      "(Iteration 27801 / 76500) loss: 2.305424\n",
      "(Iteration 27901 / 76500) loss: 2.305421\n",
      "(Iteration 28001 / 76500) loss: 2.305410\n",
      "(Iteration 28101 / 76500) loss: 2.305407\n",
      "(Iteration 28201 / 76500) loss: 2.305414\n",
      "(Iteration 28301 / 76500) loss: 2.305413\n",
      "(Epoch 37 / 100) train acc: 0.126000; val_acc: 0.110000\n",
      "(Iteration 28401 / 76500) loss: 2.305416\n",
      "(Iteration 28501 / 76500) loss: 2.305422\n",
      "(Iteration 28601 / 76500) loss: 2.305410\n",
      "(Iteration 28701 / 76500) loss: 2.305410\n",
      "(Iteration 28801 / 76500) loss: 2.305429\n",
      "(Iteration 28901 / 76500) loss: 2.305416\n",
      "(Iteration 29001 / 76500) loss: 2.305420\n",
      "(Epoch 38 / 100) train acc: 0.114000; val_acc: 0.110000\n",
      "(Iteration 29101 / 76500) loss: 2.305423\n",
      "(Iteration 29201 / 76500) loss: 2.305420\n",
      "(Iteration 29301 / 76500) loss: 2.305425\n",
      "(Iteration 29401 / 76500) loss: 2.305398\n",
      "(Iteration 29501 / 76500) loss: 2.305423\n",
      "(Iteration 29601 / 76500) loss: 2.305420\n",
      "(Iteration 29701 / 76500) loss: 2.305406\n",
      "(Iteration 29801 / 76500) loss: 2.305412\n",
      "(Epoch 39 / 100) train acc: 0.127000; val_acc: 0.110000\n",
      "(Iteration 29901 / 76500) loss: 2.305419\n",
      "(Iteration 30001 / 76500) loss: 2.305412\n",
      "(Iteration 30101 / 76500) loss: 2.305420\n",
      "(Iteration 30201 / 76500) loss: 2.305417\n",
      "(Iteration 30301 / 76500) loss: 2.305419\n",
      "(Iteration 30401 / 76500) loss: 2.305413\n",
      "(Iteration 30501 / 76500) loss: 2.305430\n",
      "(Epoch 40 / 100) train acc: 0.100000; val_acc: 0.110000\n",
      "(Iteration 30601 / 76500) loss: 2.305412\n",
      "(Iteration 30701 / 76500) loss: 2.305410\n",
      "(Iteration 30801 / 76500) loss: 2.305418\n",
      "(Iteration 30901 / 76500) loss: 2.305419\n",
      "(Iteration 31001 / 76500) loss: 2.305423\n",
      "(Iteration 31101 / 76500) loss: 2.305413\n",
      "(Iteration 31201 / 76500) loss: 2.305424\n",
      "(Iteration 31301 / 76500) loss: 2.305413\n",
      "(Epoch 41 / 100) train acc: 0.120000; val_acc: 0.110000\n",
      "(Iteration 31401 / 76500) loss: 2.305422\n",
      "(Iteration 31501 / 76500) loss: 2.305421\n",
      "(Iteration 31601 / 76500) loss: 2.305425\n",
      "(Iteration 31701 / 76500) loss: 2.305417\n",
      "(Iteration 31801 / 76500) loss: 2.305418\n",
      "(Iteration 31901 / 76500) loss: 2.305414\n",
      "(Iteration 32001 / 76500) loss: 2.305421\n",
      "(Iteration 32101 / 76500) loss: 2.305421\n",
      "(Epoch 42 / 100) train acc: 0.108000; val_acc: 0.110000\n",
      "(Iteration 32201 / 76500) loss: 2.305415\n",
      "(Iteration 32301 / 76500) loss: 2.305410\n",
      "(Iteration 32401 / 76500) loss: 2.305411\n",
      "(Iteration 32501 / 76500) loss: 2.305407\n",
      "(Iteration 32601 / 76500) loss: 2.305402\n",
      "(Iteration 32701 / 76500) loss: 2.305412\n",
      "(Iteration 32801 / 76500) loss: 2.305420\n",
      "(Epoch 43 / 100) train acc: 0.109000; val_acc: 0.110000\n",
      "(Iteration 32901 / 76500) loss: 2.305424\n",
      "(Iteration 33001 / 76500) loss: 2.305417\n",
      "(Iteration 33101 / 76500) loss: 2.305415\n",
      "(Iteration 33201 / 76500) loss: 2.305420\n",
      "(Iteration 33301 / 76500) loss: 2.305418\n",
      "(Iteration 33401 / 76500) loss: 2.305425\n",
      "(Iteration 33501 / 76500) loss: 2.305417\n",
      "(Iteration 33601 / 76500) loss: 2.305420\n",
      "(Epoch 44 / 100) train acc: 0.090000; val_acc: 0.110000\n",
      "(Iteration 33701 / 76500) loss: 2.305413\n",
      "(Iteration 33801 / 76500) loss: 2.305418\n",
      "(Iteration 33901 / 76500) loss: 2.305421\n",
      "(Iteration 34001 / 76500) loss: 2.305406\n",
      "(Iteration 34101 / 76500) loss: 2.305416\n",
      "(Iteration 34201 / 76500) loss: 2.305416\n",
      "(Iteration 34301 / 76500) loss: 2.305418\n",
      "(Iteration 34401 / 76500) loss: 2.305425\n",
      "(Epoch 45 / 100) train acc: 0.112000; val_acc: 0.110000\n",
      "(Iteration 34501 / 76500) loss: 2.305420\n",
      "(Iteration 34601 / 76500) loss: 2.305410\n",
      "(Iteration 34701 / 76500) loss: 2.305418\n",
      "(Iteration 34801 / 76500) loss: 2.305419\n",
      "(Iteration 34901 / 76500) loss: 2.305410\n",
      "(Iteration 35001 / 76500) loss: 2.305412\n",
      "(Iteration 35101 / 76500) loss: 2.305415\n",
      "(Epoch 46 / 100) train acc: 0.113000; val_acc: 0.110000\n",
      "(Iteration 35201 / 76500) loss: 2.305421\n",
      "(Iteration 35301 / 76500) loss: 2.305412\n",
      "(Iteration 35401 / 76500) loss: 2.305414\n",
      "(Iteration 35501 / 76500) loss: 2.305416\n",
      "(Iteration 35601 / 76500) loss: 2.305420\n",
      "(Iteration 35701 / 76500) loss: 2.305409\n",
      "(Iteration 35801 / 76500) loss: 2.305411\n",
      "(Iteration 35901 / 76500) loss: 2.305409\n",
      "(Epoch 47 / 100) train acc: 0.116000; val_acc: 0.110000\n",
      "(Iteration 36001 / 76500) loss: 2.305427\n",
      "(Iteration 36101 / 76500) loss: 2.305418\n",
      "(Iteration 36201 / 76500) loss: 2.305411\n",
      "(Iteration 36301 / 76500) loss: 2.305409\n",
      "(Iteration 36401 / 76500) loss: 2.305420\n",
      "(Iteration 36501 / 76500) loss: 2.305420\n",
      "(Iteration 36601 / 76500) loss: 2.305412\n",
      "(Iteration 36701 / 76500) loss: 2.305409\n",
      "(Epoch 48 / 100) train acc: 0.101000; val_acc: 0.110000\n",
      "(Iteration 36801 / 76500) loss: 2.305416\n",
      "(Iteration 36901 / 76500) loss: 2.305420\n",
      "(Iteration 37001 / 76500) loss: 2.305420\n",
      "(Iteration 37101 / 76500) loss: 2.305420\n",
      "(Iteration 37201 / 76500) loss: 2.305409\n",
      "(Iteration 37301 / 76500) loss: 2.305411\n",
      "(Iteration 37401 / 76500) loss: 2.305411\n",
      "(Epoch 49 / 100) train acc: 0.126000; val_acc: 0.110000\n",
      "(Iteration 37501 / 76500) loss: 2.305420\n",
      "(Iteration 37601 / 76500) loss: 2.305419\n",
      "(Iteration 37701 / 76500) loss: 2.305414\n",
      "(Iteration 37801 / 76500) loss: 2.305415\n",
      "(Iteration 37901 / 76500) loss: 2.305420\n",
      "(Iteration 38001 / 76500) loss: 2.305413\n",
      "(Iteration 38101 / 76500) loss: 2.305420\n",
      "(Iteration 38201 / 76500) loss: 2.305419\n",
      "(Epoch 50 / 100) train acc: 0.104000; val_acc: 0.110000\n",
      "(Iteration 38301 / 76500) loss: 2.305410\n",
      "(Iteration 38401 / 76500) loss: 2.305417\n",
      "(Iteration 38501 / 76500) loss: 2.305419\n",
      "(Iteration 38601 / 76500) loss: 2.305403\n",
      "(Iteration 38701 / 76500) loss: 2.305417\n",
      "(Iteration 38801 / 76500) loss: 2.305417\n",
      "(Iteration 38901 / 76500) loss: 2.305414\n",
      "(Iteration 39001 / 76500) loss: 2.305410\n",
      "(Epoch 51 / 100) train acc: 0.099000; val_acc: 0.110000\n",
      "(Iteration 39101 / 76500) loss: 2.305424\n",
      "(Iteration 39201 / 76500) loss: 2.305406\n",
      "(Iteration 39301 / 76500) loss: 2.305407\n",
      "(Iteration 39401 / 76500) loss: 2.305424\n",
      "(Iteration 39501 / 76500) loss: 2.305424\n",
      "(Iteration 39601 / 76500) loss: 2.305418\n",
      "(Iteration 39701 / 76500) loss: 2.305410\n",
      "(Epoch 52 / 100) train acc: 0.101000; val_acc: 0.110000\n",
      "(Iteration 39801 / 76500) loss: 2.305409\n",
      "(Iteration 39901 / 76500) loss: 2.305425\n",
      "(Iteration 40001 / 76500) loss: 2.305424\n",
      "(Iteration 40101 / 76500) loss: 2.305415\n",
      "(Iteration 40201 / 76500) loss: 2.305416\n",
      "(Iteration 40301 / 76500) loss: 2.305410\n",
      "(Iteration 40401 / 76500) loss: 2.305416\n",
      "(Iteration 40501 / 76500) loss: 2.305426\n",
      "(Epoch 53 / 100) train acc: 0.112000; val_acc: 0.110000\n",
      "(Iteration 40601 / 76500) loss: 2.305417\n",
      "(Iteration 40701 / 76500) loss: 2.305422\n",
      "(Iteration 40801 / 76500) loss: 2.305417\n",
      "(Iteration 40901 / 76500) loss: 2.305414\n",
      "(Iteration 41001 / 76500) loss: 2.305420\n",
      "(Iteration 41101 / 76500) loss: 2.305416\n",
      "(Iteration 41201 / 76500) loss: 2.305415\n",
      "(Iteration 41301 / 76500) loss: 2.305413\n",
      "(Epoch 54 / 100) train acc: 0.111000; val_acc: 0.110000\n",
      "(Iteration 41401 / 76500) loss: 2.305408\n",
      "(Iteration 41501 / 76500) loss: 2.305426\n",
      "(Iteration 41601 / 76500) loss: 2.305415\n",
      "(Iteration 41701 / 76500) loss: 2.305415\n",
      "(Iteration 41801 / 76500) loss: 2.305413\n",
      "(Iteration 41901 / 76500) loss: 2.305416\n",
      "(Iteration 42001 / 76500) loss: 2.305408\n",
      "(Epoch 55 / 100) train acc: 0.105000; val_acc: 0.110000\n",
      "(Iteration 42101 / 76500) loss: 2.305414\n",
      "(Iteration 42201 / 76500) loss: 2.305420\n",
      "(Iteration 42301 / 76500) loss: 2.305418\n",
      "(Iteration 42401 / 76500) loss: 2.305414\n",
      "(Iteration 42501 / 76500) loss: 2.305419\n",
      "(Iteration 42601 / 76500) loss: 2.305425\n",
      "(Iteration 42701 / 76500) loss: 2.305430\n",
      "(Iteration 42801 / 76500) loss: 2.305417\n",
      "(Epoch 56 / 100) train acc: 0.120000; val_acc: 0.110000\n",
      "(Iteration 42901 / 76500) loss: 2.305423\n",
      "(Iteration 43001 / 76500) loss: 2.305410\n",
      "(Iteration 43101 / 76500) loss: 2.305408\n",
      "(Iteration 43201 / 76500) loss: 2.305410\n",
      "(Iteration 43301 / 76500) loss: 2.305414\n",
      "(Iteration 43401 / 76500) loss: 2.305416\n",
      "(Iteration 43501 / 76500) loss: 2.305419\n",
      "(Iteration 43601 / 76500) loss: 2.305410\n",
      "(Epoch 57 / 100) train acc: 0.105000; val_acc: 0.110000\n",
      "(Iteration 43701 / 76500) loss: 2.305415\n",
      "(Iteration 43801 / 76500) loss: 2.305408\n",
      "(Iteration 43901 / 76500) loss: 2.305417\n",
      "(Iteration 44001 / 76500) loss: 2.305422\n",
      "(Iteration 44101 / 76500) loss: 2.305408\n",
      "(Iteration 44201 / 76500) loss: 2.305416\n",
      "(Iteration 44301 / 76500) loss: 2.305415\n",
      "(Epoch 58 / 100) train acc: 0.120000; val_acc: 0.110000\n",
      "(Iteration 44401 / 76500) loss: 2.305421\n",
      "(Iteration 44501 / 76500) loss: 2.305411\n",
      "(Iteration 44601 / 76500) loss: 2.305422\n",
      "(Iteration 44701 / 76500) loss: 2.305416\n",
      "(Iteration 44801 / 76500) loss: 2.305417\n",
      "(Iteration 44901 / 76500) loss: 2.305420\n",
      "(Iteration 45001 / 76500) loss: 2.305417\n",
      "(Iteration 45101 / 76500) loss: 2.305423\n",
      "(Epoch 59 / 100) train acc: 0.098000; val_acc: 0.110000\n",
      "(Iteration 45201 / 76500) loss: 2.305417\n",
      "(Iteration 45301 / 76500) loss: 2.305415\n",
      "(Iteration 45401 / 76500) loss: 2.305398\n",
      "(Iteration 45501 / 76500) loss: 2.305413\n",
      "(Iteration 45601 / 76500) loss: 2.305414\n",
      "(Iteration 45701 / 76500) loss: 2.305408\n",
      "(Iteration 45801 / 76500) loss: 2.305418\n",
      "(Epoch 60 / 100) train acc: 0.136000; val_acc: 0.110000\n",
      "(Iteration 45901 / 76500) loss: 2.305416\n",
      "(Iteration 46001 / 76500) loss: 2.305417\n",
      "(Iteration 46101 / 76500) loss: 2.305404\n",
      "(Iteration 46201 / 76500) loss: 2.305410\n",
      "(Iteration 46301 / 76500) loss: 2.305412\n",
      "(Iteration 46401 / 76500) loss: 2.305423\n",
      "(Iteration 46501 / 76500) loss: 2.305418\n",
      "(Iteration 46601 / 76500) loss: 2.305409\n",
      "(Epoch 61 / 100) train acc: 0.115000; val_acc: 0.110000\n",
      "(Iteration 46701 / 76500) loss: 2.305414\n",
      "(Iteration 46801 / 76500) loss: 2.305414\n",
      "(Iteration 46901 / 76500) loss: 2.305416\n",
      "(Iteration 47001 / 76500) loss: 2.305420\n",
      "(Iteration 47101 / 76500) loss: 2.305415\n",
      "(Iteration 47201 / 76500) loss: 2.305416\n",
      "(Iteration 47301 / 76500) loss: 2.305418\n",
      "(Iteration 47401 / 76500) loss: 2.305422\n",
      "(Epoch 62 / 100) train acc: 0.096000; val_acc: 0.110000\n",
      "(Iteration 47501 / 76500) loss: 2.305427\n",
      "(Iteration 47601 / 76500) loss: 2.305418\n",
      "(Iteration 47701 / 76500) loss: 2.305416\n",
      "(Iteration 47801 / 76500) loss: 2.305411\n",
      "(Iteration 47901 / 76500) loss: 2.305424\n",
      "(Iteration 48001 / 76500) loss: 2.305398\n",
      "(Iteration 48101 / 76500) loss: 2.305424\n",
      "(Epoch 63 / 100) train acc: 0.096000; val_acc: 0.110000\n",
      "(Iteration 48201 / 76500) loss: 2.305420\n",
      "(Iteration 48301 / 76500) loss: 2.305404\n",
      "(Iteration 48401 / 76500) loss: 2.305416\n",
      "(Iteration 48501 / 76500) loss: 2.305424\n",
      "(Iteration 48601 / 76500) loss: 2.305411\n",
      "(Iteration 48701 / 76500) loss: 2.305413\n",
      "(Iteration 48801 / 76500) loss: 2.305427\n",
      "(Iteration 48901 / 76500) loss: 2.305411\n",
      "(Epoch 64 / 100) train acc: 0.125000; val_acc: 0.110000\n",
      "(Iteration 49001 / 76500) loss: 2.305417\n",
      "(Iteration 49101 / 76500) loss: 2.305419\n",
      "(Iteration 49201 / 76500) loss: 2.305413\n",
      "(Iteration 49301 / 76500) loss: 2.305419\n",
      "(Iteration 49401 / 76500) loss: 2.305422\n",
      "(Iteration 49501 / 76500) loss: 2.305424\n",
      "(Iteration 49601 / 76500) loss: 2.305416\n",
      "(Iteration 49701 / 76500) loss: 2.305413\n",
      "(Epoch 65 / 100) train acc: 0.115000; val_acc: 0.110000\n",
      "(Iteration 49801 / 76500) loss: 2.305423\n",
      "(Iteration 49901 / 76500) loss: 2.305425\n",
      "(Iteration 50001 / 76500) loss: 2.305412\n",
      "(Iteration 50101 / 76500) loss: 2.305417\n",
      "(Iteration 50201 / 76500) loss: 2.305418\n",
      "(Iteration 50301 / 76500) loss: 2.305412\n",
      "(Iteration 50401 / 76500) loss: 2.305408\n",
      "(Epoch 66 / 100) train acc: 0.118000; val_acc: 0.110000\n",
      "(Iteration 50501 / 76500) loss: 2.305431\n",
      "(Iteration 50601 / 76500) loss: 2.305413\n",
      "(Iteration 50701 / 76500) loss: 2.305415\n",
      "(Iteration 50801 / 76500) loss: 2.305421\n",
      "(Iteration 50901 / 76500) loss: 2.305425\n",
      "(Iteration 51001 / 76500) loss: 2.305403\n",
      "(Iteration 51101 / 76500) loss: 2.305417\n",
      "(Iteration 51201 / 76500) loss: 2.305425\n",
      "(Epoch 67 / 100) train acc: 0.117000; val_acc: 0.110000\n",
      "(Iteration 51301 / 76500) loss: 2.305418\n",
      "(Iteration 51401 / 76500) loss: 2.305419\n",
      "(Iteration 51501 / 76500) loss: 2.305413\n",
      "(Iteration 51601 / 76500) loss: 2.305406\n",
      "(Iteration 51701 / 76500) loss: 2.305420\n",
      "(Iteration 51801 / 76500) loss: 2.305421\n",
      "(Iteration 51901 / 76500) loss: 2.305433\n",
      "(Iteration 52001 / 76500) loss: 2.305421\n",
      "(Epoch 68 / 100) train acc: 0.117000; val_acc: 0.110000\n",
      "(Iteration 52101 / 76500) loss: 2.305413\n",
      "(Iteration 52201 / 76500) loss: 2.305409\n",
      "(Iteration 52301 / 76500) loss: 2.305425\n",
      "(Iteration 52401 / 76500) loss: 2.305407\n",
      "(Iteration 52501 / 76500) loss: 2.305409\n",
      "(Iteration 52601 / 76500) loss: 2.305415\n",
      "(Iteration 52701 / 76500) loss: 2.305414\n",
      "(Epoch 69 / 100) train acc: 0.106000; val_acc: 0.110000\n",
      "(Iteration 52801 / 76500) loss: 2.305421\n",
      "(Iteration 52901 / 76500) loss: 2.305428\n",
      "(Iteration 53001 / 76500) loss: 2.305414\n",
      "(Iteration 53101 / 76500) loss: 2.305412\n",
      "(Iteration 53201 / 76500) loss: 2.305408\n",
      "(Iteration 53301 / 76500) loss: 2.305420\n",
      "(Iteration 53401 / 76500) loss: 2.305419\n",
      "(Iteration 53501 / 76500) loss: 2.305412\n",
      "(Epoch 70 / 100) train acc: 0.096000; val_acc: 0.110000\n",
      "(Iteration 53601 / 76500) loss: 2.305419\n",
      "(Iteration 53701 / 76500) loss: 2.305419\n",
      "(Iteration 53801 / 76500) loss: 2.305405\n",
      "(Iteration 53901 / 76500) loss: 2.305407\n",
      "(Iteration 54001 / 76500) loss: 2.305407\n",
      "(Iteration 54101 / 76500) loss: 2.305416\n",
      "(Iteration 54201 / 76500) loss: 2.305420\n",
      "(Iteration 54301 / 76500) loss: 2.305417\n",
      "(Epoch 71 / 100) train acc: 0.107000; val_acc: 0.110000\n",
      "(Iteration 54401 / 76500) loss: 2.305416\n",
      "(Iteration 54501 / 76500) loss: 2.305410\n",
      "(Iteration 54601 / 76500) loss: 2.305424\n",
      "(Iteration 54701 / 76500) loss: 2.305415\n",
      "(Iteration 54801 / 76500) loss: 2.305416\n",
      "(Iteration 54901 / 76500) loss: 2.305405\n",
      "(Iteration 55001 / 76500) loss: 2.305421\n",
      "(Epoch 72 / 100) train acc: 0.113000; val_acc: 0.110000\n",
      "(Iteration 55101 / 76500) loss: 2.305422\n",
      "(Iteration 55201 / 76500) loss: 2.305417\n",
      "(Iteration 55301 / 76500) loss: 2.305408\n",
      "(Iteration 55401 / 76500) loss: 2.305423\n",
      "(Iteration 55501 / 76500) loss: 2.305401\n",
      "(Iteration 55601 / 76500) loss: 2.305395\n",
      "(Iteration 55701 / 76500) loss: 2.305422\n",
      "(Iteration 55801 / 76500) loss: 2.305417\n",
      "(Epoch 73 / 100) train acc: 0.109000; val_acc: 0.110000\n",
      "(Iteration 55901 / 76500) loss: 2.305430\n",
      "(Iteration 56001 / 76500) loss: 2.305418\n",
      "(Iteration 56101 / 76500) loss: 2.305410\n",
      "(Iteration 56201 / 76500) loss: 2.305422\n",
      "(Iteration 56301 / 76500) loss: 2.305415\n",
      "(Iteration 56401 / 76500) loss: 2.305406\n",
      "(Iteration 56501 / 76500) loss: 2.305424\n",
      "(Iteration 56601 / 76500) loss: 2.305402\n",
      "(Epoch 74 / 100) train acc: 0.120000; val_acc: 0.110000\n",
      "(Iteration 56701 / 76500) loss: 2.305414\n",
      "(Iteration 56801 / 76500) loss: 2.305407\n",
      "(Iteration 56901 / 76500) loss: 2.305412\n",
      "(Iteration 57001 / 76500) loss: 2.305421\n",
      "(Iteration 57101 / 76500) loss: 2.305409\n",
      "(Iteration 57201 / 76500) loss: 2.305407\n",
      "(Iteration 57301 / 76500) loss: 2.305410\n",
      "(Epoch 75 / 100) train acc: 0.120000; val_acc: 0.110000\n",
      "(Iteration 57401 / 76500) loss: 2.305418\n",
      "(Iteration 57501 / 76500) loss: 2.305416\n",
      "(Iteration 57601 / 76500) loss: 2.305412\n",
      "(Iteration 57701 / 76500) loss: 2.305425\n",
      "(Iteration 57801 / 76500) loss: 2.305415\n",
      "(Iteration 57901 / 76500) loss: 2.305409\n",
      "(Iteration 58001 / 76500) loss: 2.305419\n",
      "(Iteration 58101 / 76500) loss: 2.305413\n",
      "(Epoch 76 / 100) train acc: 0.110000; val_acc: 0.110000\n",
      "(Iteration 58201 / 76500) loss: 2.305420\n",
      "(Iteration 58301 / 76500) loss: 2.305423\n",
      "(Iteration 58401 / 76500) loss: 2.305411\n",
      "(Iteration 58501 / 76500) loss: 2.305419\n",
      "(Iteration 58601 / 76500) loss: 2.305417\n",
      "(Iteration 58701 / 76500) loss: 2.305404\n",
      "(Iteration 58801 / 76500) loss: 2.305420\n",
      "(Iteration 58901 / 76500) loss: 2.305414\n",
      "(Epoch 77 / 100) train acc: 0.105000; val_acc: 0.110000\n",
      "(Iteration 59001 / 76500) loss: 2.305412\n",
      "(Iteration 59101 / 76500) loss: 2.305418\n",
      "(Iteration 59201 / 76500) loss: 2.305413\n",
      "(Iteration 59301 / 76500) loss: 2.305410\n",
      "(Iteration 59401 / 76500) loss: 2.305408\n",
      "(Iteration 59501 / 76500) loss: 2.305411\n",
      "(Iteration 59601 / 76500) loss: 2.305418\n",
      "(Epoch 78 / 100) train acc: 0.117000; val_acc: 0.110000\n",
      "(Iteration 59701 / 76500) loss: 2.305421\n",
      "(Iteration 59801 / 76500) loss: 2.305414\n",
      "(Iteration 59901 / 76500) loss: 2.305421\n",
      "(Iteration 60001 / 76500) loss: 2.305419\n",
      "(Iteration 60101 / 76500) loss: 2.305413\n",
      "(Iteration 60201 / 76500) loss: 2.305423\n",
      "(Iteration 60301 / 76500) loss: 2.305416\n",
      "(Iteration 60401 / 76500) loss: 2.305417\n",
      "(Epoch 79 / 100) train acc: 0.103000; val_acc: 0.110000\n",
      "(Iteration 60501 / 76500) loss: 2.305417\n",
      "(Iteration 60601 / 76500) loss: 2.305415\n",
      "(Iteration 60701 / 76500) loss: 2.305419\n",
      "(Iteration 60801 / 76500) loss: 2.305421\n",
      "(Iteration 60901 / 76500) loss: 2.305417\n",
      "(Iteration 61001 / 76500) loss: 2.305421\n",
      "(Iteration 61101 / 76500) loss: 2.305404\n",
      "(Epoch 80 / 100) train acc: 0.107000; val_acc: 0.110000\n",
      "(Iteration 61201 / 76500) loss: 2.305404\n",
      "(Iteration 61301 / 76500) loss: 2.305419\n",
      "(Iteration 61401 / 76500) loss: 2.305417\n",
      "(Iteration 61501 / 76500) loss: 2.305420\n",
      "(Iteration 61601 / 76500) loss: 2.305414\n",
      "(Iteration 61701 / 76500) loss: 2.305406\n",
      "(Iteration 61801 / 76500) loss: 2.305403\n",
      "(Iteration 61901 / 76500) loss: 2.305410\n",
      "(Epoch 81 / 100) train acc: 0.119000; val_acc: 0.110000\n",
      "(Iteration 62001 / 76500) loss: 2.305427\n",
      "(Iteration 62101 / 76500) loss: 2.305422\n",
      "(Iteration 62201 / 76500) loss: 2.305417\n",
      "(Iteration 62301 / 76500) loss: 2.305413\n",
      "(Iteration 62401 / 76500) loss: 2.305417\n",
      "(Iteration 62501 / 76500) loss: 2.305419\n",
      "(Iteration 62601 / 76500) loss: 2.305416\n",
      "(Iteration 62701 / 76500) loss: 2.305418\n",
      "(Epoch 82 / 100) train acc: 0.106000; val_acc: 0.110000\n",
      "(Iteration 62801 / 76500) loss: 2.305429\n",
      "(Iteration 62901 / 76500) loss: 2.305413\n",
      "(Iteration 63001 / 76500) loss: 2.305413\n",
      "(Iteration 63101 / 76500) loss: 2.305409\n",
      "(Iteration 63201 / 76500) loss: 2.305418\n",
      "(Iteration 63301 / 76500) loss: 2.305412\n",
      "(Iteration 63401 / 76500) loss: 2.305408\n",
      "(Epoch 83 / 100) train acc: 0.103000; val_acc: 0.110000\n",
      "(Iteration 63501 / 76500) loss: 2.305412\n",
      "(Iteration 63601 / 76500) loss: 2.305418\n",
      "(Iteration 63701 / 76500) loss: 2.305410\n",
      "(Iteration 63801 / 76500) loss: 2.305420\n",
      "(Iteration 63901 / 76500) loss: 2.305424\n",
      "(Iteration 64001 / 76500) loss: 2.305416\n",
      "(Iteration 64101 / 76500) loss: 2.305413\n",
      "(Iteration 64201 / 76500) loss: 2.305413\n",
      "(Epoch 84 / 100) train acc: 0.111000; val_acc: 0.110000\n",
      "(Iteration 64301 / 76500) loss: 2.305418\n",
      "(Iteration 64401 / 76500) loss: 2.305414\n",
      "(Iteration 64501 / 76500) loss: 2.305419\n",
      "(Iteration 64601 / 76500) loss: 2.305410\n",
      "(Iteration 64701 / 76500) loss: 2.305421\n",
      "(Iteration 64801 / 76500) loss: 2.305397\n",
      "(Iteration 64901 / 76500) loss: 2.305415\n",
      "(Iteration 65001 / 76500) loss: 2.305415\n",
      "(Epoch 85 / 100) train acc: 0.107000; val_acc: 0.110000\n",
      "(Iteration 65101 / 76500) loss: 2.305413\n",
      "(Iteration 65201 / 76500) loss: 2.305422\n",
      "(Iteration 65301 / 76500) loss: 2.305403\n",
      "(Iteration 65401 / 76500) loss: 2.305407\n",
      "(Iteration 65501 / 76500) loss: 2.305428\n",
      "(Iteration 65601 / 76500) loss: 2.305412\n",
      "(Iteration 65701 / 76500) loss: 2.305408\n",
      "(Epoch 86 / 100) train acc: 0.118000; val_acc: 0.110000\n",
      "(Iteration 65801 / 76500) loss: 2.305420\n",
      "(Iteration 65901 / 76500) loss: 2.305413\n",
      "(Iteration 66001 / 76500) loss: 2.305419\n",
      "(Iteration 66101 / 76500) loss: 2.305419\n",
      "(Iteration 66201 / 76500) loss: 2.305415\n",
      "(Iteration 66301 / 76500) loss: 2.305414\n",
      "(Iteration 66401 / 76500) loss: 2.305420\n",
      "(Iteration 66501 / 76500) loss: 2.305417\n",
      "(Epoch 87 / 100) train acc: 0.110000; val_acc: 0.110000\n",
      "(Iteration 66601 / 76500) loss: 2.305412\n",
      "(Iteration 66701 / 76500) loss: 2.305414\n",
      "(Iteration 66801 / 76500) loss: 2.305426\n",
      "(Iteration 66901 / 76500) loss: 2.305407\n",
      "(Iteration 67001 / 76500) loss: 2.305421\n",
      "(Iteration 67101 / 76500) loss: 2.305418\n",
      "(Iteration 67201 / 76500) loss: 2.305428\n",
      "(Iteration 67301 / 76500) loss: 2.305415\n",
      "(Epoch 88 / 100) train acc: 0.107000; val_acc: 0.110000\n",
      "(Iteration 67401 / 76500) loss: 2.305422\n",
      "(Iteration 67501 / 76500) loss: 2.305425\n",
      "(Iteration 67601 / 76500) loss: 2.305421\n",
      "(Iteration 67701 / 76500) loss: 2.305413\n",
      "(Iteration 67801 / 76500) loss: 2.305414\n",
      "(Iteration 67901 / 76500) loss: 2.305406\n",
      "(Iteration 68001 / 76500) loss: 2.305417\n",
      "(Epoch 89 / 100) train acc: 0.105000; val_acc: 0.110000\n",
      "(Iteration 68101 / 76500) loss: 2.305417\n",
      "(Iteration 68201 / 76500) loss: 2.305417\n",
      "(Iteration 68301 / 76500) loss: 2.305413\n",
      "(Iteration 68401 / 76500) loss: 2.305419\n",
      "(Iteration 68501 / 76500) loss: 2.305418\n",
      "(Iteration 68601 / 76500) loss: 2.305413\n",
      "(Iteration 68701 / 76500) loss: 2.305418\n",
      "(Iteration 68801 / 76500) loss: 2.305412\n",
      "(Epoch 90 / 100) train acc: 0.111000; val_acc: 0.110000\n",
      "(Iteration 68901 / 76500) loss: 2.305414\n",
      "(Iteration 69001 / 76500) loss: 2.305419\n",
      "(Iteration 69101 / 76500) loss: 2.305409\n",
      "(Iteration 69201 / 76500) loss: 2.305420\n",
      "(Iteration 69301 / 76500) loss: 2.305397\n",
      "(Iteration 69401 / 76500) loss: 2.305416\n",
      "(Iteration 69501 / 76500) loss: 2.305408\n",
      "(Iteration 69601 / 76500) loss: 2.305423\n",
      "(Epoch 91 / 100) train acc: 0.116000; val_acc: 0.110000\n",
      "(Iteration 69701 / 76500) loss: 2.305411\n",
      "(Iteration 69801 / 76500) loss: 2.305420\n",
      "(Iteration 69901 / 76500) loss: 2.305426\n",
      "(Iteration 70001 / 76500) loss: 2.305422\n",
      "(Iteration 70101 / 76500) loss: 2.305409\n",
      "(Iteration 70201 / 76500) loss: 2.305427\n",
      "(Iteration 70301 / 76500) loss: 2.305411\n",
      "(Epoch 92 / 100) train acc: 0.137000; val_acc: 0.110000\n",
      "(Iteration 70401 / 76500) loss: 2.305417\n",
      "(Iteration 70501 / 76500) loss: 2.305411\n",
      "(Iteration 70601 / 76500) loss: 2.305416\n",
      "(Iteration 70701 / 76500) loss: 2.305414\n",
      "(Iteration 70801 / 76500) loss: 2.305422\n",
      "(Iteration 70901 / 76500) loss: 2.305420\n",
      "(Iteration 71001 / 76500) loss: 2.305417\n",
      "(Iteration 71101 / 76500) loss: 2.305414\n",
      "(Epoch 93 / 100) train acc: 0.104000; val_acc: 0.110000\n",
      "(Iteration 71201 / 76500) loss: 2.305414\n",
      "(Iteration 71301 / 76500) loss: 2.305410\n",
      "(Iteration 71401 / 76500) loss: 2.305422\n",
      "(Iteration 71501 / 76500) loss: 2.305421\n",
      "(Iteration 71601 / 76500) loss: 2.305417\n",
      "(Iteration 71701 / 76500) loss: 2.305411\n",
      "(Iteration 71801 / 76500) loss: 2.305415\n",
      "(Iteration 71901 / 76500) loss: 2.305420\n",
      "(Epoch 94 / 100) train acc: 0.119000; val_acc: 0.110000\n",
      "(Iteration 72001 / 76500) loss: 2.305414\n",
      "(Iteration 72101 / 76500) loss: 2.305422\n",
      "(Iteration 72201 / 76500) loss: 2.305411\n",
      "(Iteration 72301 / 76500) loss: 2.305423\n",
      "(Iteration 72401 / 76500) loss: 2.305419\n",
      "(Iteration 72501 / 76500) loss: 2.305417\n",
      "(Iteration 72601 / 76500) loss: 2.305404\n",
      "(Epoch 95 / 100) train acc: 0.097000; val_acc: 0.110000\n",
      "(Iteration 72701 / 76500) loss: 2.305421\n",
      "(Iteration 72801 / 76500) loss: 2.305430\n",
      "(Iteration 72901 / 76500) loss: 2.305425\n",
      "(Iteration 73001 / 76500) loss: 2.305421\n",
      "(Iteration 73101 / 76500) loss: 2.305404\n",
      "(Iteration 73201 / 76500) loss: 2.305417\n",
      "(Iteration 73301 / 76500) loss: 2.305410\n",
      "(Iteration 73401 / 76500) loss: 2.305423\n",
      "(Epoch 96 / 100) train acc: 0.116000; val_acc: 0.110000\n",
      "(Iteration 73501 / 76500) loss: 2.305420\n",
      "(Iteration 73601 / 76500) loss: 2.305415\n",
      "(Iteration 73701 / 76500) loss: 2.305421\n",
      "(Iteration 73801 / 76500) loss: 2.305411\n",
      "(Iteration 73901 / 76500) loss: 2.305426\n",
      "(Iteration 74001 / 76500) loss: 2.305410\n",
      "(Iteration 74101 / 76500) loss: 2.305421\n",
      "(Iteration 74201 / 76500) loss: 2.305424\n",
      "(Epoch 97 / 100) train acc: 0.111000; val_acc: 0.110000\n",
      "(Iteration 74301 / 76500) loss: 2.305410\n",
      "(Iteration 74401 / 76500) loss: 2.305417\n",
      "(Iteration 74501 / 76500) loss: 2.305416\n",
      "(Iteration 74601 / 76500) loss: 2.305428\n",
      "(Iteration 74701 / 76500) loss: 2.305431\n",
      "(Iteration 74801 / 76500) loss: 2.305413\n",
      "(Iteration 74901 / 76500) loss: 2.305424\n",
      "(Epoch 98 / 100) train acc: 0.107000; val_acc: 0.110000\n",
      "(Iteration 75001 / 76500) loss: 2.305409\n",
      "(Iteration 75101 / 76500) loss: 2.305416\n",
      "(Iteration 75201 / 76500) loss: 2.305410\n",
      "(Iteration 75301 / 76500) loss: 2.305416\n",
      "(Iteration 75401 / 76500) loss: 2.305419\n",
      "(Iteration 75501 / 76500) loss: 2.305424\n",
      "(Iteration 75601 / 76500) loss: 2.305414\n",
      "(Iteration 75701 / 76500) loss: 2.305415\n",
      "(Epoch 99 / 100) train acc: 0.128000; val_acc: 0.110000\n",
      "(Iteration 75801 / 76500) loss: 2.305404\n",
      "(Iteration 75901 / 76500) loss: 2.305424\n",
      "(Iteration 76001 / 76500) loss: 2.305414\n",
      "(Iteration 76101 / 76500) loss: 2.305417\n",
      "(Iteration 76201 / 76500) loss: 2.305417\n",
      "(Iteration 76301 / 76500) loss: 2.305426\n",
      "(Iteration 76401 / 76500) loss: 2.305422\n",
      "(Epoch 100 / 100) train acc: 0.091000; val_acc: 0.110000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.305544\n",
      "(Epoch 0 / 100) train acc: 0.111000; val_acc: 0.119000\n",
      "(Iteration 101 / 38200) loss: 2.305545\n",
      "(Iteration 201 / 38200) loss: 2.305542\n",
      "(Iteration 301 / 38200) loss: 2.305534\n",
      "(Epoch 1 / 100) train acc: 0.115000; val_acc: 0.119000\n",
      "(Iteration 401 / 38200) loss: 2.305539\n",
      "(Iteration 501 / 38200) loss: 2.305540\n",
      "(Iteration 601 / 38200) loss: 2.305536\n",
      "(Iteration 701 / 38200) loss: 2.305547\n",
      "(Epoch 2 / 100) train acc: 0.122000; val_acc: 0.119000\n",
      "(Iteration 801 / 38200) loss: 2.305535\n",
      "(Iteration 901 / 38200) loss: 2.305534\n",
      "(Iteration 1001 / 38200) loss: 2.305538\n",
      "(Iteration 1101 / 38200) loss: 2.305539\n",
      "(Epoch 3 / 100) train acc: 0.107000; val_acc: 0.119000\n",
      "(Iteration 1201 / 38200) loss: 2.305529\n",
      "(Iteration 1301 / 38200) loss: 2.305537\n",
      "(Iteration 1401 / 38200) loss: 2.305532\n",
      "(Iteration 1501 / 38200) loss: 2.305542\n",
      "(Epoch 4 / 100) train acc: 0.111000; val_acc: 0.119000\n",
      "(Iteration 1601 / 38200) loss: 2.305539\n",
      "(Iteration 1701 / 38200) loss: 2.305543\n",
      "(Iteration 1801 / 38200) loss: 2.305544\n",
      "(Iteration 1901 / 38200) loss: 2.305537\n",
      "(Epoch 5 / 100) train acc: 0.120000; val_acc: 0.119000\n",
      "(Iteration 2001 / 38200) loss: 2.305538\n",
      "(Iteration 2101 / 38200) loss: 2.305532\n",
      "(Iteration 2201 / 38200) loss: 2.305540\n",
      "(Epoch 6 / 100) train acc: 0.106000; val_acc: 0.119000\n",
      "(Iteration 2301 / 38200) loss: 2.305544\n",
      "(Iteration 2401 / 38200) loss: 2.305539\n",
      "(Iteration 2501 / 38200) loss: 2.305539\n",
      "(Iteration 2601 / 38200) loss: 2.305536\n",
      "(Epoch 7 / 100) train acc: 0.113000; val_acc: 0.119000\n",
      "(Iteration 2701 / 38200) loss: 2.305539\n",
      "(Iteration 2801 / 38200) loss: 2.305538\n",
      "(Iteration 2901 / 38200) loss: 2.305543\n",
      "(Iteration 3001 / 38200) loss: 2.305539\n",
      "(Epoch 8 / 100) train acc: 0.108000; val_acc: 0.119000\n",
      "(Iteration 3101 / 38200) loss: 2.305534\n",
      "(Iteration 3201 / 38200) loss: 2.305544\n",
      "(Iteration 3301 / 38200) loss: 2.305542\n",
      "(Iteration 3401 / 38200) loss: 2.305534\n",
      "(Epoch 9 / 100) train acc: 0.100000; val_acc: 0.119000\n",
      "(Iteration 3501 / 38200) loss: 2.305543\n",
      "(Iteration 3601 / 38200) loss: 2.305542\n",
      "(Iteration 3701 / 38200) loss: 2.305529\n",
      "(Iteration 3801 / 38200) loss: 2.305536\n",
      "(Epoch 10 / 100) train acc: 0.117000; val_acc: 0.119000\n",
      "(Iteration 3901 / 38200) loss: 2.305538\n",
      "(Iteration 4001 / 38200) loss: 2.305535\n",
      "(Iteration 4101 / 38200) loss: 2.305536\n",
      "(Iteration 4201 / 38200) loss: 2.305541\n",
      "(Epoch 11 / 100) train acc: 0.091000; val_acc: 0.119000\n",
      "(Iteration 4301 / 38200) loss: 2.305537\n",
      "(Iteration 4401 / 38200) loss: 2.305535\n",
      "(Iteration 4501 / 38200) loss: 2.305539\n",
      "(Epoch 12 / 100) train acc: 0.126000; val_acc: 0.119000\n",
      "(Iteration 4601 / 38200) loss: 2.305531\n",
      "(Iteration 4701 / 38200) loss: 2.305539\n",
      "(Iteration 4801 / 38200) loss: 2.305535\n",
      "(Iteration 4901 / 38200) loss: 2.305539\n",
      "(Epoch 13 / 100) train acc: 0.130000; val_acc: 0.119000\n",
      "(Iteration 5001 / 38200) loss: 2.305538\n",
      "(Iteration 5101 / 38200) loss: 2.305542\n",
      "(Iteration 5201 / 38200) loss: 2.305537\n",
      "(Iteration 5301 / 38200) loss: 2.305541\n",
      "(Epoch 14 / 100) train acc: 0.104000; val_acc: 0.119000\n",
      "(Iteration 5401 / 38200) loss: 2.305540\n",
      "(Iteration 5501 / 38200) loss: 2.305541\n",
      "(Iteration 5601 / 38200) loss: 2.305545\n",
      "(Iteration 5701 / 38200) loss: 2.305526\n",
      "(Epoch 15 / 100) train acc: 0.102000; val_acc: 0.119000\n",
      "(Iteration 5801 / 38200) loss: 2.305540\n",
      "(Iteration 5901 / 38200) loss: 2.305535\n",
      "(Iteration 6001 / 38200) loss: 2.305549\n",
      "(Iteration 6101 / 38200) loss: 2.305537\n",
      "(Epoch 16 / 100) train acc: 0.104000; val_acc: 0.119000\n",
      "(Iteration 6201 / 38200) loss: 2.305539\n",
      "(Iteration 6301 / 38200) loss: 2.305533\n",
      "(Iteration 6401 / 38200) loss: 2.305542\n",
      "(Epoch 17 / 100) train acc: 0.127000; val_acc: 0.119000\n",
      "(Iteration 6501 / 38200) loss: 2.305539\n",
      "(Iteration 6601 / 38200) loss: 2.305532\n",
      "(Iteration 6701 / 38200) loss: 2.305545\n",
      "(Iteration 6801 / 38200) loss: 2.305540\n",
      "(Epoch 18 / 100) train acc: 0.118000; val_acc: 0.119000\n",
      "(Iteration 6901 / 38200) loss: 2.305546\n",
      "(Iteration 7001 / 38200) loss: 2.305540\n",
      "(Iteration 7101 / 38200) loss: 2.305537\n",
      "(Iteration 7201 / 38200) loss: 2.305542\n",
      "(Epoch 19 / 100) train acc: 0.101000; val_acc: 0.119000\n",
      "(Iteration 7301 / 38200) loss: 2.305536\n",
      "(Iteration 7401 / 38200) loss: 2.305536\n",
      "(Iteration 7501 / 38200) loss: 2.305546\n",
      "(Iteration 7601 / 38200) loss: 2.305542\n",
      "(Epoch 20 / 100) train acc: 0.107000; val_acc: 0.119000\n",
      "(Iteration 7701 / 38200) loss: 2.305537\n",
      "(Iteration 7801 / 38200) loss: 2.305540\n",
      "(Iteration 7901 / 38200) loss: 2.305544\n",
      "(Iteration 8001 / 38200) loss: 2.305533\n",
      "(Epoch 21 / 100) train acc: 0.120000; val_acc: 0.119000\n",
      "(Iteration 8101 / 38200) loss: 2.305529\n",
      "(Iteration 8201 / 38200) loss: 2.305535\n",
      "(Iteration 8301 / 38200) loss: 2.305531\n",
      "(Iteration 8401 / 38200) loss: 2.305537\n",
      "(Epoch 22 / 100) train acc: 0.112000; val_acc: 0.119000\n",
      "(Iteration 8501 / 38200) loss: 2.305545\n",
      "(Iteration 8601 / 38200) loss: 2.305535\n",
      "(Iteration 8701 / 38200) loss: 2.305539\n",
      "(Epoch 23 / 100) train acc: 0.121000; val_acc: 0.119000\n",
      "(Iteration 8801 / 38200) loss: 2.305542\n",
      "(Iteration 8901 / 38200) loss: 2.305533\n",
      "(Iteration 9001 / 38200) loss: 2.305540\n",
      "(Iteration 9101 / 38200) loss: 2.305543\n",
      "(Epoch 24 / 100) train acc: 0.129000; val_acc: 0.119000\n",
      "(Iteration 9201 / 38200) loss: 2.305543\n",
      "(Iteration 9301 / 38200) loss: 2.305548\n",
      "(Iteration 9401 / 38200) loss: 2.305538\n",
      "(Iteration 9501 / 38200) loss: 2.305535\n",
      "(Epoch 25 / 100) train acc: 0.129000; val_acc: 0.119000\n",
      "(Iteration 9601 / 38200) loss: 2.305537\n",
      "(Iteration 9701 / 38200) loss: 2.305542\n",
      "(Iteration 9801 / 38200) loss: 2.305538\n",
      "(Iteration 9901 / 38200) loss: 2.305541\n",
      "(Epoch 26 / 100) train acc: 0.115000; val_acc: 0.119000\n",
      "(Iteration 10001 / 38200) loss: 2.305537\n",
      "(Iteration 10101 / 38200) loss: 2.305538\n",
      "(Iteration 10201 / 38200) loss: 2.305536\n",
      "(Iteration 10301 / 38200) loss: 2.305543\n",
      "(Epoch 27 / 100) train acc: 0.104000; val_acc: 0.119000\n",
      "(Iteration 10401 / 38200) loss: 2.305536\n",
      "(Iteration 10501 / 38200) loss: 2.305528\n",
      "(Iteration 10601 / 38200) loss: 2.305541\n",
      "(Epoch 28 / 100) train acc: 0.115000; val_acc: 0.119000\n",
      "(Iteration 10701 / 38200) loss: 2.305538\n",
      "(Iteration 10801 / 38200) loss: 2.305540\n",
      "(Iteration 10901 / 38200) loss: 2.305541\n",
      "(Iteration 11001 / 38200) loss: 2.305532\n",
      "(Epoch 29 / 100) train acc: 0.116000; val_acc: 0.119000\n",
      "(Iteration 11101 / 38200) loss: 2.305540\n",
      "(Iteration 11201 / 38200) loss: 2.305540\n",
      "(Iteration 11301 / 38200) loss: 2.305538\n",
      "(Iteration 11401 / 38200) loss: 2.305534\n",
      "(Epoch 30 / 100) train acc: 0.110000; val_acc: 0.119000\n",
      "(Iteration 11501 / 38200) loss: 2.305536\n",
      "(Iteration 11601 / 38200) loss: 2.305543\n",
      "(Iteration 11701 / 38200) loss: 2.305538\n",
      "(Iteration 11801 / 38200) loss: 2.305537\n",
      "(Epoch 31 / 100) train acc: 0.105000; val_acc: 0.119000\n",
      "(Iteration 11901 / 38200) loss: 2.305542\n",
      "(Iteration 12001 / 38200) loss: 2.305533\n",
      "(Iteration 12101 / 38200) loss: 2.305535\n",
      "(Iteration 12201 / 38200) loss: 2.305546\n",
      "(Epoch 32 / 100) train acc: 0.102000; val_acc: 0.119000\n",
      "(Iteration 12301 / 38200) loss: 2.305538\n",
      "(Iteration 12401 / 38200) loss: 2.305527\n",
      "(Iteration 12501 / 38200) loss: 2.305543\n",
      "(Iteration 12601 / 38200) loss: 2.305539\n",
      "(Epoch 33 / 100) train acc: 0.100000; val_acc: 0.119000\n",
      "(Iteration 12701 / 38200) loss: 2.305537\n",
      "(Iteration 12801 / 38200) loss: 2.305538\n",
      "(Iteration 12901 / 38200) loss: 2.305538\n",
      "(Epoch 34 / 100) train acc: 0.115000; val_acc: 0.119000\n",
      "(Iteration 13001 / 38200) loss: 2.305537\n",
      "(Iteration 13101 / 38200) loss: 2.305534\n",
      "(Iteration 13201 / 38200) loss: 2.305541\n",
      "(Iteration 13301 / 38200) loss: 2.305537\n",
      "(Epoch 35 / 100) train acc: 0.099000; val_acc: 0.119000\n",
      "(Iteration 13401 / 38200) loss: 2.305544\n",
      "(Iteration 13501 / 38200) loss: 2.305542\n",
      "(Iteration 13601 / 38200) loss: 2.305542\n",
      "(Iteration 13701 / 38200) loss: 2.305540\n",
      "(Epoch 36 / 100) train acc: 0.120000; val_acc: 0.119000\n",
      "(Iteration 13801 / 38200) loss: 2.305534\n",
      "(Iteration 13901 / 38200) loss: 2.305541\n",
      "(Iteration 14001 / 38200) loss: 2.305537\n",
      "(Iteration 14101 / 38200) loss: 2.305542\n",
      "(Epoch 37 / 100) train acc: 0.102000; val_acc: 0.119000\n",
      "(Iteration 14201 / 38200) loss: 2.305539\n",
      "(Iteration 14301 / 38200) loss: 2.305546\n",
      "(Iteration 14401 / 38200) loss: 2.305543\n",
      "(Iteration 14501 / 38200) loss: 2.305541\n",
      "(Epoch 38 / 100) train acc: 0.101000; val_acc: 0.119000\n",
      "(Iteration 14601 / 38200) loss: 2.305533\n",
      "(Iteration 14701 / 38200) loss: 2.305535\n",
      "(Iteration 14801 / 38200) loss: 2.305541\n",
      "(Epoch 39 / 100) train acc: 0.122000; val_acc: 0.119000\n",
      "(Iteration 14901 / 38200) loss: 2.305533\n",
      "(Iteration 15001 / 38200) loss: 2.305538\n",
      "(Iteration 15101 / 38200) loss: 2.305532\n",
      "(Iteration 15201 / 38200) loss: 2.305535\n",
      "(Epoch 40 / 100) train acc: 0.120000; val_acc: 0.119000\n",
      "(Iteration 15301 / 38200) loss: 2.305540\n",
      "(Iteration 15401 / 38200) loss: 2.305541\n",
      "(Iteration 15501 / 38200) loss: 2.305544\n",
      "(Iteration 15601 / 38200) loss: 2.305529\n",
      "(Epoch 41 / 100) train acc: 0.102000; val_acc: 0.119000\n",
      "(Iteration 15701 / 38200) loss: 2.305533\n",
      "(Iteration 15801 / 38200) loss: 2.305548\n",
      "(Iteration 15901 / 38200) loss: 2.305540\n",
      "(Iteration 16001 / 38200) loss: 2.305530\n",
      "(Epoch 42 / 100) train acc: 0.109000; val_acc: 0.119000\n",
      "(Iteration 16101 / 38200) loss: 2.305540\n",
      "(Iteration 16201 / 38200) loss: 2.305542\n",
      "(Iteration 16301 / 38200) loss: 2.305540\n",
      "(Iteration 16401 / 38200) loss: 2.305531\n",
      "(Epoch 43 / 100) train acc: 0.099000; val_acc: 0.119000\n",
      "(Iteration 16501 / 38200) loss: 2.305540\n",
      "(Iteration 16601 / 38200) loss: 2.305536\n",
      "(Iteration 16701 / 38200) loss: 2.305540\n",
      "(Iteration 16801 / 38200) loss: 2.305538\n",
      "(Epoch 44 / 100) train acc: 0.111000; val_acc: 0.119000\n",
      "(Iteration 16901 / 38200) loss: 2.305542\n",
      "(Iteration 17001 / 38200) loss: 2.305530\n",
      "(Iteration 17101 / 38200) loss: 2.305538\n",
      "(Epoch 45 / 100) train acc: 0.111000; val_acc: 0.119000\n",
      "(Iteration 17201 / 38200) loss: 2.305536\n",
      "(Iteration 17301 / 38200) loss: 2.305542\n",
      "(Iteration 17401 / 38200) loss: 2.305539\n",
      "(Iteration 17501 / 38200) loss: 2.305538\n",
      "(Epoch 46 / 100) train acc: 0.118000; val_acc: 0.119000\n",
      "(Iteration 17601 / 38200) loss: 2.305541\n",
      "(Iteration 17701 / 38200) loss: 2.305543\n",
      "(Iteration 17801 / 38200) loss: 2.305540\n",
      "(Iteration 17901 / 38200) loss: 2.305542\n",
      "(Epoch 47 / 100) train acc: 0.103000; val_acc: 0.119000\n",
      "(Iteration 18001 / 38200) loss: 2.305536\n",
      "(Iteration 18101 / 38200) loss: 2.305544\n",
      "(Iteration 18201 / 38200) loss: 2.305533\n",
      "(Iteration 18301 / 38200) loss: 2.305540\n",
      "(Epoch 48 / 100) train acc: 0.104000; val_acc: 0.119000\n",
      "(Iteration 18401 / 38200) loss: 2.305544\n",
      "(Iteration 18501 / 38200) loss: 2.305537\n",
      "(Iteration 18601 / 38200) loss: 2.305536\n",
      "(Iteration 18701 / 38200) loss: 2.305539\n",
      "(Epoch 49 / 100) train acc: 0.137000; val_acc: 0.119000\n",
      "(Iteration 18801 / 38200) loss: 2.305538\n",
      "(Iteration 18901 / 38200) loss: 2.305537\n",
      "(Iteration 19001 / 38200) loss: 2.305537\n",
      "(Epoch 50 / 100) train acc: 0.108000; val_acc: 0.119000\n",
      "(Iteration 19101 / 38200) loss: 2.305542\n",
      "(Iteration 19201 / 38200) loss: 2.305535\n",
      "(Iteration 19301 / 38200) loss: 2.305544\n",
      "(Iteration 19401 / 38200) loss: 2.305537\n",
      "(Epoch 51 / 100) train acc: 0.118000; val_acc: 0.119000\n",
      "(Iteration 19501 / 38200) loss: 2.305544\n",
      "(Iteration 19601 / 38200) loss: 2.305540\n",
      "(Iteration 19701 / 38200) loss: 2.305535\n",
      "(Iteration 19801 / 38200) loss: 2.305533\n",
      "(Epoch 52 / 100) train acc: 0.089000; val_acc: 0.119000\n",
      "(Iteration 19901 / 38200) loss: 2.305547\n",
      "(Iteration 20001 / 38200) loss: 2.305539\n",
      "(Iteration 20101 / 38200) loss: 2.305529\n",
      "(Iteration 20201 / 38200) loss: 2.305546\n",
      "(Epoch 53 / 100) train acc: 0.127000; val_acc: 0.119000\n",
      "(Iteration 20301 / 38200) loss: 2.305533\n",
      "(Iteration 20401 / 38200) loss: 2.305535\n",
      "(Iteration 20501 / 38200) loss: 2.305540\n",
      "(Iteration 20601 / 38200) loss: 2.305538\n",
      "(Epoch 54 / 100) train acc: 0.110000; val_acc: 0.119000\n",
      "(Iteration 20701 / 38200) loss: 2.305535\n",
      "(Iteration 20801 / 38200) loss: 2.305536\n",
      "(Iteration 20901 / 38200) loss: 2.305543\n",
      "(Iteration 21001 / 38200) loss: 2.305540\n",
      "(Epoch 55 / 100) train acc: 0.102000; val_acc: 0.119000\n",
      "(Iteration 21101 / 38200) loss: 2.305538\n",
      "(Iteration 21201 / 38200) loss: 2.305534\n",
      "(Iteration 21301 / 38200) loss: 2.305538\n",
      "(Epoch 56 / 100) train acc: 0.103000; val_acc: 0.119000\n",
      "(Iteration 21401 / 38200) loss: 2.305538\n",
      "(Iteration 21501 / 38200) loss: 2.305535\n",
      "(Iteration 21601 / 38200) loss: 2.305535\n",
      "(Iteration 21701 / 38200) loss: 2.305536\n",
      "(Epoch 57 / 100) train acc: 0.096000; val_acc: 0.119000\n",
      "(Iteration 21801 / 38200) loss: 2.305542\n",
      "(Iteration 21901 / 38200) loss: 2.305537\n",
      "(Iteration 22001 / 38200) loss: 2.305541\n",
      "(Iteration 22101 / 38200) loss: 2.305541\n",
      "(Epoch 58 / 100) train acc: 0.102000; val_acc: 0.119000\n",
      "(Iteration 22201 / 38200) loss: 2.305542\n",
      "(Iteration 22301 / 38200) loss: 2.305539\n",
      "(Iteration 22401 / 38200) loss: 2.305537\n",
      "(Iteration 22501 / 38200) loss: 2.305536\n",
      "(Epoch 59 / 100) train acc: 0.114000; val_acc: 0.119000\n",
      "(Iteration 22601 / 38200) loss: 2.305539\n",
      "(Iteration 22701 / 38200) loss: 2.305538\n",
      "(Iteration 22801 / 38200) loss: 2.305540\n",
      "(Iteration 22901 / 38200) loss: 2.305530\n",
      "(Epoch 60 / 100) train acc: 0.139000; val_acc: 0.119000\n",
      "(Iteration 23001 / 38200) loss: 2.305539\n",
      "(Iteration 23101 / 38200) loss: 2.305546\n",
      "(Iteration 23201 / 38200) loss: 2.305532\n",
      "(Iteration 23301 / 38200) loss: 2.305530\n",
      "(Epoch 61 / 100) train acc: 0.118000; val_acc: 0.119000\n",
      "(Iteration 23401 / 38200) loss: 2.305545\n",
      "(Iteration 23501 / 38200) loss: 2.305536\n",
      "(Iteration 23601 / 38200) loss: 2.305544\n",
      "(Epoch 62 / 100) train acc: 0.107000; val_acc: 0.119000\n",
      "(Iteration 23701 / 38200) loss: 2.305545\n",
      "(Iteration 23801 / 38200) loss: 2.305538\n",
      "(Iteration 23901 / 38200) loss: 2.305548\n",
      "(Iteration 24001 / 38200) loss: 2.305542\n",
      "(Epoch 63 / 100) train acc: 0.113000; val_acc: 0.119000\n",
      "(Iteration 24101 / 38200) loss: 2.305530\n",
      "(Iteration 24201 / 38200) loss: 2.305531\n",
      "(Iteration 24301 / 38200) loss: 2.305537\n",
      "(Iteration 24401 / 38200) loss: 2.305543\n",
      "(Epoch 64 / 100) train acc: 0.095000; val_acc: 0.119000\n",
      "(Iteration 24501 / 38200) loss: 2.305544\n",
      "(Iteration 24601 / 38200) loss: 2.305534\n",
      "(Iteration 24701 / 38200) loss: 2.305533\n",
      "(Iteration 24801 / 38200) loss: 2.305532\n",
      "(Epoch 65 / 100) train acc: 0.128000; val_acc: 0.119000\n",
      "(Iteration 24901 / 38200) loss: 2.305545\n",
      "(Iteration 25001 / 38200) loss: 2.305536\n",
      "(Iteration 25101 / 38200) loss: 2.305529\n",
      "(Iteration 25201 / 38200) loss: 2.305536\n",
      "(Epoch 66 / 100) train acc: 0.106000; val_acc: 0.119000\n",
      "(Iteration 25301 / 38200) loss: 2.305539\n",
      "(Iteration 25401 / 38200) loss: 2.305532\n",
      "(Iteration 25501 / 38200) loss: 2.305534\n",
      "(Epoch 67 / 100) train acc: 0.099000; val_acc: 0.119000\n",
      "(Iteration 25601 / 38200) loss: 2.305537\n",
      "(Iteration 25701 / 38200) loss: 2.305536\n",
      "(Iteration 25801 / 38200) loss: 2.305542\n",
      "(Iteration 25901 / 38200) loss: 2.305533\n",
      "(Epoch 68 / 100) train acc: 0.109000; val_acc: 0.119000\n",
      "(Iteration 26001 / 38200) loss: 2.305539\n",
      "(Iteration 26101 / 38200) loss: 2.305538\n",
      "(Iteration 26201 / 38200) loss: 2.305534\n",
      "(Iteration 26301 / 38200) loss: 2.305535\n",
      "(Epoch 69 / 100) train acc: 0.102000; val_acc: 0.119000\n",
      "(Iteration 26401 / 38200) loss: 2.305536\n",
      "(Iteration 26501 / 38200) loss: 2.305535\n",
      "(Iteration 26601 / 38200) loss: 2.305545\n",
      "(Iteration 26701 / 38200) loss: 2.305532\n",
      "(Epoch 70 / 100) train acc: 0.099000; val_acc: 0.119000\n",
      "(Iteration 26801 / 38200) loss: 2.305534\n",
      "(Iteration 26901 / 38200) loss: 2.305527\n",
      "(Iteration 27001 / 38200) loss: 2.305533\n",
      "(Iteration 27101 / 38200) loss: 2.305539\n",
      "(Epoch 71 / 100) train acc: 0.105000; val_acc: 0.119000\n",
      "(Iteration 27201 / 38200) loss: 2.305539\n",
      "(Iteration 27301 / 38200) loss: 2.305542\n",
      "(Iteration 27401 / 38200) loss: 2.305535\n",
      "(Iteration 27501 / 38200) loss: 2.305543\n",
      "(Epoch 72 / 100) train acc: 0.126000; val_acc: 0.119000\n",
      "(Iteration 27601 / 38200) loss: 2.305537\n",
      "(Iteration 27701 / 38200) loss: 2.305534\n",
      "(Iteration 27801 / 38200) loss: 2.305541\n",
      "(Epoch 73 / 100) train acc: 0.114000; val_acc: 0.119000\n",
      "(Iteration 27901 / 38200) loss: 2.305537\n",
      "(Iteration 28001 / 38200) loss: 2.305540\n",
      "(Iteration 28101 / 38200) loss: 2.305539\n",
      "(Iteration 28201 / 38200) loss: 2.305537\n",
      "(Epoch 74 / 100) train acc: 0.120000; val_acc: 0.119000\n",
      "(Iteration 28301 / 38200) loss: 2.305536\n",
      "(Iteration 28401 / 38200) loss: 2.305543\n",
      "(Iteration 28501 / 38200) loss: 2.305540\n",
      "(Iteration 28601 / 38200) loss: 2.305539\n",
      "(Epoch 75 / 100) train acc: 0.108000; val_acc: 0.119000\n",
      "(Iteration 28701 / 38200) loss: 2.305533\n",
      "(Iteration 28801 / 38200) loss: 2.305530\n",
      "(Iteration 28901 / 38200) loss: 2.305531\n",
      "(Iteration 29001 / 38200) loss: 2.305539\n",
      "(Epoch 76 / 100) train acc: 0.105000; val_acc: 0.119000\n",
      "(Iteration 29101 / 38200) loss: 2.305545\n",
      "(Iteration 29201 / 38200) loss: 2.305529\n",
      "(Iteration 29301 / 38200) loss: 2.305542\n",
      "(Iteration 29401 / 38200) loss: 2.305539\n",
      "(Epoch 77 / 100) train acc: 0.092000; val_acc: 0.119000\n",
      "(Iteration 29501 / 38200) loss: 2.305539\n",
      "(Iteration 29601 / 38200) loss: 2.305532\n",
      "(Iteration 29701 / 38200) loss: 2.305528\n",
      "(Epoch 78 / 100) train acc: 0.095000; val_acc: 0.119000\n",
      "(Iteration 29801 / 38200) loss: 2.305539\n",
      "(Iteration 29901 / 38200) loss: 2.305539\n",
      "(Iteration 30001 / 38200) loss: 2.305537\n",
      "(Iteration 30101 / 38200) loss: 2.305537\n",
      "(Epoch 79 / 100) train acc: 0.092000; val_acc: 0.119000\n",
      "(Iteration 30201 / 38200) loss: 2.305533\n",
      "(Iteration 30301 / 38200) loss: 2.305539\n",
      "(Iteration 30401 / 38200) loss: 2.305537\n",
      "(Iteration 30501 / 38200) loss: 2.305542\n",
      "(Epoch 80 / 100) train acc: 0.102000; val_acc: 0.119000\n",
      "(Iteration 30601 / 38200) loss: 2.305541\n",
      "(Iteration 30701 / 38200) loss: 2.305540\n",
      "(Iteration 30801 / 38200) loss: 2.305538\n",
      "(Iteration 30901 / 38200) loss: 2.305535\n",
      "(Epoch 81 / 100) train acc: 0.110000; val_acc: 0.119000\n",
      "(Iteration 31001 / 38200) loss: 2.305547\n",
      "(Iteration 31101 / 38200) loss: 2.305536\n",
      "(Iteration 31201 / 38200) loss: 2.305532\n",
      "(Iteration 31301 / 38200) loss: 2.305542\n",
      "(Epoch 82 / 100) train acc: 0.095000; val_acc: 0.119000\n",
      "(Iteration 31401 / 38200) loss: 2.305541\n",
      "(Iteration 31501 / 38200) loss: 2.305535\n",
      "(Iteration 31601 / 38200) loss: 2.305532\n",
      "(Iteration 31701 / 38200) loss: 2.305539\n",
      "(Epoch 83 / 100) train acc: 0.127000; val_acc: 0.119000\n",
      "(Iteration 31801 / 38200) loss: 2.305529\n",
      "(Iteration 31901 / 38200) loss: 2.305534\n",
      "(Iteration 32001 / 38200) loss: 2.305536\n",
      "(Epoch 84 / 100) train acc: 0.099000; val_acc: 0.119000\n",
      "(Iteration 32101 / 38200) loss: 2.305541\n",
      "(Iteration 32201 / 38200) loss: 2.305538\n",
      "(Iteration 32301 / 38200) loss: 2.305539\n",
      "(Iteration 32401 / 38200) loss: 2.305536\n",
      "(Epoch 85 / 100) train acc: 0.104000; val_acc: 0.119000\n",
      "(Iteration 32501 / 38200) loss: 2.305542\n",
      "(Iteration 32601 / 38200) loss: 2.305537\n",
      "(Iteration 32701 / 38200) loss: 2.305541\n",
      "(Iteration 32801 / 38200) loss: 2.305535\n",
      "(Epoch 86 / 100) train acc: 0.105000; val_acc: 0.119000\n",
      "(Iteration 32901 / 38200) loss: 2.305537\n",
      "(Iteration 33001 / 38200) loss: 2.305533\n",
      "(Iteration 33101 / 38200) loss: 2.305537\n",
      "(Iteration 33201 / 38200) loss: 2.305547\n",
      "(Epoch 87 / 100) train acc: 0.120000; val_acc: 0.119000\n",
      "(Iteration 33301 / 38200) loss: 2.305533\n",
      "(Iteration 33401 / 38200) loss: 2.305542\n",
      "(Iteration 33501 / 38200) loss: 2.305535\n",
      "(Iteration 33601 / 38200) loss: 2.305531\n",
      "(Epoch 88 / 100) train acc: 0.093000; val_acc: 0.119000\n",
      "(Iteration 33701 / 38200) loss: 2.305533\n",
      "(Iteration 33801 / 38200) loss: 2.305539\n",
      "(Iteration 33901 / 38200) loss: 2.305533\n",
      "(Epoch 89 / 100) train acc: 0.098000; val_acc: 0.119000\n",
      "(Iteration 34001 / 38200) loss: 2.305538\n",
      "(Iteration 34101 / 38200) loss: 2.305541\n",
      "(Iteration 34201 / 38200) loss: 2.305541\n",
      "(Iteration 34301 / 38200) loss: 2.305536\n",
      "(Epoch 90 / 100) train acc: 0.107000; val_acc: 0.119000\n",
      "(Iteration 34401 / 38200) loss: 2.305540\n",
      "(Iteration 34501 / 38200) loss: 2.305540\n",
      "(Iteration 34601 / 38200) loss: 2.305544\n",
      "(Iteration 34701 / 38200) loss: 2.305541\n",
      "(Epoch 91 / 100) train acc: 0.124000; val_acc: 0.119000\n",
      "(Iteration 34801 / 38200) loss: 2.305533\n",
      "(Iteration 34901 / 38200) loss: 2.305537\n",
      "(Iteration 35001 / 38200) loss: 2.305531\n",
      "(Iteration 35101 / 38200) loss: 2.305540\n",
      "(Epoch 92 / 100) train acc: 0.104000; val_acc: 0.119000\n",
      "(Iteration 35201 / 38200) loss: 2.305526\n",
      "(Iteration 35301 / 38200) loss: 2.305537\n",
      "(Iteration 35401 / 38200) loss: 2.305538\n",
      "(Iteration 35501 / 38200) loss: 2.305535\n",
      "(Epoch 93 / 100) train acc: 0.097000; val_acc: 0.119000\n",
      "(Iteration 35601 / 38200) loss: 2.305541\n",
      "(Iteration 35701 / 38200) loss: 2.305532\n",
      "(Iteration 35801 / 38200) loss: 2.305537\n",
      "(Iteration 35901 / 38200) loss: 2.305541\n",
      "(Epoch 94 / 100) train acc: 0.115000; val_acc: 0.119000\n",
      "(Iteration 36001 / 38200) loss: 2.305538\n",
      "(Iteration 36101 / 38200) loss: 2.305538\n",
      "(Iteration 36201 / 38200) loss: 2.305538\n",
      "(Epoch 95 / 100) train acc: 0.124000; val_acc: 0.119000\n",
      "(Iteration 36301 / 38200) loss: 2.305547\n",
      "(Iteration 36401 / 38200) loss: 2.305538\n",
      "(Iteration 36501 / 38200) loss: 2.305536\n",
      "(Iteration 36601 / 38200) loss: 2.305542\n",
      "(Epoch 96 / 100) train acc: 0.123000; val_acc: 0.119000\n",
      "(Iteration 36701 / 38200) loss: 2.305540\n",
      "(Iteration 36801 / 38200) loss: 2.305537\n",
      "(Iteration 36901 / 38200) loss: 2.305537\n",
      "(Iteration 37001 / 38200) loss: 2.305532\n",
      "(Epoch 97 / 100) train acc: 0.101000; val_acc: 0.119000\n",
      "(Iteration 37101 / 38200) loss: 2.305534\n",
      "(Iteration 37201 / 38200) loss: 2.305545\n",
      "(Iteration 37301 / 38200) loss: 2.305540\n",
      "(Iteration 37401 / 38200) loss: 2.305533\n",
      "(Epoch 98 / 100) train acc: 0.123000; val_acc: 0.119000\n",
      "(Iteration 37501 / 38200) loss: 2.305537\n",
      "(Iteration 37601 / 38200) loss: 2.305538\n",
      "(Iteration 37701 / 38200) loss: 2.305531\n",
      "(Iteration 37801 / 38200) loss: 2.305533\n",
      "(Epoch 99 / 100) train acc: 0.116000; val_acc: 0.119000\n",
      "(Iteration 37901 / 38200) loss: 2.305541\n",
      "(Iteration 38001 / 38200) loss: 2.305538\n",
      "(Iteration 38101 / 38200) loss: 2.305534\n",
      "(Epoch 100 / 100) train acc: 0.107000; val_acc: 0.119000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.305401\n",
      "(Epoch 0 / 100) train acc: 0.080000; val_acc: 0.073000\n",
      "(Iteration 101 / 76500) loss: 2.305412\n",
      "(Iteration 201 / 76500) loss: 2.305421\n",
      "(Iteration 301 / 76500) loss: 2.305415\n",
      "(Iteration 401 / 76500) loss: 2.305411\n",
      "(Iteration 501 / 76500) loss: 2.305416\n",
      "(Iteration 601 / 76500) loss: 2.305422\n",
      "(Iteration 701 / 76500) loss: 2.305422\n",
      "(Epoch 1 / 100) train acc: 0.077000; val_acc: 0.073000\n",
      "(Iteration 801 / 76500) loss: 2.305411\n",
      "(Iteration 901 / 76500) loss: 2.305413\n",
      "(Iteration 1001 / 76500) loss: 2.305416\n",
      "(Iteration 1101 / 76500) loss: 2.305419\n",
      "(Iteration 1201 / 76500) loss: 2.305415\n",
      "(Iteration 1301 / 76500) loss: 2.305411\n",
      "(Iteration 1401 / 76500) loss: 2.305414\n",
      "(Iteration 1501 / 76500) loss: 2.305411\n",
      "(Epoch 2 / 100) train acc: 0.093000; val_acc: 0.073000\n",
      "(Iteration 1601 / 76500) loss: 2.305411\n",
      "(Iteration 1701 / 76500) loss: 2.305402\n",
      "(Iteration 1801 / 76500) loss: 2.305418\n",
      "(Iteration 1901 / 76500) loss: 2.305423\n",
      "(Iteration 2001 / 76500) loss: 2.305417\n",
      "(Iteration 2101 / 76500) loss: 2.305414\n",
      "(Iteration 2201 / 76500) loss: 2.305420\n",
      "(Epoch 3 / 100) train acc: 0.084000; val_acc: 0.072000\n",
      "(Iteration 2301 / 76500) loss: 2.305402\n",
      "(Iteration 2401 / 76500) loss: 2.305410\n",
      "(Iteration 2501 / 76500) loss: 2.305400\n",
      "(Iteration 2601 / 76500) loss: 2.305421\n",
      "(Iteration 2701 / 76500) loss: 2.305407\n",
      "(Iteration 2801 / 76500) loss: 2.305419\n",
      "(Iteration 2901 / 76500) loss: 2.305413\n",
      "(Iteration 3001 / 76500) loss: 2.305403\n",
      "(Epoch 4 / 100) train acc: 0.084000; val_acc: 0.072000\n",
      "(Iteration 3101 / 76500) loss: 2.305404\n",
      "(Iteration 3201 / 76500) loss: 2.305411\n",
      "(Iteration 3301 / 76500) loss: 2.305414\n",
      "(Iteration 3401 / 76500) loss: 2.305417\n",
      "(Iteration 3501 / 76500) loss: 2.305395\n",
      "(Iteration 3601 / 76500) loss: 2.305405\n",
      "(Iteration 3701 / 76500) loss: 2.305414\n",
      "(Iteration 3801 / 76500) loss: 2.305396\n",
      "(Epoch 5 / 100) train acc: 0.083000; val_acc: 0.072000\n",
      "(Iteration 3901 / 76500) loss: 2.305414\n",
      "(Iteration 4001 / 76500) loss: 2.305418\n",
      "(Iteration 4101 / 76500) loss: 2.305395\n",
      "(Iteration 4201 / 76500) loss: 2.305422\n",
      "(Iteration 4301 / 76500) loss: 2.305406\n",
      "(Iteration 4401 / 76500) loss: 2.305408\n",
      "(Iteration 4501 / 76500) loss: 2.305417\n",
      "(Epoch 6 / 100) train acc: 0.080000; val_acc: 0.072000\n",
      "(Iteration 4601 / 76500) loss: 2.305393\n",
      "(Iteration 4701 / 76500) loss: 2.305401\n",
      "(Iteration 4801 / 76500) loss: 2.305423\n",
      "(Iteration 4901 / 76500) loss: 2.305403\n",
      "(Iteration 5001 / 76500) loss: 2.305410\n",
      "(Iteration 5101 / 76500) loss: 2.305411\n",
      "(Iteration 5201 / 76500) loss: 2.305417\n",
      "(Iteration 5301 / 76500) loss: 2.305398\n",
      "(Epoch 7 / 100) train acc: 0.078000; val_acc: 0.072000\n",
      "(Iteration 5401 / 76500) loss: 2.305407\n",
      "(Iteration 5501 / 76500) loss: 2.305421\n",
      "(Iteration 5601 / 76500) loss: 2.305421\n",
      "(Iteration 5701 / 76500) loss: 2.305405\n",
      "(Iteration 5801 / 76500) loss: 2.305399\n",
      "(Iteration 5901 / 76500) loss: 2.305408\n",
      "(Iteration 6001 / 76500) loss: 2.305411\n",
      "(Iteration 6101 / 76500) loss: 2.305413\n",
      "(Epoch 8 / 100) train acc: 0.081000; val_acc: 0.072000\n",
      "(Iteration 6201 / 76500) loss: 2.305401\n",
      "(Iteration 6301 / 76500) loss: 2.305411\n",
      "(Iteration 6401 / 76500) loss: 2.305405\n",
      "(Iteration 6501 / 76500) loss: 2.305405\n",
      "(Iteration 6601 / 76500) loss: 2.305406\n",
      "(Iteration 6701 / 76500) loss: 2.305418\n",
      "(Iteration 6801 / 76500) loss: 2.305408\n",
      "(Epoch 9 / 100) train acc: 0.090000; val_acc: 0.072000\n",
      "(Iteration 6901 / 76500) loss: 2.305415\n",
      "(Iteration 7001 / 76500) loss: 2.305405\n",
      "(Iteration 7101 / 76500) loss: 2.305411\n",
      "(Iteration 7201 / 76500) loss: 2.305402\n",
      "(Iteration 7301 / 76500) loss: 2.305416\n",
      "(Iteration 7401 / 76500) loss: 2.305400\n",
      "(Iteration 7501 / 76500) loss: 2.305414\n",
      "(Iteration 7601 / 76500) loss: 2.305409\n",
      "(Epoch 10 / 100) train acc: 0.079000; val_acc: 0.072000\n",
      "(Iteration 7701 / 76500) loss: 2.305416\n",
      "(Iteration 7801 / 76500) loss: 2.305419\n",
      "(Iteration 7901 / 76500) loss: 2.305411\n",
      "(Iteration 8001 / 76500) loss: 2.305402\n",
      "(Iteration 8101 / 76500) loss: 2.305401\n",
      "(Iteration 8201 / 76500) loss: 2.305426\n",
      "(Iteration 8301 / 76500) loss: 2.305403\n",
      "(Iteration 8401 / 76500) loss: 2.305406\n",
      "(Epoch 11 / 100) train acc: 0.083000; val_acc: 0.072000\n",
      "(Iteration 8501 / 76500) loss: 2.305407\n",
      "(Iteration 8601 / 76500) loss: 2.305420\n",
      "(Iteration 8701 / 76500) loss: 2.305407\n",
      "(Iteration 8801 / 76500) loss: 2.305406\n",
      "(Iteration 8901 / 76500) loss: 2.305406\n",
      "(Iteration 9001 / 76500) loss: 2.305403\n",
      "(Iteration 9101 / 76500) loss: 2.305415\n",
      "(Epoch 12 / 100) train acc: 0.084000; val_acc: 0.072000\n",
      "(Iteration 9201 / 76500) loss: 2.305420\n",
      "(Iteration 9301 / 76500) loss: 2.305389\n",
      "(Iteration 9401 / 76500) loss: 2.305406\n",
      "(Iteration 9501 / 76500) loss: 2.305400\n",
      "(Iteration 9601 / 76500) loss: 2.305416\n",
      "(Iteration 9701 / 76500) loss: 2.305409\n",
      "(Iteration 9801 / 76500) loss: 2.305402\n",
      "(Iteration 9901 / 76500) loss: 2.305406\n",
      "(Epoch 13 / 100) train acc: 0.095000; val_acc: 0.072000\n",
      "(Iteration 10001 / 76500) loss: 2.305390\n",
      "(Iteration 10101 / 76500) loss: 2.305415\n",
      "(Iteration 10201 / 76500) loss: 2.305408\n",
      "(Iteration 10301 / 76500) loss: 2.305404\n",
      "(Iteration 10401 / 76500) loss: 2.305412\n",
      "(Iteration 10501 / 76500) loss: 2.305406\n",
      "(Iteration 10601 / 76500) loss: 2.305398\n",
      "(Iteration 10701 / 76500) loss: 2.305400\n",
      "(Epoch 14 / 100) train acc: 0.073000; val_acc: 0.072000\n",
      "(Iteration 10801 / 76500) loss: 2.305408\n",
      "(Iteration 10901 / 76500) loss: 2.305412\n",
      "(Iteration 11001 / 76500) loss: 2.305414\n",
      "(Iteration 11101 / 76500) loss: 2.305427\n",
      "(Iteration 11201 / 76500) loss: 2.305409\n",
      "(Iteration 11301 / 76500) loss: 2.305399\n",
      "(Iteration 11401 / 76500) loss: 2.305410\n",
      "(Epoch 15 / 100) train acc: 0.076000; val_acc: 0.072000\n",
      "(Iteration 11501 / 76500) loss: 2.305422\n",
      "(Iteration 11601 / 76500) loss: 2.305413\n",
      "(Iteration 11701 / 76500) loss: 2.305399\n",
      "(Iteration 11801 / 76500) loss: 2.305402\n",
      "(Iteration 11901 / 76500) loss: 2.305402\n",
      "(Iteration 12001 / 76500) loss: 2.305410\n",
      "(Iteration 12101 / 76500) loss: 2.305399\n",
      "(Iteration 12201 / 76500) loss: 2.305401\n",
      "(Epoch 16 / 100) train acc: 0.077000; val_acc: 0.072000\n",
      "(Iteration 12301 / 76500) loss: 2.305398\n",
      "(Iteration 12401 / 76500) loss: 2.305395\n",
      "(Iteration 12501 / 76500) loss: 2.305409\n",
      "(Iteration 12601 / 76500) loss: 2.305406\n",
      "(Iteration 12701 / 76500) loss: 2.305419\n",
      "(Iteration 12801 / 76500) loss: 2.305411\n",
      "(Iteration 12901 / 76500) loss: 2.305422\n",
      "(Iteration 13001 / 76500) loss: 2.305405\n",
      "(Epoch 17 / 100) train acc: 0.071000; val_acc: 0.072000\n",
      "(Iteration 13101 / 76500) loss: 2.305408\n",
      "(Iteration 13201 / 76500) loss: 2.305408\n",
      "(Iteration 13301 / 76500) loss: 2.305404\n",
      "(Iteration 13401 / 76500) loss: 2.305403\n",
      "(Iteration 13501 / 76500) loss: 2.305402\n",
      "(Iteration 13601 / 76500) loss: 2.305402\n",
      "(Iteration 13701 / 76500) loss: 2.305418\n",
      "(Epoch 18 / 100) train acc: 0.064000; val_acc: 0.072000\n",
      "(Iteration 13801 / 76500) loss: 2.305403\n",
      "(Iteration 13901 / 76500) loss: 2.305407\n",
      "(Iteration 14001 / 76500) loss: 2.305419\n",
      "(Iteration 14101 / 76500) loss: 2.305401\n",
      "(Iteration 14201 / 76500) loss: 2.305402\n",
      "(Iteration 14301 / 76500) loss: 2.305409\n",
      "(Iteration 14401 / 76500) loss: 2.305416\n",
      "(Iteration 14501 / 76500) loss: 2.305407\n",
      "(Epoch 19 / 100) train acc: 0.069000; val_acc: 0.072000\n",
      "(Iteration 14601 / 76500) loss: 2.305407\n",
      "(Iteration 14701 / 76500) loss: 2.305401\n",
      "(Iteration 14801 / 76500) loss: 2.305400\n",
      "(Iteration 14901 / 76500) loss: 2.305410\n",
      "(Iteration 15001 / 76500) loss: 2.305400\n",
      "(Iteration 15101 / 76500) loss: 2.305401\n",
      "(Iteration 15201 / 76500) loss: 2.305404\n",
      "(Epoch 20 / 100) train acc: 0.078000; val_acc: 0.072000\n",
      "(Iteration 15301 / 76500) loss: 2.305392\n",
      "(Iteration 15401 / 76500) loss: 2.305401\n",
      "(Iteration 15501 / 76500) loss: 2.305406\n",
      "(Iteration 15601 / 76500) loss: 2.305398\n",
      "(Iteration 15701 / 76500) loss: 2.305392\n",
      "(Iteration 15801 / 76500) loss: 2.305413\n",
      "(Iteration 15901 / 76500) loss: 2.305407\n",
      "(Iteration 16001 / 76500) loss: 2.305407\n",
      "(Epoch 21 / 100) train acc: 0.099000; val_acc: 0.072000\n",
      "(Iteration 16101 / 76500) loss: 2.305399\n",
      "(Iteration 16201 / 76500) loss: 2.305415\n",
      "(Iteration 16301 / 76500) loss: 2.305412\n",
      "(Iteration 16401 / 76500) loss: 2.305404\n",
      "(Iteration 16501 / 76500) loss: 2.305414\n",
      "(Iteration 16601 / 76500) loss: 2.305405\n",
      "(Iteration 16701 / 76500) loss: 2.305408\n",
      "(Iteration 16801 / 76500) loss: 2.305398\n",
      "(Epoch 22 / 100) train acc: 0.076000; val_acc: 0.072000\n",
      "(Iteration 16901 / 76500) loss: 2.305408\n",
      "(Iteration 17001 / 76500) loss: 2.305402\n",
      "(Iteration 17101 / 76500) loss: 2.305405\n",
      "(Iteration 17201 / 76500) loss: 2.305406\n",
      "(Iteration 17301 / 76500) loss: 2.305396\n",
      "(Iteration 17401 / 76500) loss: 2.305406\n",
      "(Iteration 17501 / 76500) loss: 2.305417\n",
      "(Epoch 23 / 100) train acc: 0.083000; val_acc: 0.072000\n",
      "(Iteration 17601 / 76500) loss: 2.305410\n",
      "(Iteration 17701 / 76500) loss: 2.305401\n",
      "(Iteration 17801 / 76500) loss: 2.305410\n",
      "(Iteration 17901 / 76500) loss: 2.305399\n",
      "(Iteration 18001 / 76500) loss: 2.305411\n",
      "(Iteration 18101 / 76500) loss: 2.305401\n",
      "(Iteration 18201 / 76500) loss: 2.305407\n",
      "(Iteration 18301 / 76500) loss: 2.305407\n",
      "(Epoch 24 / 100) train acc: 0.091000; val_acc: 0.072000\n",
      "(Iteration 18401 / 76500) loss: 2.305402\n",
      "(Iteration 18501 / 76500) loss: 2.305410\n",
      "(Iteration 18601 / 76500) loss: 2.305416\n",
      "(Iteration 18701 / 76500) loss: 2.305401\n",
      "(Iteration 18801 / 76500) loss: 2.305411\n",
      "(Iteration 18901 / 76500) loss: 2.305410\n",
      "(Iteration 19001 / 76500) loss: 2.305406\n",
      "(Iteration 19101 / 76500) loss: 2.305414\n",
      "(Epoch 25 / 100) train acc: 0.084000; val_acc: 0.072000\n",
      "(Iteration 19201 / 76500) loss: 2.305391\n",
      "(Iteration 19301 / 76500) loss: 2.305407\n",
      "(Iteration 19401 / 76500) loss: 2.305404\n",
      "(Iteration 19501 / 76500) loss: 2.305402\n",
      "(Iteration 19601 / 76500) loss: 2.305420\n",
      "(Iteration 19701 / 76500) loss: 2.305406\n",
      "(Iteration 19801 / 76500) loss: 2.305409\n",
      "(Epoch 26 / 100) train acc: 0.089000; val_acc: 0.072000\n",
      "(Iteration 19901 / 76500) loss: 2.305413\n",
      "(Iteration 20001 / 76500) loss: 2.305399\n",
      "(Iteration 20101 / 76500) loss: 2.305409\n",
      "(Iteration 20201 / 76500) loss: 2.305403\n",
      "(Iteration 20301 / 76500) loss: 2.305406\n",
      "(Iteration 20401 / 76500) loss: 2.305402\n",
      "(Iteration 20501 / 76500) loss: 2.305401\n",
      "(Iteration 20601 / 76500) loss: 2.305417\n",
      "(Epoch 27 / 100) train acc: 0.076000; val_acc: 0.072000\n",
      "(Iteration 20701 / 76500) loss: 2.305393\n",
      "(Iteration 20801 / 76500) loss: 2.305405\n",
      "(Iteration 20901 / 76500) loss: 2.305423\n",
      "(Iteration 21001 / 76500) loss: 2.305410\n",
      "(Iteration 21101 / 76500) loss: 2.305410\n",
      "(Iteration 21201 / 76500) loss: 2.305411\n",
      "(Iteration 21301 / 76500) loss: 2.305416\n",
      "(Iteration 21401 / 76500) loss: 2.305417\n",
      "(Epoch 28 / 100) train acc: 0.083000; val_acc: 0.072000\n",
      "(Iteration 21501 / 76500) loss: 2.305413\n",
      "(Iteration 21601 / 76500) loss: 2.305412\n",
      "(Iteration 21701 / 76500) loss: 2.305408\n",
      "(Iteration 21801 / 76500) loss: 2.305390\n",
      "(Iteration 21901 / 76500) loss: 2.305413\n",
      "(Iteration 22001 / 76500) loss: 2.305402\n",
      "(Iteration 22101 / 76500) loss: 2.305416\n",
      "(Epoch 29 / 100) train acc: 0.076000; val_acc: 0.072000\n",
      "(Iteration 22201 / 76500) loss: 2.305412\n",
      "(Iteration 22301 / 76500) loss: 2.305400\n",
      "(Iteration 22401 / 76500) loss: 2.305400\n",
      "(Iteration 22501 / 76500) loss: 2.305403\n",
      "(Iteration 22601 / 76500) loss: 2.305408\n",
      "(Iteration 22701 / 76500) loss: 2.305406\n",
      "(Iteration 22801 / 76500) loss: 2.305398\n",
      "(Iteration 22901 / 76500) loss: 2.305407\n",
      "(Epoch 30 / 100) train acc: 0.069000; val_acc: 0.072000\n",
      "(Iteration 23001 / 76500) loss: 2.305395\n",
      "(Iteration 23101 / 76500) loss: 2.305406\n",
      "(Iteration 23201 / 76500) loss: 2.305412\n",
      "(Iteration 23301 / 76500) loss: 2.305401\n",
      "(Iteration 23401 / 76500) loss: 2.305403\n",
      "(Iteration 23501 / 76500) loss: 2.305414\n",
      "(Iteration 23601 / 76500) loss: 2.305402\n",
      "(Iteration 23701 / 76500) loss: 2.305425\n",
      "(Epoch 31 / 100) train acc: 0.096000; val_acc: 0.072000\n",
      "(Iteration 23801 / 76500) loss: 2.305404\n",
      "(Iteration 23901 / 76500) loss: 2.305407\n",
      "(Iteration 24001 / 76500) loss: 2.305404\n",
      "(Iteration 24101 / 76500) loss: 2.305397\n",
      "(Iteration 24201 / 76500) loss: 2.305412\n",
      "(Iteration 24301 / 76500) loss: 2.305411\n",
      "(Iteration 24401 / 76500) loss: 2.305410\n",
      "(Epoch 32 / 100) train acc: 0.085000; val_acc: 0.072000\n",
      "(Iteration 24501 / 76500) loss: 2.305408\n",
      "(Iteration 24601 / 76500) loss: 2.305391\n",
      "(Iteration 24701 / 76500) loss: 2.305402\n",
      "(Iteration 24801 / 76500) loss: 2.305398\n",
      "(Iteration 24901 / 76500) loss: 2.305416\n",
      "(Iteration 25001 / 76500) loss: 2.305409\n",
      "(Iteration 25101 / 76500) loss: 2.305400\n",
      "(Iteration 25201 / 76500) loss: 2.305411\n",
      "(Epoch 33 / 100) train acc: 0.082000; val_acc: 0.072000\n",
      "(Iteration 25301 / 76500) loss: 2.305399\n",
      "(Iteration 25401 / 76500) loss: 2.305413\n",
      "(Iteration 25501 / 76500) loss: 2.305419\n",
      "(Iteration 25601 / 76500) loss: 2.305407\n",
      "(Iteration 25701 / 76500) loss: 2.305407\n",
      "(Iteration 25801 / 76500) loss: 2.305410\n",
      "(Iteration 25901 / 76500) loss: 2.305411\n",
      "(Iteration 26001 / 76500) loss: 2.305408\n",
      "(Epoch 34 / 100) train acc: 0.101000; val_acc: 0.072000\n",
      "(Iteration 26101 / 76500) loss: 2.305405\n",
      "(Iteration 26201 / 76500) loss: 2.305404\n",
      "(Iteration 26301 / 76500) loss: 2.305401\n",
      "(Iteration 26401 / 76500) loss: 2.305415\n",
      "(Iteration 26501 / 76500) loss: 2.305415\n",
      "(Iteration 26601 / 76500) loss: 2.305414\n",
      "(Iteration 26701 / 76500) loss: 2.305412\n",
      "(Epoch 35 / 100) train acc: 0.086000; val_acc: 0.072000\n",
      "(Iteration 26801 / 76500) loss: 2.305402\n",
      "(Iteration 26901 / 76500) loss: 2.305405\n",
      "(Iteration 27001 / 76500) loss: 2.305414\n",
      "(Iteration 27101 / 76500) loss: 2.305408\n",
      "(Iteration 27201 / 76500) loss: 2.305405\n",
      "(Iteration 27301 / 76500) loss: 2.305419\n",
      "(Iteration 27401 / 76500) loss: 2.305402\n",
      "(Iteration 27501 / 76500) loss: 2.305405\n",
      "(Epoch 36 / 100) train acc: 0.088000; val_acc: 0.072000\n",
      "(Iteration 27601 / 76500) loss: 2.305407\n",
      "(Iteration 27701 / 76500) loss: 2.305410\n",
      "(Iteration 27801 / 76500) loss: 2.305404\n",
      "(Iteration 27901 / 76500) loss: 2.305396\n",
      "(Iteration 28001 / 76500) loss: 2.305389\n",
      "(Iteration 28101 / 76500) loss: 2.305417\n",
      "(Iteration 28201 / 76500) loss: 2.305402\n",
      "(Iteration 28301 / 76500) loss: 2.305410\n",
      "(Epoch 37 / 100) train acc: 0.084000; val_acc: 0.072000\n",
      "(Iteration 28401 / 76500) loss: 2.305399\n",
      "(Iteration 28501 / 76500) loss: 2.305408\n",
      "(Iteration 28601 / 76500) loss: 2.305411\n",
      "(Iteration 28701 / 76500) loss: 2.305401\n",
      "(Iteration 28801 / 76500) loss: 2.305400\n",
      "(Iteration 28901 / 76500) loss: 2.305404\n",
      "(Iteration 29001 / 76500) loss: 2.305395\n",
      "(Epoch 38 / 100) train acc: 0.085000; val_acc: 0.072000\n",
      "(Iteration 29101 / 76500) loss: 2.305409\n",
      "(Iteration 29201 / 76500) loss: 2.305408\n",
      "(Iteration 29301 / 76500) loss: 2.305404\n",
      "(Iteration 29401 / 76500) loss: 2.305405\n",
      "(Iteration 29501 / 76500) loss: 2.305400\n",
      "(Iteration 29601 / 76500) loss: 2.305398\n",
      "(Iteration 29701 / 76500) loss: 2.305405\n",
      "(Iteration 29801 / 76500) loss: 2.305398\n",
      "(Epoch 39 / 100) train acc: 0.077000; val_acc: 0.072000\n",
      "(Iteration 29901 / 76500) loss: 2.305398\n",
      "(Iteration 30001 / 76500) loss: 2.305405\n",
      "(Iteration 30101 / 76500) loss: 2.305403\n",
      "(Iteration 30201 / 76500) loss: 2.305410\n",
      "(Iteration 30301 / 76500) loss: 2.305413\n",
      "(Iteration 30401 / 76500) loss: 2.305406\n",
      "(Iteration 30501 / 76500) loss: 2.305407\n",
      "(Epoch 40 / 100) train acc: 0.071000; val_acc: 0.072000\n",
      "(Iteration 30601 / 76500) loss: 2.305409\n",
      "(Iteration 30701 / 76500) loss: 2.305403\n",
      "(Iteration 30801 / 76500) loss: 2.305403\n",
      "(Iteration 30901 / 76500) loss: 2.305406\n",
      "(Iteration 31001 / 76500) loss: 2.305403\n",
      "(Iteration 31101 / 76500) loss: 2.305407\n",
      "(Iteration 31201 / 76500) loss: 2.305405\n",
      "(Iteration 31301 / 76500) loss: 2.305408\n",
      "(Epoch 41 / 100) train acc: 0.072000; val_acc: 0.072000\n",
      "(Iteration 31401 / 76500) loss: 2.305403\n",
      "(Iteration 31501 / 76500) loss: 2.305410\n",
      "(Iteration 31601 / 76500) loss: 2.305423\n",
      "(Iteration 31701 / 76500) loss: 2.305413\n",
      "(Iteration 31801 / 76500) loss: 2.305410\n",
      "(Iteration 31901 / 76500) loss: 2.305399\n",
      "(Iteration 32001 / 76500) loss: 2.305399\n",
      "(Iteration 32101 / 76500) loss: 2.305407\n",
      "(Epoch 42 / 100) train acc: 0.090000; val_acc: 0.072000\n",
      "(Iteration 32201 / 76500) loss: 2.305404\n",
      "(Iteration 32301 / 76500) loss: 2.305410\n",
      "(Iteration 32401 / 76500) loss: 2.305401\n",
      "(Iteration 32501 / 76500) loss: 2.305403\n",
      "(Iteration 32601 / 76500) loss: 2.305408\n",
      "(Iteration 32701 / 76500) loss: 2.305411\n",
      "(Iteration 32801 / 76500) loss: 2.305416\n",
      "(Epoch 43 / 100) train acc: 0.095000; val_acc: 0.072000\n",
      "(Iteration 32901 / 76500) loss: 2.305422\n",
      "(Iteration 33001 / 76500) loss: 2.305401\n",
      "(Iteration 33101 / 76500) loss: 2.305413\n",
      "(Iteration 33201 / 76500) loss: 2.305416\n",
      "(Iteration 33301 / 76500) loss: 2.305409\n",
      "(Iteration 33401 / 76500) loss: 2.305400\n",
      "(Iteration 33501 / 76500) loss: 2.305395\n",
      "(Iteration 33601 / 76500) loss: 2.305398\n",
      "(Epoch 44 / 100) train acc: 0.071000; val_acc: 0.072000\n",
      "(Iteration 33701 / 76500) loss: 2.305395\n",
      "(Iteration 33801 / 76500) loss: 2.305404\n",
      "(Iteration 33901 / 76500) loss: 2.305412\n",
      "(Iteration 34001 / 76500) loss: 2.305411\n",
      "(Iteration 34101 / 76500) loss: 2.305424\n",
      "(Iteration 34201 / 76500) loss: 2.305409\n",
      "(Iteration 34301 / 76500) loss: 2.305403\n",
      "(Iteration 34401 / 76500) loss: 2.305417\n",
      "(Epoch 45 / 100) train acc: 0.083000; val_acc: 0.072000\n",
      "(Iteration 34501 / 76500) loss: 2.305405\n",
      "(Iteration 34601 / 76500) loss: 2.305399\n",
      "(Iteration 34701 / 76500) loss: 2.305410\n",
      "(Iteration 34801 / 76500) loss: 2.305399\n",
      "(Iteration 34901 / 76500) loss: 2.305411\n",
      "(Iteration 35001 / 76500) loss: 2.305396\n",
      "(Iteration 35101 / 76500) loss: 2.305403\n",
      "(Epoch 46 / 100) train acc: 0.078000; val_acc: 0.072000\n",
      "(Iteration 35201 / 76500) loss: 2.305400\n",
      "(Iteration 35301 / 76500) loss: 2.305408\n",
      "(Iteration 35401 / 76500) loss: 2.305408\n",
      "(Iteration 35501 / 76500) loss: 2.305392\n",
      "(Iteration 35601 / 76500) loss: 2.305407\n",
      "(Iteration 35701 / 76500) loss: 2.305400\n",
      "(Iteration 35801 / 76500) loss: 2.305397\n",
      "(Iteration 35901 / 76500) loss: 2.305412\n",
      "(Epoch 47 / 100) train acc: 0.099000; val_acc: 0.072000\n",
      "(Iteration 36001 / 76500) loss: 2.305400\n",
      "(Iteration 36101 / 76500) loss: 2.305403\n",
      "(Iteration 36201 / 76500) loss: 2.305403\n",
      "(Iteration 36301 / 76500) loss: 2.305400\n",
      "(Iteration 36401 / 76500) loss: 2.305404\n",
      "(Iteration 36501 / 76500) loss: 2.305403\n",
      "(Iteration 36601 / 76500) loss: 2.305406\n",
      "(Iteration 36701 / 76500) loss: 2.305412\n",
      "(Epoch 48 / 100) train acc: 0.085000; val_acc: 0.072000\n",
      "(Iteration 36801 / 76500) loss: 2.305405\n",
      "(Iteration 36901 / 76500) loss: 2.305407\n",
      "(Iteration 37001 / 76500) loss: 2.305392\n",
      "(Iteration 37101 / 76500) loss: 2.305384\n",
      "(Iteration 37201 / 76500) loss: 2.305417\n",
      "(Iteration 37301 / 76500) loss: 2.305403\n",
      "(Iteration 37401 / 76500) loss: 2.305407\n",
      "(Epoch 49 / 100) train acc: 0.082000; val_acc: 0.072000\n",
      "(Iteration 37501 / 76500) loss: 2.305401\n",
      "(Iteration 37601 / 76500) loss: 2.305404\n",
      "(Iteration 37701 / 76500) loss: 2.305403\n",
      "(Iteration 37801 / 76500) loss: 2.305407\n",
      "(Iteration 37901 / 76500) loss: 2.305393\n",
      "(Iteration 38001 / 76500) loss: 2.305393\n",
      "(Iteration 38101 / 76500) loss: 2.305410\n",
      "(Iteration 38201 / 76500) loss: 2.305411\n",
      "(Epoch 50 / 100) train acc: 0.075000; val_acc: 0.072000\n",
      "(Iteration 38301 / 76500) loss: 2.305404\n",
      "(Iteration 38401 / 76500) loss: 2.305402\n",
      "(Iteration 38501 / 76500) loss: 2.305409\n",
      "(Iteration 38601 / 76500) loss: 2.305399\n",
      "(Iteration 38701 / 76500) loss: 2.305407\n",
      "(Iteration 38801 / 76500) loss: 2.305401\n",
      "(Iteration 38901 / 76500) loss: 2.305408\n",
      "(Iteration 39001 / 76500) loss: 2.305409\n",
      "(Epoch 51 / 100) train acc: 0.086000; val_acc: 0.072000\n",
      "(Iteration 39101 / 76500) loss: 2.305415\n",
      "(Iteration 39201 / 76500) loss: 2.305397\n",
      "(Iteration 39301 / 76500) loss: 2.305411\n",
      "(Iteration 39401 / 76500) loss: 2.305404\n",
      "(Iteration 39501 / 76500) loss: 2.305401\n",
      "(Iteration 39601 / 76500) loss: 2.305419\n",
      "(Iteration 39701 / 76500) loss: 2.305401\n",
      "(Epoch 52 / 100) train acc: 0.085000; val_acc: 0.072000\n",
      "(Iteration 39801 / 76500) loss: 2.305406\n",
      "(Iteration 39901 / 76500) loss: 2.305400\n",
      "(Iteration 40001 / 76500) loss: 2.305393\n",
      "(Iteration 40101 / 76500) loss: 2.305411\n",
      "(Iteration 40201 / 76500) loss: 2.305413\n",
      "(Iteration 40301 / 76500) loss: 2.305395\n",
      "(Iteration 40401 / 76500) loss: 2.305406\n",
      "(Iteration 40501 / 76500) loss: 2.305395\n",
      "(Epoch 53 / 100) train acc: 0.070000; val_acc: 0.072000\n",
      "(Iteration 40601 / 76500) loss: 2.305393\n",
      "(Iteration 40701 / 76500) loss: 2.305409\n",
      "(Iteration 40801 / 76500) loss: 2.305415\n",
      "(Iteration 40901 / 76500) loss: 2.305412\n",
      "(Iteration 41001 / 76500) loss: 2.305412\n",
      "(Iteration 41101 / 76500) loss: 2.305405\n",
      "(Iteration 41201 / 76500) loss: 2.305401\n",
      "(Iteration 41301 / 76500) loss: 2.305408\n",
      "(Epoch 54 / 100) train acc: 0.081000; val_acc: 0.072000\n",
      "(Iteration 41401 / 76500) loss: 2.305409\n",
      "(Iteration 41501 / 76500) loss: 2.305415\n",
      "(Iteration 41601 / 76500) loss: 2.305400\n",
      "(Iteration 41701 / 76500) loss: 2.305415\n",
      "(Iteration 41801 / 76500) loss: 2.305411\n",
      "(Iteration 41901 / 76500) loss: 2.305409\n",
      "(Iteration 42001 / 76500) loss: 2.305400\n",
      "(Epoch 55 / 100) train acc: 0.098000; val_acc: 0.072000\n",
      "(Iteration 42101 / 76500) loss: 2.305414\n",
      "(Iteration 42201 / 76500) loss: 2.305406\n",
      "(Iteration 42301 / 76500) loss: 2.305394\n",
      "(Iteration 42401 / 76500) loss: 2.305411\n",
      "(Iteration 42501 / 76500) loss: 2.305417\n",
      "(Iteration 42601 / 76500) loss: 2.305403\n",
      "(Iteration 42701 / 76500) loss: 2.305391\n",
      "(Iteration 42801 / 76500) loss: 2.305402\n",
      "(Epoch 56 / 100) train acc: 0.088000; val_acc: 0.072000\n",
      "(Iteration 42901 / 76500) loss: 2.305405\n",
      "(Iteration 43001 / 76500) loss: 2.305395\n",
      "(Iteration 43101 / 76500) loss: 2.305406\n",
      "(Iteration 43201 / 76500) loss: 2.305404\n",
      "(Iteration 43301 / 76500) loss: 2.305396\n",
      "(Iteration 43401 / 76500) loss: 2.305409\n",
      "(Iteration 43501 / 76500) loss: 2.305403\n",
      "(Iteration 43601 / 76500) loss: 2.305398\n",
      "(Epoch 57 / 100) train acc: 0.077000; val_acc: 0.072000\n",
      "(Iteration 43701 / 76500) loss: 2.305402\n",
      "(Iteration 43801 / 76500) loss: 2.305396\n",
      "(Iteration 43901 / 76500) loss: 2.305403\n",
      "(Iteration 44001 / 76500) loss: 2.305406\n",
      "(Iteration 44101 / 76500) loss: 2.305409\n",
      "(Iteration 44201 / 76500) loss: 2.305408\n",
      "(Iteration 44301 / 76500) loss: 2.305404\n",
      "(Epoch 58 / 100) train acc: 0.097000; val_acc: 0.072000\n",
      "(Iteration 44401 / 76500) loss: 2.305412\n",
      "(Iteration 44501 / 76500) loss: 2.305404\n",
      "(Iteration 44601 / 76500) loss: 2.305390\n",
      "(Iteration 44701 / 76500) loss: 2.305413\n",
      "(Iteration 44801 / 76500) loss: 2.305401\n",
      "(Iteration 44901 / 76500) loss: 2.305406\n",
      "(Iteration 45001 / 76500) loss: 2.305410\n",
      "(Iteration 45101 / 76500) loss: 2.305408\n",
      "(Epoch 59 / 100) train acc: 0.089000; val_acc: 0.072000\n",
      "(Iteration 45201 / 76500) loss: 2.305402\n",
      "(Iteration 45301 / 76500) loss: 2.305417\n",
      "(Iteration 45401 / 76500) loss: 2.305408\n",
      "(Iteration 45501 / 76500) loss: 2.305401\n",
      "(Iteration 45601 / 76500) loss: 2.305401\n",
      "(Iteration 45701 / 76500) loss: 2.305408\n",
      "(Iteration 45801 / 76500) loss: 2.305411\n",
      "(Epoch 60 / 100) train acc: 0.072000; val_acc: 0.072000\n",
      "(Iteration 45901 / 76500) loss: 2.305399\n",
      "(Iteration 46001 / 76500) loss: 2.305399\n",
      "(Iteration 46101 / 76500) loss: 2.305406\n",
      "(Iteration 46201 / 76500) loss: 2.305407\n",
      "(Iteration 46301 / 76500) loss: 2.305419\n",
      "(Iteration 46401 / 76500) loss: 2.305401\n",
      "(Iteration 46501 / 76500) loss: 2.305413\n",
      "(Iteration 46601 / 76500) loss: 2.305401\n",
      "(Epoch 61 / 100) train acc: 0.089000; val_acc: 0.072000\n",
      "(Iteration 46701 / 76500) loss: 2.305409\n",
      "(Iteration 46801 / 76500) loss: 2.305413\n",
      "(Iteration 46901 / 76500) loss: 2.305402\n",
      "(Iteration 47001 / 76500) loss: 2.305391\n",
      "(Iteration 47101 / 76500) loss: 2.305412\n",
      "(Iteration 47201 / 76500) loss: 2.305403\n",
      "(Iteration 47301 / 76500) loss: 2.305421\n",
      "(Iteration 47401 / 76500) loss: 2.305408\n",
      "(Epoch 62 / 100) train acc: 0.069000; val_acc: 0.072000\n",
      "(Iteration 47501 / 76500) loss: 2.305408\n",
      "(Iteration 47601 / 76500) loss: 2.305401\n",
      "(Iteration 47701 / 76500) loss: 2.305411\n",
      "(Iteration 47801 / 76500) loss: 2.305405\n",
      "(Iteration 47901 / 76500) loss: 2.305412\n",
      "(Iteration 48001 / 76500) loss: 2.305423\n",
      "(Iteration 48101 / 76500) loss: 2.305411\n",
      "(Epoch 63 / 100) train acc: 0.094000; val_acc: 0.072000\n",
      "(Iteration 48201 / 76500) loss: 2.305418\n",
      "(Iteration 48301 / 76500) loss: 2.305401\n",
      "(Iteration 48401 / 76500) loss: 2.305412\n",
      "(Iteration 48501 / 76500) loss: 2.305383\n",
      "(Iteration 48601 / 76500) loss: 2.305407\n",
      "(Iteration 48701 / 76500) loss: 2.305413\n",
      "(Iteration 48801 / 76500) loss: 2.305406\n",
      "(Iteration 48901 / 76500) loss: 2.305396\n",
      "(Epoch 64 / 100) train acc: 0.084000; val_acc: 0.072000\n",
      "(Iteration 49001 / 76500) loss: 2.305395\n",
      "(Iteration 49101 / 76500) loss: 2.305411\n",
      "(Iteration 49201 / 76500) loss: 2.305411\n",
      "(Iteration 49301 / 76500) loss: 2.305398\n",
      "(Iteration 49401 / 76500) loss: 2.305396\n",
      "(Iteration 49501 / 76500) loss: 2.305403\n",
      "(Iteration 49601 / 76500) loss: 2.305392\n",
      "(Iteration 49701 / 76500) loss: 2.305397\n",
      "(Epoch 65 / 100) train acc: 0.083000; val_acc: 0.072000\n",
      "(Iteration 49801 / 76500) loss: 2.305407\n",
      "(Iteration 49901 / 76500) loss: 2.305401\n",
      "(Iteration 50001 / 76500) loss: 2.305409\n",
      "(Iteration 50101 / 76500) loss: 2.305402\n",
      "(Iteration 50201 / 76500) loss: 2.305407\n",
      "(Iteration 50301 / 76500) loss: 2.305402\n",
      "(Iteration 50401 / 76500) loss: 2.305421\n",
      "(Epoch 66 / 100) train acc: 0.091000; val_acc: 0.072000\n",
      "(Iteration 50501 / 76500) loss: 2.305404\n",
      "(Iteration 50601 / 76500) loss: 2.305407\n",
      "(Iteration 50701 / 76500) loss: 2.305401\n",
      "(Iteration 50801 / 76500) loss: 2.305405\n",
      "(Iteration 50901 / 76500) loss: 2.305399\n",
      "(Iteration 51001 / 76500) loss: 2.305408\n",
      "(Iteration 51101 / 76500) loss: 2.305405\n",
      "(Iteration 51201 / 76500) loss: 2.305403\n",
      "(Epoch 67 / 100) train acc: 0.084000; val_acc: 0.072000\n",
      "(Iteration 51301 / 76500) loss: 2.305394\n",
      "(Iteration 51401 / 76500) loss: 2.305418\n",
      "(Iteration 51501 / 76500) loss: 2.305398\n",
      "(Iteration 51601 / 76500) loss: 2.305417\n",
      "(Iteration 51701 / 76500) loss: 2.305399\n",
      "(Iteration 51801 / 76500) loss: 2.305402\n",
      "(Iteration 51901 / 76500) loss: 2.305407\n",
      "(Iteration 52001 / 76500) loss: 2.305389\n",
      "(Epoch 68 / 100) train acc: 0.084000; val_acc: 0.072000\n",
      "(Iteration 52101 / 76500) loss: 2.305400\n",
      "(Iteration 52201 / 76500) loss: 2.305410\n",
      "(Iteration 52301 / 76500) loss: 2.305403\n",
      "(Iteration 52401 / 76500) loss: 2.305409\n",
      "(Iteration 52501 / 76500) loss: 2.305408\n",
      "(Iteration 52601 / 76500) loss: 2.305394\n",
      "(Iteration 52701 / 76500) loss: 2.305399\n",
      "(Epoch 69 / 100) train acc: 0.076000; val_acc: 0.072000\n",
      "(Iteration 52801 / 76500) loss: 2.305399\n",
      "(Iteration 52901 / 76500) loss: 2.305412\n",
      "(Iteration 53001 / 76500) loss: 2.305408\n",
      "(Iteration 53101 / 76500) loss: 2.305404\n",
      "(Iteration 53201 / 76500) loss: 2.305414\n",
      "(Iteration 53301 / 76500) loss: 2.305394\n",
      "(Iteration 53401 / 76500) loss: 2.305396\n",
      "(Iteration 53501 / 76500) loss: 2.305402\n",
      "(Epoch 70 / 100) train acc: 0.084000; val_acc: 0.072000\n",
      "(Iteration 53601 / 76500) loss: 2.305409\n",
      "(Iteration 53701 / 76500) loss: 2.305392\n",
      "(Iteration 53801 / 76500) loss: 2.305405\n",
      "(Iteration 53901 / 76500) loss: 2.305406\n",
      "(Iteration 54001 / 76500) loss: 2.305405\n",
      "(Iteration 54101 / 76500) loss: 2.305397\n",
      "(Iteration 54201 / 76500) loss: 2.305408\n",
      "(Iteration 54301 / 76500) loss: 2.305403\n",
      "(Epoch 71 / 100) train acc: 0.076000; val_acc: 0.072000\n",
      "(Iteration 54401 / 76500) loss: 2.305416\n",
      "(Iteration 54501 / 76500) loss: 2.305393\n",
      "(Iteration 54601 / 76500) loss: 2.305400\n",
      "(Iteration 54701 / 76500) loss: 2.305399\n",
      "(Iteration 54801 / 76500) loss: 2.305410\n",
      "(Iteration 54901 / 76500) loss: 2.305405\n",
      "(Iteration 55001 / 76500) loss: 2.305405\n",
      "(Epoch 72 / 100) train acc: 0.085000; val_acc: 0.072000\n",
      "(Iteration 55101 / 76500) loss: 2.305396\n",
      "(Iteration 55201 / 76500) loss: 2.305406\n",
      "(Iteration 55301 / 76500) loss: 2.305403\n",
      "(Iteration 55401 / 76500) loss: 2.305409\n",
      "(Iteration 55501 / 76500) loss: 2.305395\n",
      "(Iteration 55601 / 76500) loss: 2.305408\n",
      "(Iteration 55701 / 76500) loss: 2.305413\n",
      "(Iteration 55801 / 76500) loss: 2.305400\n",
      "(Epoch 73 / 100) train acc: 0.081000; val_acc: 0.072000\n",
      "(Iteration 55901 / 76500) loss: 2.305405\n",
      "(Iteration 56001 / 76500) loss: 2.305415\n",
      "(Iteration 56101 / 76500) loss: 2.305400\n",
      "(Iteration 56201 / 76500) loss: 2.305417\n",
      "(Iteration 56301 / 76500) loss: 2.305416\n",
      "(Iteration 56401 / 76500) loss: 2.305393\n",
      "(Iteration 56501 / 76500) loss: 2.305418\n",
      "(Iteration 56601 / 76500) loss: 2.305412\n",
      "(Epoch 74 / 100) train acc: 0.069000; val_acc: 0.072000\n",
      "(Iteration 56701 / 76500) loss: 2.305415\n",
      "(Iteration 56801 / 76500) loss: 2.305405\n",
      "(Iteration 56901 / 76500) loss: 2.305420\n",
      "(Iteration 57001 / 76500) loss: 2.305398\n",
      "(Iteration 57101 / 76500) loss: 2.305413\n",
      "(Iteration 57201 / 76500) loss: 2.305402\n",
      "(Iteration 57301 / 76500) loss: 2.305403\n",
      "(Epoch 75 / 100) train acc: 0.068000; val_acc: 0.072000\n",
      "(Iteration 57401 / 76500) loss: 2.305410\n",
      "(Iteration 57501 / 76500) loss: 2.305387\n",
      "(Iteration 57601 / 76500) loss: 2.305407\n",
      "(Iteration 57701 / 76500) loss: 2.305398\n",
      "(Iteration 57801 / 76500) loss: 2.305400\n",
      "(Iteration 57901 / 76500) loss: 2.305396\n",
      "(Iteration 58001 / 76500) loss: 2.305408\n",
      "(Iteration 58101 / 76500) loss: 2.305413\n",
      "(Epoch 76 / 100) train acc: 0.081000; val_acc: 0.072000\n",
      "(Iteration 58201 / 76500) loss: 2.305422\n",
      "(Iteration 58301 / 76500) loss: 2.305408\n",
      "(Iteration 58401 / 76500) loss: 2.305400\n",
      "(Iteration 58501 / 76500) loss: 2.305401\n",
      "(Iteration 58601 / 76500) loss: 2.305407\n",
      "(Iteration 58701 / 76500) loss: 2.305404\n",
      "(Iteration 58801 / 76500) loss: 2.305405\n",
      "(Iteration 58901 / 76500) loss: 2.305400\n",
      "(Epoch 77 / 100) train acc: 0.080000; val_acc: 0.072000\n",
      "(Iteration 59001 / 76500) loss: 2.305395\n",
      "(Iteration 59101 / 76500) loss: 2.305403\n",
      "(Iteration 59201 / 76500) loss: 2.305399\n",
      "(Iteration 59301 / 76500) loss: 2.305394\n",
      "(Iteration 59401 / 76500) loss: 2.305407\n",
      "(Iteration 59501 / 76500) loss: 2.305408\n",
      "(Iteration 59601 / 76500) loss: 2.305412\n",
      "(Epoch 78 / 100) train acc: 0.080000; val_acc: 0.072000\n",
      "(Iteration 59701 / 76500) loss: 2.305405\n",
      "(Iteration 59801 / 76500) loss: 2.305401\n",
      "(Iteration 59901 / 76500) loss: 2.305406\n",
      "(Iteration 60001 / 76500) loss: 2.305397\n",
      "(Iteration 60101 / 76500) loss: 2.305401\n",
      "(Iteration 60201 / 76500) loss: 2.305410\n",
      "(Iteration 60301 / 76500) loss: 2.305413\n",
      "(Iteration 60401 / 76500) loss: 2.305408\n",
      "(Epoch 79 / 100) train acc: 0.087000; val_acc: 0.072000\n",
      "(Iteration 60501 / 76500) loss: 2.305414\n",
      "(Iteration 60601 / 76500) loss: 2.305410\n",
      "(Iteration 60701 / 76500) loss: 2.305412\n",
      "(Iteration 60801 / 76500) loss: 2.305401\n",
      "(Iteration 60901 / 76500) loss: 2.305392\n",
      "(Iteration 61001 / 76500) loss: 2.305414\n",
      "(Iteration 61101 / 76500) loss: 2.305408\n",
      "(Epoch 80 / 100) train acc: 0.073000; val_acc: 0.072000\n",
      "(Iteration 61201 / 76500) loss: 2.305410\n",
      "(Iteration 61301 / 76500) loss: 2.305403\n",
      "(Iteration 61401 / 76500) loss: 2.305399\n",
      "(Iteration 61501 / 76500) loss: 2.305408\n",
      "(Iteration 61601 / 76500) loss: 2.305399\n",
      "(Iteration 61701 / 76500) loss: 2.305399\n",
      "(Iteration 61801 / 76500) loss: 2.305397\n",
      "(Iteration 61901 / 76500) loss: 2.305399\n",
      "(Epoch 81 / 100) train acc: 0.087000; val_acc: 0.072000\n",
      "(Iteration 62001 / 76500) loss: 2.305406\n",
      "(Iteration 62101 / 76500) loss: 2.305417\n",
      "(Iteration 62201 / 76500) loss: 2.305411\n",
      "(Iteration 62301 / 76500) loss: 2.305414\n",
      "(Iteration 62401 / 76500) loss: 2.305394\n",
      "(Iteration 62501 / 76500) loss: 2.305400\n",
      "(Iteration 62601 / 76500) loss: 2.305408\n",
      "(Iteration 62701 / 76500) loss: 2.305402\n",
      "(Epoch 82 / 100) train acc: 0.079000; val_acc: 0.072000\n",
      "(Iteration 62801 / 76500) loss: 2.305411\n",
      "(Iteration 62901 / 76500) loss: 2.305410\n",
      "(Iteration 63001 / 76500) loss: 2.305406\n",
      "(Iteration 63101 / 76500) loss: 2.305416\n",
      "(Iteration 63201 / 76500) loss: 2.305401\n",
      "(Iteration 63301 / 76500) loss: 2.305390\n",
      "(Iteration 63401 / 76500) loss: 2.305421\n",
      "(Epoch 83 / 100) train acc: 0.074000; val_acc: 0.072000\n",
      "(Iteration 63501 / 76500) loss: 2.305407\n",
      "(Iteration 63601 / 76500) loss: 2.305415\n",
      "(Iteration 63701 / 76500) loss: 2.305415\n",
      "(Iteration 63801 / 76500) loss: 2.305408\n",
      "(Iteration 63901 / 76500) loss: 2.305408\n",
      "(Iteration 64001 / 76500) loss: 2.305414\n",
      "(Iteration 64101 / 76500) loss: 2.305403\n",
      "(Iteration 64201 / 76500) loss: 2.305401\n",
      "(Epoch 84 / 100) train acc: 0.063000; val_acc: 0.072000\n",
      "(Iteration 64301 / 76500) loss: 2.305417\n",
      "(Iteration 64401 / 76500) loss: 2.305405\n",
      "(Iteration 64501 / 76500) loss: 2.305399\n",
      "(Iteration 64601 / 76500) loss: 2.305399\n",
      "(Iteration 64701 / 76500) loss: 2.305417\n",
      "(Iteration 64801 / 76500) loss: 2.305396\n",
      "(Iteration 64901 / 76500) loss: 2.305404\n",
      "(Iteration 65001 / 76500) loss: 2.305421\n",
      "(Epoch 85 / 100) train acc: 0.105000; val_acc: 0.072000\n",
      "(Iteration 65101 / 76500) loss: 2.305404\n",
      "(Iteration 65201 / 76500) loss: 2.305413\n",
      "(Iteration 65301 / 76500) loss: 2.305398\n",
      "(Iteration 65401 / 76500) loss: 2.305401\n",
      "(Iteration 65501 / 76500) loss: 2.305411\n",
      "(Iteration 65601 / 76500) loss: 2.305400\n",
      "(Iteration 65701 / 76500) loss: 2.305402\n",
      "(Epoch 86 / 100) train acc: 0.076000; val_acc: 0.072000\n",
      "(Iteration 65801 / 76500) loss: 2.305400\n",
      "(Iteration 65901 / 76500) loss: 2.305404\n",
      "(Iteration 66001 / 76500) loss: 2.305406\n",
      "(Iteration 66101 / 76500) loss: 2.305408\n",
      "(Iteration 66201 / 76500) loss: 2.305405\n",
      "(Iteration 66301 / 76500) loss: 2.305415\n",
      "(Iteration 66401 / 76500) loss: 2.305404\n",
      "(Iteration 66501 / 76500) loss: 2.305403\n",
      "(Epoch 87 / 100) train acc: 0.069000; val_acc: 0.072000\n",
      "(Iteration 66601 / 76500) loss: 2.305406\n",
      "(Iteration 66701 / 76500) loss: 2.305407\n",
      "(Iteration 66801 / 76500) loss: 2.305414\n",
      "(Iteration 66901 / 76500) loss: 2.305399\n",
      "(Iteration 67001 / 76500) loss: 2.305416\n",
      "(Iteration 67101 / 76500) loss: 2.305406\n",
      "(Iteration 67201 / 76500) loss: 2.305405\n",
      "(Iteration 67301 / 76500) loss: 2.305397\n",
      "(Epoch 88 / 100) train acc: 0.094000; val_acc: 0.072000\n",
      "(Iteration 67401 / 76500) loss: 2.305407\n",
      "(Iteration 67501 / 76500) loss: 2.305403\n",
      "(Iteration 67601 / 76500) loss: 2.305401\n",
      "(Iteration 67701 / 76500) loss: 2.305407\n",
      "(Iteration 67801 / 76500) loss: 2.305425\n",
      "(Iteration 67901 / 76500) loss: 2.305409\n",
      "(Iteration 68001 / 76500) loss: 2.305412\n",
      "(Epoch 89 / 100) train acc: 0.086000; val_acc: 0.072000\n",
      "(Iteration 68101 / 76500) loss: 2.305405\n",
      "(Iteration 68201 / 76500) loss: 2.305408\n",
      "(Iteration 68301 / 76500) loss: 2.305409\n",
      "(Iteration 68401 / 76500) loss: 2.305411\n",
      "(Iteration 68501 / 76500) loss: 2.305396\n",
      "(Iteration 68601 / 76500) loss: 2.305403\n",
      "(Iteration 68701 / 76500) loss: 2.305402\n",
      "(Iteration 68801 / 76500) loss: 2.305402\n",
      "(Epoch 90 / 100) train acc: 0.094000; val_acc: 0.072000\n",
      "(Iteration 68901 / 76500) loss: 2.305394\n",
      "(Iteration 69001 / 76500) loss: 2.305404\n",
      "(Iteration 69101 / 76500) loss: 2.305400\n",
      "(Iteration 69201 / 76500) loss: 2.305409\n",
      "(Iteration 69301 / 76500) loss: 2.305406\n",
      "(Iteration 69401 / 76500) loss: 2.305410\n",
      "(Iteration 69501 / 76500) loss: 2.305419\n",
      "(Iteration 69601 / 76500) loss: 2.305394\n",
      "(Epoch 91 / 100) train acc: 0.091000; val_acc: 0.072000\n",
      "(Iteration 69701 / 76500) loss: 2.305410\n",
      "(Iteration 69801 / 76500) loss: 2.305412\n",
      "(Iteration 69901 / 76500) loss: 2.305405\n",
      "(Iteration 70001 / 76500) loss: 2.305403\n",
      "(Iteration 70101 / 76500) loss: 2.305412\n",
      "(Iteration 70201 / 76500) loss: 2.305392\n",
      "(Iteration 70301 / 76500) loss: 2.305409\n",
      "(Epoch 92 / 100) train acc: 0.083000; val_acc: 0.072000\n",
      "(Iteration 70401 / 76500) loss: 2.305406\n",
      "(Iteration 70501 / 76500) loss: 2.305401\n",
      "(Iteration 70601 / 76500) loss: 2.305416\n",
      "(Iteration 70701 / 76500) loss: 2.305406\n",
      "(Iteration 70801 / 76500) loss: 2.305385\n",
      "(Iteration 70901 / 76500) loss: 2.305409\n",
      "(Iteration 71001 / 76500) loss: 2.305412\n",
      "(Iteration 71101 / 76500) loss: 2.305405\n",
      "(Epoch 93 / 100) train acc: 0.076000; val_acc: 0.072000\n",
      "(Iteration 71201 / 76500) loss: 2.305407\n",
      "(Iteration 71301 / 76500) loss: 2.305404\n",
      "(Iteration 71401 / 76500) loss: 2.305400\n",
      "(Iteration 71501 / 76500) loss: 2.305407\n",
      "(Iteration 71601 / 76500) loss: 2.305412\n",
      "(Iteration 71701 / 76500) loss: 2.305404\n",
      "(Iteration 71801 / 76500) loss: 2.305410\n",
      "(Iteration 71901 / 76500) loss: 2.305403\n",
      "(Epoch 94 / 100) train acc: 0.090000; val_acc: 0.072000\n",
      "(Iteration 72001 / 76500) loss: 2.305409\n",
      "(Iteration 72101 / 76500) loss: 2.305419\n",
      "(Iteration 72201 / 76500) loss: 2.305402\n",
      "(Iteration 72301 / 76500) loss: 2.305407\n",
      "(Iteration 72401 / 76500) loss: 2.305409\n",
      "(Iteration 72501 / 76500) loss: 2.305385\n",
      "(Iteration 72601 / 76500) loss: 2.305408\n",
      "(Epoch 95 / 100) train acc: 0.090000; val_acc: 0.072000\n",
      "(Iteration 72701 / 76500) loss: 2.305406\n",
      "(Iteration 72801 / 76500) loss: 2.305404\n",
      "(Iteration 72901 / 76500) loss: 2.305419\n",
      "(Iteration 73001 / 76500) loss: 2.305406\n",
      "(Iteration 73101 / 76500) loss: 2.305399\n",
      "(Iteration 73201 / 76500) loss: 2.305406\n",
      "(Iteration 73301 / 76500) loss: 2.305404\n",
      "(Iteration 73401 / 76500) loss: 2.305407\n",
      "(Epoch 96 / 100) train acc: 0.071000; val_acc: 0.072000\n",
      "(Iteration 73501 / 76500) loss: 2.305397\n",
      "(Iteration 73601 / 76500) loss: 2.305415\n",
      "(Iteration 73701 / 76500) loss: 2.305413\n",
      "(Iteration 73801 / 76500) loss: 2.305396\n",
      "(Iteration 73901 / 76500) loss: 2.305406\n",
      "(Iteration 74001 / 76500) loss: 2.305422\n",
      "(Iteration 74101 / 76500) loss: 2.305413\n",
      "(Iteration 74201 / 76500) loss: 2.305396\n",
      "(Epoch 97 / 100) train acc: 0.083000; val_acc: 0.072000\n",
      "(Iteration 74301 / 76500) loss: 2.305403\n",
      "(Iteration 74401 / 76500) loss: 2.305398\n",
      "(Iteration 74501 / 76500) loss: 2.305397\n",
      "(Iteration 74601 / 76500) loss: 2.305400\n",
      "(Iteration 74701 / 76500) loss: 2.305417\n",
      "(Iteration 74801 / 76500) loss: 2.305400\n",
      "(Iteration 74901 / 76500) loss: 2.305404\n",
      "(Epoch 98 / 100) train acc: 0.088000; val_acc: 0.072000\n",
      "(Iteration 75001 / 76500) loss: 2.305407\n",
      "(Iteration 75101 / 76500) loss: 2.305422\n",
      "(Iteration 75201 / 76500) loss: 2.305410\n",
      "(Iteration 75301 / 76500) loss: 2.305401\n",
      "(Iteration 75401 / 76500) loss: 2.305404\n",
      "(Iteration 75501 / 76500) loss: 2.305400\n",
      "(Iteration 75601 / 76500) loss: 2.305399\n",
      "(Iteration 75701 / 76500) loss: 2.305408\n",
      "(Epoch 99 / 100) train acc: 0.091000; val_acc: 0.072000\n",
      "(Iteration 75801 / 76500) loss: 2.305406\n",
      "(Iteration 75901 / 76500) loss: 2.305402\n",
      "(Iteration 76001 / 76500) loss: 2.305397\n",
      "(Iteration 76101 / 76500) loss: 2.305402\n",
      "(Iteration 76201 / 76500) loss: 2.305393\n",
      "(Iteration 76301 / 76500) loss: 2.305412\n",
      "(Iteration 76401 / 76500) loss: 2.305408\n",
      "(Epoch 100 / 100) train acc: 0.090000; val_acc: 0.072000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.305471\n",
      "(Epoch 0 / 100) train acc: 0.109000; val_acc: 0.114000\n",
      "(Iteration 101 / 38200) loss: 2.305472\n",
      "(Iteration 201 / 38200) loss: 2.305469\n",
      "(Iteration 301 / 38200) loss: 2.305462\n",
      "(Epoch 1 / 100) train acc: 0.105000; val_acc: 0.114000\n",
      "(Iteration 401 / 38200) loss: 2.305473\n",
      "(Iteration 501 / 38200) loss: 2.305470\n",
      "(Iteration 601 / 38200) loss: 2.305480\n",
      "(Iteration 701 / 38200) loss: 2.305478\n",
      "(Epoch 2 / 100) train acc: 0.123000; val_acc: 0.114000\n",
      "(Iteration 801 / 38200) loss: 2.305473\n",
      "(Iteration 901 / 38200) loss: 2.305465\n",
      "(Iteration 1001 / 38200) loss: 2.305465\n",
      "(Iteration 1101 / 38200) loss: 2.305473\n",
      "(Epoch 3 / 100) train acc: 0.126000; val_acc: 0.114000\n",
      "(Iteration 1201 / 38200) loss: 2.305482\n",
      "(Iteration 1301 / 38200) loss: 2.305473\n",
      "(Iteration 1401 / 38200) loss: 2.305459\n",
      "(Iteration 1501 / 38200) loss: 2.305457\n",
      "(Epoch 4 / 100) train acc: 0.099000; val_acc: 0.114000\n",
      "(Iteration 1601 / 38200) loss: 2.305475\n",
      "(Iteration 1701 / 38200) loss: 2.305470\n",
      "(Iteration 1801 / 38200) loss: 2.305477\n",
      "(Iteration 1901 / 38200) loss: 2.305465\n",
      "(Epoch 5 / 100) train acc: 0.104000; val_acc: 0.114000\n",
      "(Iteration 2001 / 38200) loss: 2.305476\n",
      "(Iteration 2101 / 38200) loss: 2.305469\n",
      "(Iteration 2201 / 38200) loss: 2.305479\n",
      "(Epoch 6 / 100) train acc: 0.099000; val_acc: 0.114000\n",
      "(Iteration 2301 / 38200) loss: 2.305468\n",
      "(Iteration 2401 / 38200) loss: 2.305472\n",
      "(Iteration 2501 / 38200) loss: 2.305468\n",
      "(Iteration 2601 / 38200) loss: 2.305475\n",
      "(Epoch 7 / 100) train acc: 0.124000; val_acc: 0.114000\n",
      "(Iteration 2701 / 38200) loss: 2.305474\n",
      "(Iteration 2801 / 38200) loss: 2.305470\n",
      "(Iteration 2901 / 38200) loss: 2.305479\n",
      "(Iteration 3001 / 38200) loss: 2.305471\n",
      "(Epoch 8 / 100) train acc: 0.116000; val_acc: 0.114000\n",
      "(Iteration 3101 / 38200) loss: 2.305463\n",
      "(Iteration 3201 / 38200) loss: 2.305476\n",
      "(Iteration 3301 / 38200) loss: 2.305479\n",
      "(Iteration 3401 / 38200) loss: 2.305473\n",
      "(Epoch 9 / 100) train acc: 0.104000; val_acc: 0.114000\n",
      "(Iteration 3501 / 38200) loss: 2.305470\n",
      "(Iteration 3601 / 38200) loss: 2.305476\n",
      "(Iteration 3701 / 38200) loss: 2.305483\n",
      "(Iteration 3801 / 38200) loss: 2.305472\n",
      "(Epoch 10 / 100) train acc: 0.112000; val_acc: 0.114000\n",
      "(Iteration 3901 / 38200) loss: 2.305466\n",
      "(Iteration 4001 / 38200) loss: 2.305480\n",
      "(Iteration 4101 / 38200) loss: 2.305468\n",
      "(Iteration 4201 / 38200) loss: 2.305474\n",
      "(Epoch 11 / 100) train acc: 0.103000; val_acc: 0.114000\n",
      "(Iteration 4301 / 38200) loss: 2.305463\n",
      "(Iteration 4401 / 38200) loss: 2.305463\n",
      "(Iteration 4501 / 38200) loss: 2.305473\n",
      "(Epoch 12 / 100) train acc: 0.119000; val_acc: 0.114000\n",
      "(Iteration 4601 / 38200) loss: 2.305465\n",
      "(Iteration 4701 / 38200) loss: 2.305468\n",
      "(Iteration 4801 / 38200) loss: 2.305464\n",
      "(Iteration 4901 / 38200) loss: 2.305464\n",
      "(Epoch 13 / 100) train acc: 0.120000; val_acc: 0.114000\n",
      "(Iteration 5001 / 38200) loss: 2.305471\n",
      "(Iteration 5101 / 38200) loss: 2.305469\n",
      "(Iteration 5201 / 38200) loss: 2.305478\n",
      "(Iteration 5301 / 38200) loss: 2.305468\n",
      "(Epoch 14 / 100) train acc: 0.118000; val_acc: 0.114000\n",
      "(Iteration 5401 / 38200) loss: 2.305476\n",
      "(Iteration 5501 / 38200) loss: 2.305471\n",
      "(Iteration 5601 / 38200) loss: 2.305472\n",
      "(Iteration 5701 / 38200) loss: 2.305480\n",
      "(Epoch 15 / 100) train acc: 0.111000; val_acc: 0.114000\n",
      "(Iteration 5801 / 38200) loss: 2.305473\n",
      "(Iteration 5901 / 38200) loss: 2.305482\n",
      "(Iteration 6001 / 38200) loss: 2.305474\n",
      "(Iteration 6101 / 38200) loss: 2.305472\n",
      "(Epoch 16 / 100) train acc: 0.104000; val_acc: 0.114000\n",
      "(Iteration 6201 / 38200) loss: 2.305483\n",
      "(Iteration 6301 / 38200) loss: 2.305479\n",
      "(Iteration 6401 / 38200) loss: 2.305470\n",
      "(Epoch 17 / 100) train acc: 0.114000; val_acc: 0.114000\n",
      "(Iteration 6501 / 38200) loss: 2.305474\n",
      "(Iteration 6601 / 38200) loss: 2.305473\n",
      "(Iteration 6701 / 38200) loss: 2.305468\n",
      "(Iteration 6801 / 38200) loss: 2.305472\n",
      "(Epoch 18 / 100) train acc: 0.114000; val_acc: 0.114000\n",
      "(Iteration 6901 / 38200) loss: 2.305469\n",
      "(Iteration 7001 / 38200) loss: 2.305467\n",
      "(Iteration 7101 / 38200) loss: 2.305476\n",
      "(Iteration 7201 / 38200) loss: 2.305467\n",
      "(Epoch 19 / 100) train acc: 0.113000; val_acc: 0.114000\n",
      "(Iteration 7301 / 38200) loss: 2.305469\n",
      "(Iteration 7401 / 38200) loss: 2.305478\n",
      "(Iteration 7501 / 38200) loss: 2.305470\n",
      "(Iteration 7601 / 38200) loss: 2.305468\n",
      "(Epoch 20 / 100) train acc: 0.118000; val_acc: 0.114000\n",
      "(Iteration 7701 / 38200) loss: 2.305474\n",
      "(Iteration 7801 / 38200) loss: 2.305465\n",
      "(Iteration 7901 / 38200) loss: 2.305473\n",
      "(Iteration 8001 / 38200) loss: 2.305472\n",
      "(Epoch 21 / 100) train acc: 0.112000; val_acc: 0.114000\n",
      "(Iteration 8101 / 38200) loss: 2.305467\n",
      "(Iteration 8201 / 38200) loss: 2.305468\n",
      "(Iteration 8301 / 38200) loss: 2.305473\n",
      "(Iteration 8401 / 38200) loss: 2.305475\n",
      "(Epoch 22 / 100) train acc: 0.143000; val_acc: 0.114000\n",
      "(Iteration 8501 / 38200) loss: 2.305467\n",
      "(Iteration 8601 / 38200) loss: 2.305473\n",
      "(Iteration 8701 / 38200) loss: 2.305475\n",
      "(Epoch 23 / 100) train acc: 0.125000; val_acc: 0.114000\n",
      "(Iteration 8801 / 38200) loss: 2.305469\n",
      "(Iteration 8901 / 38200) loss: 2.305473\n",
      "(Iteration 9001 / 38200) loss: 2.305485\n",
      "(Iteration 9101 / 38200) loss: 2.305467\n",
      "(Epoch 24 / 100) train acc: 0.111000; val_acc: 0.114000\n",
      "(Iteration 9201 / 38200) loss: 2.305468\n",
      "(Iteration 9301 / 38200) loss: 2.305470\n",
      "(Iteration 9401 / 38200) loss: 2.305473\n",
      "(Iteration 9501 / 38200) loss: 2.305475\n",
      "(Epoch 25 / 100) train acc: 0.106000; val_acc: 0.114000\n",
      "(Iteration 9601 / 38200) loss: 2.305471\n",
      "(Iteration 9701 / 38200) loss: 2.305469\n",
      "(Iteration 9801 / 38200) loss: 2.305477\n",
      "(Iteration 9901 / 38200) loss: 2.305461\n",
      "(Epoch 26 / 100) train acc: 0.102000; val_acc: 0.114000\n",
      "(Iteration 10001 / 38200) loss: 2.305462\n",
      "(Iteration 10101 / 38200) loss: 2.305471\n",
      "(Iteration 10201 / 38200) loss: 2.305470\n",
      "(Iteration 10301 / 38200) loss: 2.305469\n",
      "(Epoch 27 / 100) train acc: 0.098000; val_acc: 0.114000\n",
      "(Iteration 10401 / 38200) loss: 2.305476\n",
      "(Iteration 10501 / 38200) loss: 2.305474\n",
      "(Iteration 10601 / 38200) loss: 2.305474\n",
      "(Epoch 28 / 100) train acc: 0.115000; val_acc: 0.114000\n",
      "(Iteration 10701 / 38200) loss: 2.305466\n",
      "(Iteration 10801 / 38200) loss: 2.305464\n",
      "(Iteration 10901 / 38200) loss: 2.305471\n",
      "(Iteration 11001 / 38200) loss: 2.305468\n",
      "(Epoch 29 / 100) train acc: 0.111000; val_acc: 0.114000\n",
      "(Iteration 11101 / 38200) loss: 2.305466\n",
      "(Iteration 11201 / 38200) loss: 2.305470\n",
      "(Iteration 11301 / 38200) loss: 2.305466\n",
      "(Iteration 11401 / 38200) loss: 2.305475\n",
      "(Epoch 30 / 100) train acc: 0.098000; val_acc: 0.114000\n",
      "(Iteration 11501 / 38200) loss: 2.305467\n",
      "(Iteration 11601 / 38200) loss: 2.305472\n",
      "(Iteration 11701 / 38200) loss: 2.305469\n",
      "(Iteration 11801 / 38200) loss: 2.305469\n",
      "(Epoch 31 / 100) train acc: 0.106000; val_acc: 0.114000\n",
      "(Iteration 11901 / 38200) loss: 2.305468\n",
      "(Iteration 12001 / 38200) loss: 2.305469\n",
      "(Iteration 12101 / 38200) loss: 2.305474\n",
      "(Iteration 12201 / 38200) loss: 2.305471\n",
      "(Epoch 32 / 100) train acc: 0.105000; val_acc: 0.114000\n",
      "(Iteration 12301 / 38200) loss: 2.305477\n",
      "(Iteration 12401 / 38200) loss: 2.305479\n",
      "(Iteration 12501 / 38200) loss: 2.305469\n",
      "(Iteration 12601 / 38200) loss: 2.305466\n",
      "(Epoch 33 / 100) train acc: 0.096000; val_acc: 0.114000\n",
      "(Iteration 12701 / 38200) loss: 2.305481\n",
      "(Iteration 12801 / 38200) loss: 2.305470\n",
      "(Iteration 12901 / 38200) loss: 2.305469\n",
      "(Epoch 34 / 100) train acc: 0.094000; val_acc: 0.114000\n",
      "(Iteration 13001 / 38200) loss: 2.305487\n",
      "(Iteration 13101 / 38200) loss: 2.305466\n",
      "(Iteration 13201 / 38200) loss: 2.305460\n",
      "(Iteration 13301 / 38200) loss: 2.305469\n",
      "(Epoch 35 / 100) train acc: 0.105000; val_acc: 0.114000\n",
      "(Iteration 13401 / 38200) loss: 2.305466\n",
      "(Iteration 13501 / 38200) loss: 2.305472\n",
      "(Iteration 13601 / 38200) loss: 2.305471\n",
      "(Iteration 13701 / 38200) loss: 2.305475\n",
      "(Epoch 36 / 100) train acc: 0.110000; val_acc: 0.114000\n",
      "(Iteration 13801 / 38200) loss: 2.305461\n",
      "(Iteration 13901 / 38200) loss: 2.305476\n",
      "(Iteration 14001 / 38200) loss: 2.305470\n",
      "(Iteration 14101 / 38200) loss: 2.305460\n",
      "(Epoch 37 / 100) train acc: 0.107000; val_acc: 0.114000\n",
      "(Iteration 14201 / 38200) loss: 2.305479\n",
      "(Iteration 14301 / 38200) loss: 2.305472\n",
      "(Iteration 14401 / 38200) loss: 2.305481\n",
      "(Iteration 14501 / 38200) loss: 2.305465\n",
      "(Epoch 38 / 100) train acc: 0.099000; val_acc: 0.114000\n",
      "(Iteration 14601 / 38200) loss: 2.305469\n",
      "(Iteration 14701 / 38200) loss: 2.305475\n",
      "(Iteration 14801 / 38200) loss: 2.305469\n",
      "(Epoch 39 / 100) train acc: 0.113000; val_acc: 0.114000\n",
      "(Iteration 14901 / 38200) loss: 2.305467\n",
      "(Iteration 15001 / 38200) loss: 2.305471\n",
      "(Iteration 15101 / 38200) loss: 2.305472\n",
      "(Iteration 15201 / 38200) loss: 2.305474\n",
      "(Epoch 40 / 100) train acc: 0.108000; val_acc: 0.114000\n",
      "(Iteration 15301 / 38200) loss: 2.305472\n",
      "(Iteration 15401 / 38200) loss: 2.305461\n",
      "(Iteration 15501 / 38200) loss: 2.305475\n",
      "(Iteration 15601 / 38200) loss: 2.305471\n",
      "(Epoch 41 / 100) train acc: 0.120000; val_acc: 0.114000\n",
      "(Iteration 15701 / 38200) loss: 2.305476\n",
      "(Iteration 15801 / 38200) loss: 2.305480\n",
      "(Iteration 15901 / 38200) loss: 2.305470\n",
      "(Iteration 16001 / 38200) loss: 2.305474\n",
      "(Epoch 42 / 100) train acc: 0.101000; val_acc: 0.114000\n",
      "(Iteration 16101 / 38200) loss: 2.305466\n",
      "(Iteration 16201 / 38200) loss: 2.305468\n",
      "(Iteration 16301 / 38200) loss: 2.305468\n",
      "(Iteration 16401 / 38200) loss: 2.305462\n",
      "(Epoch 43 / 100) train acc: 0.119000; val_acc: 0.114000\n",
      "(Iteration 16501 / 38200) loss: 2.305472\n",
      "(Iteration 16601 / 38200) loss: 2.305479\n",
      "(Iteration 16701 / 38200) loss: 2.305472\n",
      "(Iteration 16801 / 38200) loss: 2.305467\n",
      "(Epoch 44 / 100) train acc: 0.123000; val_acc: 0.114000\n",
      "(Iteration 16901 / 38200) loss: 2.305467\n",
      "(Iteration 17001 / 38200) loss: 2.305470\n",
      "(Iteration 17101 / 38200) loss: 2.305471\n",
      "(Epoch 45 / 100) train acc: 0.095000; val_acc: 0.114000\n",
      "(Iteration 17201 / 38200) loss: 2.305472\n",
      "(Iteration 17301 / 38200) loss: 2.305477\n",
      "(Iteration 17401 / 38200) loss: 2.305467\n",
      "(Iteration 17501 / 38200) loss: 2.305465\n",
      "(Epoch 46 / 100) train acc: 0.110000; val_acc: 0.114000\n",
      "(Iteration 17601 / 38200) loss: 2.305472\n",
      "(Iteration 17701 / 38200) loss: 2.305470\n",
      "(Iteration 17801 / 38200) loss: 2.305473\n",
      "(Iteration 17901 / 38200) loss: 2.305470\n",
      "(Epoch 47 / 100) train acc: 0.116000; val_acc: 0.114000\n",
      "(Iteration 18001 / 38200) loss: 2.305473\n",
      "(Iteration 18101 / 38200) loss: 2.305471\n",
      "(Iteration 18201 / 38200) loss: 2.305473\n",
      "(Iteration 18301 / 38200) loss: 2.305474\n",
      "(Epoch 48 / 100) train acc: 0.114000; val_acc: 0.114000\n",
      "(Iteration 18401 / 38200) loss: 2.305470\n",
      "(Iteration 18501 / 38200) loss: 2.305467\n",
      "(Iteration 18601 / 38200) loss: 2.305463\n",
      "(Iteration 18701 / 38200) loss: 2.305469\n",
      "(Epoch 49 / 100) train acc: 0.097000; val_acc: 0.114000\n",
      "(Iteration 18801 / 38200) loss: 2.305467\n",
      "(Iteration 18901 / 38200) loss: 2.305465\n",
      "(Iteration 19001 / 38200) loss: 2.305468\n",
      "(Epoch 50 / 100) train acc: 0.116000; val_acc: 0.114000\n",
      "(Iteration 19101 / 38200) loss: 2.305468\n",
      "(Iteration 19201 / 38200) loss: 2.305481\n",
      "(Iteration 19301 / 38200) loss: 2.305466\n",
      "(Iteration 19401 / 38200) loss: 2.305477\n",
      "(Epoch 51 / 100) train acc: 0.111000; val_acc: 0.114000\n",
      "(Iteration 19501 / 38200) loss: 2.305464\n",
      "(Iteration 19601 / 38200) loss: 2.305486\n",
      "(Iteration 19701 / 38200) loss: 2.305477\n",
      "(Iteration 19801 / 38200) loss: 2.305469\n",
      "(Epoch 52 / 100) train acc: 0.105000; val_acc: 0.114000\n",
      "(Iteration 19901 / 38200) loss: 2.305473\n",
      "(Iteration 20001 / 38200) loss: 2.305470\n",
      "(Iteration 20101 / 38200) loss: 2.305470\n",
      "(Iteration 20201 / 38200) loss: 2.305467\n",
      "(Epoch 53 / 100) train acc: 0.115000; val_acc: 0.114000\n",
      "(Iteration 20301 / 38200) loss: 2.305476\n",
      "(Iteration 20401 / 38200) loss: 2.305467\n",
      "(Iteration 20501 / 38200) loss: 2.305464\n",
      "(Iteration 20601 / 38200) loss: 2.305474\n",
      "(Epoch 54 / 100) train acc: 0.119000; val_acc: 0.114000\n",
      "(Iteration 20701 / 38200) loss: 2.305472\n",
      "(Iteration 20801 / 38200) loss: 2.305466\n",
      "(Iteration 20901 / 38200) loss: 2.305473\n",
      "(Iteration 21001 / 38200) loss: 2.305474\n",
      "(Epoch 55 / 100) train acc: 0.102000; val_acc: 0.114000\n",
      "(Iteration 21101 / 38200) loss: 2.305465\n",
      "(Iteration 21201 / 38200) loss: 2.305468\n",
      "(Iteration 21301 / 38200) loss: 2.305471\n",
      "(Epoch 56 / 100) train acc: 0.093000; val_acc: 0.114000\n",
      "(Iteration 21401 / 38200) loss: 2.305471\n",
      "(Iteration 21501 / 38200) loss: 2.305464\n",
      "(Iteration 21601 / 38200) loss: 2.305474\n",
      "(Iteration 21701 / 38200) loss: 2.305463\n",
      "(Epoch 57 / 100) train acc: 0.104000; val_acc: 0.114000\n",
      "(Iteration 21801 / 38200) loss: 2.305470\n",
      "(Iteration 21901 / 38200) loss: 2.305475\n",
      "(Iteration 22001 / 38200) loss: 2.305471\n",
      "(Iteration 22101 / 38200) loss: 2.305469\n",
      "(Epoch 58 / 100) train acc: 0.108000; val_acc: 0.114000\n",
      "(Iteration 22201 / 38200) loss: 2.305465\n",
      "(Iteration 22301 / 38200) loss: 2.305473\n",
      "(Iteration 22401 / 38200) loss: 2.305470\n",
      "(Iteration 22501 / 38200) loss: 2.305459\n",
      "(Epoch 59 / 100) train acc: 0.111000; val_acc: 0.114000\n",
      "(Iteration 22601 / 38200) loss: 2.305474\n",
      "(Iteration 22701 / 38200) loss: 2.305470\n",
      "(Iteration 22801 / 38200) loss: 2.305472\n",
      "(Iteration 22901 / 38200) loss: 2.305467\n",
      "(Epoch 60 / 100) train acc: 0.111000; val_acc: 0.114000\n",
      "(Iteration 23001 / 38200) loss: 2.305461\n",
      "(Iteration 23101 / 38200) loss: 2.305476\n",
      "(Iteration 23201 / 38200) loss: 2.305475\n",
      "(Iteration 23301 / 38200) loss: 2.305469\n",
      "(Epoch 61 / 100) train acc: 0.106000; val_acc: 0.114000\n",
      "(Iteration 23401 / 38200) loss: 2.305467\n",
      "(Iteration 23501 / 38200) loss: 2.305467\n",
      "(Iteration 23601 / 38200) loss: 2.305464\n",
      "(Epoch 62 / 100) train acc: 0.102000; val_acc: 0.114000\n",
      "(Iteration 23701 / 38200) loss: 2.305475\n",
      "(Iteration 23801 / 38200) loss: 2.305469\n",
      "(Iteration 23901 / 38200) loss: 2.305465\n",
      "(Iteration 24001 / 38200) loss: 2.305467\n",
      "(Epoch 63 / 100) train acc: 0.104000; val_acc: 0.114000\n",
      "(Iteration 24101 / 38200) loss: 2.305465\n",
      "(Iteration 24201 / 38200) loss: 2.305465\n",
      "(Iteration 24301 / 38200) loss: 2.305471\n",
      "(Iteration 24401 / 38200) loss: 2.305466\n",
      "(Epoch 64 / 100) train acc: 0.117000; val_acc: 0.114000\n",
      "(Iteration 24501 / 38200) loss: 2.305466\n",
      "(Iteration 24601 / 38200) loss: 2.305468\n",
      "(Iteration 24701 / 38200) loss: 2.305479\n",
      "(Iteration 24801 / 38200) loss: 2.305467\n",
      "(Epoch 65 / 100) train acc: 0.116000; val_acc: 0.114000\n",
      "(Iteration 24901 / 38200) loss: 2.305462\n",
      "(Iteration 25001 / 38200) loss: 2.305469\n",
      "(Iteration 25101 / 38200) loss: 2.305469\n",
      "(Iteration 25201 / 38200) loss: 2.305477\n",
      "(Epoch 66 / 100) train acc: 0.117000; val_acc: 0.114000\n",
      "(Iteration 25301 / 38200) loss: 2.305474\n",
      "(Iteration 25401 / 38200) loss: 2.305477\n",
      "(Iteration 25501 / 38200) loss: 2.305463\n",
      "(Epoch 67 / 100) train acc: 0.108000; val_acc: 0.114000\n",
      "(Iteration 25601 / 38200) loss: 2.305481\n",
      "(Iteration 25701 / 38200) loss: 2.305475\n",
      "(Iteration 25801 / 38200) loss: 2.305473\n",
      "(Iteration 25901 / 38200) loss: 2.305475\n",
      "(Epoch 68 / 100) train acc: 0.112000; val_acc: 0.114000\n",
      "(Iteration 26001 / 38200) loss: 2.305471\n",
      "(Iteration 26101 / 38200) loss: 2.305473\n",
      "(Iteration 26201 / 38200) loss: 2.305465\n",
      "(Iteration 26301 / 38200) loss: 2.305472\n",
      "(Epoch 69 / 100) train acc: 0.120000; val_acc: 0.114000\n",
      "(Iteration 26401 / 38200) loss: 2.305468\n",
      "(Iteration 26501 / 38200) loss: 2.305479\n",
      "(Iteration 26601 / 38200) loss: 2.305486\n",
      "(Iteration 26701 / 38200) loss: 2.305466\n",
      "(Epoch 70 / 100) train acc: 0.108000; val_acc: 0.114000\n",
      "(Iteration 26801 / 38200) loss: 2.305472\n",
      "(Iteration 26901 / 38200) loss: 2.305472\n",
      "(Iteration 27001 / 38200) loss: 2.305477\n",
      "(Iteration 27101 / 38200) loss: 2.305474\n",
      "(Epoch 71 / 100) train acc: 0.107000; val_acc: 0.114000\n",
      "(Iteration 27201 / 38200) loss: 2.305468\n",
      "(Iteration 27301 / 38200) loss: 2.305466\n",
      "(Iteration 27401 / 38200) loss: 2.305473\n",
      "(Iteration 27501 / 38200) loss: 2.305469\n",
      "(Epoch 72 / 100) train acc: 0.114000; val_acc: 0.114000\n",
      "(Iteration 27601 / 38200) loss: 2.305468\n",
      "(Iteration 27701 / 38200) loss: 2.305474\n",
      "(Iteration 27801 / 38200) loss: 2.305467\n",
      "(Epoch 73 / 100) train acc: 0.105000; val_acc: 0.114000\n",
      "(Iteration 27901 / 38200) loss: 2.305466\n",
      "(Iteration 28001 / 38200) loss: 2.305476\n",
      "(Iteration 28101 / 38200) loss: 2.305458\n",
      "(Iteration 28201 / 38200) loss: 2.305469\n",
      "(Epoch 74 / 100) train acc: 0.106000; val_acc: 0.114000\n",
      "(Iteration 28301 / 38200) loss: 2.305464\n",
      "(Iteration 28401 / 38200) loss: 2.305469\n",
      "(Iteration 28501 / 38200) loss: 2.305468\n",
      "(Iteration 28601 / 38200) loss: 2.305461\n",
      "(Epoch 75 / 100) train acc: 0.079000; val_acc: 0.114000\n",
      "(Iteration 28701 / 38200) loss: 2.305467\n",
      "(Iteration 28801 / 38200) loss: 2.305475\n",
      "(Iteration 28901 / 38200) loss: 2.305460\n",
      "(Iteration 29001 / 38200) loss: 2.305466\n",
      "(Epoch 76 / 100) train acc: 0.121000; val_acc: 0.114000\n",
      "(Iteration 29101 / 38200) loss: 2.305468\n",
      "(Iteration 29201 / 38200) loss: 2.305477\n",
      "(Iteration 29301 / 38200) loss: 2.305466\n",
      "(Iteration 29401 / 38200) loss: 2.305480\n",
      "(Epoch 77 / 100) train acc: 0.098000; val_acc: 0.114000\n",
      "(Iteration 29501 / 38200) loss: 2.305472\n",
      "(Iteration 29601 / 38200) loss: 2.305473\n",
      "(Iteration 29701 / 38200) loss: 2.305472\n",
      "(Epoch 78 / 100) train acc: 0.110000; val_acc: 0.114000\n",
      "(Iteration 29801 / 38200) loss: 2.305465\n",
      "(Iteration 29901 / 38200) loss: 2.305469\n",
      "(Iteration 30001 / 38200) loss: 2.305475\n",
      "(Iteration 30101 / 38200) loss: 2.305467\n",
      "(Epoch 79 / 100) train acc: 0.099000; val_acc: 0.114000\n",
      "(Iteration 30201 / 38200) loss: 2.305467\n",
      "(Iteration 30301 / 38200) loss: 2.305470\n",
      "(Iteration 30401 / 38200) loss: 2.305463\n",
      "(Iteration 30501 / 38200) loss: 2.305470\n",
      "(Epoch 80 / 100) train acc: 0.099000; val_acc: 0.114000\n",
      "(Iteration 30601 / 38200) loss: 2.305472\n",
      "(Iteration 30701 / 38200) loss: 2.305476\n",
      "(Iteration 30801 / 38200) loss: 2.305470\n",
      "(Iteration 30901 / 38200) loss: 2.305474\n",
      "(Epoch 81 / 100) train acc: 0.108000; val_acc: 0.114000\n",
      "(Iteration 31001 / 38200) loss: 2.305465\n",
      "(Iteration 31101 / 38200) loss: 2.305470\n",
      "(Iteration 31201 / 38200) loss: 2.305467\n",
      "(Iteration 31301 / 38200) loss: 2.305467\n",
      "(Epoch 82 / 100) train acc: 0.099000; val_acc: 0.114000\n",
      "(Iteration 31401 / 38200) loss: 2.305472\n",
      "(Iteration 31501 / 38200) loss: 2.305464\n",
      "(Iteration 31601 / 38200) loss: 2.305468\n",
      "(Iteration 31701 / 38200) loss: 2.305467\n",
      "(Epoch 83 / 100) train acc: 0.112000; val_acc: 0.114000\n",
      "(Iteration 31801 / 38200) loss: 2.305472\n",
      "(Iteration 31901 / 38200) loss: 2.305472\n",
      "(Iteration 32001 / 38200) loss: 2.305464\n",
      "(Epoch 84 / 100) train acc: 0.115000; val_acc: 0.114000\n",
      "(Iteration 32101 / 38200) loss: 2.305474\n",
      "(Iteration 32201 / 38200) loss: 2.305461\n",
      "(Iteration 32301 / 38200) loss: 2.305467\n",
      "(Iteration 32401 / 38200) loss: 2.305467\n",
      "(Epoch 85 / 100) train acc: 0.081000; val_acc: 0.114000\n",
      "(Iteration 32501 / 38200) loss: 2.305466\n",
      "(Iteration 32601 / 38200) loss: 2.305468\n",
      "(Iteration 32701 / 38200) loss: 2.305472\n",
      "(Iteration 32801 / 38200) loss: 2.305471\n",
      "(Epoch 86 / 100) train acc: 0.111000; val_acc: 0.114000\n",
      "(Iteration 32901 / 38200) loss: 2.305471\n",
      "(Iteration 33001 / 38200) loss: 2.305478\n",
      "(Iteration 33101 / 38200) loss: 2.305474\n",
      "(Iteration 33201 / 38200) loss: 2.305470\n",
      "(Epoch 87 / 100) train acc: 0.106000; val_acc: 0.114000\n",
      "(Iteration 33301 / 38200) loss: 2.305477\n",
      "(Iteration 33401 / 38200) loss: 2.305472\n",
      "(Iteration 33501 / 38200) loss: 2.305472\n",
      "(Iteration 33601 / 38200) loss: 2.305474\n",
      "(Epoch 88 / 100) train acc: 0.122000; val_acc: 0.114000\n",
      "(Iteration 33701 / 38200) loss: 2.305477\n",
      "(Iteration 33801 / 38200) loss: 2.305470\n",
      "(Iteration 33901 / 38200) loss: 2.305476\n",
      "(Epoch 89 / 100) train acc: 0.088000; val_acc: 0.114000\n",
      "(Iteration 34001 / 38200) loss: 2.305476\n",
      "(Iteration 34101 / 38200) loss: 2.305467\n",
      "(Iteration 34201 / 38200) loss: 2.305472\n",
      "(Iteration 34301 / 38200) loss: 2.305469\n",
      "(Epoch 90 / 100) train acc: 0.117000; val_acc: 0.114000\n",
      "(Iteration 34401 / 38200) loss: 2.305476\n",
      "(Iteration 34501 / 38200) loss: 2.305463\n",
      "(Iteration 34601 / 38200) loss: 2.305466\n",
      "(Iteration 34701 / 38200) loss: 2.305470\n",
      "(Epoch 91 / 100) train acc: 0.112000; val_acc: 0.114000\n",
      "(Iteration 34801 / 38200) loss: 2.305472\n",
      "(Iteration 34901 / 38200) loss: 2.305472\n",
      "(Iteration 35001 / 38200) loss: 2.305467\n",
      "(Iteration 35101 / 38200) loss: 2.305463\n",
      "(Epoch 92 / 100) train acc: 0.111000; val_acc: 0.114000\n",
      "(Iteration 35201 / 38200) loss: 2.305467\n",
      "(Iteration 35301 / 38200) loss: 2.305466\n",
      "(Iteration 35401 / 38200) loss: 2.305473\n",
      "(Iteration 35501 / 38200) loss: 2.305471\n",
      "(Epoch 93 / 100) train acc: 0.122000; val_acc: 0.114000\n",
      "(Iteration 35601 / 38200) loss: 2.305470\n",
      "(Iteration 35701 / 38200) loss: 2.305464\n",
      "(Iteration 35801 / 38200) loss: 2.305478\n",
      "(Iteration 35901 / 38200) loss: 2.305476\n",
      "(Epoch 94 / 100) train acc: 0.095000; val_acc: 0.114000\n",
      "(Iteration 36001 / 38200) loss: 2.305460\n",
      "(Iteration 36101 / 38200) loss: 2.305475\n",
      "(Iteration 36201 / 38200) loss: 2.305477\n",
      "(Epoch 95 / 100) train acc: 0.113000; val_acc: 0.114000\n",
      "(Iteration 36301 / 38200) loss: 2.305467\n",
      "(Iteration 36401 / 38200) loss: 2.305474\n",
      "(Iteration 36501 / 38200) loss: 2.305468\n",
      "(Iteration 36601 / 38200) loss: 2.305472\n",
      "(Epoch 96 / 100) train acc: 0.100000; val_acc: 0.114000\n",
      "(Iteration 36701 / 38200) loss: 2.305474\n",
      "(Iteration 36801 / 38200) loss: 2.305469\n",
      "(Iteration 36901 / 38200) loss: 2.305465\n",
      "(Iteration 37001 / 38200) loss: 2.305482\n",
      "(Epoch 97 / 100) train acc: 0.121000; val_acc: 0.114000\n",
      "(Iteration 37101 / 38200) loss: 2.305472\n",
      "(Iteration 37201 / 38200) loss: 2.305464\n",
      "(Iteration 37301 / 38200) loss: 2.305472\n",
      "(Iteration 37401 / 38200) loss: 2.305471\n",
      "(Epoch 98 / 100) train acc: 0.115000; val_acc: 0.114000\n",
      "(Iteration 37501 / 38200) loss: 2.305475\n",
      "(Iteration 37601 / 38200) loss: 2.305473\n",
      "(Iteration 37701 / 38200) loss: 2.305469\n",
      "(Iteration 37801 / 38200) loss: 2.305469\n",
      "(Epoch 99 / 100) train acc: 0.114000; val_acc: 0.114000\n",
      "(Iteration 37901 / 38200) loss: 2.305475\n",
      "(Iteration 38001 / 38200) loss: 2.305463\n",
      "(Iteration 38101 / 38200) loss: 2.305471\n",
      "(Epoch 100 / 100) train acc: 0.105000; val_acc: 0.114000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.304591\n",
      "(Epoch 0 / 80) train acc: 0.092000; val_acc: 0.106000\n",
      "(Iteration 101 / 61200) loss: 2.304554\n",
      "(Iteration 201 / 61200) loss: 2.304533\n",
      "(Iteration 301 / 61200) loss: 2.304524\n",
      "(Iteration 401 / 61200) loss: 2.304505\n",
      "(Iteration 501 / 61200) loss: 2.304474\n",
      "(Iteration 601 / 61200) loss: 2.304463\n",
      "(Iteration 701 / 61200) loss: 2.304442\n",
      "(Epoch 1 / 80) train acc: 0.088000; val_acc: 0.086000\n",
      "(Iteration 801 / 61200) loss: 2.304430\n",
      "(Iteration 901 / 61200) loss: 2.304401\n",
      "(Iteration 1001 / 61200) loss: 2.304394\n",
      "(Iteration 1101 / 61200) loss: 2.304394\n",
      "(Iteration 1201 / 61200) loss: 2.304356\n",
      "(Iteration 1301 / 61200) loss: 2.304356\n",
      "(Iteration 1401 / 61200) loss: 2.304393\n",
      "(Iteration 1501 / 61200) loss: 2.304311\n",
      "(Epoch 2 / 80) train acc: 0.098000; val_acc: 0.097000\n",
      "(Iteration 1601 / 61200) loss: 2.304303\n",
      "(Iteration 1701 / 61200) loss: 2.304337\n",
      "(Iteration 1801 / 61200) loss: 2.304205\n",
      "(Iteration 1901 / 61200) loss: 2.304322\n",
      "(Iteration 2001 / 61200) loss: 2.304253\n",
      "(Iteration 2101 / 61200) loss: 2.304234\n",
      "(Iteration 2201 / 61200) loss: 2.304185\n",
      "(Epoch 3 / 80) train acc: 0.091000; val_acc: 0.096000\n",
      "(Iteration 2301 / 61200) loss: 2.304202\n",
      "(Iteration 2401 / 61200) loss: 2.304180\n",
      "(Iteration 2501 / 61200) loss: 2.304185\n",
      "(Iteration 2601 / 61200) loss: 2.304160\n",
      "(Iteration 2701 / 61200) loss: 2.304180\n",
      "(Iteration 2801 / 61200) loss: 2.304109\n",
      "(Iteration 2901 / 61200) loss: 2.304141\n",
      "(Iteration 3001 / 61200) loss: 2.304135\n",
      "(Epoch 4 / 80) train acc: 0.078000; val_acc: 0.079000\n",
      "(Iteration 3101 / 61200) loss: 2.304160\n",
      "(Iteration 3201 / 61200) loss: 2.304119\n",
      "(Iteration 3301 / 61200) loss: 2.304143\n",
      "(Iteration 3401 / 61200) loss: 2.304099\n",
      "(Iteration 3501 / 61200) loss: 2.304027\n",
      "(Iteration 3601 / 61200) loss: 2.304073\n",
      "(Iteration 3701 / 61200) loss: 2.304099\n",
      "(Iteration 3801 / 61200) loss: 2.304018\n",
      "(Epoch 5 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 3901 / 61200) loss: 2.303980\n",
      "(Iteration 4001 / 61200) loss: 2.304052\n",
      "(Iteration 4101 / 61200) loss: 2.304008\n",
      "(Iteration 4201 / 61200) loss: 2.303988\n",
      "(Iteration 4301 / 61200) loss: 2.303986\n",
      "(Iteration 4401 / 61200) loss: 2.304064\n",
      "(Iteration 4501 / 61200) loss: 2.304032\n",
      "(Epoch 6 / 80) train acc: 0.115000; val_acc: 0.079000\n",
      "(Iteration 4601 / 61200) loss: 2.304012\n",
      "(Iteration 4701 / 61200) loss: 2.303921\n",
      "(Iteration 4801 / 61200) loss: 2.303990\n",
      "(Iteration 4901 / 61200) loss: 2.303957\n",
      "(Iteration 5001 / 61200) loss: 2.303909\n",
      "(Iteration 5101 / 61200) loss: 2.303929\n",
      "(Iteration 5201 / 61200) loss: 2.303967\n",
      "(Iteration 5301 / 61200) loss: 2.303985\n",
      "(Epoch 7 / 80) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 5401 / 61200) loss: 2.303886\n",
      "(Iteration 5501 / 61200) loss: 2.303951\n",
      "(Iteration 5601 / 61200) loss: 2.303915\n",
      "(Iteration 5701 / 61200) loss: 2.303843\n",
      "(Iteration 5801 / 61200) loss: 2.303924\n",
      "(Iteration 5901 / 61200) loss: 2.303906\n",
      "(Iteration 6001 / 61200) loss: 2.303866\n",
      "(Iteration 6101 / 61200) loss: 2.303872\n",
      "(Epoch 8 / 80) train acc: 0.090000; val_acc: 0.079000\n",
      "(Iteration 6201 / 61200) loss: 2.303884\n",
      "(Iteration 6301 / 61200) loss: 2.303868\n",
      "(Iteration 6401 / 61200) loss: 2.303831\n",
      "(Iteration 6501 / 61200) loss: 2.303872\n",
      "(Iteration 6601 / 61200) loss: 2.303852\n",
      "(Iteration 6701 / 61200) loss: 2.303833\n",
      "(Iteration 6801 / 61200) loss: 2.303767\n",
      "(Epoch 9 / 80) train acc: 0.078000; val_acc: 0.079000\n",
      "(Iteration 6901 / 61200) loss: 2.303824\n",
      "(Iteration 7001 / 61200) loss: 2.303848\n",
      "(Iteration 7101 / 61200) loss: 2.303830\n",
      "(Iteration 7201 / 61200) loss: 2.303817\n",
      "(Iteration 7301 / 61200) loss: 2.303750\n",
      "(Iteration 7401 / 61200) loss: 2.303869\n",
      "(Iteration 7501 / 61200) loss: 2.303865\n",
      "(Iteration 7601 / 61200) loss: 2.303777\n",
      "(Epoch 10 / 80) train acc: 0.086000; val_acc: 0.079000\n",
      "(Iteration 7701 / 61200) loss: 2.303798\n",
      "(Iteration 7801 / 61200) loss: 2.303728\n",
      "(Iteration 7901 / 61200) loss: 2.303790\n",
      "(Iteration 8001 / 61200) loss: 2.303759\n",
      "(Iteration 8101 / 61200) loss: 2.303748\n",
      "(Iteration 8201 / 61200) loss: 2.303798\n",
      "(Iteration 8301 / 61200) loss: 2.303719\n",
      "(Iteration 8401 / 61200) loss: 2.303746\n",
      "(Epoch 11 / 80) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 8501 / 61200) loss: 2.303793\n",
      "(Iteration 8601 / 61200) loss: 2.303714\n",
      "(Iteration 8701 / 61200) loss: 2.303804\n",
      "(Iteration 8801 / 61200) loss: 2.303766\n",
      "(Iteration 8901 / 61200) loss: 2.303779\n",
      "(Iteration 9001 / 61200) loss: 2.303714\n",
      "(Iteration 9101 / 61200) loss: 2.303705\n",
      "(Epoch 12 / 80) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 9201 / 61200) loss: 2.303707\n",
      "(Iteration 9301 / 61200) loss: 2.303713\n",
      "(Iteration 9401 / 61200) loss: 2.303639\n",
      "(Iteration 9501 / 61200) loss: 2.303729\n",
      "(Iteration 9601 / 61200) loss: 2.303655\n",
      "(Iteration 9701 / 61200) loss: 2.303782\n",
      "(Iteration 9801 / 61200) loss: 2.303713\n",
      "(Iteration 9901 / 61200) loss: 2.303688\n",
      "(Epoch 13 / 80) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 10001 / 61200) loss: 2.303726\n",
      "(Iteration 10101 / 61200) loss: 2.303701\n",
      "(Iteration 10201 / 61200) loss: 2.303676\n",
      "(Iteration 10301 / 61200) loss: 2.303735\n",
      "(Iteration 10401 / 61200) loss: 2.303719\n",
      "(Iteration 10501 / 61200) loss: 2.303704\n",
      "(Iteration 10601 / 61200) loss: 2.303738\n",
      "(Iteration 10701 / 61200) loss: 2.303643\n",
      "(Epoch 14 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 10801 / 61200) loss: 2.303641\n",
      "(Iteration 10901 / 61200) loss: 2.303652\n",
      "(Iteration 11001 / 61200) loss: 2.303604\n",
      "(Iteration 11101 / 61200) loss: 2.303638\n",
      "(Iteration 11201 / 61200) loss: 2.303623\n",
      "(Iteration 11301 / 61200) loss: 2.303626\n",
      "(Iteration 11401 / 61200) loss: 2.303608\n",
      "(Epoch 15 / 80) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 11501 / 61200) loss: 2.303663\n",
      "(Iteration 11601 / 61200) loss: 2.303749\n",
      "(Iteration 11701 / 61200) loss: 2.303675\n",
      "(Iteration 11801 / 61200) loss: 2.303597\n",
      "(Iteration 11901 / 61200) loss: 2.303700\n",
      "(Iteration 12001 / 61200) loss: 2.303631\n",
      "(Iteration 12101 / 61200) loss: 2.303680\n",
      "(Iteration 12201 / 61200) loss: 2.303625\n",
      "(Epoch 16 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 12301 / 61200) loss: 2.303637\n",
      "(Iteration 12401 / 61200) loss: 2.303623\n",
      "(Iteration 12501 / 61200) loss: 2.303720\n",
      "(Iteration 12601 / 61200) loss: 2.303661\n",
      "(Iteration 12701 / 61200) loss: 2.303662\n",
      "(Iteration 12801 / 61200) loss: 2.303639\n",
      "(Iteration 12901 / 61200) loss: 2.303605\n",
      "(Iteration 13001 / 61200) loss: 2.303582\n",
      "(Epoch 17 / 80) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 13101 / 61200) loss: 2.303595\n",
      "(Iteration 13201 / 61200) loss: 2.303664\n",
      "(Iteration 13301 / 61200) loss: 2.303631\n",
      "(Iteration 13401 / 61200) loss: 2.303648\n",
      "(Iteration 13501 / 61200) loss: 2.303644\n",
      "(Iteration 13601 / 61200) loss: 2.303613\n",
      "(Iteration 13701 / 61200) loss: 2.303691\n",
      "(Epoch 18 / 80) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 13801 / 61200) loss: 2.303580\n",
      "(Iteration 13901 / 61200) loss: 2.303576\n",
      "(Iteration 14001 / 61200) loss: 2.303567\n",
      "(Iteration 14101 / 61200) loss: 2.303603\n",
      "(Iteration 14201 / 61200) loss: 2.303620\n",
      "(Iteration 14301 / 61200) loss: 2.303676\n",
      "(Iteration 14401 / 61200) loss: 2.303632\n",
      "(Iteration 14501 / 61200) loss: 2.303652\n",
      "(Epoch 19 / 80) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 14601 / 61200) loss: 2.303559\n",
      "(Iteration 14701 / 61200) loss: 2.303575\n",
      "(Iteration 14801 / 61200) loss: 2.303595\n",
      "(Iteration 14901 / 61200) loss: 2.303619\n",
      "(Iteration 15001 / 61200) loss: 2.303634\n",
      "(Iteration 15101 / 61200) loss: 2.303592\n",
      "(Iteration 15201 / 61200) loss: 2.303686\n",
      "(Epoch 20 / 80) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 15301 / 61200) loss: 2.303629\n",
      "(Iteration 15401 / 61200) loss: 2.303567\n",
      "(Iteration 15501 / 61200) loss: 2.303687\n",
      "(Iteration 15601 / 61200) loss: 2.303602\n",
      "(Iteration 15701 / 61200) loss: 2.303680\n",
      "(Iteration 15801 / 61200) loss: 2.303512\n",
      "(Iteration 15901 / 61200) loss: 2.303552\n",
      "(Iteration 16001 / 61200) loss: 2.303576\n",
      "(Epoch 21 / 80) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 16101 / 61200) loss: 2.303689\n",
      "(Iteration 16201 / 61200) loss: 2.303659\n",
      "(Iteration 16301 / 61200) loss: 2.303602\n",
      "(Iteration 16401 / 61200) loss: 2.303602\n",
      "(Iteration 16501 / 61200) loss: 2.303560\n",
      "(Iteration 16601 / 61200) loss: 2.303615\n",
      "(Iteration 16701 / 61200) loss: 2.303617\n",
      "(Iteration 16801 / 61200) loss: 2.303614\n",
      "(Epoch 22 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 16901 / 61200) loss: 2.303546\n",
      "(Iteration 17001 / 61200) loss: 2.303683\n",
      "(Iteration 17101 / 61200) loss: 2.303555\n",
      "(Iteration 17201 / 61200) loss: 2.303550\n",
      "(Iteration 17301 / 61200) loss: 2.303539\n",
      "(Iteration 17401 / 61200) loss: 2.303596\n",
      "(Iteration 17501 / 61200) loss: 2.303624\n",
      "(Epoch 23 / 80) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 17601 / 61200) loss: 2.303565\n",
      "(Iteration 17701 / 61200) loss: 2.303656\n",
      "(Iteration 17801 / 61200) loss: 2.303585\n",
      "(Iteration 17901 / 61200) loss: 2.303612\n",
      "(Iteration 18001 / 61200) loss: 2.303573\n",
      "(Iteration 18101 / 61200) loss: 2.303552\n",
      "(Iteration 18201 / 61200) loss: 2.303546\n",
      "(Iteration 18301 / 61200) loss: 2.303482\n",
      "(Epoch 24 / 80) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 18401 / 61200) loss: 2.303505\n",
      "(Iteration 18501 / 61200) loss: 2.303501\n",
      "(Iteration 18601 / 61200) loss: 2.303643\n",
      "(Iteration 18701 / 61200) loss: 2.303549\n",
      "(Iteration 18801 / 61200) loss: 2.303592\n",
      "(Iteration 18901 / 61200) loss: 2.303499\n",
      "(Iteration 19001 / 61200) loss: 2.303550\n",
      "(Iteration 19101 / 61200) loss: 2.303580\n",
      "(Epoch 25 / 80) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 19201 / 61200) loss: 2.303591\n",
      "(Iteration 19301 / 61200) loss: 2.303727\n",
      "(Iteration 19401 / 61200) loss: 2.303554\n",
      "(Iteration 19501 / 61200) loss: 2.303516\n",
      "(Iteration 19601 / 61200) loss: 2.303504\n",
      "(Iteration 19701 / 61200) loss: 2.303600\n",
      "(Iteration 19801 / 61200) loss: 2.303519\n",
      "(Epoch 26 / 80) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 19901 / 61200) loss: 2.303551\n",
      "(Iteration 20001 / 61200) loss: 2.303609\n",
      "(Iteration 20101 / 61200) loss: 2.303500\n",
      "(Iteration 20201 / 61200) loss: 2.303413\n",
      "(Iteration 20301 / 61200) loss: 2.303560\n",
      "(Iteration 20401 / 61200) loss: 2.303469\n",
      "(Iteration 20501 / 61200) loss: 2.303599\n",
      "(Iteration 20601 / 61200) loss: 2.303452\n",
      "(Epoch 27 / 80) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 20701 / 61200) loss: 2.303497\n",
      "(Iteration 20801 / 61200) loss: 2.303552\n",
      "(Iteration 20901 / 61200) loss: 2.303497\n",
      "(Iteration 21001 / 61200) loss: 2.303478\n",
      "(Iteration 21101 / 61200) loss: 2.303474\n",
      "(Iteration 21201 / 61200) loss: 2.303570\n",
      "(Iteration 21301 / 61200) loss: 2.303550\n",
      "(Iteration 21401 / 61200) loss: 2.303586\n",
      "(Epoch 28 / 80) train acc: 0.082000; val_acc: 0.079000\n",
      "(Iteration 21501 / 61200) loss: 2.303514\n",
      "(Iteration 21601 / 61200) loss: 2.303488\n",
      "(Iteration 21701 / 61200) loss: 2.303557\n",
      "(Iteration 21801 / 61200) loss: 2.303543\n",
      "(Iteration 21901 / 61200) loss: 2.303468\n",
      "(Iteration 22001 / 61200) loss: 2.303561\n",
      "(Iteration 22101 / 61200) loss: 2.303558\n",
      "(Epoch 29 / 80) train acc: 0.092000; val_acc: 0.079000\n",
      "(Iteration 22201 / 61200) loss: 2.303495\n",
      "(Iteration 22301 / 61200) loss: 2.303538\n",
      "(Iteration 22401 / 61200) loss: 2.303555\n",
      "(Iteration 22501 / 61200) loss: 2.303555\n",
      "(Iteration 22601 / 61200) loss: 2.303670\n",
      "(Iteration 22701 / 61200) loss: 2.303544\n",
      "(Iteration 22801 / 61200) loss: 2.303552\n",
      "(Iteration 22901 / 61200) loss: 2.303616\n",
      "(Epoch 30 / 80) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 23001 / 61200) loss: 2.303546\n",
      "(Iteration 23101 / 61200) loss: 2.303552\n",
      "(Iteration 23201 / 61200) loss: 2.303573\n",
      "(Iteration 23301 / 61200) loss: 2.303470\n",
      "(Iteration 23401 / 61200) loss: 2.303548\n",
      "(Iteration 23501 / 61200) loss: 2.303604\n",
      "(Iteration 23601 / 61200) loss: 2.303557\n",
      "(Iteration 23701 / 61200) loss: 2.303642\n",
      "(Epoch 31 / 80) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 23801 / 61200) loss: 2.303485\n",
      "(Iteration 23901 / 61200) loss: 2.303571\n",
      "(Iteration 24001 / 61200) loss: 2.303502\n",
      "(Iteration 24101 / 61200) loss: 2.303582\n",
      "(Iteration 24201 / 61200) loss: 2.303523\n",
      "(Iteration 24301 / 61200) loss: 2.303541\n",
      "(Iteration 24401 / 61200) loss: 2.303598\n",
      "(Epoch 32 / 80) train acc: 0.091000; val_acc: 0.079000\n",
      "(Iteration 24501 / 61200) loss: 2.303537\n",
      "(Iteration 24601 / 61200) loss: 2.303502\n",
      "(Iteration 24701 / 61200) loss: 2.303537\n",
      "(Iteration 24801 / 61200) loss: 2.303579\n",
      "(Iteration 24901 / 61200) loss: 2.303556\n",
      "(Iteration 25001 / 61200) loss: 2.303533\n",
      "(Iteration 25101 / 61200) loss: 2.303518\n",
      "(Iteration 25201 / 61200) loss: 2.303563\n",
      "(Epoch 33 / 80) train acc: 0.120000; val_acc: 0.079000\n",
      "(Iteration 25301 / 61200) loss: 2.303512\n",
      "(Iteration 25401 / 61200) loss: 2.303605\n",
      "(Iteration 25501 / 61200) loss: 2.303530\n",
      "(Iteration 25601 / 61200) loss: 2.303566\n",
      "(Iteration 25701 / 61200) loss: 2.303536\n",
      "(Iteration 25801 / 61200) loss: 2.303482\n",
      "(Iteration 25901 / 61200) loss: 2.303496\n",
      "(Iteration 26001 / 61200) loss: 2.303552\n",
      "(Epoch 34 / 80) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 26101 / 61200) loss: 2.303498\n",
      "(Iteration 26201 / 61200) loss: 2.303487\n",
      "(Iteration 26301 / 61200) loss: 2.303641\n",
      "(Iteration 26401 / 61200) loss: 2.303574\n",
      "(Iteration 26501 / 61200) loss: 2.303512\n",
      "(Iteration 26601 / 61200) loss: 2.303565\n",
      "(Iteration 26701 / 61200) loss: 2.303547\n",
      "(Epoch 35 / 80) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 26801 / 61200) loss: 2.303451\n",
      "(Iteration 26901 / 61200) loss: 2.303480\n",
      "(Iteration 27001 / 61200) loss: 2.303590\n",
      "(Iteration 27101 / 61200) loss: 2.303558\n",
      "(Iteration 27201 / 61200) loss: 2.303549\n",
      "(Iteration 27301 / 61200) loss: 2.303533\n",
      "(Iteration 27401 / 61200) loss: 2.303577\n",
      "(Iteration 27501 / 61200) loss: 2.303571\n",
      "(Epoch 36 / 80) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 27601 / 61200) loss: 2.303506\n",
      "(Iteration 27701 / 61200) loss: 2.303594\n",
      "(Iteration 27801 / 61200) loss: 2.303510\n",
      "(Iteration 27901 / 61200) loss: 2.303486\n",
      "(Iteration 28001 / 61200) loss: 2.303527\n",
      "(Iteration 28101 / 61200) loss: 2.303515\n",
      "(Iteration 28201 / 61200) loss: 2.303501\n",
      "(Iteration 28301 / 61200) loss: 2.303509\n",
      "(Epoch 37 / 80) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 28401 / 61200) loss: 2.303540\n",
      "(Iteration 28501 / 61200) loss: 2.303419\n",
      "(Iteration 28601 / 61200) loss: 2.303624\n",
      "(Iteration 28701 / 61200) loss: 2.303483\n",
      "(Iteration 28801 / 61200) loss: 2.303496\n",
      "(Iteration 28901 / 61200) loss: 2.303560\n",
      "(Iteration 29001 / 61200) loss: 2.303497\n",
      "(Epoch 38 / 80) train acc: 0.112000; val_acc: 0.079000\n",
      "(Iteration 29101 / 61200) loss: 2.303567\n",
      "(Iteration 29201 / 61200) loss: 2.303427\n",
      "(Iteration 29301 / 61200) loss: 2.303457\n",
      "(Iteration 29401 / 61200) loss: 2.303490\n",
      "(Iteration 29501 / 61200) loss: 2.303510\n",
      "(Iteration 29601 / 61200) loss: 2.303591\n",
      "(Iteration 29701 / 61200) loss: 2.303472\n",
      "(Iteration 29801 / 61200) loss: 2.303597\n",
      "(Epoch 39 / 80) train acc: 0.088000; val_acc: 0.079000\n",
      "(Iteration 29901 / 61200) loss: 2.303557\n",
      "(Iteration 30001 / 61200) loss: 2.303561\n",
      "(Iteration 30101 / 61200) loss: 2.303515\n",
      "(Iteration 30201 / 61200) loss: 2.303572\n",
      "(Iteration 30301 / 61200) loss: 2.303486\n",
      "(Iteration 30401 / 61200) loss: 2.303595\n",
      "(Iteration 30501 / 61200) loss: 2.303454\n",
      "(Epoch 40 / 80) train acc: 0.110000; val_acc: 0.079000\n",
      "(Iteration 30601 / 61200) loss: 2.303472\n",
      "(Iteration 30701 / 61200) loss: 2.303623\n",
      "(Iteration 30801 / 61200) loss: 2.303520\n",
      "(Iteration 30901 / 61200) loss: 2.303492\n",
      "(Iteration 31001 / 61200) loss: 2.303557\n",
      "(Iteration 31101 / 61200) loss: 2.303562\n",
      "(Iteration 31201 / 61200) loss: 2.303549\n",
      "(Iteration 31301 / 61200) loss: 2.303549\n",
      "(Epoch 41 / 80) train acc: 0.092000; val_acc: 0.079000\n",
      "(Iteration 31401 / 61200) loss: 2.303572\n",
      "(Iteration 31501 / 61200) loss: 2.303513\n",
      "(Iteration 31601 / 61200) loss: 2.303488\n",
      "(Iteration 31701 / 61200) loss: 2.303480\n",
      "(Iteration 31801 / 61200) loss: 2.303495\n",
      "(Iteration 31901 / 61200) loss: 2.303613\n",
      "(Iteration 32001 / 61200) loss: 2.303547\n",
      "(Iteration 32101 / 61200) loss: 2.303509\n",
      "(Epoch 42 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 32201 / 61200) loss: 2.303568\n",
      "(Iteration 32301 / 61200) loss: 2.303571\n",
      "(Iteration 32401 / 61200) loss: 2.303572\n",
      "(Iteration 32501 / 61200) loss: 2.303556\n",
      "(Iteration 32601 / 61200) loss: 2.303582\n",
      "(Iteration 32701 / 61200) loss: 2.303578\n",
      "(Iteration 32801 / 61200) loss: 2.303495\n",
      "(Epoch 43 / 80) train acc: 0.088000; val_acc: 0.079000\n",
      "(Iteration 32901 / 61200) loss: 2.303503\n",
      "(Iteration 33001 / 61200) loss: 2.303547\n",
      "(Iteration 33101 / 61200) loss: 2.303537\n",
      "(Iteration 33201 / 61200) loss: 2.303514\n",
      "(Iteration 33301 / 61200) loss: 2.303542\n",
      "(Iteration 33401 / 61200) loss: 2.303503\n",
      "(Iteration 33501 / 61200) loss: 2.303515\n",
      "(Iteration 33601 / 61200) loss: 2.303516\n",
      "(Epoch 44 / 80) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 33701 / 61200) loss: 2.303493\n",
      "(Iteration 33801 / 61200) loss: 2.303543\n",
      "(Iteration 33901 / 61200) loss: 2.303582\n",
      "(Iteration 34001 / 61200) loss: 2.303478\n",
      "(Iteration 34101 / 61200) loss: 2.303489\n",
      "(Iteration 34201 / 61200) loss: 2.303509\n",
      "(Iteration 34301 / 61200) loss: 2.303499\n",
      "(Iteration 34401 / 61200) loss: 2.303522\n",
      "(Epoch 45 / 80) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 34501 / 61200) loss: 2.303512\n",
      "(Iteration 34601 / 61200) loss: 2.303536\n",
      "(Iteration 34701 / 61200) loss: 2.303563\n",
      "(Iteration 34801 / 61200) loss: 2.303438\n",
      "(Iteration 34901 / 61200) loss: 2.303431\n",
      "(Iteration 35001 / 61200) loss: 2.303537\n",
      "(Iteration 35101 / 61200) loss: 2.303638\n",
      "(Epoch 46 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 35201 / 61200) loss: 2.303501\n",
      "(Iteration 35301 / 61200) loss: 2.303563\n",
      "(Iteration 35401 / 61200) loss: 2.303483\n",
      "(Iteration 35501 / 61200) loss: 2.303538\n",
      "(Iteration 35601 / 61200) loss: 2.303479\n",
      "(Iteration 35701 / 61200) loss: 2.303514\n",
      "(Iteration 35801 / 61200) loss: 2.303500\n",
      "(Iteration 35901 / 61200) loss: 2.303484\n",
      "(Epoch 47 / 80) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 36001 / 61200) loss: 2.303481\n",
      "(Iteration 36101 / 61200) loss: 2.303434\n",
      "(Iteration 36201 / 61200) loss: 2.303432\n",
      "(Iteration 36301 / 61200) loss: 2.303516\n",
      "(Iteration 36401 / 61200) loss: 2.303576\n",
      "(Iteration 36501 / 61200) loss: 2.303552\n",
      "(Iteration 36601 / 61200) loss: 2.303492\n",
      "(Iteration 36701 / 61200) loss: 2.303496\n",
      "(Epoch 48 / 80) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 36801 / 61200) loss: 2.303513\n",
      "(Iteration 36901 / 61200) loss: 2.303528\n",
      "(Iteration 37001 / 61200) loss: 2.303535\n",
      "(Iteration 37101 / 61200) loss: 2.303366\n",
      "(Iteration 37201 / 61200) loss: 2.303546\n",
      "(Iteration 37301 / 61200) loss: 2.303514\n",
      "(Iteration 37401 / 61200) loss: 2.303511\n",
      "(Epoch 49 / 80) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 37501 / 61200) loss: 2.303459\n",
      "(Iteration 37601 / 61200) loss: 2.303502\n",
      "(Iteration 37701 / 61200) loss: 2.303554\n",
      "(Iteration 37801 / 61200) loss: 2.303571\n",
      "(Iteration 37901 / 61200) loss: 2.303469\n",
      "(Iteration 38001 / 61200) loss: 2.303499\n",
      "(Iteration 38101 / 61200) loss: 2.303529\n",
      "(Iteration 38201 / 61200) loss: 2.303554\n",
      "(Epoch 50 / 80) train acc: 0.113000; val_acc: 0.079000\n",
      "(Iteration 38301 / 61200) loss: 2.303403\n",
      "(Iteration 38401 / 61200) loss: 2.303568\n",
      "(Iteration 38501 / 61200) loss: 2.303476\n",
      "(Iteration 38601 / 61200) loss: 2.303504\n",
      "(Iteration 38701 / 61200) loss: 2.303452\n",
      "(Iteration 38801 / 61200) loss: 2.303547\n",
      "(Iteration 38901 / 61200) loss: 2.303505\n",
      "(Iteration 39001 / 61200) loss: 2.303515\n",
      "(Epoch 51 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 39101 / 61200) loss: 2.303550\n",
      "(Iteration 39201 / 61200) loss: 2.303514\n",
      "(Iteration 39301 / 61200) loss: 2.303507\n",
      "(Iteration 39401 / 61200) loss: 2.303569\n",
      "(Iteration 39501 / 61200) loss: 2.303496\n",
      "(Iteration 39601 / 61200) loss: 2.303632\n",
      "(Iteration 39701 / 61200) loss: 2.303420\n",
      "(Epoch 52 / 80) train acc: 0.113000; val_acc: 0.079000\n",
      "(Iteration 39801 / 61200) loss: 2.303520\n",
      "(Iteration 39901 / 61200) loss: 2.303532\n",
      "(Iteration 40001 / 61200) loss: 2.303404\n",
      "(Iteration 40101 / 61200) loss: 2.303635\n",
      "(Iteration 40201 / 61200) loss: 2.303556\n",
      "(Iteration 40301 / 61200) loss: 2.303571\n",
      "(Iteration 40401 / 61200) loss: 2.303552\n",
      "(Iteration 40501 / 61200) loss: 2.303495\n",
      "(Epoch 53 / 80) train acc: 0.110000; val_acc: 0.079000\n",
      "(Iteration 40601 / 61200) loss: 2.303541\n",
      "(Iteration 40701 / 61200) loss: 2.303523\n",
      "(Iteration 40801 / 61200) loss: 2.303549\n",
      "(Iteration 40901 / 61200) loss: 2.303489\n",
      "(Iteration 41001 / 61200) loss: 2.303490\n",
      "(Iteration 41101 / 61200) loss: 2.303452\n",
      "(Iteration 41201 / 61200) loss: 2.303522\n",
      "(Iteration 41301 / 61200) loss: 2.303552\n",
      "(Epoch 54 / 80) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 41401 / 61200) loss: 2.303452\n",
      "(Iteration 41501 / 61200) loss: 2.303509\n",
      "(Iteration 41601 / 61200) loss: 2.303444\n",
      "(Iteration 41701 / 61200) loss: 2.303572\n",
      "(Iteration 41801 / 61200) loss: 2.303507\n",
      "(Iteration 41901 / 61200) loss: 2.303482\n",
      "(Iteration 42001 / 61200) loss: 2.303455\n",
      "(Epoch 55 / 80) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 42101 / 61200) loss: 2.303439\n",
      "(Iteration 42201 / 61200) loss: 2.303588\n",
      "(Iteration 42301 / 61200) loss: 2.303471\n",
      "(Iteration 42401 / 61200) loss: 2.303520\n",
      "(Iteration 42501 / 61200) loss: 2.303560\n",
      "(Iteration 42601 / 61200) loss: 2.303446\n",
      "(Iteration 42701 / 61200) loss: 2.303499\n",
      "(Iteration 42801 / 61200) loss: 2.303484\n",
      "(Epoch 56 / 80) train acc: 0.093000; val_acc: 0.079000\n",
      "(Iteration 42901 / 61200) loss: 2.303467\n",
      "(Iteration 43001 / 61200) loss: 2.303441\n",
      "(Iteration 43101 / 61200) loss: 2.303462\n",
      "(Iteration 43201 / 61200) loss: 2.303496\n",
      "(Iteration 43301 / 61200) loss: 2.303518\n",
      "(Iteration 43401 / 61200) loss: 2.303504\n",
      "(Iteration 43501 / 61200) loss: 2.303537\n",
      "(Iteration 43601 / 61200) loss: 2.303570\n",
      "(Epoch 57 / 80) train acc: 0.089000; val_acc: 0.079000\n",
      "(Iteration 43701 / 61200) loss: 2.303559\n",
      "(Iteration 43801 / 61200) loss: 2.303536\n",
      "(Iteration 43901 / 61200) loss: 2.303413\n",
      "(Iteration 44001 / 61200) loss: 2.303555\n",
      "(Iteration 44101 / 61200) loss: 2.303547\n",
      "(Iteration 44201 / 61200) loss: 2.303472\n",
      "(Iteration 44301 / 61200) loss: 2.303421\n",
      "(Epoch 58 / 80) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 44401 / 61200) loss: 2.303554\n",
      "(Iteration 44501 / 61200) loss: 2.303618\n",
      "(Iteration 44601 / 61200) loss: 2.303632\n",
      "(Iteration 44701 / 61200) loss: 2.303417\n",
      "(Iteration 44801 / 61200) loss: 2.303574\n",
      "(Iteration 44901 / 61200) loss: 2.303457\n",
      "(Iteration 45001 / 61200) loss: 2.303516\n",
      "(Iteration 45101 / 61200) loss: 2.303539\n",
      "(Epoch 59 / 80) train acc: 0.115000; val_acc: 0.079000\n",
      "(Iteration 45201 / 61200) loss: 2.303597\n",
      "(Iteration 45301 / 61200) loss: 2.303537\n",
      "(Iteration 45401 / 61200) loss: 2.303529\n",
      "(Iteration 45501 / 61200) loss: 2.303530\n",
      "(Iteration 45601 / 61200) loss: 2.303492\n",
      "(Iteration 45701 / 61200) loss: 2.303508\n",
      "(Iteration 45801 / 61200) loss: 2.303534\n",
      "(Epoch 60 / 80) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 45901 / 61200) loss: 2.303489\n",
      "(Iteration 46001 / 61200) loss: 2.303482\n",
      "(Iteration 46101 / 61200) loss: 2.303506\n",
      "(Iteration 46201 / 61200) loss: 2.303503\n",
      "(Iteration 46301 / 61200) loss: 2.303600\n",
      "(Iteration 46401 / 61200) loss: 2.303543\n",
      "(Iteration 46501 / 61200) loss: 2.303600\n",
      "(Iteration 46601 / 61200) loss: 2.303522\n",
      "(Epoch 61 / 80) train acc: 0.117000; val_acc: 0.079000\n",
      "(Iteration 46701 / 61200) loss: 2.303522\n",
      "(Iteration 46801 / 61200) loss: 2.303560\n",
      "(Iteration 46901 / 61200) loss: 2.303460\n",
      "(Iteration 47001 / 61200) loss: 2.303462\n",
      "(Iteration 47101 / 61200) loss: 2.303543\n",
      "(Iteration 47201 / 61200) loss: 2.303559\n",
      "(Iteration 47301 / 61200) loss: 2.303514\n",
      "(Iteration 47401 / 61200) loss: 2.303480\n",
      "(Epoch 62 / 80) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 47501 / 61200) loss: 2.303479\n",
      "(Iteration 47601 / 61200) loss: 2.303553\n",
      "(Iteration 47701 / 61200) loss: 2.303568\n",
      "(Iteration 47801 / 61200) loss: 2.303543\n",
      "(Iteration 47901 / 61200) loss: 2.303551\n",
      "(Iteration 48001 / 61200) loss: 2.303504\n",
      "(Iteration 48101 / 61200) loss: 2.303538\n",
      "(Epoch 63 / 80) train acc: 0.090000; val_acc: 0.079000\n",
      "(Iteration 48201 / 61200) loss: 2.303503\n",
      "(Iteration 48301 / 61200) loss: 2.303548\n",
      "(Iteration 48401 / 61200) loss: 2.303494\n",
      "(Iteration 48501 / 61200) loss: 2.303565\n",
      "(Iteration 48601 / 61200) loss: 2.303455\n",
      "(Iteration 48701 / 61200) loss: 2.303562\n",
      "(Iteration 48801 / 61200) loss: 2.303469\n",
      "(Iteration 48901 / 61200) loss: 2.303555\n",
      "(Epoch 64 / 80) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 49001 / 61200) loss: 2.303504\n",
      "(Iteration 49101 / 61200) loss: 2.303512\n",
      "(Iteration 49201 / 61200) loss: 2.303549\n",
      "(Iteration 49301 / 61200) loss: 2.303562\n",
      "(Iteration 49401 / 61200) loss: 2.303594\n",
      "(Iteration 49501 / 61200) loss: 2.303487\n",
      "(Iteration 49601 / 61200) loss: 2.303502\n",
      "(Iteration 49701 / 61200) loss: 2.303505\n",
      "(Epoch 65 / 80) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 49801 / 61200) loss: 2.303568\n",
      "(Iteration 49901 / 61200) loss: 2.303613\n",
      "(Iteration 50001 / 61200) loss: 2.303538\n",
      "(Iteration 50101 / 61200) loss: 2.303542\n",
      "(Iteration 50201 / 61200) loss: 2.303518\n",
      "(Iteration 50301 / 61200) loss: 2.303478\n",
      "(Iteration 50401 / 61200) loss: 2.303441\n",
      "(Epoch 66 / 80) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 50501 / 61200) loss: 2.303506\n",
      "(Iteration 50601 / 61200) loss: 2.303540\n",
      "(Iteration 50701 / 61200) loss: 2.303529\n",
      "(Iteration 50801 / 61200) loss: 2.303560\n",
      "(Iteration 50901 / 61200) loss: 2.303442\n",
      "(Iteration 51001 / 61200) loss: 2.303480\n",
      "(Iteration 51101 / 61200) loss: 2.303573\n",
      "(Iteration 51201 / 61200) loss: 2.303423\n",
      "(Epoch 67 / 80) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 51301 / 61200) loss: 2.303528\n",
      "(Iteration 51401 / 61200) loss: 2.303403\n",
      "(Iteration 51501 / 61200) loss: 2.303474\n",
      "(Iteration 51601 / 61200) loss: 2.303449\n",
      "(Iteration 51701 / 61200) loss: 2.303562\n",
      "(Iteration 51801 / 61200) loss: 2.303489\n",
      "(Iteration 51901 / 61200) loss: 2.303528\n",
      "(Iteration 52001 / 61200) loss: 2.303492\n",
      "(Epoch 68 / 80) train acc: 0.110000; val_acc: 0.079000\n",
      "(Iteration 52101 / 61200) loss: 2.303514\n",
      "(Iteration 52201 / 61200) loss: 2.303502\n",
      "(Iteration 52301 / 61200) loss: 2.303397\n",
      "(Iteration 52401 / 61200) loss: 2.303474\n",
      "(Iteration 52501 / 61200) loss: 2.303548\n",
      "(Iteration 52601 / 61200) loss: 2.303486\n",
      "(Iteration 52701 / 61200) loss: 2.303559\n",
      "(Epoch 69 / 80) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 52801 / 61200) loss: 2.303480\n",
      "(Iteration 52901 / 61200) loss: 2.303576\n",
      "(Iteration 53001 / 61200) loss: 2.303498\n",
      "(Iteration 53101 / 61200) loss: 2.303529\n",
      "(Iteration 53201 / 61200) loss: 2.303462\n",
      "(Iteration 53301 / 61200) loss: 2.303563\n",
      "(Iteration 53401 / 61200) loss: 2.303538\n",
      "(Iteration 53501 / 61200) loss: 2.303496\n",
      "(Epoch 70 / 80) train acc: 0.109000; val_acc: 0.079000\n",
      "(Iteration 53601 / 61200) loss: 2.303538\n",
      "(Iteration 53701 / 61200) loss: 2.303467\n",
      "(Iteration 53801 / 61200) loss: 2.303471\n",
      "(Iteration 53901 / 61200) loss: 2.303600\n",
      "(Iteration 54001 / 61200) loss: 2.303467\n",
      "(Iteration 54101 / 61200) loss: 2.303586\n",
      "(Iteration 54201 / 61200) loss: 2.303476\n",
      "(Iteration 54301 / 61200) loss: 2.303482\n",
      "(Epoch 71 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 54401 / 61200) loss: 2.303534\n",
      "(Iteration 54501 / 61200) loss: 2.303545\n",
      "(Iteration 54601 / 61200) loss: 2.303522\n",
      "(Iteration 54701 / 61200) loss: 2.303385\n",
      "(Iteration 54801 / 61200) loss: 2.303574\n",
      "(Iteration 54901 / 61200) loss: 2.303445\n",
      "(Iteration 55001 / 61200) loss: 2.303476\n",
      "(Epoch 72 / 80) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 55101 / 61200) loss: 2.303579\n",
      "(Iteration 55201 / 61200) loss: 2.303469\n",
      "(Iteration 55301 / 61200) loss: 2.303554\n",
      "(Iteration 55401 / 61200) loss: 2.303601\n",
      "(Iteration 55501 / 61200) loss: 2.303490\n",
      "(Iteration 55601 / 61200) loss: 2.303573\n",
      "(Iteration 55701 / 61200) loss: 2.303445\n",
      "(Iteration 55801 / 61200) loss: 2.303568\n",
      "(Epoch 73 / 80) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 55901 / 61200) loss: 2.303513\n",
      "(Iteration 56001 / 61200) loss: 2.303503\n",
      "(Iteration 56101 / 61200) loss: 2.303489\n",
      "(Iteration 56201 / 61200) loss: 2.303503\n",
      "(Iteration 56301 / 61200) loss: 2.303582\n",
      "(Iteration 56401 / 61200) loss: 2.303506\n",
      "(Iteration 56501 / 61200) loss: 2.303438\n",
      "(Iteration 56601 / 61200) loss: 2.303421\n",
      "(Epoch 74 / 80) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 56701 / 61200) loss: 2.303504\n",
      "(Iteration 56801 / 61200) loss: 2.303449\n",
      "(Iteration 56901 / 61200) loss: 2.303484\n",
      "(Iteration 57001 / 61200) loss: 2.303465\n",
      "(Iteration 57101 / 61200) loss: 2.303521\n",
      "(Iteration 57201 / 61200) loss: 2.303538\n",
      "(Iteration 57301 / 61200) loss: 2.303529\n",
      "(Epoch 75 / 80) train acc: 0.116000; val_acc: 0.079000\n",
      "(Iteration 57401 / 61200) loss: 2.303516\n",
      "(Iteration 57501 / 61200) loss: 2.303508\n",
      "(Iteration 57601 / 61200) loss: 2.303417\n",
      "(Iteration 57701 / 61200) loss: 2.303485\n",
      "(Iteration 57801 / 61200) loss: 2.303524\n",
      "(Iteration 57901 / 61200) loss: 2.303473\n",
      "(Iteration 58001 / 61200) loss: 2.303552\n",
      "(Iteration 58101 / 61200) loss: 2.303479\n",
      "(Epoch 76 / 80) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 58201 / 61200) loss: 2.303496\n",
      "(Iteration 58301 / 61200) loss: 2.303456\n",
      "(Iteration 58401 / 61200) loss: 2.303547\n",
      "(Iteration 58501 / 61200) loss: 2.303522\n",
      "(Iteration 58601 / 61200) loss: 2.303485\n",
      "(Iteration 58701 / 61200) loss: 2.303508\n",
      "(Iteration 58801 / 61200) loss: 2.303505\n",
      "(Iteration 58901 / 61200) loss: 2.303507\n",
      "(Epoch 77 / 80) train acc: 0.084000; val_acc: 0.079000\n",
      "(Iteration 59001 / 61200) loss: 2.303510\n",
      "(Iteration 59101 / 61200) loss: 2.303575\n",
      "(Iteration 59201 / 61200) loss: 2.303562\n",
      "(Iteration 59301 / 61200) loss: 2.303571\n",
      "(Iteration 59401 / 61200) loss: 2.303492\n",
      "(Iteration 59501 / 61200) loss: 2.303541\n",
      "(Iteration 59601 / 61200) loss: 2.303514\n",
      "(Epoch 78 / 80) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 59701 / 61200) loss: 2.303522\n",
      "(Iteration 59801 / 61200) loss: 2.303491\n",
      "(Iteration 59901 / 61200) loss: 2.303551\n",
      "(Iteration 60001 / 61200) loss: 2.303527\n",
      "(Iteration 60101 / 61200) loss: 2.303542\n",
      "(Iteration 60201 / 61200) loss: 2.303492\n",
      "(Iteration 60301 / 61200) loss: 2.303456\n",
      "(Iteration 60401 / 61200) loss: 2.303524\n",
      "(Epoch 79 / 80) train acc: 0.091000; val_acc: 0.079000\n",
      "(Iteration 60501 / 61200) loss: 2.303486\n",
      "(Iteration 60601 / 61200) loss: 2.303488\n",
      "(Iteration 60701 / 61200) loss: 2.303470\n",
      "(Iteration 60801 / 61200) loss: 2.303494\n",
      "(Iteration 60901 / 61200) loss: 2.303612\n",
      "(Iteration 61001 / 61200) loss: 2.303532\n",
      "(Iteration 61101 / 61200) loss: 2.303455\n",
      "(Epoch 80 / 80) train acc: 0.116000; val_acc: 0.079000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.304651\n",
      "(Epoch 0 / 80) train acc: 0.095000; val_acc: 0.115000\n",
      "(Iteration 101 / 30560) loss: 2.304618\n",
      "(Iteration 201 / 30560) loss: 2.304598\n",
      "(Iteration 301 / 30560) loss: 2.304570\n",
      "(Epoch 1 / 80) train acc: 0.146000; val_acc: 0.118000\n",
      "(Iteration 401 / 30560) loss: 2.304557\n",
      "(Iteration 501 / 30560) loss: 2.304550\n",
      "(Iteration 601 / 30560) loss: 2.304531\n",
      "(Iteration 701 / 30560) loss: 2.304522\n",
      "(Epoch 2 / 80) train acc: 0.133000; val_acc: 0.127000\n",
      "(Iteration 801 / 30560) loss: 2.304487\n",
      "(Iteration 901 / 30560) loss: 2.304488\n",
      "(Iteration 1001 / 30560) loss: 2.304450\n",
      "(Iteration 1101 / 30560) loss: 2.304457\n",
      "(Epoch 3 / 80) train acc: 0.142000; val_acc: 0.113000\n",
      "(Iteration 1201 / 30560) loss: 2.304446\n",
      "(Iteration 1301 / 30560) loss: 2.304423\n",
      "(Iteration 1401 / 30560) loss: 2.304405\n",
      "(Iteration 1501 / 30560) loss: 2.304402\n",
      "(Epoch 4 / 80) train acc: 0.110000; val_acc: 0.123000\n",
      "(Iteration 1601 / 30560) loss: 2.304386\n",
      "(Iteration 1701 / 30560) loss: 2.304383\n",
      "(Iteration 1801 / 30560) loss: 2.304387\n",
      "(Iteration 1901 / 30560) loss: 2.304344\n",
      "(Epoch 5 / 80) train acc: 0.116000; val_acc: 0.120000\n",
      "(Iteration 2001 / 30560) loss: 2.304321\n",
      "(Iteration 2101 / 30560) loss: 2.304321\n",
      "(Iteration 2201 / 30560) loss: 2.304315\n",
      "(Epoch 6 / 80) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 2301 / 30560) loss: 2.304299\n",
      "(Iteration 2401 / 30560) loss: 2.304277\n",
      "(Iteration 2501 / 30560) loss: 2.304281\n",
      "(Iteration 2601 / 30560) loss: 2.304269\n",
      "(Epoch 7 / 80) train acc: 0.080000; val_acc: 0.110000\n",
      "(Iteration 2701 / 30560) loss: 2.304259\n",
      "(Iteration 2801 / 30560) loss: 2.304270\n",
      "(Iteration 2901 / 30560) loss: 2.304249\n",
      "(Iteration 3001 / 30560) loss: 2.304237\n",
      "(Epoch 8 / 80) train acc: 0.105000; val_acc: 0.092000\n",
      "(Iteration 3101 / 30560) loss: 2.304231\n",
      "(Iteration 3201 / 30560) loss: 2.304232\n",
      "(Iteration 3301 / 30560) loss: 2.304225\n",
      "(Iteration 3401 / 30560) loss: 2.304203\n",
      "(Epoch 9 / 80) train acc: 0.084000; val_acc: 0.094000\n",
      "(Iteration 3501 / 30560) loss: 2.304206\n",
      "(Iteration 3601 / 30560) loss: 2.304207\n",
      "(Iteration 3701 / 30560) loss: 2.304195\n",
      "(Iteration 3801 / 30560) loss: 2.304204\n",
      "(Epoch 10 / 80) train acc: 0.112000; val_acc: 0.105000\n",
      "(Iteration 3901 / 30560) loss: 2.304182\n",
      "(Iteration 4001 / 30560) loss: 2.304178\n",
      "(Iteration 4101 / 30560) loss: 2.304170\n",
      "(Iteration 4201 / 30560) loss: 2.304161\n",
      "(Epoch 11 / 80) train acc: 0.108000; val_acc: 0.101000\n",
      "(Iteration 4301 / 30560) loss: 2.304161\n",
      "(Iteration 4401 / 30560) loss: 2.304144\n",
      "(Iteration 4501 / 30560) loss: 2.304162\n",
      "(Epoch 12 / 80) train acc: 0.103000; val_acc: 0.106000\n",
      "(Iteration 4601 / 30560) loss: 2.304136\n",
      "(Iteration 4701 / 30560) loss: 2.304148\n",
      "(Iteration 4801 / 30560) loss: 2.304140\n",
      "(Iteration 4901 / 30560) loss: 2.304130\n",
      "(Epoch 13 / 80) train acc: 0.116000; val_acc: 0.106000\n",
      "(Iteration 5001 / 30560) loss: 2.304139\n",
      "(Iteration 5101 / 30560) loss: 2.304130\n",
      "(Iteration 5201 / 30560) loss: 2.304124\n",
      "(Iteration 5301 / 30560) loss: 2.304111\n",
      "(Epoch 14 / 80) train acc: 0.112000; val_acc: 0.109000\n",
      "(Iteration 5401 / 30560) loss: 2.304110\n",
      "(Iteration 5501 / 30560) loss: 2.304117\n",
      "(Iteration 5601 / 30560) loss: 2.304106\n",
      "(Iteration 5701 / 30560) loss: 2.304097\n",
      "(Epoch 15 / 80) train acc: 0.119000; val_acc: 0.109000\n",
      "(Iteration 5801 / 30560) loss: 2.304088\n",
      "(Iteration 5901 / 30560) loss: 2.304093\n",
      "(Iteration 6001 / 30560) loss: 2.304094\n",
      "(Iteration 6101 / 30560) loss: 2.304093\n",
      "(Epoch 16 / 80) train acc: 0.121000; val_acc: 0.109000\n",
      "(Iteration 6201 / 30560) loss: 2.304099\n",
      "(Iteration 6301 / 30560) loss: 2.304087\n",
      "(Iteration 6401 / 30560) loss: 2.304108\n",
      "(Epoch 17 / 80) train acc: 0.111000; val_acc: 0.108000\n",
      "(Iteration 6501 / 30560) loss: 2.304097\n",
      "(Iteration 6601 / 30560) loss: 2.304064\n",
      "(Iteration 6701 / 30560) loss: 2.304069\n",
      "(Iteration 6801 / 30560) loss: 2.304093\n",
      "(Epoch 18 / 80) train acc: 0.120000; val_acc: 0.113000\n",
      "(Iteration 6901 / 30560) loss: 2.304072\n",
      "(Iteration 7001 / 30560) loss: 2.304068\n",
      "(Iteration 7101 / 30560) loss: 2.304056\n",
      "(Iteration 7201 / 30560) loss: 2.304059\n",
      "(Epoch 19 / 80) train acc: 0.110000; val_acc: 0.116000\n",
      "(Iteration 7301 / 30560) loss: 2.304034\n",
      "(Iteration 7401 / 30560) loss: 2.304063\n",
      "(Iteration 7501 / 30560) loss: 2.304067\n",
      "(Iteration 7601 / 30560) loss: 2.304061\n",
      "(Epoch 20 / 80) train acc: 0.108000; val_acc: 0.113000\n",
      "(Iteration 7701 / 30560) loss: 2.304048\n",
      "(Iteration 7801 / 30560) loss: 2.304046\n",
      "(Iteration 7901 / 30560) loss: 2.304036\n",
      "(Iteration 8001 / 30560) loss: 2.304044\n",
      "(Epoch 21 / 80) train acc: 0.117000; val_acc: 0.107000\n",
      "(Iteration 8101 / 30560) loss: 2.304029\n",
      "(Iteration 8201 / 30560) loss: 2.304052\n",
      "(Iteration 8301 / 30560) loss: 2.304022\n",
      "(Iteration 8401 / 30560) loss: 2.304038\n",
      "(Epoch 22 / 80) train acc: 0.117000; val_acc: 0.110000\n",
      "(Iteration 8501 / 30560) loss: 2.304031\n",
      "(Iteration 8601 / 30560) loss: 2.304042\n",
      "(Iteration 8701 / 30560) loss: 2.304055\n",
      "(Epoch 23 / 80) train acc: 0.108000; val_acc: 0.116000\n",
      "(Iteration 8801 / 30560) loss: 2.304037\n",
      "(Iteration 8901 / 30560) loss: 2.304053\n",
      "(Iteration 9001 / 30560) loss: 2.304051\n",
      "(Iteration 9101 / 30560) loss: 2.304028\n",
      "(Epoch 24 / 80) train acc: 0.131000; val_acc: 0.110000\n",
      "(Iteration 9201 / 30560) loss: 2.304027\n",
      "(Iteration 9301 / 30560) loss: 2.304027\n",
      "(Iteration 9401 / 30560) loss: 2.304018\n",
      "(Iteration 9501 / 30560) loss: 2.304031\n",
      "(Epoch 25 / 80) train acc: 0.118000; val_acc: 0.110000\n",
      "(Iteration 9601 / 30560) loss: 2.304017\n",
      "(Iteration 9701 / 30560) loss: 2.304010\n",
      "(Iteration 9801 / 30560) loss: 2.304019\n",
      "(Iteration 9901 / 30560) loss: 2.304021\n",
      "(Epoch 26 / 80) train acc: 0.126000; val_acc: 0.109000\n",
      "(Iteration 10001 / 30560) loss: 2.304013\n",
      "(Iteration 10101 / 30560) loss: 2.304019\n",
      "(Iteration 10201 / 30560) loss: 2.304021\n",
      "(Iteration 10301 / 30560) loss: 2.304029\n",
      "(Epoch 27 / 80) train acc: 0.112000; val_acc: 0.102000\n",
      "(Iteration 10401 / 30560) loss: 2.304027\n",
      "(Iteration 10501 / 30560) loss: 2.303996\n",
      "(Iteration 10601 / 30560) loss: 2.304028\n",
      "(Epoch 28 / 80) train acc: 0.118000; val_acc: 0.101000\n",
      "(Iteration 10701 / 30560) loss: 2.304009\n",
      "(Iteration 10801 / 30560) loss: 2.304022\n",
      "(Iteration 10901 / 30560) loss: 2.304004\n",
      "(Iteration 11001 / 30560) loss: 2.304019\n",
      "(Epoch 29 / 80) train acc: 0.120000; val_acc: 0.102000\n",
      "(Iteration 11101 / 30560) loss: 2.304011\n",
      "(Iteration 11201 / 30560) loss: 2.304025\n",
      "(Iteration 11301 / 30560) loss: 2.304002\n",
      "(Iteration 11401 / 30560) loss: 2.304015\n",
      "(Epoch 30 / 80) train acc: 0.117000; val_acc: 0.104000\n",
      "(Iteration 11501 / 30560) loss: 2.304024\n",
      "(Iteration 11601 / 30560) loss: 2.304014\n",
      "(Iteration 11701 / 30560) loss: 2.304008\n",
      "(Iteration 11801 / 30560) loss: 2.304000\n",
      "(Epoch 31 / 80) train acc: 0.126000; val_acc: 0.103000\n",
      "(Iteration 11901 / 30560) loss: 2.304010\n",
      "(Iteration 12001 / 30560) loss: 2.304033\n",
      "(Iteration 12101 / 30560) loss: 2.303996\n",
      "(Iteration 12201 / 30560) loss: 2.304002\n",
      "(Epoch 32 / 80) train acc: 0.125000; val_acc: 0.107000\n",
      "(Iteration 12301 / 30560) loss: 2.304008\n",
      "(Iteration 12401 / 30560) loss: 2.304007\n",
      "(Iteration 12501 / 30560) loss: 2.303995\n",
      "(Iteration 12601 / 30560) loss: 2.304026\n",
      "(Epoch 33 / 80) train acc: 0.117000; val_acc: 0.111000\n",
      "(Iteration 12701 / 30560) loss: 2.303998\n",
      "(Iteration 12801 / 30560) loss: 2.303995\n",
      "(Iteration 12901 / 30560) loss: 2.303994\n",
      "(Epoch 34 / 80) train acc: 0.110000; val_acc: 0.108000\n",
      "(Iteration 13001 / 30560) loss: 2.303994\n",
      "(Iteration 13101 / 30560) loss: 2.303969\n",
      "(Iteration 13201 / 30560) loss: 2.303989\n",
      "(Iteration 13301 / 30560) loss: 2.304002\n",
      "(Epoch 35 / 80) train acc: 0.110000; val_acc: 0.110000\n",
      "(Iteration 13401 / 30560) loss: 2.304005\n",
      "(Iteration 13501 / 30560) loss: 2.303996\n",
      "(Iteration 13601 / 30560) loss: 2.303996\n",
      "(Iteration 13701 / 30560) loss: 2.303987\n",
      "(Epoch 36 / 80) train acc: 0.094000; val_acc: 0.110000\n",
      "(Iteration 13801 / 30560) loss: 2.303998\n",
      "(Iteration 13901 / 30560) loss: 2.303996\n",
      "(Iteration 14001 / 30560) loss: 2.304023\n",
      "(Iteration 14101 / 30560) loss: 2.304003\n",
      "(Epoch 37 / 80) train acc: 0.115000; val_acc: 0.109000\n",
      "(Iteration 14201 / 30560) loss: 2.304012\n",
      "(Iteration 14301 / 30560) loss: 2.303987\n",
      "(Iteration 14401 / 30560) loss: 2.303998\n",
      "(Iteration 14501 / 30560) loss: 2.303996\n",
      "(Epoch 38 / 80) train acc: 0.134000; val_acc: 0.106000\n",
      "(Iteration 14601 / 30560) loss: 2.304014\n",
      "(Iteration 14701 / 30560) loss: 2.304007\n",
      "(Iteration 14801 / 30560) loss: 2.303990\n",
      "(Epoch 39 / 80) train acc: 0.118000; val_acc: 0.107000\n",
      "(Iteration 14901 / 30560) loss: 2.303994\n",
      "(Iteration 15001 / 30560) loss: 2.303995\n",
      "(Iteration 15101 / 30560) loss: 2.303993\n",
      "(Iteration 15201 / 30560) loss: 2.303974\n",
      "(Epoch 40 / 80) train acc: 0.115000; val_acc: 0.106000\n",
      "(Iteration 15301 / 30560) loss: 2.303996\n",
      "(Iteration 15401 / 30560) loss: 2.304005\n",
      "(Iteration 15501 / 30560) loss: 2.303985\n",
      "(Iteration 15601 / 30560) loss: 2.303991\n",
      "(Epoch 41 / 80) train acc: 0.131000; val_acc: 0.106000\n",
      "(Iteration 15701 / 30560) loss: 2.303997\n",
      "(Iteration 15801 / 30560) loss: 2.303993\n",
      "(Iteration 15901 / 30560) loss: 2.304009\n",
      "(Iteration 16001 / 30560) loss: 2.303974\n",
      "(Epoch 42 / 80) train acc: 0.126000; val_acc: 0.111000\n",
      "(Iteration 16101 / 30560) loss: 2.303993\n",
      "(Iteration 16201 / 30560) loss: 2.303995\n",
      "(Iteration 16301 / 30560) loss: 2.304001\n",
      "(Iteration 16401 / 30560) loss: 2.303982\n",
      "(Epoch 43 / 80) train acc: 0.109000; val_acc: 0.111000\n",
      "(Iteration 16501 / 30560) loss: 2.303991\n",
      "(Iteration 16601 / 30560) loss: 2.303982\n",
      "(Iteration 16701 / 30560) loss: 2.303972\n",
      "(Iteration 16801 / 30560) loss: 2.303987\n",
      "(Epoch 44 / 80) train acc: 0.116000; val_acc: 0.109000\n",
      "(Iteration 16901 / 30560) loss: 2.303993\n",
      "(Iteration 17001 / 30560) loss: 2.303986\n",
      "(Iteration 17101 / 30560) loss: 2.303993\n",
      "(Epoch 45 / 80) train acc: 0.120000; val_acc: 0.108000\n",
      "(Iteration 17201 / 30560) loss: 2.303988\n",
      "(Iteration 17301 / 30560) loss: 2.303991\n",
      "(Iteration 17401 / 30560) loss: 2.303991\n",
      "(Iteration 17501 / 30560) loss: 2.304003\n",
      "(Epoch 46 / 80) train acc: 0.120000; val_acc: 0.107000\n",
      "(Iteration 17601 / 30560) loss: 2.303990\n",
      "(Iteration 17701 / 30560) loss: 2.303993\n",
      "(Iteration 17801 / 30560) loss: 2.303986\n",
      "(Iteration 17901 / 30560) loss: 2.304004\n",
      "(Epoch 47 / 80) train acc: 0.105000; val_acc: 0.107000\n",
      "(Iteration 18001 / 30560) loss: 2.303984\n",
      "(Iteration 18101 / 30560) loss: 2.303980\n",
      "(Iteration 18201 / 30560) loss: 2.303983\n",
      "(Iteration 18301 / 30560) loss: 2.303982\n",
      "(Epoch 48 / 80) train acc: 0.127000; val_acc: 0.107000\n",
      "(Iteration 18401 / 30560) loss: 2.304010\n",
      "(Iteration 18501 / 30560) loss: 2.303987\n",
      "(Iteration 18601 / 30560) loss: 2.303996\n",
      "(Iteration 18701 / 30560) loss: 2.303994\n",
      "(Epoch 49 / 80) train acc: 0.108000; val_acc: 0.107000\n",
      "(Iteration 18801 / 30560) loss: 2.304005\n",
      "(Iteration 18901 / 30560) loss: 2.303991\n",
      "(Iteration 19001 / 30560) loss: 2.304001\n",
      "(Epoch 50 / 80) train acc: 0.128000; val_acc: 0.107000\n",
      "(Iteration 19101 / 30560) loss: 2.303993\n",
      "(Iteration 19201 / 30560) loss: 2.303982\n",
      "(Iteration 19301 / 30560) loss: 2.303986\n",
      "(Iteration 19401 / 30560) loss: 2.304005\n",
      "(Epoch 51 / 80) train acc: 0.129000; val_acc: 0.107000\n",
      "(Iteration 19501 / 30560) loss: 2.304006\n",
      "(Iteration 19601 / 30560) loss: 2.303992\n",
      "(Iteration 19701 / 30560) loss: 2.303991\n",
      "(Iteration 19801 / 30560) loss: 2.304000\n",
      "(Epoch 52 / 80) train acc: 0.113000; val_acc: 0.106000\n",
      "(Iteration 19901 / 30560) loss: 2.303966\n",
      "(Iteration 20001 / 30560) loss: 2.303977\n",
      "(Iteration 20101 / 30560) loss: 2.303990\n",
      "(Iteration 20201 / 30560) loss: 2.303997\n",
      "(Epoch 53 / 80) train acc: 0.094000; val_acc: 0.106000\n",
      "(Iteration 20301 / 30560) loss: 2.303989\n",
      "(Iteration 20401 / 30560) loss: 2.303986\n",
      "(Iteration 20501 / 30560) loss: 2.303981\n",
      "(Iteration 20601 / 30560) loss: 2.304004\n",
      "(Epoch 54 / 80) train acc: 0.108000; val_acc: 0.106000\n",
      "(Iteration 20701 / 30560) loss: 2.303980\n",
      "(Iteration 20801 / 30560) loss: 2.303995\n",
      "(Iteration 20901 / 30560) loss: 2.303991\n",
      "(Iteration 21001 / 30560) loss: 2.303991\n",
      "(Epoch 55 / 80) train acc: 0.112000; val_acc: 0.106000\n",
      "(Iteration 21101 / 30560) loss: 2.303965\n",
      "(Iteration 21201 / 30560) loss: 2.304011\n",
      "(Iteration 21301 / 30560) loss: 2.303997\n",
      "(Epoch 56 / 80) train acc: 0.125000; val_acc: 0.106000\n",
      "(Iteration 21401 / 30560) loss: 2.304013\n",
      "(Iteration 21501 / 30560) loss: 2.303993\n",
      "(Iteration 21601 / 30560) loss: 2.303993\n",
      "(Iteration 21701 / 30560) loss: 2.303995\n",
      "(Epoch 57 / 80) train acc: 0.112000; val_acc: 0.106000\n",
      "(Iteration 21801 / 30560) loss: 2.303983\n",
      "(Iteration 21901 / 30560) loss: 2.303999\n",
      "(Iteration 22001 / 30560) loss: 2.303982\n",
      "(Iteration 22101 / 30560) loss: 2.303970\n",
      "(Epoch 58 / 80) train acc: 0.116000; val_acc: 0.106000\n",
      "(Iteration 22201 / 30560) loss: 2.303994\n",
      "(Iteration 22301 / 30560) loss: 2.303989\n",
      "(Iteration 22401 / 30560) loss: 2.303993\n",
      "(Iteration 22501 / 30560) loss: 2.303973\n",
      "(Epoch 59 / 80) train acc: 0.115000; val_acc: 0.106000\n",
      "(Iteration 22601 / 30560) loss: 2.303982\n",
      "(Iteration 22701 / 30560) loss: 2.303973\n",
      "(Iteration 22801 / 30560) loss: 2.303983\n",
      "(Iteration 22901 / 30560) loss: 2.303997\n",
      "(Epoch 60 / 80) train acc: 0.125000; val_acc: 0.106000\n",
      "(Iteration 23001 / 30560) loss: 2.303982\n",
      "(Iteration 23101 / 30560) loss: 2.303981\n",
      "(Iteration 23201 / 30560) loss: 2.303977\n",
      "(Iteration 23301 / 30560) loss: 2.303997\n",
      "(Epoch 61 / 80) train acc: 0.138000; val_acc: 0.106000\n",
      "(Iteration 23401 / 30560) loss: 2.303988\n",
      "(Iteration 23501 / 30560) loss: 2.303995\n",
      "(Iteration 23601 / 30560) loss: 2.304000\n",
      "(Epoch 62 / 80) train acc: 0.119000; val_acc: 0.106000\n",
      "(Iteration 23701 / 30560) loss: 2.304003\n",
      "(Iteration 23801 / 30560) loss: 2.303982\n",
      "(Iteration 23901 / 30560) loss: 2.303994\n",
      "(Iteration 24001 / 30560) loss: 2.303964\n",
      "(Epoch 63 / 80) train acc: 0.129000; val_acc: 0.106000\n",
      "(Iteration 24101 / 30560) loss: 2.303987\n",
      "(Iteration 24201 / 30560) loss: 2.303976\n",
      "(Iteration 24301 / 30560) loss: 2.304002\n",
      "(Iteration 24401 / 30560) loss: 2.303998\n",
      "(Epoch 64 / 80) train acc: 0.120000; val_acc: 0.106000\n",
      "(Iteration 24501 / 30560) loss: 2.303981\n",
      "(Iteration 24601 / 30560) loss: 2.303971\n",
      "(Iteration 24701 / 30560) loss: 2.303964\n",
      "(Iteration 24801 / 30560) loss: 2.303976\n",
      "(Epoch 65 / 80) train acc: 0.107000; val_acc: 0.106000\n",
      "(Iteration 24901 / 30560) loss: 2.303990\n",
      "(Iteration 25001 / 30560) loss: 2.303984\n",
      "(Iteration 25101 / 30560) loss: 2.303995\n",
      "(Iteration 25201 / 30560) loss: 2.303999\n",
      "(Epoch 66 / 80) train acc: 0.107000; val_acc: 0.106000\n",
      "(Iteration 25301 / 30560) loss: 2.304003\n",
      "(Iteration 25401 / 30560) loss: 2.304014\n",
      "(Iteration 25501 / 30560) loss: 2.303992\n",
      "(Epoch 67 / 80) train acc: 0.117000; val_acc: 0.106000\n",
      "(Iteration 25601 / 30560) loss: 2.303991\n",
      "(Iteration 25701 / 30560) loss: 2.303982\n",
      "(Iteration 25801 / 30560) loss: 2.303984\n",
      "(Iteration 25901 / 30560) loss: 2.303990\n",
      "(Epoch 68 / 80) train acc: 0.133000; val_acc: 0.106000\n",
      "(Iteration 26001 / 30560) loss: 2.303985\n",
      "(Iteration 26101 / 30560) loss: 2.303968\n",
      "(Iteration 26201 / 30560) loss: 2.303996\n",
      "(Iteration 26301 / 30560) loss: 2.303988\n",
      "(Epoch 69 / 80) train acc: 0.125000; val_acc: 0.106000\n",
      "(Iteration 26401 / 30560) loss: 2.303981\n",
      "(Iteration 26501 / 30560) loss: 2.303965\n",
      "(Iteration 26601 / 30560) loss: 2.303999\n",
      "(Iteration 26701 / 30560) loss: 2.303969\n",
      "(Epoch 70 / 80) train acc: 0.114000; val_acc: 0.106000\n",
      "(Iteration 26801 / 30560) loss: 2.303990\n",
      "(Iteration 26901 / 30560) loss: 2.303979\n",
      "(Iteration 27001 / 30560) loss: 2.304000\n",
      "(Iteration 27101 / 30560) loss: 2.303977\n",
      "(Epoch 71 / 80) train acc: 0.122000; val_acc: 0.106000\n",
      "(Iteration 27201 / 30560) loss: 2.303965\n",
      "(Iteration 27301 / 30560) loss: 2.303989\n",
      "(Iteration 27401 / 30560) loss: 2.303997\n",
      "(Iteration 27501 / 30560) loss: 2.304001\n",
      "(Epoch 72 / 80) train acc: 0.104000; val_acc: 0.106000\n",
      "(Iteration 27601 / 30560) loss: 2.303962\n",
      "(Iteration 27701 / 30560) loss: 2.303986\n",
      "(Iteration 27801 / 30560) loss: 2.303980\n",
      "(Epoch 73 / 80) train acc: 0.093000; val_acc: 0.106000\n",
      "(Iteration 27901 / 30560) loss: 2.303981\n",
      "(Iteration 28001 / 30560) loss: 2.303998\n",
      "(Iteration 28101 / 30560) loss: 2.303998\n",
      "(Iteration 28201 / 30560) loss: 2.303986\n",
      "(Epoch 74 / 80) train acc: 0.108000; val_acc: 0.106000\n",
      "(Iteration 28301 / 30560) loss: 2.303991\n",
      "(Iteration 28401 / 30560) loss: 2.303993\n",
      "(Iteration 28501 / 30560) loss: 2.303971\n",
      "(Iteration 28601 / 30560) loss: 2.303961\n",
      "(Epoch 75 / 80) train acc: 0.109000; val_acc: 0.106000\n",
      "(Iteration 28701 / 30560) loss: 2.304011\n",
      "(Iteration 28801 / 30560) loss: 2.304003\n",
      "(Iteration 28901 / 30560) loss: 2.303971\n",
      "(Iteration 29001 / 30560) loss: 2.303982\n",
      "(Epoch 76 / 80) train acc: 0.131000; val_acc: 0.106000\n",
      "(Iteration 29101 / 30560) loss: 2.303989\n",
      "(Iteration 29201 / 30560) loss: 2.303975\n",
      "(Iteration 29301 / 30560) loss: 2.303985\n",
      "(Iteration 29401 / 30560) loss: 2.303967\n",
      "(Epoch 77 / 80) train acc: 0.121000; val_acc: 0.106000\n",
      "(Iteration 29501 / 30560) loss: 2.303989\n",
      "(Iteration 29601 / 30560) loss: 2.303994\n",
      "(Iteration 29701 / 30560) loss: 2.303967\n",
      "(Epoch 78 / 80) train acc: 0.119000; val_acc: 0.106000\n",
      "(Iteration 29801 / 30560) loss: 2.304000\n",
      "(Iteration 29901 / 30560) loss: 2.303994\n",
      "(Iteration 30001 / 30560) loss: 2.303992\n",
      "(Iteration 30101 / 30560) loss: 2.303972\n",
      "(Epoch 79 / 80) train acc: 0.127000; val_acc: 0.106000\n",
      "(Iteration 30201 / 30560) loss: 2.303989\n",
      "(Iteration 30301 / 30560) loss: 2.303995\n",
      "(Iteration 30401 / 30560) loss: 2.303970\n",
      "(Iteration 30501 / 30560) loss: 2.303966\n",
      "(Epoch 80 / 80) train acc: 0.123000; val_acc: 0.106000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.304619\n",
      "(Epoch 0 / 80) train acc: 0.109000; val_acc: 0.111000\n",
      "(Iteration 101 / 61200) loss: 2.304603\n",
      "(Iteration 201 / 61200) loss: 2.304575\n",
      "(Iteration 301 / 61200) loss: 2.304561\n",
      "(Iteration 401 / 61200) loss: 2.304527\n",
      "(Iteration 501 / 61200) loss: 2.304535\n",
      "(Iteration 601 / 61200) loss: 2.304516\n",
      "(Iteration 701 / 61200) loss: 2.304480\n",
      "(Epoch 1 / 80) train acc: 0.084000; val_acc: 0.092000\n",
      "(Iteration 801 / 61200) loss: 2.304461\n",
      "(Iteration 901 / 61200) loss: 2.304424\n",
      "(Iteration 1001 / 61200) loss: 2.304422\n",
      "(Iteration 1101 / 61200) loss: 2.304385\n",
      "(Iteration 1201 / 61200) loss: 2.304422\n",
      "(Iteration 1301 / 61200) loss: 2.304406\n",
      "(Iteration 1401 / 61200) loss: 2.304369\n",
      "(Iteration 1501 / 61200) loss: 2.304396\n",
      "(Epoch 2 / 80) train acc: 0.078000; val_acc: 0.102000\n",
      "(Iteration 1601 / 61200) loss: 2.304344\n",
      "(Iteration 1701 / 61200) loss: 2.304307\n",
      "(Iteration 1801 / 61200) loss: 2.304315\n",
      "(Iteration 1901 / 61200) loss: 2.304293\n",
      "(Iteration 2001 / 61200) loss: 2.304261\n",
      "(Iteration 2101 / 61200) loss: 2.304236\n",
      "(Iteration 2201 / 61200) loss: 2.304218\n",
      "(Epoch 3 / 80) train acc: 0.100000; val_acc: 0.097000\n",
      "(Iteration 2301 / 61200) loss: 2.304211\n",
      "(Iteration 2401 / 61200) loss: 2.304202\n",
      "(Iteration 2501 / 61200) loss: 2.304169\n",
      "(Iteration 2601 / 61200) loss: 2.304188\n",
      "(Iteration 2701 / 61200) loss: 2.304168\n",
      "(Iteration 2801 / 61200) loss: 2.304165\n",
      "(Iteration 2901 / 61200) loss: 2.304153\n",
      "(Iteration 3001 / 61200) loss: 2.304128\n",
      "(Epoch 4 / 80) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 3101 / 61200) loss: 2.304146\n",
      "(Iteration 3201 / 61200) loss: 2.304125\n",
      "(Iteration 3301 / 61200) loss: 2.304061\n",
      "(Iteration 3401 / 61200) loss: 2.304062\n",
      "(Iteration 3501 / 61200) loss: 2.304106\n",
      "(Iteration 3601 / 61200) loss: 2.304046\n",
      "(Iteration 3701 / 61200) loss: 2.304068\n",
      "(Iteration 3801 / 61200) loss: 2.304076\n",
      "(Epoch 5 / 80) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 3901 / 61200) loss: 2.303979\n",
      "(Iteration 4001 / 61200) loss: 2.303989\n",
      "(Iteration 4101 / 61200) loss: 2.303998\n",
      "(Iteration 4201 / 61200) loss: 2.303960\n",
      "(Iteration 4301 / 61200) loss: 2.303980\n",
      "(Iteration 4401 / 61200) loss: 2.304042\n",
      "(Iteration 4501 / 61200) loss: 2.303940\n",
      "(Epoch 6 / 80) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 4601 / 61200) loss: 2.303901\n",
      "(Iteration 4701 / 61200) loss: 2.303931\n",
      "(Iteration 4801 / 61200) loss: 2.303973\n",
      "(Iteration 4901 / 61200) loss: 2.303857\n",
      "(Iteration 5001 / 61200) loss: 2.303892\n",
      "(Iteration 5101 / 61200) loss: 2.303884\n",
      "(Iteration 5201 / 61200) loss: 2.303935\n",
      "(Iteration 5301 / 61200) loss: 2.303879\n",
      "(Epoch 7 / 80) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 5401 / 61200) loss: 2.303832\n",
      "(Iteration 5501 / 61200) loss: 2.303847\n",
      "(Iteration 5601 / 61200) loss: 2.303917\n",
      "(Iteration 5701 / 61200) loss: 2.303873\n",
      "(Iteration 5801 / 61200) loss: 2.303814\n",
      "(Iteration 5901 / 61200) loss: 2.303827\n",
      "(Iteration 6001 / 61200) loss: 2.303788\n",
      "(Iteration 6101 / 61200) loss: 2.303790\n",
      "(Epoch 8 / 80) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 6201 / 61200) loss: 2.303828\n",
      "(Iteration 6301 / 61200) loss: 2.303813\n",
      "(Iteration 6401 / 61200) loss: 2.303768\n",
      "(Iteration 6501 / 61200) loss: 2.303759\n",
      "(Iteration 6601 / 61200) loss: 2.303830\n",
      "(Iteration 6701 / 61200) loss: 2.303743\n",
      "(Iteration 6801 / 61200) loss: 2.303777\n",
      "(Epoch 9 / 80) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 6901 / 61200) loss: 2.303756\n",
      "(Iteration 7001 / 61200) loss: 2.303740\n",
      "(Iteration 7101 / 61200) loss: 2.303780\n",
      "(Iteration 7201 / 61200) loss: 2.303730\n",
      "(Iteration 7301 / 61200) loss: 2.303728\n",
      "(Iteration 7401 / 61200) loss: 2.303663\n",
      "(Iteration 7501 / 61200) loss: 2.303667\n",
      "(Iteration 7601 / 61200) loss: 2.303640\n",
      "(Epoch 10 / 80) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 7701 / 61200) loss: 2.303718\n",
      "(Iteration 7801 / 61200) loss: 2.303704\n",
      "(Iteration 7901 / 61200) loss: 2.303649\n",
      "(Iteration 8001 / 61200) loss: 2.303691\n",
      "(Iteration 8101 / 61200) loss: 2.303666\n",
      "(Iteration 8201 / 61200) loss: 2.303701\n",
      "(Iteration 8301 / 61200) loss: 2.303524\n",
      "(Iteration 8401 / 61200) loss: 2.303530\n",
      "(Epoch 11 / 80) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 8501 / 61200) loss: 2.303738\n",
      "(Iteration 8601 / 61200) loss: 2.303590\n",
      "(Iteration 8701 / 61200) loss: 2.303630\n",
      "(Iteration 8801 / 61200) loss: 2.303555\n",
      "(Iteration 8901 / 61200) loss: 2.303601\n",
      "(Iteration 9001 / 61200) loss: 2.303612\n",
      "(Iteration 9101 / 61200) loss: 2.303585\n",
      "(Epoch 12 / 80) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 9201 / 61200) loss: 2.303575\n",
      "(Iteration 9301 / 61200) loss: 2.303581\n",
      "(Iteration 9401 / 61200) loss: 2.303515\n",
      "(Iteration 9501 / 61200) loss: 2.303528\n",
      "(Iteration 9601 / 61200) loss: 2.303516\n",
      "(Iteration 9701 / 61200) loss: 2.303544\n",
      "(Iteration 9801 / 61200) loss: 2.303477\n",
      "(Iteration 9901 / 61200) loss: 2.303581\n",
      "(Epoch 13 / 80) train acc: 0.111000; val_acc: 0.078000\n",
      "(Iteration 10001 / 61200) loss: 2.303559\n",
      "(Iteration 10101 / 61200) loss: 2.303570\n",
      "(Iteration 10201 / 61200) loss: 2.303498\n",
      "(Iteration 10301 / 61200) loss: 2.303559\n",
      "(Iteration 10401 / 61200) loss: 2.303552\n",
      "(Iteration 10501 / 61200) loss: 2.303494\n",
      "(Iteration 10601 / 61200) loss: 2.303530\n",
      "(Iteration 10701 / 61200) loss: 2.303566\n",
      "(Epoch 14 / 80) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 10801 / 61200) loss: 2.303518\n",
      "(Iteration 10901 / 61200) loss: 2.303532\n",
      "(Iteration 11001 / 61200) loss: 2.303526\n",
      "(Iteration 11101 / 61200) loss: 2.303479\n",
      "(Iteration 11201 / 61200) loss: 2.303510\n",
      "(Iteration 11301 / 61200) loss: 2.303375\n",
      "(Iteration 11401 / 61200) loss: 2.303407\n",
      "(Epoch 15 / 80) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 11501 / 61200) loss: 2.303473\n",
      "(Iteration 11601 / 61200) loss: 2.303447\n",
      "(Iteration 11701 / 61200) loss: 2.303511\n",
      "(Iteration 11801 / 61200) loss: 2.303435\n",
      "(Iteration 11901 / 61200) loss: 2.303361\n",
      "(Iteration 12001 / 61200) loss: 2.303499\n",
      "(Iteration 12101 / 61200) loss: 2.303530\n",
      "(Iteration 12201 / 61200) loss: 2.303462\n",
      "(Epoch 16 / 80) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 12301 / 61200) loss: 2.303501\n",
      "(Iteration 12401 / 61200) loss: 2.303343\n",
      "(Iteration 12501 / 61200) loss: 2.303441\n",
      "(Iteration 12601 / 61200) loss: 2.303362\n",
      "(Iteration 12701 / 61200) loss: 2.303463\n",
      "(Iteration 12801 / 61200) loss: 2.303446\n",
      "(Iteration 12901 / 61200) loss: 2.303368\n",
      "(Iteration 13001 / 61200) loss: 2.303386\n",
      "(Epoch 17 / 80) train acc: 0.084000; val_acc: 0.078000\n",
      "(Iteration 13101 / 61200) loss: 2.303347\n",
      "(Iteration 13201 / 61200) loss: 2.303393\n",
      "(Iteration 13301 / 61200) loss: 2.303321\n",
      "(Iteration 13401 / 61200) loss: 2.303452\n",
      "(Iteration 13501 / 61200) loss: 2.303317\n",
      "(Iteration 13601 / 61200) loss: 2.303389\n",
      "(Iteration 13701 / 61200) loss: 2.303325\n",
      "(Epoch 18 / 80) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 13801 / 61200) loss: 2.303333\n",
      "(Iteration 13901 / 61200) loss: 2.303341\n",
      "(Iteration 14001 / 61200) loss: 2.303530\n",
      "(Iteration 14101 / 61200) loss: 2.303308\n",
      "(Iteration 14201 / 61200) loss: 2.303449\n",
      "(Iteration 14301 / 61200) loss: 2.303384\n",
      "(Iteration 14401 / 61200) loss: 2.303353\n",
      "(Iteration 14501 / 61200) loss: 2.303368\n",
      "(Epoch 19 / 80) train acc: 0.112000; val_acc: 0.078000\n",
      "(Iteration 14601 / 61200) loss: 2.303376\n",
      "(Iteration 14701 / 61200) loss: 2.303426\n",
      "(Iteration 14801 / 61200) loss: 2.303358\n",
      "(Iteration 14901 / 61200) loss: 2.303361\n",
      "(Iteration 15001 / 61200) loss: 2.303296\n",
      "(Iteration 15101 / 61200) loss: 2.303452\n",
      "(Iteration 15201 / 61200) loss: 2.303318\n",
      "(Epoch 20 / 80) train acc: 0.117000; val_acc: 0.078000\n",
      "(Iteration 15301 / 61200) loss: 2.303168\n",
      "(Iteration 15401 / 61200) loss: 2.303212\n",
      "(Iteration 15501 / 61200) loss: 2.303263\n",
      "(Iteration 15601 / 61200) loss: 2.303318\n",
      "(Iteration 15701 / 61200) loss: 2.303368\n",
      "(Iteration 15801 / 61200) loss: 2.303266\n",
      "(Iteration 15901 / 61200) loss: 2.303236\n",
      "(Iteration 16001 / 61200) loss: 2.303357\n",
      "(Epoch 21 / 80) train acc: 0.085000; val_acc: 0.078000\n",
      "(Iteration 16101 / 61200) loss: 2.303316\n",
      "(Iteration 16201 / 61200) loss: 2.303409\n",
      "(Iteration 16301 / 61200) loss: 2.303276\n",
      "(Iteration 16401 / 61200) loss: 2.303342\n",
      "(Iteration 16501 / 61200) loss: 2.303377\n",
      "(Iteration 16601 / 61200) loss: 2.303379\n",
      "(Iteration 16701 / 61200) loss: 2.303208\n",
      "(Iteration 16801 / 61200) loss: 2.303399\n",
      "(Epoch 22 / 80) train acc: 0.115000; val_acc: 0.078000\n",
      "(Iteration 16901 / 61200) loss: 2.303390\n",
      "(Iteration 17001 / 61200) loss: 2.303302\n",
      "(Iteration 17101 / 61200) loss: 2.303399\n",
      "(Iteration 17201 / 61200) loss: 2.303257\n",
      "(Iteration 17301 / 61200) loss: 2.303295\n",
      "(Iteration 17401 / 61200) loss: 2.303277\n",
      "(Iteration 17501 / 61200) loss: 2.303197\n",
      "(Epoch 23 / 80) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 17601 / 61200) loss: 2.303233\n",
      "(Iteration 17701 / 61200) loss: 2.303326\n",
      "(Iteration 17801 / 61200) loss: 2.303241\n",
      "(Iteration 17901 / 61200) loss: 2.303263\n",
      "(Iteration 18001 / 61200) loss: 2.303194\n",
      "(Iteration 18101 / 61200) loss: 2.303242\n",
      "(Iteration 18201 / 61200) loss: 2.303301\n",
      "(Iteration 18301 / 61200) loss: 2.303235\n",
      "(Epoch 24 / 80) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 18401 / 61200) loss: 2.303180\n",
      "(Iteration 18501 / 61200) loss: 2.303278\n",
      "(Iteration 18601 / 61200) loss: 2.303221\n",
      "(Iteration 18701 / 61200) loss: 2.303230\n",
      "(Iteration 18801 / 61200) loss: 2.303281\n",
      "(Iteration 18901 / 61200) loss: 2.303117\n",
      "(Iteration 19001 / 61200) loss: 2.303246\n",
      "(Iteration 19101 / 61200) loss: 2.303227\n",
      "(Epoch 25 / 80) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 19201 / 61200) loss: 2.303205\n",
      "(Iteration 19301 / 61200) loss: 2.303278\n",
      "(Iteration 19401 / 61200) loss: 2.303248\n",
      "(Iteration 19501 / 61200) loss: 2.303174\n",
      "(Iteration 19601 / 61200) loss: 2.303284\n",
      "(Iteration 19701 / 61200) loss: 2.303312\n",
      "(Iteration 19801 / 61200) loss: 2.303295\n",
      "(Epoch 26 / 80) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 19901 / 61200) loss: 2.303254\n",
      "(Iteration 20001 / 61200) loss: 2.303161\n",
      "(Iteration 20101 / 61200) loss: 2.303369\n",
      "(Iteration 20201 / 61200) loss: 2.303211\n",
      "(Iteration 20301 / 61200) loss: 2.303308\n",
      "(Iteration 20401 / 61200) loss: 2.303164\n",
      "(Iteration 20501 / 61200) loss: 2.303258\n",
      "(Iteration 20601 / 61200) loss: 2.303175\n",
      "(Epoch 27 / 80) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 20701 / 61200) loss: 2.303288\n",
      "(Iteration 20801 / 61200) loss: 2.303259\n",
      "(Iteration 20901 / 61200) loss: 2.303256\n",
      "(Iteration 21001 / 61200) loss: 2.303220\n",
      "(Iteration 21101 / 61200) loss: 2.303220\n",
      "(Iteration 21201 / 61200) loss: 2.303179\n",
      "(Iteration 21301 / 61200) loss: 2.303292\n",
      "(Iteration 21401 / 61200) loss: 2.303168\n",
      "(Epoch 28 / 80) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 21501 / 61200) loss: 2.303156\n",
      "(Iteration 21601 / 61200) loss: 2.303325\n",
      "(Iteration 21701 / 61200) loss: 2.303217\n",
      "(Iteration 21801 / 61200) loss: 2.303267\n",
      "(Iteration 21901 / 61200) loss: 2.303233\n",
      "(Iteration 22001 / 61200) loss: 2.303203\n",
      "(Iteration 22101 / 61200) loss: 2.303089\n",
      "(Epoch 29 / 80) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 22201 / 61200) loss: 2.303211\n",
      "(Iteration 22301 / 61200) loss: 2.303158\n",
      "(Iteration 22401 / 61200) loss: 2.303110\n",
      "(Iteration 22501 / 61200) loss: 2.303232\n",
      "(Iteration 22601 / 61200) loss: 2.303236\n",
      "(Iteration 22701 / 61200) loss: 2.303283\n",
      "(Iteration 22801 / 61200) loss: 2.303171\n",
      "(Iteration 22901 / 61200) loss: 2.303204\n",
      "(Epoch 30 / 80) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 23001 / 61200) loss: 2.303179\n",
      "(Iteration 23101 / 61200) loss: 2.303319\n",
      "(Iteration 23201 / 61200) loss: 2.303289\n",
      "(Iteration 23301 / 61200) loss: 2.303139\n",
      "(Iteration 23401 / 61200) loss: 2.303183\n",
      "(Iteration 23501 / 61200) loss: 2.303088\n",
      "(Iteration 23601 / 61200) loss: 2.303209\n",
      "(Iteration 23701 / 61200) loss: 2.303087\n",
      "(Epoch 31 / 80) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 23801 / 61200) loss: 2.303135\n",
      "(Iteration 23901 / 61200) loss: 2.303272\n",
      "(Iteration 24001 / 61200) loss: 2.303132\n",
      "(Iteration 24101 / 61200) loss: 2.303163\n",
      "(Iteration 24201 / 61200) loss: 2.303308\n",
      "(Iteration 24301 / 61200) loss: 2.303206\n",
      "(Iteration 24401 / 61200) loss: 2.303085\n",
      "(Epoch 32 / 80) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 24501 / 61200) loss: 2.303270\n",
      "(Iteration 24601 / 61200) loss: 2.303199\n",
      "(Iteration 24701 / 61200) loss: 2.303221\n",
      "(Iteration 24801 / 61200) loss: 2.303113\n",
      "(Iteration 24901 / 61200) loss: 2.303241\n",
      "(Iteration 25001 / 61200) loss: 2.303170\n",
      "(Iteration 25101 / 61200) loss: 2.303316\n",
      "(Iteration 25201 / 61200) loss: 2.303178\n",
      "(Epoch 33 / 80) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 25301 / 61200) loss: 2.303033\n",
      "(Iteration 25401 / 61200) loss: 2.303061\n",
      "(Iteration 25501 / 61200) loss: 2.303226\n",
      "(Iteration 25601 / 61200) loss: 2.303033\n",
      "(Iteration 25701 / 61200) loss: 2.303099\n",
      "(Iteration 25801 / 61200) loss: 2.303342\n",
      "(Iteration 25901 / 61200) loss: 2.303241\n",
      "(Iteration 26001 / 61200) loss: 2.303195\n",
      "(Epoch 34 / 80) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 26101 / 61200) loss: 2.303303\n",
      "(Iteration 26201 / 61200) loss: 2.303103\n",
      "(Iteration 26301 / 61200) loss: 2.303234\n",
      "(Iteration 26401 / 61200) loss: 2.303229\n",
      "(Iteration 26501 / 61200) loss: 2.303275\n",
      "(Iteration 26601 / 61200) loss: 2.303172\n",
      "(Iteration 26701 / 61200) loss: 2.303111\n",
      "(Epoch 35 / 80) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 26801 / 61200) loss: 2.303218\n",
      "(Iteration 26901 / 61200) loss: 2.303168\n",
      "(Iteration 27001 / 61200) loss: 2.303050\n",
      "(Iteration 27101 / 61200) loss: 2.303119\n",
      "(Iteration 27201 / 61200) loss: 2.303163\n",
      "(Iteration 27301 / 61200) loss: 2.303017\n",
      "(Iteration 27401 / 61200) loss: 2.303218\n",
      "(Iteration 27501 / 61200) loss: 2.303105\n",
      "(Epoch 36 / 80) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 27601 / 61200) loss: 2.303204\n",
      "(Iteration 27701 / 61200) loss: 2.303135\n",
      "(Iteration 27801 / 61200) loss: 2.303147\n",
      "(Iteration 27901 / 61200) loss: 2.303118\n",
      "(Iteration 28001 / 61200) loss: 2.303100\n",
      "(Iteration 28101 / 61200) loss: 2.303167\n",
      "(Iteration 28201 / 61200) loss: 2.303093\n",
      "(Iteration 28301 / 61200) loss: 2.303213\n",
      "(Epoch 37 / 80) train acc: 0.085000; val_acc: 0.078000\n",
      "(Iteration 28401 / 61200) loss: 2.303247\n",
      "(Iteration 28501 / 61200) loss: 2.303029\n",
      "(Iteration 28601 / 61200) loss: 2.303119\n",
      "(Iteration 28701 / 61200) loss: 2.303192\n",
      "(Iteration 28801 / 61200) loss: 2.303158\n",
      "(Iteration 28901 / 61200) loss: 2.303089\n",
      "(Iteration 29001 / 61200) loss: 2.303097\n",
      "(Epoch 38 / 80) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 29101 / 61200) loss: 2.303262\n",
      "(Iteration 29201 / 61200) loss: 2.303136\n",
      "(Iteration 29301 / 61200) loss: 2.303039\n",
      "(Iteration 29401 / 61200) loss: 2.303206\n",
      "(Iteration 29501 / 61200) loss: 2.303084\n",
      "(Iteration 29601 / 61200) loss: 2.303065\n",
      "(Iteration 29701 / 61200) loss: 2.303161\n",
      "(Iteration 29801 / 61200) loss: 2.303091\n",
      "(Epoch 39 / 80) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 29901 / 61200) loss: 2.303093\n",
      "(Iteration 30001 / 61200) loss: 2.303199\n",
      "(Iteration 30101 / 61200) loss: 2.303086\n",
      "(Iteration 30201 / 61200) loss: 2.303103\n",
      "(Iteration 30301 / 61200) loss: 2.303176\n",
      "(Iteration 30401 / 61200) loss: 2.303118\n",
      "(Iteration 30501 / 61200) loss: 2.303137\n",
      "(Epoch 40 / 80) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 30601 / 61200) loss: 2.303047\n",
      "(Iteration 30701 / 61200) loss: 2.303229\n",
      "(Iteration 30801 / 61200) loss: 2.303212\n",
      "(Iteration 30901 / 61200) loss: 2.303076\n",
      "(Iteration 31001 / 61200) loss: 2.303219\n",
      "(Iteration 31101 / 61200) loss: 2.303118\n",
      "(Iteration 31201 / 61200) loss: 2.303114\n",
      "(Iteration 31301 / 61200) loss: 2.303078\n",
      "(Epoch 41 / 80) train acc: 0.085000; val_acc: 0.078000\n",
      "(Iteration 31401 / 61200) loss: 2.303096\n",
      "(Iteration 31501 / 61200) loss: 2.303203\n",
      "(Iteration 31601 / 61200) loss: 2.303110\n",
      "(Iteration 31701 / 61200) loss: 2.303103\n",
      "(Iteration 31801 / 61200) loss: 2.303149\n",
      "(Iteration 31901 / 61200) loss: 2.303165\n",
      "(Iteration 32001 / 61200) loss: 2.303124\n",
      "(Iteration 32101 / 61200) loss: 2.303092\n",
      "(Epoch 42 / 80) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 32201 / 61200) loss: 2.303205\n",
      "(Iteration 32301 / 61200) loss: 2.303060\n",
      "(Iteration 32401 / 61200) loss: 2.303209\n",
      "(Iteration 32501 / 61200) loss: 2.303107\n",
      "(Iteration 32601 / 61200) loss: 2.303019\n",
      "(Iteration 32701 / 61200) loss: 2.303134\n",
      "(Iteration 32801 / 61200) loss: 2.303193\n",
      "(Epoch 43 / 80) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 32901 / 61200) loss: 2.303212\n",
      "(Iteration 33001 / 61200) loss: 2.303264\n",
      "(Iteration 33101 / 61200) loss: 2.303054\n",
      "(Iteration 33201 / 61200) loss: 2.303136\n",
      "(Iteration 33301 / 61200) loss: 2.303006\n",
      "(Iteration 33401 / 61200) loss: 2.302998\n",
      "(Iteration 33501 / 61200) loss: 2.303103\n",
      "(Iteration 33601 / 61200) loss: 2.303139\n",
      "(Epoch 44 / 80) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 33701 / 61200) loss: 2.303111\n",
      "(Iteration 33801 / 61200) loss: 2.303066\n",
      "(Iteration 33901 / 61200) loss: 2.303202\n",
      "(Iteration 34001 / 61200) loss: 2.303119\n",
      "(Iteration 34101 / 61200) loss: 2.303055\n",
      "(Iteration 34201 / 61200) loss: 2.303149\n",
      "(Iteration 34301 / 61200) loss: 2.303082\n",
      "(Iteration 34401 / 61200) loss: 2.303025\n",
      "(Epoch 45 / 80) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 34501 / 61200) loss: 2.303039\n",
      "(Iteration 34601 / 61200) loss: 2.303036\n",
      "(Iteration 34701 / 61200) loss: 2.303173\n",
      "(Iteration 34801 / 61200) loss: 2.303013\n",
      "(Iteration 34901 / 61200) loss: 2.303004\n",
      "(Iteration 35001 / 61200) loss: 2.303077\n",
      "(Iteration 35101 / 61200) loss: 2.303075\n",
      "(Epoch 46 / 80) train acc: 0.085000; val_acc: 0.078000\n",
      "(Iteration 35201 / 61200) loss: 2.303151\n",
      "(Iteration 35301 / 61200) loss: 2.303014\n",
      "(Iteration 35401 / 61200) loss: 2.303110\n",
      "(Iteration 35501 / 61200) loss: 2.303011\n",
      "(Iteration 35601 / 61200) loss: 2.303107\n",
      "(Iteration 35701 / 61200) loss: 2.303070\n",
      "(Iteration 35801 / 61200) loss: 2.303035\n",
      "(Iteration 35901 / 61200) loss: 2.303040\n",
      "(Epoch 47 / 80) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 36001 / 61200) loss: 2.303081\n",
      "(Iteration 36101 / 61200) loss: 2.303013\n",
      "(Iteration 36201 / 61200) loss: 2.302989\n",
      "(Iteration 36301 / 61200) loss: 2.303207\n",
      "(Iteration 36401 / 61200) loss: 2.303176\n",
      "(Iteration 36501 / 61200) loss: 2.303118\n",
      "(Iteration 36601 / 61200) loss: 2.302938\n",
      "(Iteration 36701 / 61200) loss: 2.303100\n",
      "(Epoch 48 / 80) train acc: 0.086000; val_acc: 0.078000\n",
      "(Iteration 36801 / 61200) loss: 2.303031\n",
      "(Iteration 36901 / 61200) loss: 2.303031\n",
      "(Iteration 37001 / 61200) loss: 2.303066\n",
      "(Iteration 37101 / 61200) loss: 2.303067\n",
      "(Iteration 37201 / 61200) loss: 2.303124\n",
      "(Iteration 37301 / 61200) loss: 2.303083\n",
      "(Iteration 37401 / 61200) loss: 2.303037\n",
      "(Epoch 49 / 80) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 37501 / 61200) loss: 2.303045\n",
      "(Iteration 37601 / 61200) loss: 2.303103\n",
      "(Iteration 37701 / 61200) loss: 2.302983\n",
      "(Iteration 37801 / 61200) loss: 2.303141\n",
      "(Iteration 37901 / 61200) loss: 2.302940\n",
      "(Iteration 38001 / 61200) loss: 2.303058\n",
      "(Iteration 38101 / 61200) loss: 2.303175\n",
      "(Iteration 38201 / 61200) loss: 2.303171\n",
      "(Epoch 50 / 80) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 38301 / 61200) loss: 2.303011\n",
      "(Iteration 38401 / 61200) loss: 2.303063\n",
      "(Iteration 38501 / 61200) loss: 2.303098\n",
      "(Iteration 38601 / 61200) loss: 2.303080\n",
      "(Iteration 38701 / 61200) loss: 2.303016\n",
      "(Iteration 38801 / 61200) loss: 2.303081\n",
      "(Iteration 38901 / 61200) loss: 2.303142\n",
      "(Iteration 39001 / 61200) loss: 2.303107\n",
      "(Epoch 51 / 80) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 39101 / 61200) loss: 2.303056\n",
      "(Iteration 39201 / 61200) loss: 2.303033\n",
      "(Iteration 39301 / 61200) loss: 2.303092\n",
      "(Iteration 39401 / 61200) loss: 2.303011\n",
      "(Iteration 39501 / 61200) loss: 2.302961\n",
      "(Iteration 39601 / 61200) loss: 2.302993\n",
      "(Iteration 39701 / 61200) loss: 2.302988\n",
      "(Epoch 52 / 80) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 39801 / 61200) loss: 2.303135\n",
      "(Iteration 39901 / 61200) loss: 2.303033\n",
      "(Iteration 40001 / 61200) loss: 2.303031\n",
      "(Iteration 40101 / 61200) loss: 2.303190\n",
      "(Iteration 40201 / 61200) loss: 2.303058\n",
      "(Iteration 40301 / 61200) loss: 2.303101\n",
      "(Iteration 40401 / 61200) loss: 2.303069\n",
      "(Iteration 40501 / 61200) loss: 2.302895\n",
      "(Epoch 53 / 80) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 40601 / 61200) loss: 2.303098\n",
      "(Iteration 40701 / 61200) loss: 2.303060\n",
      "(Iteration 40801 / 61200) loss: 2.303012\n",
      "(Iteration 40901 / 61200) loss: 2.303136\n",
      "(Iteration 41001 / 61200) loss: 2.303115\n",
      "(Iteration 41101 / 61200) loss: 2.303078\n",
      "(Iteration 41201 / 61200) loss: 2.303054\n",
      "(Iteration 41301 / 61200) loss: 2.303124\n",
      "(Epoch 54 / 80) train acc: 0.112000; val_acc: 0.078000\n",
      "(Iteration 41401 / 61200) loss: 2.303147\n",
      "(Iteration 41501 / 61200) loss: 2.303037\n",
      "(Iteration 41601 / 61200) loss: 2.303096\n",
      "(Iteration 41701 / 61200) loss: 2.302971\n",
      "(Iteration 41801 / 61200) loss: 2.303156\n",
      "(Iteration 41901 / 61200) loss: 2.302950\n",
      "(Iteration 42001 / 61200) loss: 2.303037\n",
      "(Epoch 55 / 80) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 42101 / 61200) loss: 2.303049\n",
      "(Iteration 42201 / 61200) loss: 2.303155\n",
      "(Iteration 42301 / 61200) loss: 2.303092\n",
      "(Iteration 42401 / 61200) loss: 2.303130\n",
      "(Iteration 42501 / 61200) loss: 2.302962\n",
      "(Iteration 42601 / 61200) loss: 2.303226\n",
      "(Iteration 42701 / 61200) loss: 2.302889\n",
      "(Iteration 42801 / 61200) loss: 2.303005\n",
      "(Epoch 56 / 80) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 42901 / 61200) loss: 2.303134\n",
      "(Iteration 43001 / 61200) loss: 2.302983\n",
      "(Iteration 43101 / 61200) loss: 2.303141\n",
      "(Iteration 43201 / 61200) loss: 2.303046\n",
      "(Iteration 43301 / 61200) loss: 2.303096\n",
      "(Iteration 43401 / 61200) loss: 2.303004\n",
      "(Iteration 43501 / 61200) loss: 2.303072\n",
      "(Iteration 43601 / 61200) loss: 2.303000\n",
      "(Epoch 57 / 80) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 43701 / 61200) loss: 2.302991\n",
      "(Iteration 43801 / 61200) loss: 2.302980\n",
      "(Iteration 43901 / 61200) loss: 2.303100\n",
      "(Iteration 44001 / 61200) loss: 2.302979\n",
      "(Iteration 44101 / 61200) loss: 2.303086\n",
      "(Iteration 44201 / 61200) loss: 2.303107\n",
      "(Iteration 44301 / 61200) loss: 2.302979\n",
      "(Epoch 58 / 80) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 44401 / 61200) loss: 2.303135\n",
      "(Iteration 44501 / 61200) loss: 2.303066\n",
      "(Iteration 44601 / 61200) loss: 2.303100\n",
      "(Iteration 44701 / 61200) loss: 2.303036\n",
      "(Iteration 44801 / 61200) loss: 2.303079\n",
      "(Iteration 44901 / 61200) loss: 2.303010\n",
      "(Iteration 45001 / 61200) loss: 2.303102\n",
      "(Iteration 45101 / 61200) loss: 2.302976\n",
      "(Epoch 59 / 80) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 45201 / 61200) loss: 2.303095\n",
      "(Iteration 45301 / 61200) loss: 2.303084\n",
      "(Iteration 45401 / 61200) loss: 2.302955\n",
      "(Iteration 45501 / 61200) loss: 2.303104\n",
      "(Iteration 45601 / 61200) loss: 2.303064\n",
      "(Iteration 45701 / 61200) loss: 2.303076\n",
      "(Iteration 45801 / 61200) loss: 2.302918\n",
      "(Epoch 60 / 80) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 45901 / 61200) loss: 2.303015\n",
      "(Iteration 46001 / 61200) loss: 2.303043\n",
      "(Iteration 46101 / 61200) loss: 2.303116\n",
      "(Iteration 46201 / 61200) loss: 2.303041\n",
      "(Iteration 46301 / 61200) loss: 2.303016\n",
      "(Iteration 46401 / 61200) loss: 2.303025\n",
      "(Iteration 46501 / 61200) loss: 2.303039\n",
      "(Iteration 46601 / 61200) loss: 2.303033\n",
      "(Epoch 61 / 80) train acc: 0.082000; val_acc: 0.078000\n",
      "(Iteration 46701 / 61200) loss: 2.303012\n",
      "(Iteration 46801 / 61200) loss: 2.303190\n",
      "(Iteration 46901 / 61200) loss: 2.303002\n",
      "(Iteration 47001 / 61200) loss: 2.303160\n",
      "(Iteration 47101 / 61200) loss: 2.303141\n",
      "(Iteration 47201 / 61200) loss: 2.302894\n",
      "(Iteration 47301 / 61200) loss: 2.302980\n",
      "(Iteration 47401 / 61200) loss: 2.303112\n",
      "(Epoch 62 / 80) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 47501 / 61200) loss: 2.303149\n",
      "(Iteration 47601 / 61200) loss: 2.302956\n",
      "(Iteration 47701 / 61200) loss: 2.303036\n",
      "(Iteration 47801 / 61200) loss: 2.303122\n",
      "(Iteration 47901 / 61200) loss: 2.303043\n",
      "(Iteration 48001 / 61200) loss: 2.303036\n",
      "(Iteration 48101 / 61200) loss: 2.303005\n",
      "(Epoch 63 / 80) train acc: 0.086000; val_acc: 0.078000\n",
      "(Iteration 48201 / 61200) loss: 2.302915\n",
      "(Iteration 48301 / 61200) loss: 2.302988\n",
      "(Iteration 48401 / 61200) loss: 2.303007\n",
      "(Iteration 48501 / 61200) loss: 2.302983\n",
      "(Iteration 48601 / 61200) loss: 2.303052\n",
      "(Iteration 48701 / 61200) loss: 2.302983\n",
      "(Iteration 48801 / 61200) loss: 2.303068\n",
      "(Iteration 48901 / 61200) loss: 2.303151\n",
      "(Epoch 64 / 80) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 49001 / 61200) loss: 2.302988\n",
      "(Iteration 49101 / 61200) loss: 2.303122\n",
      "(Iteration 49201 / 61200) loss: 2.303106\n",
      "(Iteration 49301 / 61200) loss: 2.303052\n",
      "(Iteration 49401 / 61200) loss: 2.303135\n",
      "(Iteration 49501 / 61200) loss: 2.303074\n",
      "(Iteration 49601 / 61200) loss: 2.303012\n",
      "(Iteration 49701 / 61200) loss: 2.303058\n",
      "(Epoch 65 / 80) train acc: 0.111000; val_acc: 0.078000\n",
      "(Iteration 49801 / 61200) loss: 2.303065\n",
      "(Iteration 49901 / 61200) loss: 2.303051\n",
      "(Iteration 50001 / 61200) loss: 2.302977\n",
      "(Iteration 50101 / 61200) loss: 2.303038\n",
      "(Iteration 50201 / 61200) loss: 2.303050\n",
      "(Iteration 50301 / 61200) loss: 2.302924\n",
      "(Iteration 50401 / 61200) loss: 2.303104\n",
      "(Epoch 66 / 80) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 50501 / 61200) loss: 2.303072\n",
      "(Iteration 50601 / 61200) loss: 2.303023\n",
      "(Iteration 50701 / 61200) loss: 2.302947\n",
      "(Iteration 50801 / 61200) loss: 2.303049\n",
      "(Iteration 50901 / 61200) loss: 2.303019\n",
      "(Iteration 51001 / 61200) loss: 2.303084\n",
      "(Iteration 51101 / 61200) loss: 2.303079\n",
      "(Iteration 51201 / 61200) loss: 2.303020\n",
      "(Epoch 67 / 80) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 51301 / 61200) loss: 2.303013\n",
      "(Iteration 51401 / 61200) loss: 2.303101\n",
      "(Iteration 51501 / 61200) loss: 2.302936\n",
      "(Iteration 51601 / 61200) loss: 2.303006\n",
      "(Iteration 51701 / 61200) loss: 2.302976\n",
      "(Iteration 51801 / 61200) loss: 2.302934\n",
      "(Iteration 51901 / 61200) loss: 2.302950\n",
      "(Iteration 52001 / 61200) loss: 2.303030\n",
      "(Epoch 68 / 80) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 52101 / 61200) loss: 2.303063\n",
      "(Iteration 52201 / 61200) loss: 2.303004\n",
      "(Iteration 52301 / 61200) loss: 2.302967\n",
      "(Iteration 52401 / 61200) loss: 2.303049\n",
      "(Iteration 52501 / 61200) loss: 2.303124\n",
      "(Iteration 52601 / 61200) loss: 2.303141\n",
      "(Iteration 52701 / 61200) loss: 2.302993\n",
      "(Epoch 69 / 80) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 52801 / 61200) loss: 2.303114\n",
      "(Iteration 52901 / 61200) loss: 2.303049\n",
      "(Iteration 53001 / 61200) loss: 2.303043\n",
      "(Iteration 53101 / 61200) loss: 2.303030\n",
      "(Iteration 53201 / 61200) loss: 2.303051\n",
      "(Iteration 53301 / 61200) loss: 2.303044\n",
      "(Iteration 53401 / 61200) loss: 2.303190\n",
      "(Iteration 53501 / 61200) loss: 2.303018\n",
      "(Epoch 70 / 80) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 53601 / 61200) loss: 2.303177\n",
      "(Iteration 53701 / 61200) loss: 2.303041\n",
      "(Iteration 53801 / 61200) loss: 2.302973\n",
      "(Iteration 53901 / 61200) loss: 2.302953\n",
      "(Iteration 54001 / 61200) loss: 2.303038\n",
      "(Iteration 54101 / 61200) loss: 2.303054\n",
      "(Iteration 54201 / 61200) loss: 2.303093\n",
      "(Iteration 54301 / 61200) loss: 2.303121\n",
      "(Epoch 71 / 80) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 54401 / 61200) loss: 2.303033\n",
      "(Iteration 54501 / 61200) loss: 2.302981\n",
      "(Iteration 54601 / 61200) loss: 2.302947\n",
      "(Iteration 54701 / 61200) loss: 2.303161\n",
      "(Iteration 54801 / 61200) loss: 2.303230\n",
      "(Iteration 54901 / 61200) loss: 2.302983\n",
      "(Iteration 55001 / 61200) loss: 2.302987\n",
      "(Epoch 72 / 80) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 55101 / 61200) loss: 2.302920\n",
      "(Iteration 55201 / 61200) loss: 2.302908\n",
      "(Iteration 55301 / 61200) loss: 2.302922\n",
      "(Iteration 55401 / 61200) loss: 2.303055\n",
      "(Iteration 55501 / 61200) loss: 2.302949\n",
      "(Iteration 55601 / 61200) loss: 2.303148\n",
      "(Iteration 55701 / 61200) loss: 2.302978\n",
      "(Iteration 55801 / 61200) loss: 2.303041\n",
      "(Epoch 73 / 80) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 55901 / 61200) loss: 2.302922\n",
      "(Iteration 56001 / 61200) loss: 2.303032\n",
      "(Iteration 56101 / 61200) loss: 2.303053\n",
      "(Iteration 56201 / 61200) loss: 2.303079\n",
      "(Iteration 56301 / 61200) loss: 2.303159\n",
      "(Iteration 56401 / 61200) loss: 2.303046\n",
      "(Iteration 56501 / 61200) loss: 2.303027\n",
      "(Iteration 56601 / 61200) loss: 2.303224\n",
      "(Epoch 74 / 80) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 56701 / 61200) loss: 2.303165\n",
      "(Iteration 56801 / 61200) loss: 2.303096\n",
      "(Iteration 56901 / 61200) loss: 2.302950\n",
      "(Iteration 57001 / 61200) loss: 2.303069\n",
      "(Iteration 57101 / 61200) loss: 2.303005\n",
      "(Iteration 57201 / 61200) loss: 2.303021\n",
      "(Iteration 57301 / 61200) loss: 2.303037\n",
      "(Epoch 75 / 80) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 57401 / 61200) loss: 2.302910\n",
      "(Iteration 57501 / 61200) loss: 2.303115\n",
      "(Iteration 57601 / 61200) loss: 2.302974\n",
      "(Iteration 57701 / 61200) loss: 2.303064\n",
      "(Iteration 57801 / 61200) loss: 2.303010\n",
      "(Iteration 57901 / 61200) loss: 2.303036\n",
      "(Iteration 58001 / 61200) loss: 2.303052\n",
      "(Iteration 58101 / 61200) loss: 2.302980\n",
      "(Epoch 76 / 80) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 58201 / 61200) loss: 2.303049\n",
      "(Iteration 58301 / 61200) loss: 2.303054\n",
      "(Iteration 58401 / 61200) loss: 2.303120\n",
      "(Iteration 58501 / 61200) loss: 2.303016\n",
      "(Iteration 58601 / 61200) loss: 2.303011\n",
      "(Iteration 58701 / 61200) loss: 2.303025\n",
      "(Iteration 58801 / 61200) loss: 2.302931\n",
      "(Iteration 58901 / 61200) loss: 2.303003\n",
      "(Epoch 77 / 80) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 59001 / 61200) loss: 2.303004\n",
      "(Iteration 59101 / 61200) loss: 2.303046\n",
      "(Iteration 59201 / 61200) loss: 2.303050\n",
      "(Iteration 59301 / 61200) loss: 2.303089\n",
      "(Iteration 59401 / 61200) loss: 2.302939\n",
      "(Iteration 59501 / 61200) loss: 2.302955\n",
      "(Iteration 59601 / 61200) loss: 2.303129\n",
      "(Epoch 78 / 80) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 59701 / 61200) loss: 2.303025\n",
      "(Iteration 59801 / 61200) loss: 2.302871\n",
      "(Iteration 59901 / 61200) loss: 2.302846\n",
      "(Iteration 60001 / 61200) loss: 2.303137\n",
      "(Iteration 60101 / 61200) loss: 2.303259\n",
      "(Iteration 60201 / 61200) loss: 2.303034\n",
      "(Iteration 60301 / 61200) loss: 2.303028\n",
      "(Iteration 60401 / 61200) loss: 2.302934\n",
      "(Epoch 79 / 80) train acc: 0.115000; val_acc: 0.078000\n",
      "(Iteration 60501 / 61200) loss: 2.303134\n",
      "(Iteration 60601 / 61200) loss: 2.303003\n",
      "(Iteration 60701 / 61200) loss: 2.303002\n",
      "(Iteration 60801 / 61200) loss: 2.303023\n",
      "(Iteration 60901 / 61200) loss: 2.303212\n",
      "(Iteration 61001 / 61200) loss: 2.302951\n",
      "(Iteration 61101 / 61200) loss: 2.303062\n",
      "(Epoch 80 / 80) train acc: 0.097000; val_acc: 0.078000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.304613\n",
      "(Epoch 0 / 80) train acc: 0.092000; val_acc: 0.089000\n",
      "(Iteration 101 / 30560) loss: 2.304603\n",
      "(Iteration 201 / 30560) loss: 2.304571\n",
      "(Iteration 301 / 30560) loss: 2.304564\n",
      "(Epoch 1 / 80) train acc: 0.110000; val_acc: 0.094000\n",
      "(Iteration 401 / 30560) loss: 2.304527\n",
      "(Iteration 501 / 30560) loss: 2.304523\n",
      "(Iteration 601 / 30560) loss: 2.304494\n",
      "(Iteration 701 / 30560) loss: 2.304482\n",
      "(Epoch 2 / 80) train acc: 0.099000; val_acc: 0.104000\n",
      "(Iteration 801 / 30560) loss: 2.304468\n",
      "(Iteration 901 / 30560) loss: 2.304443\n",
      "(Iteration 1001 / 30560) loss: 2.304430\n",
      "(Iteration 1101 / 30560) loss: 2.304412\n",
      "(Epoch 3 / 80) train acc: 0.111000; val_acc: 0.107000\n",
      "(Iteration 1201 / 30560) loss: 2.304401\n",
      "(Iteration 1301 / 30560) loss: 2.304371\n",
      "(Iteration 1401 / 30560) loss: 2.304370\n",
      "(Iteration 1501 / 30560) loss: 2.304351\n",
      "(Epoch 4 / 80) train acc: 0.111000; val_acc: 0.083000\n",
      "(Iteration 1601 / 30560) loss: 2.304340\n",
      "(Iteration 1701 / 30560) loss: 2.304313\n",
      "(Iteration 1801 / 30560) loss: 2.304304\n",
      "(Iteration 1901 / 30560) loss: 2.304289\n",
      "(Epoch 5 / 80) train acc: 0.088000; val_acc: 0.084000\n",
      "(Iteration 2001 / 30560) loss: 2.304281\n",
      "(Iteration 2101 / 30560) loss: 2.304277\n",
      "(Iteration 2201 / 30560) loss: 2.304248\n",
      "(Epoch 6 / 80) train acc: 0.122000; val_acc: 0.103000\n",
      "(Iteration 2301 / 30560) loss: 2.304232\n",
      "(Iteration 2401 / 30560) loss: 2.304221\n",
      "(Iteration 2501 / 30560) loss: 2.304223\n",
      "(Iteration 2601 / 30560) loss: 2.304205\n",
      "(Epoch 7 / 80) train acc: 0.114000; val_acc: 0.102000\n",
      "(Iteration 2701 / 30560) loss: 2.304192\n",
      "(Iteration 2801 / 30560) loss: 2.304191\n",
      "(Iteration 2901 / 30560) loss: 2.304166\n",
      "(Iteration 3001 / 30560) loss: 2.304148\n",
      "(Epoch 8 / 80) train acc: 0.108000; val_acc: 0.095000\n",
      "(Iteration 3101 / 30560) loss: 2.304150\n",
      "(Iteration 3201 / 30560) loss: 2.304141\n",
      "(Iteration 3301 / 30560) loss: 2.304118\n",
      "(Iteration 3401 / 30560) loss: 2.304111\n",
      "(Epoch 9 / 80) train acc: 0.121000; val_acc: 0.100000\n",
      "(Iteration 3501 / 30560) loss: 2.304110\n",
      "(Iteration 3601 / 30560) loss: 2.304089\n",
      "(Iteration 3701 / 30560) loss: 2.304072\n",
      "(Iteration 3801 / 30560) loss: 2.304079\n",
      "(Epoch 10 / 80) train acc: 0.115000; val_acc: 0.094000\n",
      "(Iteration 3901 / 30560) loss: 2.304065\n",
      "(Iteration 4001 / 30560) loss: 2.304053\n",
      "(Iteration 4101 / 30560) loss: 2.304053\n",
      "(Iteration 4201 / 30560) loss: 2.304042\n",
      "(Epoch 11 / 80) train acc: 0.097000; val_acc: 0.104000\n",
      "(Iteration 4301 / 30560) loss: 2.304030\n",
      "(Iteration 4401 / 30560) loss: 2.304013\n",
      "(Iteration 4501 / 30560) loss: 2.304006\n",
      "(Epoch 12 / 80) train acc: 0.087000; val_acc: 0.084000\n",
      "(Iteration 4601 / 30560) loss: 2.304000\n",
      "(Iteration 4701 / 30560) loss: 2.303987\n",
      "(Iteration 4801 / 30560) loss: 2.303997\n",
      "(Iteration 4901 / 30560) loss: 2.303978\n",
      "(Epoch 13 / 80) train acc: 0.116000; val_acc: 0.085000\n",
      "(Iteration 5001 / 30560) loss: 2.303979\n",
      "(Iteration 5101 / 30560) loss: 2.303944\n",
      "(Iteration 5201 / 30560) loss: 2.303961\n",
      "(Iteration 5301 / 30560) loss: 2.303966\n",
      "(Epoch 14 / 80) train acc: 0.106000; val_acc: 0.080000\n",
      "(Iteration 5401 / 30560) loss: 2.303976\n",
      "(Iteration 5501 / 30560) loss: 2.303938\n",
      "(Iteration 5601 / 30560) loss: 2.303930\n",
      "(Iteration 5701 / 30560) loss: 2.303925\n",
      "(Epoch 15 / 80) train acc: 0.089000; val_acc: 0.080000\n",
      "(Iteration 5801 / 30560) loss: 2.303929\n",
      "(Iteration 5901 / 30560) loss: 2.303917\n",
      "(Iteration 6001 / 30560) loss: 2.303889\n",
      "(Iteration 6101 / 30560) loss: 2.303900\n",
      "(Epoch 16 / 80) train acc: 0.112000; val_acc: 0.084000\n",
      "(Iteration 6201 / 30560) loss: 2.303906\n",
      "(Iteration 6301 / 30560) loss: 2.303900\n",
      "(Iteration 6401 / 30560) loss: 2.303910\n",
      "(Epoch 17 / 80) train acc: 0.106000; val_acc: 0.083000\n",
      "(Iteration 6501 / 30560) loss: 2.303928\n",
      "(Iteration 6601 / 30560) loss: 2.303870\n",
      "(Iteration 6701 / 30560) loss: 2.303867\n",
      "(Iteration 6801 / 30560) loss: 2.303863\n",
      "(Epoch 18 / 80) train acc: 0.103000; val_acc: 0.080000\n",
      "(Iteration 6901 / 30560) loss: 2.303883\n",
      "(Iteration 7001 / 30560) loss: 2.303859\n",
      "(Iteration 7101 / 30560) loss: 2.303841\n",
      "(Iteration 7201 / 30560) loss: 2.303838\n",
      "(Epoch 19 / 80) train acc: 0.098000; val_acc: 0.080000\n",
      "(Iteration 7301 / 30560) loss: 2.303833\n",
      "(Iteration 7401 / 30560) loss: 2.303843\n",
      "(Iteration 7501 / 30560) loss: 2.303830\n",
      "(Iteration 7601 / 30560) loss: 2.303830\n",
      "(Epoch 20 / 80) train acc: 0.102000; val_acc: 0.082000\n",
      "(Iteration 7701 / 30560) loss: 2.303814\n",
      "(Iteration 7801 / 30560) loss: 2.303822\n",
      "(Iteration 7901 / 30560) loss: 2.303812\n",
      "(Iteration 8001 / 30560) loss: 2.303809\n",
      "(Epoch 21 / 80) train acc: 0.101000; val_acc: 0.080000\n",
      "(Iteration 8101 / 30560) loss: 2.303824\n",
      "(Iteration 8201 / 30560) loss: 2.303812\n",
      "(Iteration 8301 / 30560) loss: 2.303792\n",
      "(Iteration 8401 / 30560) loss: 2.303797\n",
      "(Epoch 22 / 80) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 8501 / 30560) loss: 2.303781\n",
      "(Iteration 8601 / 30560) loss: 2.303789\n",
      "(Iteration 8701 / 30560) loss: 2.303773\n",
      "(Epoch 23 / 80) train acc: 0.105000; val_acc: 0.080000\n",
      "(Iteration 8801 / 30560) loss: 2.303779\n",
      "(Iteration 8901 / 30560) loss: 2.303798\n",
      "(Iteration 9001 / 30560) loss: 2.303780\n",
      "(Iteration 9101 / 30560) loss: 2.303759\n",
      "(Epoch 24 / 80) train acc: 0.097000; val_acc: 0.082000\n",
      "(Iteration 9201 / 30560) loss: 2.303756\n",
      "(Iteration 9301 / 30560) loss: 2.303760\n",
      "(Iteration 9401 / 30560) loss: 2.303751\n",
      "(Iteration 9501 / 30560) loss: 2.303725\n",
      "(Epoch 25 / 80) train acc: 0.112000; val_acc: 0.078000\n",
      "(Iteration 9601 / 30560) loss: 2.303759\n",
      "(Iteration 9701 / 30560) loss: 2.303740\n",
      "(Iteration 9801 / 30560) loss: 2.303743\n",
      "(Iteration 9901 / 30560) loss: 2.303735\n",
      "(Epoch 26 / 80) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 10001 / 30560) loss: 2.303729\n",
      "(Iteration 10101 / 30560) loss: 2.303764\n",
      "(Iteration 10201 / 30560) loss: 2.303730\n",
      "(Iteration 10301 / 30560) loss: 2.303723\n",
      "(Epoch 27 / 80) train acc: 0.090000; val_acc: 0.079000\n",
      "(Iteration 10401 / 30560) loss: 2.303708\n",
      "(Iteration 10501 / 30560) loss: 2.303709\n",
      "(Iteration 10601 / 30560) loss: 2.303742\n",
      "(Epoch 28 / 80) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 10701 / 30560) loss: 2.303712\n",
      "(Iteration 10801 / 30560) loss: 2.303714\n",
      "(Iteration 10901 / 30560) loss: 2.303692\n",
      "(Iteration 11001 / 30560) loss: 2.303709\n",
      "(Epoch 29 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 11101 / 30560) loss: 2.303718\n",
      "(Iteration 11201 / 30560) loss: 2.303674\n",
      "(Iteration 11301 / 30560) loss: 2.303716\n",
      "(Iteration 11401 / 30560) loss: 2.303681\n",
      "(Epoch 30 / 80) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 11501 / 30560) loss: 2.303714\n",
      "(Iteration 11601 / 30560) loss: 2.303681\n",
      "(Iteration 11701 / 30560) loss: 2.303706\n",
      "(Iteration 11801 / 30560) loss: 2.303689\n",
      "(Epoch 31 / 80) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 11901 / 30560) loss: 2.303704\n",
      "(Iteration 12001 / 30560) loss: 2.303703\n",
      "(Iteration 12101 / 30560) loss: 2.303696\n",
      "(Iteration 12201 / 30560) loss: 2.303672\n",
      "(Epoch 32 / 80) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 12301 / 30560) loss: 2.303681\n",
      "(Iteration 12401 / 30560) loss: 2.303683\n",
      "(Iteration 12501 / 30560) loss: 2.303665\n",
      "(Iteration 12601 / 30560) loss: 2.303658\n",
      "(Epoch 33 / 80) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 12701 / 30560) loss: 2.303670\n",
      "(Iteration 12801 / 30560) loss: 2.303674\n",
      "(Iteration 12901 / 30560) loss: 2.303650\n",
      "(Epoch 34 / 80) train acc: 0.097000; val_acc: 0.077000\n",
      "(Iteration 13001 / 30560) loss: 2.303693\n",
      "(Iteration 13101 / 30560) loss: 2.303665\n",
      "(Iteration 13201 / 30560) loss: 2.303657\n",
      "(Iteration 13301 / 30560) loss: 2.303645\n",
      "(Epoch 35 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 13401 / 30560) loss: 2.303669\n",
      "(Iteration 13501 / 30560) loss: 2.303640\n",
      "(Iteration 13601 / 30560) loss: 2.303679\n",
      "(Iteration 13701 / 30560) loss: 2.303659\n",
      "(Epoch 36 / 80) train acc: 0.089000; val_acc: 0.079000\n",
      "(Iteration 13801 / 30560) loss: 2.303642\n",
      "(Iteration 13901 / 30560) loss: 2.303642\n",
      "(Iteration 14001 / 30560) loss: 2.303640\n",
      "(Iteration 14101 / 30560) loss: 2.303633\n",
      "(Epoch 37 / 80) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 14201 / 30560) loss: 2.303637\n",
      "(Iteration 14301 / 30560) loss: 2.303646\n",
      "(Iteration 14401 / 30560) loss: 2.303614\n",
      "(Iteration 14501 / 30560) loss: 2.303642\n",
      "(Epoch 38 / 80) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 14601 / 30560) loss: 2.303642\n",
      "(Iteration 14701 / 30560) loss: 2.303651\n",
      "(Iteration 14801 / 30560) loss: 2.303629\n",
      "(Epoch 39 / 80) train acc: 0.121000; val_acc: 0.080000\n",
      "(Iteration 14901 / 30560) loss: 2.303622\n",
      "(Iteration 15001 / 30560) loss: 2.303630\n",
      "(Iteration 15101 / 30560) loss: 2.303655\n",
      "(Iteration 15201 / 30560) loss: 2.303625\n",
      "(Epoch 40 / 80) train acc: 0.096000; val_acc: 0.080000\n",
      "(Iteration 15301 / 30560) loss: 2.303638\n",
      "(Iteration 15401 / 30560) loss: 2.303632\n",
      "(Iteration 15501 / 30560) loss: 2.303622\n",
      "(Iteration 15601 / 30560) loss: 2.303620\n",
      "(Epoch 41 / 80) train acc: 0.109000; val_acc: 0.080000\n",
      "(Iteration 15701 / 30560) loss: 2.303607\n",
      "(Iteration 15801 / 30560) loss: 2.303617\n",
      "(Iteration 15901 / 30560) loss: 2.303634\n",
      "(Iteration 16001 / 30560) loss: 2.303607\n",
      "(Epoch 42 / 80) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 16101 / 30560) loss: 2.303628\n",
      "(Iteration 16201 / 30560) loss: 2.303610\n",
      "(Iteration 16301 / 30560) loss: 2.303617\n",
      "(Iteration 16401 / 30560) loss: 2.303612\n",
      "(Epoch 43 / 80) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 16501 / 30560) loss: 2.303606\n",
      "(Iteration 16601 / 30560) loss: 2.303600\n",
      "(Iteration 16701 / 30560) loss: 2.303607\n",
      "(Iteration 16801 / 30560) loss: 2.303632\n",
      "(Epoch 44 / 80) train acc: 0.125000; val_acc: 0.078000\n",
      "(Iteration 16901 / 30560) loss: 2.303593\n",
      "(Iteration 17001 / 30560) loss: 2.303601\n",
      "(Iteration 17101 / 30560) loss: 2.303570\n",
      "(Epoch 45 / 80) train acc: 0.085000; val_acc: 0.078000\n",
      "(Iteration 17201 / 30560) loss: 2.303576\n",
      "(Iteration 17301 / 30560) loss: 2.303579\n",
      "(Iteration 17401 / 30560) loss: 2.303604\n",
      "(Iteration 17501 / 30560) loss: 2.303580\n",
      "(Epoch 46 / 80) train acc: 0.114000; val_acc: 0.079000\n",
      "(Iteration 17601 / 30560) loss: 2.303592\n",
      "(Iteration 17701 / 30560) loss: 2.303603\n",
      "(Iteration 17801 / 30560) loss: 2.303572\n",
      "(Iteration 17901 / 30560) loss: 2.303568\n",
      "(Epoch 47 / 80) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 18001 / 30560) loss: 2.303608\n",
      "(Iteration 18101 / 30560) loss: 2.303598\n",
      "(Iteration 18201 / 30560) loss: 2.303591\n",
      "(Iteration 18301 / 30560) loss: 2.303635\n",
      "(Epoch 48 / 80) train acc: 0.091000; val_acc: 0.079000\n",
      "(Iteration 18401 / 30560) loss: 2.303569\n",
      "(Iteration 18501 / 30560) loss: 2.303577\n",
      "(Iteration 18601 / 30560) loss: 2.303600\n",
      "(Iteration 18701 / 30560) loss: 2.303572\n",
      "(Epoch 49 / 80) train acc: 0.107000; val_acc: 0.080000\n",
      "(Iteration 18801 / 30560) loss: 2.303584\n",
      "(Iteration 18901 / 30560) loss: 2.303588\n",
      "(Iteration 19001 / 30560) loss: 2.303590\n",
      "(Epoch 50 / 80) train acc: 0.089000; val_acc: 0.080000\n",
      "(Iteration 19101 / 30560) loss: 2.303579\n",
      "(Iteration 19201 / 30560) loss: 2.303577\n",
      "(Iteration 19301 / 30560) loss: 2.303592\n",
      "(Iteration 19401 / 30560) loss: 2.303600\n",
      "(Epoch 51 / 80) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 19501 / 30560) loss: 2.303580\n",
      "(Iteration 19601 / 30560) loss: 2.303612\n",
      "(Iteration 19701 / 30560) loss: 2.303596\n",
      "(Iteration 19801 / 30560) loss: 2.303584\n",
      "(Epoch 52 / 80) train acc: 0.096000; val_acc: 0.080000\n",
      "(Iteration 19901 / 30560) loss: 2.303601\n",
      "(Iteration 20001 / 30560) loss: 2.303563\n",
      "(Iteration 20101 / 30560) loss: 2.303568\n",
      "(Iteration 20201 / 30560) loss: 2.303595\n",
      "(Epoch 53 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 20301 / 30560) loss: 2.303574\n",
      "(Iteration 20401 / 30560) loss: 2.303589\n",
      "(Iteration 20501 / 30560) loss: 2.303598\n",
      "(Iteration 20601 / 30560) loss: 2.303557\n",
      "(Epoch 54 / 80) train acc: 0.092000; val_acc: 0.080000\n",
      "(Iteration 20701 / 30560) loss: 2.303598\n",
      "(Iteration 20801 / 30560) loss: 2.303544\n",
      "(Iteration 20901 / 30560) loss: 2.303586\n",
      "(Iteration 21001 / 30560) loss: 2.303576\n",
      "(Epoch 55 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 21101 / 30560) loss: 2.303573\n",
      "(Iteration 21201 / 30560) loss: 2.303576\n",
      "(Iteration 21301 / 30560) loss: 2.303588\n",
      "(Epoch 56 / 80) train acc: 0.083000; val_acc: 0.079000\n",
      "(Iteration 21401 / 30560) loss: 2.303560\n",
      "(Iteration 21501 / 30560) loss: 2.303559\n",
      "(Iteration 21601 / 30560) loss: 2.303583\n",
      "(Iteration 21701 / 30560) loss: 2.303573\n",
      "(Epoch 57 / 80) train acc: 0.080000; val_acc: 0.079000\n",
      "(Iteration 21801 / 30560) loss: 2.303574\n",
      "(Iteration 21901 / 30560) loss: 2.303592\n",
      "(Iteration 22001 / 30560) loss: 2.303568\n",
      "(Iteration 22101 / 30560) loss: 2.303570\n",
      "(Epoch 58 / 80) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 22201 / 30560) loss: 2.303532\n",
      "(Iteration 22301 / 30560) loss: 2.303562\n",
      "(Iteration 22401 / 30560) loss: 2.303576\n",
      "(Iteration 22501 / 30560) loss: 2.303567\n",
      "(Epoch 59 / 80) train acc: 0.103000; val_acc: 0.080000\n",
      "(Iteration 22601 / 30560) loss: 2.303571\n",
      "(Iteration 22701 / 30560) loss: 2.303525\n",
      "(Iteration 22801 / 30560) loss: 2.303555\n",
      "(Iteration 22901 / 30560) loss: 2.303537\n",
      "(Epoch 60 / 80) train acc: 0.097000; val_acc: 0.080000\n",
      "(Iteration 23001 / 30560) loss: 2.303532\n",
      "(Iteration 23101 / 30560) loss: 2.303537\n",
      "(Iteration 23201 / 30560) loss: 2.303554\n",
      "(Iteration 23301 / 30560) loss: 2.303582\n",
      "(Epoch 61 / 80) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 23401 / 30560) loss: 2.303517\n",
      "(Iteration 23501 / 30560) loss: 2.303547\n",
      "(Iteration 23601 / 30560) loss: 2.303584\n",
      "(Epoch 62 / 80) train acc: 0.091000; val_acc: 0.079000\n",
      "(Iteration 23701 / 30560) loss: 2.303535\n",
      "(Iteration 23801 / 30560) loss: 2.303556\n",
      "(Iteration 23901 / 30560) loss: 2.303571\n",
      "(Iteration 24001 / 30560) loss: 2.303534\n",
      "(Epoch 63 / 80) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 24101 / 30560) loss: 2.303552\n",
      "(Iteration 24201 / 30560) loss: 2.303555\n",
      "(Iteration 24301 / 30560) loss: 2.303548\n",
      "(Iteration 24401 / 30560) loss: 2.303536\n",
      "(Epoch 64 / 80) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 24501 / 30560) loss: 2.303570\n",
      "(Iteration 24601 / 30560) loss: 2.303543\n",
      "(Iteration 24701 / 30560) loss: 2.303584\n",
      "(Iteration 24801 / 30560) loss: 2.303527\n",
      "(Epoch 65 / 80) train acc: 0.093000; val_acc: 0.079000\n",
      "(Iteration 24901 / 30560) loss: 2.303542\n",
      "(Iteration 25001 / 30560) loss: 2.303540\n",
      "(Iteration 25101 / 30560) loss: 2.303548\n",
      "(Iteration 25201 / 30560) loss: 2.303579\n",
      "(Epoch 66 / 80) train acc: 0.091000; val_acc: 0.079000\n",
      "(Iteration 25301 / 30560) loss: 2.303553\n",
      "(Iteration 25401 / 30560) loss: 2.303534\n",
      "(Iteration 25501 / 30560) loss: 2.303561\n",
      "(Epoch 67 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 25601 / 30560) loss: 2.303535\n",
      "(Iteration 25701 / 30560) loss: 2.303536\n",
      "(Iteration 25801 / 30560) loss: 2.303558\n",
      "(Iteration 25901 / 30560) loss: 2.303551\n",
      "(Epoch 68 / 80) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 26001 / 30560) loss: 2.303533\n",
      "(Iteration 26101 / 30560) loss: 2.303585\n",
      "(Iteration 26201 / 30560) loss: 2.303518\n",
      "(Iteration 26301 / 30560) loss: 2.303543\n",
      "(Epoch 69 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 26401 / 30560) loss: 2.303546\n",
      "(Iteration 26501 / 30560) loss: 2.303581\n",
      "(Iteration 26601 / 30560) loss: 2.303566\n",
      "(Iteration 26701 / 30560) loss: 2.303567\n",
      "(Epoch 70 / 80) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 26801 / 30560) loss: 2.303564\n",
      "(Iteration 26901 / 30560) loss: 2.303513\n",
      "(Iteration 27001 / 30560) loss: 2.303539\n",
      "(Iteration 27101 / 30560) loss: 2.303516\n",
      "(Epoch 71 / 80) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 27201 / 30560) loss: 2.303540\n",
      "(Iteration 27301 / 30560) loss: 2.303525\n",
      "(Iteration 27401 / 30560) loss: 2.303542\n",
      "(Iteration 27501 / 30560) loss: 2.303550\n",
      "(Epoch 72 / 80) train acc: 0.092000; val_acc: 0.079000\n",
      "(Iteration 27601 / 30560) loss: 2.303534\n",
      "(Iteration 27701 / 30560) loss: 2.303533\n",
      "(Iteration 27801 / 30560) loss: 2.303539\n",
      "(Epoch 73 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 27901 / 30560) loss: 2.303563\n",
      "(Iteration 28001 / 30560) loss: 2.303544\n",
      "(Iteration 28101 / 30560) loss: 2.303542\n",
      "(Iteration 28201 / 30560) loss: 2.303526\n",
      "(Epoch 74 / 80) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 28301 / 30560) loss: 2.303576\n",
      "(Iteration 28401 / 30560) loss: 2.303550\n",
      "(Iteration 28501 / 30560) loss: 2.303551\n",
      "(Iteration 28601 / 30560) loss: 2.303555\n",
      "(Epoch 75 / 80) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 28701 / 30560) loss: 2.303545\n",
      "(Iteration 28801 / 30560) loss: 2.303540\n",
      "(Iteration 28901 / 30560) loss: 2.303526\n",
      "(Iteration 29001 / 30560) loss: 2.303556\n",
      "(Epoch 76 / 80) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 29101 / 30560) loss: 2.303536\n",
      "(Iteration 29201 / 30560) loss: 2.303500\n",
      "(Iteration 29301 / 30560) loss: 2.303552\n",
      "(Iteration 29401 / 30560) loss: 2.303558\n",
      "(Epoch 77 / 80) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 29501 / 30560) loss: 2.303549\n",
      "(Iteration 29601 / 30560) loss: 2.303544\n",
      "(Iteration 29701 / 30560) loss: 2.303515\n",
      "(Epoch 78 / 80) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 29801 / 30560) loss: 2.303526\n",
      "(Iteration 29901 / 30560) loss: 2.303529\n",
      "(Iteration 30001 / 30560) loss: 2.303560\n",
      "(Iteration 30101 / 30560) loss: 2.303541\n",
      "(Epoch 79 / 80) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 30201 / 30560) loss: 2.303513\n",
      "(Iteration 30301 / 30560) loss: 2.303543\n",
      "(Iteration 30401 / 30560) loss: 2.303571\n",
      "(Iteration 30501 / 30560) loss: 2.303548\n",
      "(Epoch 80 / 80) train acc: 0.075000; val_acc: 0.078000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.305474\n",
      "(Epoch 0 / 80) train acc: 0.091000; val_acc: 0.096000\n",
      "(Iteration 101 / 61200) loss: 2.305444\n",
      "(Iteration 201 / 61200) loss: 2.305397\n",
      "(Iteration 301 / 61200) loss: 2.305361\n",
      "(Iteration 401 / 61200) loss: 2.305330\n",
      "(Iteration 501 / 61200) loss: 2.305287\n",
      "(Iteration 601 / 61200) loss: 2.305243\n",
      "(Iteration 701 / 61200) loss: 2.305197\n",
      "(Epoch 1 / 80) train acc: 0.092000; val_acc: 0.082000\n",
      "(Iteration 801 / 61200) loss: 2.305172\n",
      "(Iteration 901 / 61200) loss: 2.305147\n",
      "(Iteration 1001 / 61200) loss: 2.305108\n",
      "(Iteration 1101 / 61200) loss: 2.305081\n",
      "(Iteration 1201 / 61200) loss: 2.305049\n",
      "(Iteration 1301 / 61200) loss: 2.305019\n",
      "(Iteration 1401 / 61200) loss: 2.304992\n",
      "(Iteration 1501 / 61200) loss: 2.304970\n",
      "(Epoch 2 / 80) train acc: 0.091000; val_acc: 0.096000\n",
      "(Iteration 1601 / 61200) loss: 2.304933\n",
      "(Iteration 1701 / 61200) loss: 2.304902\n",
      "(Iteration 1801 / 61200) loss: 2.304858\n",
      "(Iteration 1901 / 61200) loss: 2.304869\n",
      "(Iteration 2001 / 61200) loss: 2.304807\n",
      "(Iteration 2101 / 61200) loss: 2.304776\n",
      "(Iteration 2201 / 61200) loss: 2.304771\n",
      "(Epoch 3 / 80) train acc: 0.092000; val_acc: 0.093000\n",
      "(Iteration 2301 / 61200) loss: 2.304703\n",
      "(Iteration 2401 / 61200) loss: 2.304730\n",
      "(Iteration 2501 / 61200) loss: 2.304737\n",
      "(Iteration 2601 / 61200) loss: 2.304731\n",
      "(Iteration 2701 / 61200) loss: 2.304693\n",
      "(Iteration 2801 / 61200) loss: 2.304697\n",
      "(Iteration 2901 / 61200) loss: 2.304652\n",
      "(Iteration 3001 / 61200) loss: 2.304607\n",
      "(Epoch 4 / 80) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 3101 / 61200) loss: 2.304552\n",
      "(Iteration 3201 / 61200) loss: 2.304577\n",
      "(Iteration 3301 / 61200) loss: 2.304543\n",
      "(Iteration 3401 / 61200) loss: 2.304545\n",
      "(Iteration 3501 / 61200) loss: 2.304531\n",
      "(Iteration 3601 / 61200) loss: 2.304513\n",
      "(Iteration 3701 / 61200) loss: 2.304437\n",
      "(Iteration 3801 / 61200) loss: 2.304451\n",
      "(Epoch 5 / 80) train acc: 0.095000; val_acc: 0.102000\n",
      "(Iteration 3901 / 61200) loss: 2.304437\n",
      "(Iteration 4001 / 61200) loss: 2.304432\n",
      "(Iteration 4101 / 61200) loss: 2.304398\n",
      "(Iteration 4201 / 61200) loss: 2.304432\n",
      "(Iteration 4301 / 61200) loss: 2.304402\n",
      "(Iteration 4401 / 61200) loss: 2.304376\n",
      "(Iteration 4501 / 61200) loss: 2.304365\n",
      "(Epoch 6 / 80) train acc: 0.094000; val_acc: 0.103000\n",
      "(Iteration 4601 / 61200) loss: 2.304312\n",
      "(Iteration 4701 / 61200) loss: 2.304340\n",
      "(Iteration 4801 / 61200) loss: 2.304304\n",
      "(Iteration 4901 / 61200) loss: 2.304326\n",
      "(Iteration 5001 / 61200) loss: 2.304298\n",
      "(Iteration 5101 / 61200) loss: 2.304226\n",
      "(Iteration 5201 / 61200) loss: 2.304212\n",
      "(Iteration 5301 / 61200) loss: 2.304295\n",
      "(Epoch 7 / 80) train acc: 0.092000; val_acc: 0.102000\n",
      "(Iteration 5401 / 61200) loss: 2.304170\n",
      "(Iteration 5501 / 61200) loss: 2.304247\n",
      "(Iteration 5601 / 61200) loss: 2.304159\n",
      "(Iteration 5701 / 61200) loss: 2.304223\n",
      "(Iteration 5801 / 61200) loss: 2.304185\n",
      "(Iteration 5901 / 61200) loss: 2.304210\n",
      "(Iteration 6001 / 61200) loss: 2.304143\n",
      "(Iteration 6101 / 61200) loss: 2.304181\n",
      "(Epoch 8 / 80) train acc: 0.088000; val_acc: 0.102000\n",
      "(Iteration 6201 / 61200) loss: 2.304130\n",
      "(Iteration 6301 / 61200) loss: 2.304102\n",
      "(Iteration 6401 / 61200) loss: 2.304101\n",
      "(Iteration 6501 / 61200) loss: 2.304133\n",
      "(Iteration 6601 / 61200) loss: 2.304129\n",
      "(Iteration 6701 / 61200) loss: 2.304080\n",
      "(Iteration 6801 / 61200) loss: 2.304159\n",
      "(Epoch 9 / 80) train acc: 0.097000; val_acc: 0.102000\n",
      "(Iteration 6901 / 61200) loss: 2.304017\n",
      "(Iteration 7001 / 61200) loss: 2.304033\n",
      "(Iteration 7101 / 61200) loss: 2.304123\n",
      "(Iteration 7201 / 61200) loss: 2.304054\n",
      "(Iteration 7301 / 61200) loss: 2.304125\n",
      "(Iteration 7401 / 61200) loss: 2.304129\n",
      "(Iteration 7501 / 61200) loss: 2.304036\n",
      "(Iteration 7601 / 61200) loss: 2.304104\n",
      "(Epoch 10 / 80) train acc: 0.105000; val_acc: 0.102000\n",
      "(Iteration 7701 / 61200) loss: 2.303988\n",
      "(Iteration 7801 / 61200) loss: 2.304013\n",
      "(Iteration 7901 / 61200) loss: 2.303994\n",
      "(Iteration 8001 / 61200) loss: 2.303991\n",
      "(Iteration 8101 / 61200) loss: 2.303966\n",
      "(Iteration 8201 / 61200) loss: 2.304022\n",
      "(Iteration 8301 / 61200) loss: 2.303925\n",
      "(Iteration 8401 / 61200) loss: 2.303945\n",
      "(Epoch 11 / 80) train acc: 0.090000; val_acc: 0.102000\n",
      "(Iteration 8501 / 61200) loss: 2.303944\n",
      "(Iteration 8601 / 61200) loss: 2.303943\n",
      "(Iteration 8701 / 61200) loss: 2.303941\n",
      "(Iteration 8801 / 61200) loss: 2.304004\n",
      "(Iteration 8901 / 61200) loss: 2.303926\n",
      "(Iteration 9001 / 61200) loss: 2.303938\n",
      "(Iteration 9101 / 61200) loss: 2.303868\n",
      "(Epoch 12 / 80) train acc: 0.099000; val_acc: 0.102000\n",
      "(Iteration 9201 / 61200) loss: 2.303925\n",
      "(Iteration 9301 / 61200) loss: 2.303887\n",
      "(Iteration 9401 / 61200) loss: 2.303862\n",
      "(Iteration 9501 / 61200) loss: 2.303951\n",
      "(Iteration 9601 / 61200) loss: 2.303856\n",
      "(Iteration 9701 / 61200) loss: 2.303907\n",
      "(Iteration 9801 / 61200) loss: 2.303882\n",
      "(Iteration 9901 / 61200) loss: 2.303899\n",
      "(Epoch 13 / 80) train acc: 0.111000; val_acc: 0.104000\n",
      "(Iteration 10001 / 61200) loss: 2.303907\n",
      "(Iteration 10101 / 61200) loss: 2.303904\n",
      "(Iteration 10201 / 61200) loss: 2.303900\n",
      "(Iteration 10301 / 61200) loss: 2.303861\n",
      "(Iteration 10401 / 61200) loss: 2.303861\n",
      "(Iteration 10501 / 61200) loss: 2.303845\n",
      "(Iteration 10601 / 61200) loss: 2.303796\n",
      "(Iteration 10701 / 61200) loss: 2.303814\n",
      "(Epoch 14 / 80) train acc: 0.089000; val_acc: 0.106000\n",
      "(Iteration 10801 / 61200) loss: 2.303860\n",
      "(Iteration 10901 / 61200) loss: 2.303795\n",
      "(Iteration 11001 / 61200) loss: 2.303757\n",
      "(Iteration 11101 / 61200) loss: 2.303881\n",
      "(Iteration 11201 / 61200) loss: 2.303918\n",
      "(Iteration 11301 / 61200) loss: 2.303761\n",
      "(Iteration 11401 / 61200) loss: 2.303815\n",
      "(Epoch 15 / 80) train acc: 0.097000; val_acc: 0.104000\n",
      "(Iteration 11501 / 61200) loss: 2.303801\n",
      "(Iteration 11601 / 61200) loss: 2.303782\n",
      "(Iteration 11701 / 61200) loss: 2.303793\n",
      "(Iteration 11801 / 61200) loss: 2.303775\n",
      "(Iteration 11901 / 61200) loss: 2.303721\n",
      "(Iteration 12001 / 61200) loss: 2.303766\n",
      "(Iteration 12101 / 61200) loss: 2.303850\n",
      "(Iteration 12201 / 61200) loss: 2.303762\n",
      "(Epoch 16 / 80) train acc: 0.117000; val_acc: 0.103000\n",
      "(Iteration 12301 / 61200) loss: 2.303817\n",
      "(Iteration 12401 / 61200) loss: 2.303811\n",
      "(Iteration 12501 / 61200) loss: 2.303784\n",
      "(Iteration 12601 / 61200) loss: 2.303804\n",
      "(Iteration 12701 / 61200) loss: 2.303830\n",
      "(Iteration 12801 / 61200) loss: 2.303756\n",
      "(Iteration 12901 / 61200) loss: 2.303730\n",
      "(Iteration 13001 / 61200) loss: 2.303776\n",
      "(Epoch 17 / 80) train acc: 0.110000; val_acc: 0.104000\n",
      "(Iteration 13101 / 61200) loss: 2.303688\n",
      "(Iteration 13201 / 61200) loss: 2.303790\n",
      "(Iteration 13301 / 61200) loss: 2.303753\n",
      "(Iteration 13401 / 61200) loss: 2.303742\n",
      "(Iteration 13501 / 61200) loss: 2.303735\n",
      "(Iteration 13601 / 61200) loss: 2.303776\n",
      "(Iteration 13701 / 61200) loss: 2.303685\n",
      "(Epoch 18 / 80) train acc: 0.106000; val_acc: 0.112000\n",
      "(Iteration 13801 / 61200) loss: 2.303796\n",
      "(Iteration 13901 / 61200) loss: 2.303710\n",
      "(Iteration 14001 / 61200) loss: 2.303747\n",
      "(Iteration 14101 / 61200) loss: 2.303809\n",
      "(Iteration 14201 / 61200) loss: 2.303806\n",
      "(Iteration 14301 / 61200) loss: 2.303778\n",
      "(Iteration 14401 / 61200) loss: 2.303729\n",
      "(Iteration 14501 / 61200) loss: 2.303672\n",
      "(Epoch 19 / 80) train acc: 0.121000; val_acc: 0.105000\n",
      "(Iteration 14601 / 61200) loss: 2.303634\n",
      "(Iteration 14701 / 61200) loss: 2.303743\n",
      "(Iteration 14801 / 61200) loss: 2.303693\n",
      "(Iteration 14901 / 61200) loss: 2.303790\n",
      "(Iteration 15001 / 61200) loss: 2.303725\n",
      "(Iteration 15101 / 61200) loss: 2.303657\n",
      "(Iteration 15201 / 61200) loss: 2.303711\n",
      "(Epoch 20 / 80) train acc: 0.106000; val_acc: 0.105000\n",
      "(Iteration 15301 / 61200) loss: 2.303740\n",
      "(Iteration 15401 / 61200) loss: 2.303733\n",
      "(Iteration 15501 / 61200) loss: 2.303694\n",
      "(Iteration 15601 / 61200) loss: 2.303690\n",
      "(Iteration 15701 / 61200) loss: 2.303673\n",
      "(Iteration 15801 / 61200) loss: 2.303748\n",
      "(Iteration 15901 / 61200) loss: 2.303683\n",
      "(Iteration 16001 / 61200) loss: 2.303714\n",
      "(Epoch 21 / 80) train acc: 0.112000; val_acc: 0.105000\n",
      "(Iteration 16101 / 61200) loss: 2.303651\n",
      "(Iteration 16201 / 61200) loss: 2.303669\n",
      "(Iteration 16301 / 61200) loss: 2.303748\n",
      "(Iteration 16401 / 61200) loss: 2.303674\n",
      "(Iteration 16501 / 61200) loss: 2.303682\n",
      "(Iteration 16601 / 61200) loss: 2.303673\n",
      "(Iteration 16701 / 61200) loss: 2.303697\n",
      "(Iteration 16801 / 61200) loss: 2.303637\n",
      "(Epoch 22 / 80) train acc: 0.112000; val_acc: 0.105000\n",
      "(Iteration 16901 / 61200) loss: 2.303706\n",
      "(Iteration 17001 / 61200) loss: 2.303664\n",
      "(Iteration 17101 / 61200) loss: 2.303758\n",
      "(Iteration 17201 / 61200) loss: 2.303677\n",
      "(Iteration 17301 / 61200) loss: 2.303681\n",
      "(Iteration 17401 / 61200) loss: 2.303661\n",
      "(Iteration 17501 / 61200) loss: 2.303652\n",
      "(Epoch 23 / 80) train acc: 0.101000; val_acc: 0.103000\n",
      "(Iteration 17601 / 61200) loss: 2.303690\n",
      "(Iteration 17701 / 61200) loss: 2.303668\n",
      "(Iteration 17801 / 61200) loss: 2.303605\n",
      "(Iteration 17901 / 61200) loss: 2.303674\n",
      "(Iteration 18001 / 61200) loss: 2.303653\n",
      "(Iteration 18101 / 61200) loss: 2.303644\n",
      "(Iteration 18201 / 61200) loss: 2.303673\n",
      "(Iteration 18301 / 61200) loss: 2.303653\n",
      "(Epoch 24 / 80) train acc: 0.090000; val_acc: 0.103000\n",
      "(Iteration 18401 / 61200) loss: 2.303624\n",
      "(Iteration 18501 / 61200) loss: 2.303606\n",
      "(Iteration 18601 / 61200) loss: 2.303707\n",
      "(Iteration 18701 / 61200) loss: 2.303582\n",
      "(Iteration 18801 / 61200) loss: 2.303683\n",
      "(Iteration 18901 / 61200) loss: 2.303689\n",
      "(Iteration 19001 / 61200) loss: 2.303609\n",
      "(Iteration 19101 / 61200) loss: 2.303642\n",
      "(Epoch 25 / 80) train acc: 0.110000; val_acc: 0.105000\n",
      "(Iteration 19201 / 61200) loss: 2.303687\n",
      "(Iteration 19301 / 61200) loss: 2.303690\n",
      "(Iteration 19401 / 61200) loss: 2.303648\n",
      "(Iteration 19501 / 61200) loss: 2.303654\n",
      "(Iteration 19601 / 61200) loss: 2.303641\n",
      "(Iteration 19701 / 61200) loss: 2.303600\n",
      "(Iteration 19801 / 61200) loss: 2.303630\n",
      "(Epoch 26 / 80) train acc: 0.105000; val_acc: 0.103000\n",
      "(Iteration 19901 / 61200) loss: 2.303628\n",
      "(Iteration 20001 / 61200) loss: 2.303608\n",
      "(Iteration 20101 / 61200) loss: 2.303669\n",
      "(Iteration 20201 / 61200) loss: 2.303608\n",
      "(Iteration 20301 / 61200) loss: 2.303617\n",
      "(Iteration 20401 / 61200) loss: 2.303650\n",
      "(Iteration 20501 / 61200) loss: 2.303653\n",
      "(Iteration 20601 / 61200) loss: 2.303634\n",
      "(Epoch 27 / 80) train acc: 0.091000; val_acc: 0.102000\n",
      "(Iteration 20701 / 61200) loss: 2.303587\n",
      "(Iteration 20801 / 61200) loss: 2.303675\n",
      "(Iteration 20901 / 61200) loss: 2.303676\n",
      "(Iteration 21001 / 61200) loss: 2.303678\n",
      "(Iteration 21101 / 61200) loss: 2.303649\n",
      "(Iteration 21201 / 61200) loss: 2.303650\n",
      "(Iteration 21301 / 61200) loss: 2.303648\n",
      "(Iteration 21401 / 61200) loss: 2.303548\n",
      "(Epoch 28 / 80) train acc: 0.103000; val_acc: 0.102000\n",
      "(Iteration 21501 / 61200) loss: 2.303637\n",
      "(Iteration 21601 / 61200) loss: 2.303583\n",
      "(Iteration 21701 / 61200) loss: 2.303702\n",
      "(Iteration 21801 / 61200) loss: 2.303560\n",
      "(Iteration 21901 / 61200) loss: 2.303633\n",
      "(Iteration 22001 / 61200) loss: 2.303586\n",
      "(Iteration 22101 / 61200) loss: 2.303569\n",
      "(Epoch 29 / 80) train acc: 0.095000; val_acc: 0.102000\n",
      "(Iteration 22201 / 61200) loss: 2.303643\n",
      "(Iteration 22301 / 61200) loss: 2.303548\n",
      "(Iteration 22401 / 61200) loss: 2.303621\n",
      "(Iteration 22501 / 61200) loss: 2.303611\n",
      "(Iteration 22601 / 61200) loss: 2.303677\n",
      "(Iteration 22701 / 61200) loss: 2.303561\n",
      "(Iteration 22801 / 61200) loss: 2.303615\n",
      "(Iteration 22901 / 61200) loss: 2.303620\n",
      "(Epoch 30 / 80) train acc: 0.107000; val_acc: 0.102000\n",
      "(Iteration 23001 / 61200) loss: 2.303627\n",
      "(Iteration 23101 / 61200) loss: 2.303592\n",
      "(Iteration 23201 / 61200) loss: 2.303615\n",
      "(Iteration 23301 / 61200) loss: 2.303579\n",
      "(Iteration 23401 / 61200) loss: 2.303653\n",
      "(Iteration 23501 / 61200) loss: 2.303605\n",
      "(Iteration 23601 / 61200) loss: 2.303657\n",
      "(Iteration 23701 / 61200) loss: 2.303529\n",
      "(Epoch 31 / 80) train acc: 0.113000; val_acc: 0.102000\n",
      "(Iteration 23801 / 61200) loss: 2.303651\n",
      "(Iteration 23901 / 61200) loss: 2.303619\n",
      "(Iteration 24001 / 61200) loss: 2.303599\n",
      "(Iteration 24101 / 61200) loss: 2.303594\n",
      "(Iteration 24201 / 61200) loss: 2.303610\n",
      "(Iteration 24301 / 61200) loss: 2.303626\n",
      "(Iteration 24401 / 61200) loss: 2.303576\n",
      "(Epoch 32 / 80) train acc: 0.095000; val_acc: 0.102000\n",
      "(Iteration 24501 / 61200) loss: 2.303670\n",
      "(Iteration 24601 / 61200) loss: 2.303660\n",
      "(Iteration 24701 / 61200) loss: 2.303633\n",
      "(Iteration 24801 / 61200) loss: 2.303610\n",
      "(Iteration 24901 / 61200) loss: 2.303638\n",
      "(Iteration 25001 / 61200) loss: 2.303626\n",
      "(Iteration 25101 / 61200) loss: 2.303593\n",
      "(Iteration 25201 / 61200) loss: 2.303658\n",
      "(Epoch 33 / 80) train acc: 0.103000; val_acc: 0.102000\n",
      "(Iteration 25301 / 61200) loss: 2.303614\n",
      "(Iteration 25401 / 61200) loss: 2.303606\n",
      "(Iteration 25501 / 61200) loss: 2.303581\n",
      "(Iteration 25601 / 61200) loss: 2.303599\n",
      "(Iteration 25701 / 61200) loss: 2.303584\n",
      "(Iteration 25801 / 61200) loss: 2.303537\n",
      "(Iteration 25901 / 61200) loss: 2.303595\n",
      "(Iteration 26001 / 61200) loss: 2.303610\n",
      "(Epoch 34 / 80) train acc: 0.104000; val_acc: 0.102000\n",
      "(Iteration 26101 / 61200) loss: 2.303618\n",
      "(Iteration 26201 / 61200) loss: 2.303630\n",
      "(Iteration 26301 / 61200) loss: 2.303605\n",
      "(Iteration 26401 / 61200) loss: 2.303630\n",
      "(Iteration 26501 / 61200) loss: 2.303591\n",
      "(Iteration 26601 / 61200) loss: 2.303582\n",
      "(Iteration 26701 / 61200) loss: 2.303569\n",
      "(Epoch 35 / 80) train acc: 0.102000; val_acc: 0.102000\n",
      "(Iteration 26801 / 61200) loss: 2.303610\n",
      "(Iteration 26901 / 61200) loss: 2.303630\n",
      "(Iteration 27001 / 61200) loss: 2.303627\n",
      "(Iteration 27101 / 61200) loss: 2.303590\n",
      "(Iteration 27201 / 61200) loss: 2.303649\n",
      "(Iteration 27301 / 61200) loss: 2.303649\n",
      "(Iteration 27401 / 61200) loss: 2.303582\n",
      "(Iteration 27501 / 61200) loss: 2.303547\n",
      "(Epoch 36 / 80) train acc: 0.097000; val_acc: 0.102000\n",
      "(Iteration 27601 / 61200) loss: 2.303617\n",
      "(Iteration 27701 / 61200) loss: 2.303612\n",
      "(Iteration 27801 / 61200) loss: 2.303567\n",
      "(Iteration 27901 / 61200) loss: 2.303640\n",
      "(Iteration 28001 / 61200) loss: 2.303583\n",
      "(Iteration 28101 / 61200) loss: 2.303589\n",
      "(Iteration 28201 / 61200) loss: 2.303521\n",
      "(Iteration 28301 / 61200) loss: 2.303680\n",
      "(Epoch 37 / 80) train acc: 0.097000; val_acc: 0.102000\n",
      "(Iteration 28401 / 61200) loss: 2.303611\n",
      "(Iteration 28501 / 61200) loss: 2.303536\n",
      "(Iteration 28601 / 61200) loss: 2.303544\n",
      "(Iteration 28701 / 61200) loss: 2.303598\n",
      "(Iteration 28801 / 61200) loss: 2.303562\n",
      "(Iteration 28901 / 61200) loss: 2.303606\n",
      "(Iteration 29001 / 61200) loss: 2.303577\n",
      "(Epoch 38 / 80) train acc: 0.109000; val_acc: 0.102000\n",
      "(Iteration 29101 / 61200) loss: 2.303648\n",
      "(Iteration 29201 / 61200) loss: 2.303571\n",
      "(Iteration 29301 / 61200) loss: 2.303507\n",
      "(Iteration 29401 / 61200) loss: 2.303654\n",
      "(Iteration 29501 / 61200) loss: 2.303563\n",
      "(Iteration 29601 / 61200) loss: 2.303674\n",
      "(Iteration 29701 / 61200) loss: 2.303584\n",
      "(Iteration 29801 / 61200) loss: 2.303660\n",
      "(Epoch 39 / 80) train acc: 0.092000; val_acc: 0.102000\n",
      "(Iteration 29901 / 61200) loss: 2.303647\n",
      "(Iteration 30001 / 61200) loss: 2.303625\n",
      "(Iteration 30101 / 61200) loss: 2.303567\n",
      "(Iteration 30201 / 61200) loss: 2.303489\n",
      "(Iteration 30301 / 61200) loss: 2.303556\n",
      "(Iteration 30401 / 61200) loss: 2.303582\n",
      "(Iteration 30501 / 61200) loss: 2.303543\n",
      "(Epoch 40 / 80) train acc: 0.095000; val_acc: 0.102000\n",
      "(Iteration 30601 / 61200) loss: 2.303609\n",
      "(Iteration 30701 / 61200) loss: 2.303635\n",
      "(Iteration 30801 / 61200) loss: 2.303624\n",
      "(Iteration 30901 / 61200) loss: 2.303617\n",
      "(Iteration 31001 / 61200) loss: 2.303658\n",
      "(Iteration 31101 / 61200) loss: 2.303687\n",
      "(Iteration 31201 / 61200) loss: 2.303632\n",
      "(Iteration 31301 / 61200) loss: 2.303590\n",
      "(Epoch 41 / 80) train acc: 0.097000; val_acc: 0.102000\n",
      "(Iteration 31401 / 61200) loss: 2.303521\n",
      "(Iteration 31501 / 61200) loss: 2.303603\n",
      "(Iteration 31601 / 61200) loss: 2.303652\n",
      "(Iteration 31701 / 61200) loss: 2.303641\n",
      "(Iteration 31801 / 61200) loss: 2.303591\n",
      "(Iteration 31901 / 61200) loss: 2.303585\n",
      "(Iteration 32001 / 61200) loss: 2.303617\n",
      "(Iteration 32101 / 61200) loss: 2.303613\n",
      "(Epoch 42 / 80) train acc: 0.100000; val_acc: 0.102000\n",
      "(Iteration 32201 / 61200) loss: 2.303585\n",
      "(Iteration 32301 / 61200) loss: 2.303531\n",
      "(Iteration 32401 / 61200) loss: 2.303645\n",
      "(Iteration 32501 / 61200) loss: 2.303560\n",
      "(Iteration 32601 / 61200) loss: 2.303606\n",
      "(Iteration 32701 / 61200) loss: 2.303581\n",
      "(Iteration 32801 / 61200) loss: 2.303599\n",
      "(Epoch 43 / 80) train acc: 0.090000; val_acc: 0.102000\n",
      "(Iteration 32901 / 61200) loss: 2.303521\n",
      "(Iteration 33001 / 61200) loss: 2.303541\n",
      "(Iteration 33101 / 61200) loss: 2.303506\n",
      "(Iteration 33201 / 61200) loss: 2.303665\n",
      "(Iteration 33301 / 61200) loss: 2.303514\n",
      "(Iteration 33401 / 61200) loss: 2.303575\n",
      "(Iteration 33501 / 61200) loss: 2.303586\n",
      "(Iteration 33601 / 61200) loss: 2.303535\n",
      "(Epoch 44 / 80) train acc: 0.113000; val_acc: 0.102000\n",
      "(Iteration 33701 / 61200) loss: 2.303646\n",
      "(Iteration 33801 / 61200) loss: 2.303637\n",
      "(Iteration 33901 / 61200) loss: 2.303597\n",
      "(Iteration 34001 / 61200) loss: 2.303584\n",
      "(Iteration 34101 / 61200) loss: 2.303614\n",
      "(Iteration 34201 / 61200) loss: 2.303593\n",
      "(Iteration 34301 / 61200) loss: 2.303575\n",
      "(Iteration 34401 / 61200) loss: 2.303567\n",
      "(Epoch 45 / 80) train acc: 0.103000; val_acc: 0.102000\n",
      "(Iteration 34501 / 61200) loss: 2.303501\n",
      "(Iteration 34601 / 61200) loss: 2.303552\n",
      "(Iteration 34701 / 61200) loss: 2.303623\n",
      "(Iteration 34801 / 61200) loss: 2.303587\n",
      "(Iteration 34901 / 61200) loss: 2.303596\n",
      "(Iteration 35001 / 61200) loss: 2.303554\n",
      "(Iteration 35101 / 61200) loss: 2.303532\n",
      "(Epoch 46 / 80) train acc: 0.093000; val_acc: 0.102000\n",
      "(Iteration 35201 / 61200) loss: 2.303582\n",
      "(Iteration 35301 / 61200) loss: 2.303559\n",
      "(Iteration 35401 / 61200) loss: 2.303583\n",
      "(Iteration 35501 / 61200) loss: 2.303564\n",
      "(Iteration 35601 / 61200) loss: 2.303595\n",
      "(Iteration 35701 / 61200) loss: 2.303618\n",
      "(Iteration 35801 / 61200) loss: 2.303577\n",
      "(Iteration 35901 / 61200) loss: 2.303540\n",
      "(Epoch 47 / 80) train acc: 0.103000; val_acc: 0.102000\n",
      "(Iteration 36001 / 61200) loss: 2.303524\n",
      "(Iteration 36101 / 61200) loss: 2.303606\n",
      "(Iteration 36201 / 61200) loss: 2.303518\n",
      "(Iteration 36301 / 61200) loss: 2.303568\n",
      "(Iteration 36401 / 61200) loss: 2.303595\n",
      "(Iteration 36501 / 61200) loss: 2.303616\n",
      "(Iteration 36601 / 61200) loss: 2.303564\n",
      "(Iteration 36701 / 61200) loss: 2.303581\n",
      "(Epoch 48 / 80) train acc: 0.084000; val_acc: 0.102000\n",
      "(Iteration 36801 / 61200) loss: 2.303614\n",
      "(Iteration 36901 / 61200) loss: 2.303550\n",
      "(Iteration 37001 / 61200) loss: 2.303587\n",
      "(Iteration 37101 / 61200) loss: 2.303521\n",
      "(Iteration 37201 / 61200) loss: 2.303623\n",
      "(Iteration 37301 / 61200) loss: 2.303575\n",
      "(Iteration 37401 / 61200) loss: 2.303583\n",
      "(Epoch 49 / 80) train acc: 0.101000; val_acc: 0.102000\n",
      "(Iteration 37501 / 61200) loss: 2.303534\n",
      "(Iteration 37601 / 61200) loss: 2.303533\n",
      "(Iteration 37701 / 61200) loss: 2.303497\n",
      "(Iteration 37801 / 61200) loss: 2.303535\n",
      "(Iteration 37901 / 61200) loss: 2.303666\n",
      "(Iteration 38001 / 61200) loss: 2.303570\n",
      "(Iteration 38101 / 61200) loss: 2.303595\n",
      "(Iteration 38201 / 61200) loss: 2.303560\n",
      "(Epoch 50 / 80) train acc: 0.102000; val_acc: 0.102000\n",
      "(Iteration 38301 / 61200) loss: 2.303584\n",
      "(Iteration 38401 / 61200) loss: 2.303596\n",
      "(Iteration 38501 / 61200) loss: 2.303553\n",
      "(Iteration 38601 / 61200) loss: 2.303596\n",
      "(Iteration 38701 / 61200) loss: 2.303605\n",
      "(Iteration 38801 / 61200) loss: 2.303594\n",
      "(Iteration 38901 / 61200) loss: 2.303597\n",
      "(Iteration 39001 / 61200) loss: 2.303529\n",
      "(Epoch 51 / 80) train acc: 0.110000; val_acc: 0.102000\n",
      "(Iteration 39101 / 61200) loss: 2.303581\n",
      "(Iteration 39201 / 61200) loss: 2.303576\n",
      "(Iteration 39301 / 61200) loss: 2.303665\n",
      "(Iteration 39401 / 61200) loss: 2.303538\n",
      "(Iteration 39501 / 61200) loss: 2.303539\n",
      "(Iteration 39601 / 61200) loss: 2.303545\n",
      "(Iteration 39701 / 61200) loss: 2.303596\n",
      "(Epoch 52 / 80) train acc: 0.095000; val_acc: 0.102000\n",
      "(Iteration 39801 / 61200) loss: 2.303551\n",
      "(Iteration 39901 / 61200) loss: 2.303540\n",
      "(Iteration 40001 / 61200) loss: 2.303621\n",
      "(Iteration 40101 / 61200) loss: 2.303609\n",
      "(Iteration 40201 / 61200) loss: 2.303564\n",
      "(Iteration 40301 / 61200) loss: 2.303582\n",
      "(Iteration 40401 / 61200) loss: 2.303573\n",
      "(Iteration 40501 / 61200) loss: 2.303577\n",
      "(Epoch 53 / 80) train acc: 0.101000; val_acc: 0.102000\n",
      "(Iteration 40601 / 61200) loss: 2.303538\n",
      "(Iteration 40701 / 61200) loss: 2.303571\n",
      "(Iteration 40801 / 61200) loss: 2.303568\n",
      "(Iteration 40901 / 61200) loss: 2.303620\n",
      "(Iteration 41001 / 61200) loss: 2.303582\n",
      "(Iteration 41101 / 61200) loss: 2.303577\n",
      "(Iteration 41201 / 61200) loss: 2.303561\n",
      "(Iteration 41301 / 61200) loss: 2.303613\n",
      "(Epoch 54 / 80) train acc: 0.105000; val_acc: 0.102000\n",
      "(Iteration 41401 / 61200) loss: 2.303615\n",
      "(Iteration 41501 / 61200) loss: 2.303531\n",
      "(Iteration 41601 / 61200) loss: 2.303583\n",
      "(Iteration 41701 / 61200) loss: 2.303534\n",
      "(Iteration 41801 / 61200) loss: 2.303546\n",
      "(Iteration 41901 / 61200) loss: 2.303583\n",
      "(Iteration 42001 / 61200) loss: 2.303594\n",
      "(Epoch 55 / 80) train acc: 0.094000; val_acc: 0.102000\n",
      "(Iteration 42101 / 61200) loss: 2.303580\n",
      "(Iteration 42201 / 61200) loss: 2.303531\n",
      "(Iteration 42301 / 61200) loss: 2.303571\n",
      "(Iteration 42401 / 61200) loss: 2.303618\n",
      "(Iteration 42501 / 61200) loss: 2.303597\n",
      "(Iteration 42601 / 61200) loss: 2.303554\n",
      "(Iteration 42701 / 61200) loss: 2.303517\n",
      "(Iteration 42801 / 61200) loss: 2.303550\n",
      "(Epoch 56 / 80) train acc: 0.094000; val_acc: 0.102000\n",
      "(Iteration 42901 / 61200) loss: 2.303559\n",
      "(Iteration 43001 / 61200) loss: 2.303585\n",
      "(Iteration 43101 / 61200) loss: 2.303507\n",
      "(Iteration 43201 / 61200) loss: 2.303568\n",
      "(Iteration 43301 / 61200) loss: 2.303664\n",
      "(Iteration 43401 / 61200) loss: 2.303633\n",
      "(Iteration 43501 / 61200) loss: 2.303471\n",
      "(Iteration 43601 / 61200) loss: 2.303484\n",
      "(Epoch 57 / 80) train acc: 0.105000; val_acc: 0.102000\n",
      "(Iteration 43701 / 61200) loss: 2.303663\n",
      "(Iteration 43801 / 61200) loss: 2.303576\n",
      "(Iteration 43901 / 61200) loss: 2.303554\n",
      "(Iteration 44001 / 61200) loss: 2.303609\n",
      "(Iteration 44101 / 61200) loss: 2.303594\n",
      "(Iteration 44201 / 61200) loss: 2.303531\n",
      "(Iteration 44301 / 61200) loss: 2.303543\n",
      "(Epoch 58 / 80) train acc: 0.096000; val_acc: 0.102000\n",
      "(Iteration 44401 / 61200) loss: 2.303596\n",
      "(Iteration 44501 / 61200) loss: 2.303624\n",
      "(Iteration 44601 / 61200) loss: 2.303542\n",
      "(Iteration 44701 / 61200) loss: 2.303479\n",
      "(Iteration 44801 / 61200) loss: 2.303655\n",
      "(Iteration 44901 / 61200) loss: 2.303515\n",
      "(Iteration 45001 / 61200) loss: 2.303542\n",
      "(Iteration 45101 / 61200) loss: 2.303574\n",
      "(Epoch 59 / 80) train acc: 0.098000; val_acc: 0.102000\n",
      "(Iteration 45201 / 61200) loss: 2.303486\n",
      "(Iteration 45301 / 61200) loss: 2.303535\n",
      "(Iteration 45401 / 61200) loss: 2.303564\n",
      "(Iteration 45501 / 61200) loss: 2.303602\n",
      "(Iteration 45601 / 61200) loss: 2.303549\n",
      "(Iteration 45701 / 61200) loss: 2.303509\n",
      "(Iteration 45801 / 61200) loss: 2.303534\n",
      "(Epoch 60 / 80) train acc: 0.110000; val_acc: 0.102000\n",
      "(Iteration 45901 / 61200) loss: 2.303594\n",
      "(Iteration 46001 / 61200) loss: 2.303539\n",
      "(Iteration 46101 / 61200) loss: 2.303567\n",
      "(Iteration 46201 / 61200) loss: 2.303640\n",
      "(Iteration 46301 / 61200) loss: 2.303496\n",
      "(Iteration 46401 / 61200) loss: 2.303535\n",
      "(Iteration 46501 / 61200) loss: 2.303517\n",
      "(Iteration 46601 / 61200) loss: 2.303609\n",
      "(Epoch 61 / 80) train acc: 0.097000; val_acc: 0.102000\n",
      "(Iteration 46701 / 61200) loss: 2.303597\n",
      "(Iteration 46801 / 61200) loss: 2.303520\n",
      "(Iteration 46901 / 61200) loss: 2.303556\n",
      "(Iteration 47001 / 61200) loss: 2.303570\n",
      "(Iteration 47101 / 61200) loss: 2.303649\n",
      "(Iteration 47201 / 61200) loss: 2.303536\n",
      "(Iteration 47301 / 61200) loss: 2.303621\n",
      "(Iteration 47401 / 61200) loss: 2.303579\n",
      "(Epoch 62 / 80) train acc: 0.122000; val_acc: 0.102000\n",
      "(Iteration 47501 / 61200) loss: 2.303536\n",
      "(Iteration 47601 / 61200) loss: 2.303565\n",
      "(Iteration 47701 / 61200) loss: 2.303563\n",
      "(Iteration 47801 / 61200) loss: 2.303587\n",
      "(Iteration 47901 / 61200) loss: 2.303552\n",
      "(Iteration 48001 / 61200) loss: 2.303606\n",
      "(Iteration 48101 / 61200) loss: 2.303608\n",
      "(Epoch 63 / 80) train acc: 0.117000; val_acc: 0.102000\n",
      "(Iteration 48201 / 61200) loss: 2.303552\n",
      "(Iteration 48301 / 61200) loss: 2.303509\n",
      "(Iteration 48401 / 61200) loss: 2.303506\n",
      "(Iteration 48501 / 61200) loss: 2.303545\n",
      "(Iteration 48601 / 61200) loss: 2.303579\n",
      "(Iteration 48701 / 61200) loss: 2.303586\n",
      "(Iteration 48801 / 61200) loss: 2.303560\n",
      "(Iteration 48901 / 61200) loss: 2.303565\n",
      "(Epoch 64 / 80) train acc: 0.104000; val_acc: 0.102000\n",
      "(Iteration 49001 / 61200) loss: 2.303603\n",
      "(Iteration 49101 / 61200) loss: 2.303567\n",
      "(Iteration 49201 / 61200) loss: 2.303595\n",
      "(Iteration 49301 / 61200) loss: 2.303579\n",
      "(Iteration 49401 / 61200) loss: 2.303630\n",
      "(Iteration 49501 / 61200) loss: 2.303593\n",
      "(Iteration 49601 / 61200) loss: 2.303614\n",
      "(Iteration 49701 / 61200) loss: 2.303561\n",
      "(Epoch 65 / 80) train acc: 0.104000; val_acc: 0.102000\n",
      "(Iteration 49801 / 61200) loss: 2.303608\n",
      "(Iteration 49901 / 61200) loss: 2.303508\n",
      "(Iteration 50001 / 61200) loss: 2.303575\n",
      "(Iteration 50101 / 61200) loss: 2.303586\n",
      "(Iteration 50201 / 61200) loss: 2.303614\n",
      "(Iteration 50301 / 61200) loss: 2.303470\n",
      "(Iteration 50401 / 61200) loss: 2.303595\n",
      "(Epoch 66 / 80) train acc: 0.103000; val_acc: 0.102000\n",
      "(Iteration 50501 / 61200) loss: 2.303539\n",
      "(Iteration 50601 / 61200) loss: 2.303698\n",
      "(Iteration 50701 / 61200) loss: 2.303643\n",
      "(Iteration 50801 / 61200) loss: 2.303573\n",
      "(Iteration 50901 / 61200) loss: 2.303545\n",
      "(Iteration 51001 / 61200) loss: 2.303624\n",
      "(Iteration 51101 / 61200) loss: 2.303602\n",
      "(Iteration 51201 / 61200) loss: 2.303592\n",
      "(Epoch 67 / 80) train acc: 0.109000; val_acc: 0.102000\n",
      "(Iteration 51301 / 61200) loss: 2.303617\n",
      "(Iteration 51401 / 61200) loss: 2.303622\n",
      "(Iteration 51501 / 61200) loss: 2.303590\n",
      "(Iteration 51601 / 61200) loss: 2.303580\n",
      "(Iteration 51701 / 61200) loss: 2.303541\n",
      "(Iteration 51801 / 61200) loss: 2.303531\n",
      "(Iteration 51901 / 61200) loss: 2.303556\n",
      "(Iteration 52001 / 61200) loss: 2.303645\n",
      "(Epoch 68 / 80) train acc: 0.120000; val_acc: 0.102000\n",
      "(Iteration 52101 / 61200) loss: 2.303546\n",
      "(Iteration 52201 / 61200) loss: 2.303578\n",
      "(Iteration 52301 / 61200) loss: 2.303488\n",
      "(Iteration 52401 / 61200) loss: 2.303612\n",
      "(Iteration 52501 / 61200) loss: 2.303595\n",
      "(Iteration 52601 / 61200) loss: 2.303538\n",
      "(Iteration 52701 / 61200) loss: 2.303541\n",
      "(Epoch 69 / 80) train acc: 0.102000; val_acc: 0.102000\n",
      "(Iteration 52801 / 61200) loss: 2.303519\n",
      "(Iteration 52901 / 61200) loss: 2.303555\n",
      "(Iteration 53001 / 61200) loss: 2.303548\n",
      "(Iteration 53101 / 61200) loss: 2.303578\n",
      "(Iteration 53201 / 61200) loss: 2.303634\n",
      "(Iteration 53301 / 61200) loss: 2.303556\n",
      "(Iteration 53401 / 61200) loss: 2.303618\n",
      "(Iteration 53501 / 61200) loss: 2.303524\n",
      "(Epoch 70 / 80) train acc: 0.111000; val_acc: 0.102000\n",
      "(Iteration 53601 / 61200) loss: 2.303552\n",
      "(Iteration 53701 / 61200) loss: 2.303597\n",
      "(Iteration 53801 / 61200) loss: 2.303527\n",
      "(Iteration 53901 / 61200) loss: 2.303597\n",
      "(Iteration 54001 / 61200) loss: 2.303626\n",
      "(Iteration 54101 / 61200) loss: 2.303599\n",
      "(Iteration 54201 / 61200) loss: 2.303504\n",
      "(Iteration 54301 / 61200) loss: 2.303625\n",
      "(Epoch 71 / 80) train acc: 0.120000; val_acc: 0.102000\n",
      "(Iteration 54401 / 61200) loss: 2.303626\n",
      "(Iteration 54501 / 61200) loss: 2.303582\n",
      "(Iteration 54601 / 61200) loss: 2.303596\n",
      "(Iteration 54701 / 61200) loss: 2.303585\n",
      "(Iteration 54801 / 61200) loss: 2.303576\n",
      "(Iteration 54901 / 61200) loss: 2.303531\n",
      "(Iteration 55001 / 61200) loss: 2.303547\n",
      "(Epoch 72 / 80) train acc: 0.105000; val_acc: 0.102000\n",
      "(Iteration 55101 / 61200) loss: 2.303589\n",
      "(Iteration 55201 / 61200) loss: 2.303644\n",
      "(Iteration 55301 / 61200) loss: 2.303601\n",
      "(Iteration 55401 / 61200) loss: 2.303533\n",
      "(Iteration 55501 / 61200) loss: 2.303533\n",
      "(Iteration 55601 / 61200) loss: 2.303563\n",
      "(Iteration 55701 / 61200) loss: 2.303620\n",
      "(Iteration 55801 / 61200) loss: 2.303599\n",
      "(Epoch 73 / 80) train acc: 0.106000; val_acc: 0.102000\n",
      "(Iteration 55901 / 61200) loss: 2.303515\n",
      "(Iteration 56001 / 61200) loss: 2.303627\n",
      "(Iteration 56101 / 61200) loss: 2.303665\n",
      "(Iteration 56201 / 61200) loss: 2.303534\n",
      "(Iteration 56301 / 61200) loss: 2.303489\n",
      "(Iteration 56401 / 61200) loss: 2.303590\n",
      "(Iteration 56501 / 61200) loss: 2.303575\n",
      "(Iteration 56601 / 61200) loss: 2.303594\n",
      "(Epoch 74 / 80) train acc: 0.113000; val_acc: 0.102000\n",
      "(Iteration 56701 / 61200) loss: 2.303551\n",
      "(Iteration 56801 / 61200) loss: 2.303560\n",
      "(Iteration 56901 / 61200) loss: 2.303650\n",
      "(Iteration 57001 / 61200) loss: 2.303580\n",
      "(Iteration 57101 / 61200) loss: 2.303584\n",
      "(Iteration 57201 / 61200) loss: 2.303566\n",
      "(Iteration 57301 / 61200) loss: 2.303514\n",
      "(Epoch 75 / 80) train acc: 0.115000; val_acc: 0.102000\n",
      "(Iteration 57401 / 61200) loss: 2.303651\n",
      "(Iteration 57501 / 61200) loss: 2.303562\n",
      "(Iteration 57601 / 61200) loss: 2.303594\n",
      "(Iteration 57701 / 61200) loss: 2.303527\n",
      "(Iteration 57801 / 61200) loss: 2.303648\n",
      "(Iteration 57901 / 61200) loss: 2.303576\n",
      "(Iteration 58001 / 61200) loss: 2.303620\n",
      "(Iteration 58101 / 61200) loss: 2.303575\n",
      "(Epoch 76 / 80) train acc: 0.115000; val_acc: 0.102000\n",
      "(Iteration 58201 / 61200) loss: 2.303623\n",
      "(Iteration 58301 / 61200) loss: 2.303563\n",
      "(Iteration 58401 / 61200) loss: 2.303596\n",
      "(Iteration 58501 / 61200) loss: 2.303548\n",
      "(Iteration 58601 / 61200) loss: 2.303611\n",
      "(Iteration 58701 / 61200) loss: 2.303551\n",
      "(Iteration 58801 / 61200) loss: 2.303568\n",
      "(Iteration 58901 / 61200) loss: 2.303651\n",
      "(Epoch 77 / 80) train acc: 0.106000; val_acc: 0.102000\n",
      "(Iteration 59001 / 61200) loss: 2.303573\n",
      "(Iteration 59101 / 61200) loss: 2.303590\n",
      "(Iteration 59201 / 61200) loss: 2.303530\n",
      "(Iteration 59301 / 61200) loss: 2.303582\n",
      "(Iteration 59401 / 61200) loss: 2.303540\n",
      "(Iteration 59501 / 61200) loss: 2.303597\n",
      "(Iteration 59601 / 61200) loss: 2.303720\n",
      "(Epoch 78 / 80) train acc: 0.115000; val_acc: 0.102000\n",
      "(Iteration 59701 / 61200) loss: 2.303612\n",
      "(Iteration 59801 / 61200) loss: 2.303584\n",
      "(Iteration 59901 / 61200) loss: 2.303564\n",
      "(Iteration 60001 / 61200) loss: 2.303561\n",
      "(Iteration 60101 / 61200) loss: 2.303610\n",
      "(Iteration 60201 / 61200) loss: 2.303577\n",
      "(Iteration 60301 / 61200) loss: 2.303546\n",
      "(Iteration 60401 / 61200) loss: 2.303623\n",
      "(Epoch 79 / 80) train acc: 0.093000; val_acc: 0.102000\n",
      "(Iteration 60501 / 61200) loss: 2.303625\n",
      "(Iteration 60601 / 61200) loss: 2.303546\n",
      "(Iteration 60701 / 61200) loss: 2.303549\n",
      "(Iteration 60801 / 61200) loss: 2.303578\n",
      "(Iteration 60901 / 61200) loss: 2.303579\n",
      "(Iteration 61001 / 61200) loss: 2.303615\n",
      "(Iteration 61101 / 61200) loss: 2.303587\n",
      "(Epoch 80 / 80) train acc: 0.113000; val_acc: 0.102000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.305458\n",
      "(Epoch 0 / 80) train acc: 0.155000; val_acc: 0.161000\n",
      "(Iteration 101 / 30560) loss: 2.305412\n",
      "(Iteration 201 / 30560) loss: 2.305361\n",
      "(Iteration 301 / 30560) loss: 2.305318\n",
      "(Epoch 1 / 80) train acc: 0.117000; val_acc: 0.124000\n",
      "(Iteration 401 / 30560) loss: 2.305297\n",
      "(Iteration 501 / 30560) loss: 2.305252\n",
      "(Iteration 601 / 30560) loss: 2.305218\n",
      "(Iteration 701 / 30560) loss: 2.305196\n",
      "(Epoch 2 / 80) train acc: 0.096000; val_acc: 0.110000\n",
      "(Iteration 801 / 30560) loss: 2.305149\n",
      "(Iteration 901 / 30560) loss: 2.305134\n",
      "(Iteration 1001 / 30560) loss: 2.305088\n",
      "(Iteration 1101 / 30560) loss: 2.305068\n",
      "(Epoch 3 / 80) train acc: 0.092000; val_acc: 0.092000\n",
      "(Iteration 1201 / 30560) loss: 2.305037\n",
      "(Iteration 1301 / 30560) loss: 2.305024\n",
      "(Iteration 1401 / 30560) loss: 2.305004\n",
      "(Iteration 1501 / 30560) loss: 2.304961\n",
      "(Epoch 4 / 80) train acc: 0.112000; val_acc: 0.094000\n",
      "(Iteration 1601 / 30560) loss: 2.304944\n",
      "(Iteration 1701 / 30560) loss: 2.304924\n",
      "(Iteration 1801 / 30560) loss: 2.304888\n",
      "(Iteration 1901 / 30560) loss: 2.304861\n",
      "(Epoch 5 / 80) train acc: 0.108000; val_acc: 0.100000\n",
      "(Iteration 2001 / 30560) loss: 2.304870\n",
      "(Iteration 2101 / 30560) loss: 2.304842\n",
      "(Iteration 2201 / 30560) loss: 2.304819\n",
      "(Epoch 6 / 80) train acc: 0.107000; val_acc: 0.096000\n",
      "(Iteration 2301 / 30560) loss: 2.304811\n",
      "(Iteration 2401 / 30560) loss: 2.304803\n",
      "(Iteration 2501 / 30560) loss: 2.304787\n",
      "(Iteration 2601 / 30560) loss: 2.304775\n",
      "(Epoch 7 / 80) train acc: 0.102000; val_acc: 0.091000\n",
      "(Iteration 2701 / 30560) loss: 2.304730\n",
      "(Iteration 2801 / 30560) loss: 2.304716\n",
      "(Iteration 2901 / 30560) loss: 2.304713\n",
      "(Iteration 3001 / 30560) loss: 2.304705\n",
      "(Epoch 8 / 80) train acc: 0.111000; val_acc: 0.089000\n",
      "(Iteration 3101 / 30560) loss: 2.304696\n",
      "(Iteration 3201 / 30560) loss: 2.304671\n",
      "(Iteration 3301 / 30560) loss: 2.304671\n",
      "(Iteration 3401 / 30560) loss: 2.304640\n",
      "(Epoch 9 / 80) train acc: 0.101000; val_acc: 0.091000\n",
      "(Iteration 3501 / 30560) loss: 2.304636\n",
      "(Iteration 3601 / 30560) loss: 2.304616\n",
      "(Iteration 3701 / 30560) loss: 2.304616\n",
      "(Iteration 3801 / 30560) loss: 2.304599\n",
      "(Epoch 10 / 80) train acc: 0.096000; val_acc: 0.090000\n",
      "(Iteration 3901 / 30560) loss: 2.304601\n",
      "(Iteration 4001 / 30560) loss: 2.304589\n",
      "(Iteration 4101 / 30560) loss: 2.304551\n",
      "(Iteration 4201 / 30560) loss: 2.304549\n",
      "(Epoch 11 / 80) train acc: 0.106000; val_acc: 0.090000\n",
      "(Iteration 4301 / 30560) loss: 2.304550\n",
      "(Iteration 4401 / 30560) loss: 2.304549\n",
      "(Iteration 4501 / 30560) loss: 2.304537\n",
      "(Epoch 12 / 80) train acc: 0.102000; val_acc: 0.091000\n",
      "(Iteration 4601 / 30560) loss: 2.304544\n",
      "(Iteration 4701 / 30560) loss: 2.304503\n",
      "(Iteration 4801 / 30560) loss: 2.304503\n",
      "(Iteration 4901 / 30560) loss: 2.304521\n",
      "(Epoch 13 / 80) train acc: 0.096000; val_acc: 0.090000\n",
      "(Iteration 5001 / 30560) loss: 2.304498\n",
      "(Iteration 5101 / 30560) loss: 2.304508\n",
      "(Iteration 5201 / 30560) loss: 2.304500\n",
      "(Iteration 5301 / 30560) loss: 2.304445\n",
      "(Epoch 14 / 80) train acc: 0.093000; val_acc: 0.089000\n",
      "(Iteration 5401 / 30560) loss: 2.304478\n",
      "(Iteration 5501 / 30560) loss: 2.304479\n",
      "(Iteration 5601 / 30560) loss: 2.304454\n",
      "(Iteration 5701 / 30560) loss: 2.304441\n",
      "(Epoch 15 / 80) train acc: 0.096000; val_acc: 0.090000\n",
      "(Iteration 5801 / 30560) loss: 2.304451\n",
      "(Iteration 5901 / 30560) loss: 2.304442\n",
      "(Iteration 6001 / 30560) loss: 2.304413\n",
      "(Iteration 6101 / 30560) loss: 2.304441\n",
      "(Epoch 16 / 80) train acc: 0.098000; val_acc: 0.088000\n",
      "(Iteration 6201 / 30560) loss: 2.304410\n",
      "(Iteration 6301 / 30560) loss: 2.304446\n",
      "(Iteration 6401 / 30560) loss: 2.304414\n",
      "(Epoch 17 / 80) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 6501 / 30560) loss: 2.304416\n",
      "(Iteration 6601 / 30560) loss: 2.304416\n",
      "(Iteration 6701 / 30560) loss: 2.304411\n",
      "(Iteration 6801 / 30560) loss: 2.304413\n",
      "(Epoch 18 / 80) train acc: 0.092000; val_acc: 0.087000\n",
      "(Iteration 6901 / 30560) loss: 2.304372\n",
      "(Iteration 7001 / 30560) loss: 2.304394\n",
      "(Iteration 7101 / 30560) loss: 2.304382\n",
      "(Iteration 7201 / 30560) loss: 2.304377\n",
      "(Epoch 19 / 80) train acc: 0.103000; val_acc: 0.090000\n",
      "(Iteration 7301 / 30560) loss: 2.304372\n",
      "(Iteration 7401 / 30560) loss: 2.304387\n",
      "(Iteration 7501 / 30560) loss: 2.304366\n",
      "(Iteration 7601 / 30560) loss: 2.304397\n",
      "(Epoch 20 / 80) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 7701 / 30560) loss: 2.304383\n",
      "(Iteration 7801 / 30560) loss: 2.304362\n",
      "(Iteration 7901 / 30560) loss: 2.304349\n",
      "(Iteration 8001 / 30560) loss: 2.304354\n",
      "(Epoch 21 / 80) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 8101 / 30560) loss: 2.304346\n",
      "(Iteration 8201 / 30560) loss: 2.304348\n",
      "(Iteration 8301 / 30560) loss: 2.304365\n",
      "(Iteration 8401 / 30560) loss: 2.304369\n",
      "(Epoch 22 / 80) train acc: 0.110000; val_acc: 0.088000\n",
      "(Iteration 8501 / 30560) loss: 2.304338\n",
      "(Iteration 8601 / 30560) loss: 2.304336\n",
      "(Iteration 8701 / 30560) loss: 2.304336\n",
      "(Epoch 23 / 80) train acc: 0.090000; val_acc: 0.090000\n",
      "(Iteration 8801 / 30560) loss: 2.304320\n",
      "(Iteration 8901 / 30560) loss: 2.304347\n",
      "(Iteration 9001 / 30560) loss: 2.304346\n",
      "(Iteration 9101 / 30560) loss: 2.304336\n",
      "(Epoch 24 / 80) train acc: 0.100000; val_acc: 0.089000\n",
      "(Iteration 9201 / 30560) loss: 2.304320\n",
      "(Iteration 9301 / 30560) loss: 2.304336\n",
      "(Iteration 9401 / 30560) loss: 2.304308\n",
      "(Iteration 9501 / 30560) loss: 2.304332\n",
      "(Epoch 25 / 80) train acc: 0.112000; val_acc: 0.089000\n",
      "(Iteration 9601 / 30560) loss: 2.304321\n",
      "(Iteration 9701 / 30560) loss: 2.304335\n",
      "(Iteration 9801 / 30560) loss: 2.304311\n",
      "(Iteration 9901 / 30560) loss: 2.304325\n",
      "(Epoch 26 / 80) train acc: 0.112000; val_acc: 0.088000\n",
      "(Iteration 10001 / 30560) loss: 2.304336\n",
      "(Iteration 10101 / 30560) loss: 2.304330\n",
      "(Iteration 10201 / 30560) loss: 2.304287\n",
      "(Iteration 10301 / 30560) loss: 2.304305\n",
      "(Epoch 27 / 80) train acc: 0.094000; val_acc: 0.088000\n",
      "(Iteration 10401 / 30560) loss: 2.304328\n",
      "(Iteration 10501 / 30560) loss: 2.304301\n",
      "(Iteration 10601 / 30560) loss: 2.304300\n",
      "(Epoch 28 / 80) train acc: 0.095000; val_acc: 0.088000\n",
      "(Iteration 10701 / 30560) loss: 2.304301\n",
      "(Iteration 10801 / 30560) loss: 2.304296\n",
      "(Iteration 10901 / 30560) loss: 2.304295\n",
      "(Iteration 11001 / 30560) loss: 2.304282\n",
      "(Epoch 29 / 80) train acc: 0.093000; val_acc: 0.088000\n",
      "(Iteration 11101 / 30560) loss: 2.304311\n",
      "(Iteration 11201 / 30560) loss: 2.304293\n",
      "(Iteration 11301 / 30560) loss: 2.304288\n",
      "(Iteration 11401 / 30560) loss: 2.304296\n",
      "(Epoch 30 / 80) train acc: 0.099000; val_acc: 0.088000\n",
      "(Iteration 11501 / 30560) loss: 2.304287\n",
      "(Iteration 11601 / 30560) loss: 2.304327\n",
      "(Iteration 11701 / 30560) loss: 2.304292\n",
      "(Iteration 11801 / 30560) loss: 2.304286\n",
      "(Epoch 31 / 80) train acc: 0.113000; val_acc: 0.088000\n",
      "(Iteration 11901 / 30560) loss: 2.304308\n",
      "(Iteration 12001 / 30560) loss: 2.304297\n",
      "(Iteration 12101 / 30560) loss: 2.304301\n",
      "(Iteration 12201 / 30560) loss: 2.304297\n",
      "(Epoch 32 / 80) train acc: 0.114000; val_acc: 0.088000\n",
      "(Iteration 12301 / 30560) loss: 2.304285\n",
      "(Iteration 12401 / 30560) loss: 2.304297\n",
      "(Iteration 12501 / 30560) loss: 2.304284\n",
      "(Iteration 12601 / 30560) loss: 2.304302\n",
      "(Epoch 33 / 80) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 12701 / 30560) loss: 2.304269\n",
      "(Iteration 12801 / 30560) loss: 2.304293\n",
      "(Iteration 12901 / 30560) loss: 2.304299\n",
      "(Epoch 34 / 80) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 13001 / 30560) loss: 2.304292\n",
      "(Iteration 13101 / 30560) loss: 2.304277\n",
      "(Iteration 13201 / 30560) loss: 2.304290\n",
      "(Iteration 13301 / 30560) loss: 2.304277\n",
      "(Epoch 35 / 80) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 13401 / 30560) loss: 2.304312\n",
      "(Iteration 13501 / 30560) loss: 2.304260\n",
      "(Iteration 13601 / 30560) loss: 2.304305\n",
      "(Iteration 13701 / 30560) loss: 2.304254\n",
      "(Epoch 36 / 80) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 13801 / 30560) loss: 2.304267\n",
      "(Iteration 13901 / 30560) loss: 2.304257\n",
      "(Iteration 14001 / 30560) loss: 2.304270\n",
      "(Iteration 14101 / 30560) loss: 2.304279\n",
      "(Epoch 37 / 80) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 14201 / 30560) loss: 2.304278\n",
      "(Iteration 14301 / 30560) loss: 2.304305\n",
      "(Iteration 14401 / 30560) loss: 2.304271\n",
      "(Iteration 14501 / 30560) loss: 2.304288\n",
      "(Epoch 38 / 80) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 14601 / 30560) loss: 2.304300\n",
      "(Iteration 14701 / 30560) loss: 2.304264\n",
      "(Iteration 14801 / 30560) loss: 2.304282\n",
      "(Epoch 39 / 80) train acc: 0.087000; val_acc: 0.087000\n",
      "(Iteration 14901 / 30560) loss: 2.304266\n",
      "(Iteration 15001 / 30560) loss: 2.304266\n",
      "(Iteration 15101 / 30560) loss: 2.304275\n",
      "(Iteration 15201 / 30560) loss: 2.304247\n",
      "(Epoch 40 / 80) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 15301 / 30560) loss: 2.304262\n",
      "(Iteration 15401 / 30560) loss: 2.304243\n",
      "(Iteration 15501 / 30560) loss: 2.304241\n",
      "(Iteration 15601 / 30560) loss: 2.304272\n",
      "(Epoch 41 / 80) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 15701 / 30560) loss: 2.304262\n",
      "(Iteration 15801 / 30560) loss: 2.304276\n",
      "(Iteration 15901 / 30560) loss: 2.304256\n",
      "(Iteration 16001 / 30560) loss: 2.304261\n",
      "(Epoch 42 / 80) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 16101 / 30560) loss: 2.304240\n",
      "(Iteration 16201 / 30560) loss: 2.304270\n",
      "(Iteration 16301 / 30560) loss: 2.304277\n",
      "(Iteration 16401 / 30560) loss: 2.304243\n",
      "(Epoch 43 / 80) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 16501 / 30560) loss: 2.304277\n",
      "(Iteration 16601 / 30560) loss: 2.304276\n",
      "(Iteration 16701 / 30560) loss: 2.304266\n",
      "(Iteration 16801 / 30560) loss: 2.304276\n",
      "(Epoch 44 / 80) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 16901 / 30560) loss: 2.304255\n",
      "(Iteration 17001 / 30560) loss: 2.304265\n",
      "(Iteration 17101 / 30560) loss: 2.304270\n",
      "(Epoch 45 / 80) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 17201 / 30560) loss: 2.304261\n",
      "(Iteration 17301 / 30560) loss: 2.304281\n",
      "(Iteration 17401 / 30560) loss: 2.304263\n",
      "(Iteration 17501 / 30560) loss: 2.304284\n",
      "(Epoch 46 / 80) train acc: 0.109000; val_acc: 0.087000\n",
      "(Iteration 17601 / 30560) loss: 2.304271\n",
      "(Iteration 17701 / 30560) loss: 2.304247\n",
      "(Iteration 17801 / 30560) loss: 2.304276\n",
      "(Iteration 17901 / 30560) loss: 2.304240\n",
      "(Epoch 47 / 80) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 18001 / 30560) loss: 2.304270\n",
      "(Iteration 18101 / 30560) loss: 2.304256\n",
      "(Iteration 18201 / 30560) loss: 2.304256\n",
      "(Iteration 18301 / 30560) loss: 2.304243\n",
      "(Epoch 48 / 80) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 18401 / 30560) loss: 2.304271\n",
      "(Iteration 18501 / 30560) loss: 2.304265\n",
      "(Iteration 18601 / 30560) loss: 2.304256\n",
      "(Iteration 18701 / 30560) loss: 2.304281\n",
      "(Epoch 49 / 80) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 18801 / 30560) loss: 2.304265\n",
      "(Iteration 18901 / 30560) loss: 2.304268\n",
      "(Iteration 19001 / 30560) loss: 2.304262\n",
      "(Epoch 50 / 80) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 19101 / 30560) loss: 2.304236\n",
      "(Iteration 19201 / 30560) loss: 2.304285\n",
      "(Iteration 19301 / 30560) loss: 2.304257\n",
      "(Iteration 19401 / 30560) loss: 2.304246\n",
      "(Epoch 51 / 80) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 19501 / 30560) loss: 2.304282\n",
      "(Iteration 19601 / 30560) loss: 2.304243\n",
      "(Iteration 19701 / 30560) loss: 2.304234\n",
      "(Iteration 19801 / 30560) loss: 2.304254\n",
      "(Epoch 52 / 80) train acc: 0.112000; val_acc: 0.087000\n",
      "(Iteration 19901 / 30560) loss: 2.304264\n",
      "(Iteration 20001 / 30560) loss: 2.304259\n",
      "(Iteration 20101 / 30560) loss: 2.304262\n",
      "(Iteration 20201 / 30560) loss: 2.304276\n",
      "(Epoch 53 / 80) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 20301 / 30560) loss: 2.304259\n",
      "(Iteration 20401 / 30560) loss: 2.304259\n",
      "(Iteration 20501 / 30560) loss: 2.304242\n",
      "(Iteration 20601 / 30560) loss: 2.304286\n",
      "(Epoch 54 / 80) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 20701 / 30560) loss: 2.304248\n",
      "(Iteration 20801 / 30560) loss: 2.304263\n",
      "(Iteration 20901 / 30560) loss: 2.304247\n",
      "(Iteration 21001 / 30560) loss: 2.304232\n",
      "(Epoch 55 / 80) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 21101 / 30560) loss: 2.304237\n",
      "(Iteration 21201 / 30560) loss: 2.304268\n",
      "(Iteration 21301 / 30560) loss: 2.304264\n",
      "(Epoch 56 / 80) train acc: 0.116000; val_acc: 0.087000\n",
      "(Iteration 21401 / 30560) loss: 2.304260\n",
      "(Iteration 21501 / 30560) loss: 2.304271\n",
      "(Iteration 21601 / 30560) loss: 2.304274\n",
      "(Iteration 21701 / 30560) loss: 2.304272\n",
      "(Epoch 57 / 80) train acc: 0.089000; val_acc: 0.087000\n",
      "(Iteration 21801 / 30560) loss: 2.304253\n",
      "(Iteration 21901 / 30560) loss: 2.304243\n",
      "(Iteration 22001 / 30560) loss: 2.304223\n",
      "(Iteration 22101 / 30560) loss: 2.304255\n",
      "(Epoch 58 / 80) train acc: 0.086000; val_acc: 0.087000\n",
      "(Iteration 22201 / 30560) loss: 2.304259\n",
      "(Iteration 22301 / 30560) loss: 2.304257\n",
      "(Iteration 22401 / 30560) loss: 2.304278\n",
      "(Iteration 22501 / 30560) loss: 2.304237\n",
      "(Epoch 59 / 80) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 22601 / 30560) loss: 2.304245\n",
      "(Iteration 22701 / 30560) loss: 2.304253\n",
      "(Iteration 22801 / 30560) loss: 2.304273\n",
      "(Iteration 22901 / 30560) loss: 2.304239\n",
      "(Epoch 60 / 80) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 23001 / 30560) loss: 2.304268\n",
      "(Iteration 23101 / 30560) loss: 2.304270\n",
      "(Iteration 23201 / 30560) loss: 2.304250\n",
      "(Iteration 23301 / 30560) loss: 2.304269\n",
      "(Epoch 61 / 80) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 23401 / 30560) loss: 2.304263\n",
      "(Iteration 23501 / 30560) loss: 2.304274\n",
      "(Iteration 23601 / 30560) loss: 2.304266\n",
      "(Epoch 62 / 80) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 23701 / 30560) loss: 2.304269\n",
      "(Iteration 23801 / 30560) loss: 2.304258\n",
      "(Iteration 23901 / 30560) loss: 2.304242\n",
      "(Iteration 24001 / 30560) loss: 2.304254\n",
      "(Epoch 63 / 80) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 24101 / 30560) loss: 2.304287\n",
      "(Iteration 24201 / 30560) loss: 2.304250\n",
      "(Iteration 24301 / 30560) loss: 2.304239\n",
      "(Iteration 24401 / 30560) loss: 2.304256\n",
      "(Epoch 64 / 80) train acc: 0.082000; val_acc: 0.087000\n",
      "(Iteration 24501 / 30560) loss: 2.304246\n",
      "(Iteration 24601 / 30560) loss: 2.304253\n",
      "(Iteration 24701 / 30560) loss: 2.304265\n",
      "(Iteration 24801 / 30560) loss: 2.304264\n",
      "(Epoch 65 / 80) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 24901 / 30560) loss: 2.304271\n",
      "(Iteration 25001 / 30560) loss: 2.304251\n",
      "(Iteration 25101 / 30560) loss: 2.304276\n",
      "(Iteration 25201 / 30560) loss: 2.304247\n",
      "(Epoch 66 / 80) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 25301 / 30560) loss: 2.304250\n",
      "(Iteration 25401 / 30560) loss: 2.304301\n",
      "(Iteration 25501 / 30560) loss: 2.304259\n",
      "(Epoch 67 / 80) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 25601 / 30560) loss: 2.304242\n",
      "(Iteration 25701 / 30560) loss: 2.304253\n",
      "(Iteration 25801 / 30560) loss: 2.304262\n",
      "(Iteration 25901 / 30560) loss: 2.304285\n",
      "(Epoch 68 / 80) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 26001 / 30560) loss: 2.304261\n",
      "(Iteration 26101 / 30560) loss: 2.304237\n",
      "(Iteration 26201 / 30560) loss: 2.304253\n",
      "(Iteration 26301 / 30560) loss: 2.304261\n",
      "(Epoch 69 / 80) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 26401 / 30560) loss: 2.304239\n",
      "(Iteration 26501 / 30560) loss: 2.304248\n",
      "(Iteration 26601 / 30560) loss: 2.304264\n",
      "(Iteration 26701 / 30560) loss: 2.304263\n",
      "(Epoch 70 / 80) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 26801 / 30560) loss: 2.304241\n",
      "(Iteration 26901 / 30560) loss: 2.304267\n",
      "(Iteration 27001 / 30560) loss: 2.304226\n",
      "(Iteration 27101 / 30560) loss: 2.304270\n",
      "(Epoch 71 / 80) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 27201 / 30560) loss: 2.304247\n",
      "(Iteration 27301 / 30560) loss: 2.304279\n",
      "(Iteration 27401 / 30560) loss: 2.304250\n",
      "(Iteration 27501 / 30560) loss: 2.304235\n",
      "(Epoch 72 / 80) train acc: 0.086000; val_acc: 0.087000\n",
      "(Iteration 27601 / 30560) loss: 2.304274\n",
      "(Iteration 27701 / 30560) loss: 2.304270\n",
      "(Iteration 27801 / 30560) loss: 2.304281\n",
      "(Epoch 73 / 80) train acc: 0.083000; val_acc: 0.087000\n",
      "(Iteration 27901 / 30560) loss: 2.304266\n",
      "(Iteration 28001 / 30560) loss: 2.304259\n",
      "(Iteration 28101 / 30560) loss: 2.304248\n",
      "(Iteration 28201 / 30560) loss: 2.304259\n",
      "(Epoch 74 / 80) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 28301 / 30560) loss: 2.304246\n",
      "(Iteration 28401 / 30560) loss: 2.304287\n",
      "(Iteration 28501 / 30560) loss: 2.304231\n",
      "(Iteration 28601 / 30560) loss: 2.304248\n",
      "(Epoch 75 / 80) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 28701 / 30560) loss: 2.304266\n",
      "(Iteration 28801 / 30560) loss: 2.304258\n",
      "(Iteration 28901 / 30560) loss: 2.304287\n",
      "(Iteration 29001 / 30560) loss: 2.304243\n",
      "(Epoch 76 / 80) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 29101 / 30560) loss: 2.304239\n",
      "(Iteration 29201 / 30560) loss: 2.304260\n",
      "(Iteration 29301 / 30560) loss: 2.304246\n",
      "(Iteration 29401 / 30560) loss: 2.304285\n",
      "(Epoch 77 / 80) train acc: 0.086000; val_acc: 0.087000\n",
      "(Iteration 29501 / 30560) loss: 2.304271\n",
      "(Iteration 29601 / 30560) loss: 2.304271\n",
      "(Iteration 29701 / 30560) loss: 2.304264\n",
      "(Epoch 78 / 80) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 29801 / 30560) loss: 2.304263\n",
      "(Iteration 29901 / 30560) loss: 2.304273\n",
      "(Iteration 30001 / 30560) loss: 2.304264\n",
      "(Iteration 30101 / 30560) loss: 2.304266\n",
      "(Epoch 79 / 80) train acc: 0.090000; val_acc: 0.087000\n",
      "(Iteration 30201 / 30560) loss: 2.304288\n",
      "(Iteration 30301 / 30560) loss: 2.304257\n",
      "(Iteration 30401 / 30560) loss: 2.304268\n",
      "(Iteration 30501 / 30560) loss: 2.304272\n",
      "(Epoch 80 / 80) train acc: 0.098000; val_acc: 0.087000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.305423\n",
      "(Epoch 0 / 80) train acc: 0.079000; val_acc: 0.079000\n",
      "(Iteration 101 / 61200) loss: 2.305396\n",
      "(Iteration 201 / 61200) loss: 2.305342\n",
      "(Iteration 301 / 61200) loss: 2.305328\n",
      "(Iteration 401 / 61200) loss: 2.305234\n",
      "(Iteration 501 / 61200) loss: 2.305221\n",
      "(Iteration 601 / 61200) loss: 2.305214\n",
      "(Iteration 701 / 61200) loss: 2.305114\n",
      "(Epoch 1 / 80) train acc: 0.083000; val_acc: 0.076000\n",
      "(Iteration 801 / 61200) loss: 2.305132\n",
      "(Iteration 901 / 61200) loss: 2.305090\n",
      "(Iteration 1001 / 61200) loss: 2.305065\n",
      "(Iteration 1101 / 61200) loss: 2.305048\n",
      "(Iteration 1201 / 61200) loss: 2.305018\n",
      "(Iteration 1301 / 61200) loss: 2.304997\n",
      "(Iteration 1401 / 61200) loss: 2.304917\n",
      "(Iteration 1501 / 61200) loss: 2.304879\n",
      "(Epoch 2 / 80) train acc: 0.109000; val_acc: 0.089000\n",
      "(Iteration 1601 / 61200) loss: 2.304845\n",
      "(Iteration 1701 / 61200) loss: 2.304824\n",
      "(Iteration 1801 / 61200) loss: 2.304816\n",
      "(Iteration 1901 / 61200) loss: 2.304794\n",
      "(Iteration 2001 / 61200) loss: 2.304735\n",
      "(Iteration 2101 / 61200) loss: 2.304729\n",
      "(Iteration 2201 / 61200) loss: 2.304682\n",
      "(Epoch 3 / 80) train acc: 0.116000; val_acc: 0.083000\n",
      "(Iteration 2301 / 61200) loss: 2.304667\n",
      "(Iteration 2401 / 61200) loss: 2.304657\n",
      "(Iteration 2501 / 61200) loss: 2.304646\n",
      "(Iteration 2601 / 61200) loss: 2.304598\n",
      "(Iteration 2701 / 61200) loss: 2.304556\n",
      "(Iteration 2801 / 61200) loss: 2.304509\n",
      "(Iteration 2901 / 61200) loss: 2.304493\n",
      "(Iteration 3001 / 61200) loss: 2.304545\n",
      "(Epoch 4 / 80) train acc: 0.088000; val_acc: 0.095000\n",
      "(Iteration 3101 / 61200) loss: 2.304506\n",
      "(Iteration 3201 / 61200) loss: 2.304495\n",
      "(Iteration 3301 / 61200) loss: 2.304433\n",
      "(Iteration 3401 / 61200) loss: 2.304377\n",
      "(Iteration 3501 / 61200) loss: 2.304495\n",
      "(Iteration 3601 / 61200) loss: 2.304327\n",
      "(Iteration 3701 / 61200) loss: 2.304288\n",
      "(Iteration 3801 / 61200) loss: 2.304339\n",
      "(Epoch 5 / 80) train acc: 0.096000; val_acc: 0.098000\n",
      "(Iteration 3901 / 61200) loss: 2.304314\n",
      "(Iteration 4001 / 61200) loss: 2.304293\n",
      "(Iteration 4101 / 61200) loss: 2.304280\n",
      "(Iteration 4201 / 61200) loss: 2.304224\n",
      "(Iteration 4301 / 61200) loss: 2.304261\n",
      "(Iteration 4401 / 61200) loss: 2.304189\n",
      "(Iteration 4501 / 61200) loss: 2.304266\n",
      "(Epoch 6 / 80) train acc: 0.101000; val_acc: 0.082000\n",
      "(Iteration 4601 / 61200) loss: 2.304197\n",
      "(Iteration 4701 / 61200) loss: 2.304176\n",
      "(Iteration 4801 / 61200) loss: 2.304197\n",
      "(Iteration 4901 / 61200) loss: 2.304203\n",
      "(Iteration 5001 / 61200) loss: 2.304106\n",
      "(Iteration 5101 / 61200) loss: 2.304102\n",
      "(Iteration 5201 / 61200) loss: 2.304092\n",
      "(Iteration 5301 / 61200) loss: 2.304104\n",
      "(Epoch 7 / 80) train acc: 0.119000; val_acc: 0.084000\n",
      "(Iteration 5401 / 61200) loss: 2.304063\n",
      "(Iteration 5501 / 61200) loss: 2.304046\n",
      "(Iteration 5601 / 61200) loss: 2.304011\n",
      "(Iteration 5701 / 61200) loss: 2.304027\n",
      "(Iteration 5801 / 61200) loss: 2.304051\n",
      "(Iteration 5901 / 61200) loss: 2.303985\n",
      "(Iteration 6001 / 61200) loss: 2.303955\n",
      "(Iteration 6101 / 61200) loss: 2.303968\n",
      "(Epoch 8 / 80) train acc: 0.083000; val_acc: 0.114000\n",
      "(Iteration 6201 / 61200) loss: 2.303946\n",
      "(Iteration 6301 / 61200) loss: 2.303924\n",
      "(Iteration 6401 / 61200) loss: 2.303941\n",
      "(Iteration 6501 / 61200) loss: 2.303862\n",
      "(Iteration 6601 / 61200) loss: 2.303947\n",
      "(Iteration 6701 / 61200) loss: 2.303857\n",
      "(Iteration 6801 / 61200) loss: 2.303837\n",
      "(Epoch 9 / 80) train acc: 0.090000; val_acc: 0.113000\n",
      "(Iteration 6901 / 61200) loss: 2.303903\n",
      "(Iteration 7001 / 61200) loss: 2.303835\n",
      "(Iteration 7101 / 61200) loss: 2.303868\n",
      "(Iteration 7201 / 61200) loss: 2.303846\n",
      "(Iteration 7301 / 61200) loss: 2.303822\n",
      "(Iteration 7401 / 61200) loss: 2.303833\n",
      "(Iteration 7501 / 61200) loss: 2.303726\n",
      "(Iteration 7601 / 61200) loss: 2.303799\n",
      "(Epoch 10 / 80) train acc: 0.133000; val_acc: 0.154000\n",
      "(Iteration 7701 / 61200) loss: 2.303753\n",
      "(Iteration 7801 / 61200) loss: 2.303734\n",
      "(Iteration 7901 / 61200) loss: 2.303814\n",
      "(Iteration 8001 / 61200) loss: 2.303701\n",
      "(Iteration 8101 / 61200) loss: 2.303777\n",
      "(Iteration 8201 / 61200) loss: 2.303737\n",
      "(Iteration 8301 / 61200) loss: 2.303682\n",
      "(Iteration 8401 / 61200) loss: 2.303776\n",
      "(Epoch 11 / 80) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 8501 / 61200) loss: 2.303734\n",
      "(Iteration 8601 / 61200) loss: 2.303610\n",
      "(Iteration 8701 / 61200) loss: 2.303714\n",
      "(Iteration 8801 / 61200) loss: 2.303706\n",
      "(Iteration 8901 / 61200) loss: 2.303623\n",
      "(Iteration 9001 / 61200) loss: 2.303611\n",
      "(Iteration 9101 / 61200) loss: 2.303724\n",
      "(Epoch 12 / 80) train acc: 0.091000; val_acc: 0.098000\n",
      "(Iteration 9201 / 61200) loss: 2.303647\n",
      "(Iteration 9301 / 61200) loss: 2.303601\n",
      "(Iteration 9401 / 61200) loss: 2.303626\n",
      "(Iteration 9501 / 61200) loss: 2.303605\n",
      "(Iteration 9601 / 61200) loss: 2.303673\n",
      "(Iteration 9701 / 61200) loss: 2.303596\n",
      "(Iteration 9801 / 61200) loss: 2.303594\n",
      "(Iteration 9901 / 61200) loss: 2.303581\n",
      "(Epoch 13 / 80) train acc: 0.105000; val_acc: 0.098000\n",
      "(Iteration 10001 / 61200) loss: 2.303607\n",
      "(Iteration 10101 / 61200) loss: 2.303592\n",
      "(Iteration 10201 / 61200) loss: 2.303559\n",
      "(Iteration 10301 / 61200) loss: 2.303596\n",
      "(Iteration 10401 / 61200) loss: 2.303559\n",
      "(Iteration 10501 / 61200) loss: 2.303573\n",
      "(Iteration 10601 / 61200) loss: 2.303561\n",
      "(Iteration 10701 / 61200) loss: 2.303525\n",
      "(Epoch 14 / 80) train acc: 0.109000; val_acc: 0.105000\n",
      "(Iteration 10801 / 61200) loss: 2.303532\n",
      "(Iteration 10901 / 61200) loss: 2.303460\n",
      "(Iteration 11001 / 61200) loss: 2.303565\n",
      "(Iteration 11101 / 61200) loss: 2.303448\n",
      "(Iteration 11201 / 61200) loss: 2.303414\n",
      "(Iteration 11301 / 61200) loss: 2.303525\n",
      "(Iteration 11401 / 61200) loss: 2.303443\n",
      "(Epoch 15 / 80) train acc: 0.101000; val_acc: 0.098000\n",
      "(Iteration 11501 / 61200) loss: 2.303556\n",
      "(Iteration 11601 / 61200) loss: 2.303446\n",
      "(Iteration 11701 / 61200) loss: 2.303528\n",
      "(Iteration 11801 / 61200) loss: 2.303469\n",
      "(Iteration 11901 / 61200) loss: 2.303407\n",
      "(Iteration 12001 / 61200) loss: 2.303334\n",
      "(Iteration 12101 / 61200) loss: 2.303562\n",
      "(Iteration 12201 / 61200) loss: 2.303373\n",
      "(Epoch 16 / 80) train acc: 0.098000; val_acc: 0.098000\n",
      "(Iteration 12301 / 61200) loss: 2.303444\n",
      "(Iteration 12401 / 61200) loss: 2.303451\n",
      "(Iteration 12501 / 61200) loss: 2.303368\n",
      "(Iteration 12601 / 61200) loss: 2.303417\n",
      "(Iteration 12701 / 61200) loss: 2.303436\n",
      "(Iteration 12801 / 61200) loss: 2.303496\n",
      "(Iteration 12901 / 61200) loss: 2.303404\n",
      "(Iteration 13001 / 61200) loss: 2.303411\n",
      "(Epoch 17 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 13101 / 61200) loss: 2.303444\n",
      "(Iteration 13201 / 61200) loss: 2.303389\n",
      "(Iteration 13301 / 61200) loss: 2.303448\n",
      "(Iteration 13401 / 61200) loss: 2.303489\n",
      "(Iteration 13501 / 61200) loss: 2.303432\n",
      "(Iteration 13601 / 61200) loss: 2.303394\n",
      "(Iteration 13701 / 61200) loss: 2.303393\n",
      "(Epoch 18 / 80) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 13801 / 61200) loss: 2.303421\n",
      "(Iteration 13901 / 61200) loss: 2.303364\n",
      "(Iteration 14001 / 61200) loss: 2.303343\n",
      "(Iteration 14101 / 61200) loss: 2.303310\n",
      "(Iteration 14201 / 61200) loss: 2.303340\n",
      "(Iteration 14301 / 61200) loss: 2.303416\n",
      "(Iteration 14401 / 61200) loss: 2.303297\n",
      "(Iteration 14501 / 61200) loss: 2.303352\n",
      "(Epoch 19 / 80) train acc: 0.102000; val_acc: 0.084000\n",
      "(Iteration 14601 / 61200) loss: 2.303300\n",
      "(Iteration 14701 / 61200) loss: 2.303347\n",
      "(Iteration 14801 / 61200) loss: 2.303401\n",
      "(Iteration 14901 / 61200) loss: 2.303335\n",
      "(Iteration 15001 / 61200) loss: 2.303327\n",
      "(Iteration 15101 / 61200) loss: 2.303213\n",
      "(Iteration 15201 / 61200) loss: 2.303305\n",
      "(Epoch 20 / 80) train acc: 0.110000; val_acc: 0.086000\n",
      "(Iteration 15301 / 61200) loss: 2.303269\n",
      "(Iteration 15401 / 61200) loss: 2.303405\n",
      "(Iteration 15501 / 61200) loss: 2.303206\n",
      "(Iteration 15601 / 61200) loss: 2.303272\n",
      "(Iteration 15701 / 61200) loss: 2.303257\n",
      "(Iteration 15801 / 61200) loss: 2.303219\n",
      "(Iteration 15901 / 61200) loss: 2.303222\n",
      "(Iteration 16001 / 61200) loss: 2.303262\n",
      "(Epoch 21 / 80) train acc: 0.125000; val_acc: 0.079000\n",
      "(Iteration 16101 / 61200) loss: 2.303393\n",
      "(Iteration 16201 / 61200) loss: 2.303254\n",
      "(Iteration 16301 / 61200) loss: 2.303241\n",
      "(Iteration 16401 / 61200) loss: 2.303202\n",
      "(Iteration 16501 / 61200) loss: 2.303229\n",
      "(Iteration 16601 / 61200) loss: 2.303183\n",
      "(Iteration 16701 / 61200) loss: 2.303255\n",
      "(Iteration 16801 / 61200) loss: 2.303227\n",
      "(Epoch 22 / 80) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 16901 / 61200) loss: 2.303199\n",
      "(Iteration 17001 / 61200) loss: 2.303243\n",
      "(Iteration 17101 / 61200) loss: 2.303206\n",
      "(Iteration 17201 / 61200) loss: 2.303259\n",
      "(Iteration 17301 / 61200) loss: 2.303305\n",
      "(Iteration 17401 / 61200) loss: 2.303255\n",
      "(Iteration 17501 / 61200) loss: 2.303227\n",
      "(Epoch 23 / 80) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 17601 / 61200) loss: 2.303192\n",
      "(Iteration 17701 / 61200) loss: 2.303167\n",
      "(Iteration 17801 / 61200) loss: 2.303170\n",
      "(Iteration 17901 / 61200) loss: 2.303254\n",
      "(Iteration 18001 / 61200) loss: 2.303155\n",
      "(Iteration 18101 / 61200) loss: 2.303145\n",
      "(Iteration 18201 / 61200) loss: 2.303195\n",
      "(Iteration 18301 / 61200) loss: 2.303190\n",
      "(Epoch 24 / 80) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 18401 / 61200) loss: 2.303227\n",
      "(Iteration 18501 / 61200) loss: 2.303187\n",
      "(Iteration 18601 / 61200) loss: 2.303186\n",
      "(Iteration 18701 / 61200) loss: 2.303204\n",
      "(Iteration 18801 / 61200) loss: 2.303172\n",
      "(Iteration 18901 / 61200) loss: 2.303141\n",
      "(Iteration 19001 / 61200) loss: 2.303169\n",
      "(Iteration 19101 / 61200) loss: 2.303168\n",
      "(Epoch 25 / 80) train acc: 0.082000; val_acc: 0.079000\n",
      "(Iteration 19201 / 61200) loss: 2.303204\n",
      "(Iteration 19301 / 61200) loss: 2.303076\n",
      "(Iteration 19401 / 61200) loss: 2.303232\n",
      "(Iteration 19501 / 61200) loss: 2.303088\n",
      "(Iteration 19601 / 61200) loss: 2.303210\n",
      "(Iteration 19701 / 61200) loss: 2.303119\n",
      "(Iteration 19801 / 61200) loss: 2.303110\n",
      "(Epoch 26 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 19901 / 61200) loss: 2.303151\n",
      "(Iteration 20001 / 61200) loss: 2.303241\n",
      "(Iteration 20101 / 61200) loss: 2.303164\n",
      "(Iteration 20201 / 61200) loss: 2.303147\n",
      "(Iteration 20301 / 61200) loss: 2.303119\n",
      "(Iteration 20401 / 61200) loss: 2.303161\n",
      "(Iteration 20501 / 61200) loss: 2.303168\n",
      "(Iteration 20601 / 61200) loss: 2.303227\n",
      "(Epoch 27 / 80) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 20701 / 61200) loss: 2.303099\n",
      "(Iteration 20801 / 61200) loss: 2.303076\n",
      "(Iteration 20901 / 61200) loss: 2.303126\n",
      "(Iteration 21001 / 61200) loss: 2.303116\n",
      "(Iteration 21101 / 61200) loss: 2.303158\n",
      "(Iteration 21201 / 61200) loss: 2.303149\n",
      "(Iteration 21301 / 61200) loss: 2.303076\n",
      "(Iteration 21401 / 61200) loss: 2.303084\n",
      "(Epoch 28 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 21501 / 61200) loss: 2.303188\n",
      "(Iteration 21601 / 61200) loss: 2.303149\n",
      "(Iteration 21701 / 61200) loss: 2.303107\n",
      "(Iteration 21801 / 61200) loss: 2.303190\n",
      "(Iteration 21901 / 61200) loss: 2.303101\n",
      "(Iteration 22001 / 61200) loss: 2.303185\n",
      "(Iteration 22101 / 61200) loss: 2.303157\n",
      "(Epoch 29 / 80) train acc: 0.108000; val_acc: 0.089000\n",
      "(Iteration 22201 / 61200) loss: 2.303150\n",
      "(Iteration 22301 / 61200) loss: 2.303101\n",
      "(Iteration 22401 / 61200) loss: 2.303136\n",
      "(Iteration 22501 / 61200) loss: 2.303079\n",
      "(Iteration 22601 / 61200) loss: 2.303007\n",
      "(Iteration 22701 / 61200) loss: 2.303110\n",
      "(Iteration 22801 / 61200) loss: 2.303109\n",
      "(Iteration 22901 / 61200) loss: 2.303056\n",
      "(Epoch 30 / 80) train acc: 0.090000; val_acc: 0.079000\n",
      "(Iteration 23001 / 61200) loss: 2.303032\n",
      "(Iteration 23101 / 61200) loss: 2.303099\n",
      "(Iteration 23201 / 61200) loss: 2.303115\n",
      "(Iteration 23301 / 61200) loss: 2.303106\n",
      "(Iteration 23401 / 61200) loss: 2.303190\n",
      "(Iteration 23501 / 61200) loss: 2.303039\n",
      "(Iteration 23601 / 61200) loss: 2.303150\n",
      "(Iteration 23701 / 61200) loss: 2.303117\n",
      "(Epoch 31 / 80) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 23801 / 61200) loss: 2.303121\n",
      "(Iteration 23901 / 61200) loss: 2.303134\n",
      "(Iteration 24001 / 61200) loss: 2.303052\n",
      "(Iteration 24101 / 61200) loss: 2.303050\n",
      "(Iteration 24201 / 61200) loss: 2.303188\n",
      "(Iteration 24301 / 61200) loss: 2.303127\n",
      "(Iteration 24401 / 61200) loss: 2.303148\n",
      "(Epoch 32 / 80) train acc: 0.116000; val_acc: 0.078000\n",
      "(Iteration 24501 / 61200) loss: 2.303099\n",
      "(Iteration 24601 / 61200) loss: 2.303026\n",
      "(Iteration 24701 / 61200) loss: 2.303040\n",
      "(Iteration 24801 / 61200) loss: 2.303017\n",
      "(Iteration 24901 / 61200) loss: 2.303122\n",
      "(Iteration 25001 / 61200) loss: 2.303118\n",
      "(Iteration 25101 / 61200) loss: 2.303143\n",
      "(Iteration 25201 / 61200) loss: 2.303069\n",
      "(Epoch 33 / 80) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 25301 / 61200) loss: 2.303077\n",
      "(Iteration 25401 / 61200) loss: 2.303118\n",
      "(Iteration 25501 / 61200) loss: 2.302969\n",
      "(Iteration 25601 / 61200) loss: 2.303067\n",
      "(Iteration 25701 / 61200) loss: 2.303053\n",
      "(Iteration 25801 / 61200) loss: 2.303109\n",
      "(Iteration 25901 / 61200) loss: 2.303138\n",
      "(Iteration 26001 / 61200) loss: 2.303072\n",
      "(Epoch 34 / 80) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 26101 / 61200) loss: 2.303127\n",
      "(Iteration 26201 / 61200) loss: 2.303001\n",
      "(Iteration 26301 / 61200) loss: 2.303108\n",
      "(Iteration 26401 / 61200) loss: 2.303100\n",
      "(Iteration 26501 / 61200) loss: 2.303105\n",
      "(Iteration 26601 / 61200) loss: 2.303097\n",
      "(Iteration 26701 / 61200) loss: 2.302941\n",
      "(Epoch 35 / 80) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 26801 / 61200) loss: 2.303064\n",
      "(Iteration 26901 / 61200) loss: 2.303050\n",
      "(Iteration 27001 / 61200) loss: 2.302998\n",
      "(Iteration 27101 / 61200) loss: 2.303134\n",
      "(Iteration 27201 / 61200) loss: 2.303072\n",
      "(Iteration 27301 / 61200) loss: 2.303075\n",
      "(Iteration 27401 / 61200) loss: 2.303030\n",
      "(Iteration 27501 / 61200) loss: 2.303075\n",
      "(Epoch 36 / 80) train acc: 0.114000; val_acc: 0.078000\n",
      "(Iteration 27601 / 61200) loss: 2.302955\n",
      "(Iteration 27701 / 61200) loss: 2.303052\n",
      "(Iteration 27801 / 61200) loss: 2.303057\n",
      "(Iteration 27901 / 61200) loss: 2.302969\n",
      "(Iteration 28001 / 61200) loss: 2.303050\n",
      "(Iteration 28101 / 61200) loss: 2.302987\n",
      "(Iteration 28201 / 61200) loss: 2.303113\n",
      "(Iteration 28301 / 61200) loss: 2.303011\n",
      "(Epoch 37 / 80) train acc: 0.120000; val_acc: 0.078000\n",
      "(Iteration 28401 / 61200) loss: 2.302986\n",
      "(Iteration 28501 / 61200) loss: 2.303034\n",
      "(Iteration 28601 / 61200) loss: 2.303050\n",
      "(Iteration 28701 / 61200) loss: 2.302988\n",
      "(Iteration 28801 / 61200) loss: 2.303049\n",
      "(Iteration 28901 / 61200) loss: 2.303039\n",
      "(Iteration 29001 / 61200) loss: 2.303106\n",
      "(Epoch 38 / 80) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 29101 / 61200) loss: 2.303019\n",
      "(Iteration 29201 / 61200) loss: 2.302997\n",
      "(Iteration 29301 / 61200) loss: 2.303069\n",
      "(Iteration 29401 / 61200) loss: 2.303017\n",
      "(Iteration 29501 / 61200) loss: 2.302925\n",
      "(Iteration 29601 / 61200) loss: 2.303123\n",
      "(Iteration 29701 / 61200) loss: 2.303059\n",
      "(Iteration 29801 / 61200) loss: 2.302921\n",
      "(Epoch 39 / 80) train acc: 0.127000; val_acc: 0.078000\n",
      "(Iteration 29901 / 61200) loss: 2.302960\n",
      "(Iteration 30001 / 61200) loss: 2.303124\n",
      "(Iteration 30101 / 61200) loss: 2.302972\n",
      "(Iteration 30201 / 61200) loss: 2.303119\n",
      "(Iteration 30301 / 61200) loss: 2.303070\n",
      "(Iteration 30401 / 61200) loss: 2.303059\n",
      "(Iteration 30501 / 61200) loss: 2.303067\n",
      "(Epoch 40 / 80) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 30601 / 61200) loss: 2.303031\n",
      "(Iteration 30701 / 61200) loss: 2.302925\n",
      "(Iteration 30801 / 61200) loss: 2.302934\n",
      "(Iteration 30901 / 61200) loss: 2.302982\n",
      "(Iteration 31001 / 61200) loss: 2.303022\n",
      "(Iteration 31101 / 61200) loss: 2.302999\n",
      "(Iteration 31201 / 61200) loss: 2.303080\n",
      "(Iteration 31301 / 61200) loss: 2.303085\n",
      "(Epoch 41 / 80) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 31401 / 61200) loss: 2.303099\n",
      "(Iteration 31501 / 61200) loss: 2.302960\n",
      "(Iteration 31601 / 61200) loss: 2.303063\n",
      "(Iteration 31701 / 61200) loss: 2.303015\n",
      "(Iteration 31801 / 61200) loss: 2.302987\n",
      "(Iteration 31901 / 61200) loss: 2.302980\n",
      "(Iteration 32001 / 61200) loss: 2.302908\n",
      "(Iteration 32101 / 61200) loss: 2.302921\n",
      "(Epoch 42 / 80) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 32201 / 61200) loss: 2.302998\n",
      "(Iteration 32301 / 61200) loss: 2.302955\n",
      "(Iteration 32401 / 61200) loss: 2.302987\n",
      "(Iteration 32501 / 61200) loss: 2.302993\n",
      "(Iteration 32601 / 61200) loss: 2.303006\n",
      "(Iteration 32701 / 61200) loss: 2.302969\n",
      "(Iteration 32801 / 61200) loss: 2.302984\n",
      "(Epoch 43 / 80) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 32901 / 61200) loss: 2.303056\n",
      "(Iteration 33001 / 61200) loss: 2.303022\n",
      "(Iteration 33101 / 61200) loss: 2.302913\n",
      "(Iteration 33201 / 61200) loss: 2.303051\n",
      "(Iteration 33301 / 61200) loss: 2.303039\n",
      "(Iteration 33401 / 61200) loss: 2.302949\n",
      "(Iteration 33501 / 61200) loss: 2.303022\n",
      "(Iteration 33601 / 61200) loss: 2.302933\n",
      "(Epoch 44 / 80) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 33701 / 61200) loss: 2.302972\n",
      "(Iteration 33801 / 61200) loss: 2.302961\n",
      "(Iteration 33901 / 61200) loss: 2.303043\n",
      "(Iteration 34001 / 61200) loss: 2.303084\n",
      "(Iteration 34101 / 61200) loss: 2.302962\n",
      "(Iteration 34201 / 61200) loss: 2.303009\n",
      "(Iteration 34301 / 61200) loss: 2.303052\n",
      "(Iteration 34401 / 61200) loss: 2.302975\n",
      "(Epoch 45 / 80) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 34501 / 61200) loss: 2.303049\n",
      "(Iteration 34601 / 61200) loss: 2.303027\n",
      "(Iteration 34701 / 61200) loss: 2.303002\n",
      "(Iteration 34801 / 61200) loss: 2.302979\n",
      "(Iteration 34901 / 61200) loss: 2.303060\n",
      "(Iteration 35001 / 61200) loss: 2.303018\n",
      "(Iteration 35101 / 61200) loss: 2.303003\n",
      "(Epoch 46 / 80) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 35201 / 61200) loss: 2.302972\n",
      "(Iteration 35301 / 61200) loss: 2.303044\n",
      "(Iteration 35401 / 61200) loss: 2.303040\n",
      "(Iteration 35501 / 61200) loss: 2.302953\n",
      "(Iteration 35601 / 61200) loss: 2.302991\n",
      "(Iteration 35701 / 61200) loss: 2.303001\n",
      "(Iteration 35801 / 61200) loss: 2.302997\n",
      "(Iteration 35901 / 61200) loss: 2.302991\n",
      "(Epoch 47 / 80) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 36001 / 61200) loss: 2.302978\n",
      "(Iteration 36101 / 61200) loss: 2.303011\n",
      "(Iteration 36201 / 61200) loss: 2.302902\n",
      "(Iteration 36301 / 61200) loss: 2.302918\n",
      "(Iteration 36401 / 61200) loss: 2.303042\n",
      "(Iteration 36501 / 61200) loss: 2.302946\n",
      "(Iteration 36601 / 61200) loss: 2.303010\n",
      "(Iteration 36701 / 61200) loss: 2.302958\n",
      "(Epoch 48 / 80) train acc: 0.118000; val_acc: 0.078000\n",
      "(Iteration 36801 / 61200) loss: 2.302942\n",
      "(Iteration 36901 / 61200) loss: 2.302963\n",
      "(Iteration 37001 / 61200) loss: 2.303007\n",
      "(Iteration 37101 / 61200) loss: 2.303040\n",
      "(Iteration 37201 / 61200) loss: 2.303022\n",
      "(Iteration 37301 / 61200) loss: 2.302988\n",
      "(Iteration 37401 / 61200) loss: 2.303037\n",
      "(Epoch 49 / 80) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 37501 / 61200) loss: 2.303012\n",
      "(Iteration 37601 / 61200) loss: 2.302965\n",
      "(Iteration 37701 / 61200) loss: 2.302929\n",
      "(Iteration 37801 / 61200) loss: 2.303022\n",
      "(Iteration 37901 / 61200) loss: 2.302945\n",
      "(Iteration 38001 / 61200) loss: 2.302951\n",
      "(Iteration 38101 / 61200) loss: 2.303047\n",
      "(Iteration 38201 / 61200) loss: 2.302958\n",
      "(Epoch 50 / 80) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 38301 / 61200) loss: 2.302963\n",
      "(Iteration 38401 / 61200) loss: 2.303087\n",
      "(Iteration 38501 / 61200) loss: 2.302898\n",
      "(Iteration 38601 / 61200) loss: 2.303134\n",
      "(Iteration 38701 / 61200) loss: 2.303034\n",
      "(Iteration 38801 / 61200) loss: 2.302982\n",
      "(Iteration 38901 / 61200) loss: 2.302873\n",
      "(Iteration 39001 / 61200) loss: 2.302994\n",
      "(Epoch 51 / 80) train acc: 0.115000; val_acc: 0.078000\n",
      "(Iteration 39101 / 61200) loss: 2.303037\n",
      "(Iteration 39201 / 61200) loss: 2.303022\n",
      "(Iteration 39301 / 61200) loss: 2.302981\n",
      "(Iteration 39401 / 61200) loss: 2.302964\n",
      "(Iteration 39501 / 61200) loss: 2.302931\n",
      "(Iteration 39601 / 61200) loss: 2.302942\n",
      "(Iteration 39701 / 61200) loss: 2.302981\n",
      "(Epoch 52 / 80) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 39801 / 61200) loss: 2.302934\n",
      "(Iteration 39901 / 61200) loss: 2.303101\n",
      "(Iteration 40001 / 61200) loss: 2.302920\n",
      "(Iteration 40101 / 61200) loss: 2.302989\n",
      "(Iteration 40201 / 61200) loss: 2.302934\n",
      "(Iteration 40301 / 61200) loss: 2.302905\n",
      "(Iteration 40401 / 61200) loss: 2.302918\n",
      "(Iteration 40501 / 61200) loss: 2.302932\n",
      "(Epoch 53 / 80) train acc: 0.116000; val_acc: 0.078000\n",
      "(Iteration 40601 / 61200) loss: 2.302883\n",
      "(Iteration 40701 / 61200) loss: 2.303057\n",
      "(Iteration 40801 / 61200) loss: 2.303082\n",
      "(Iteration 40901 / 61200) loss: 2.302952\n",
      "(Iteration 41001 / 61200) loss: 2.302977\n",
      "(Iteration 41101 / 61200) loss: 2.303023\n",
      "(Iteration 41201 / 61200) loss: 2.302996\n",
      "(Iteration 41301 / 61200) loss: 2.302941\n",
      "(Epoch 54 / 80) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 41401 / 61200) loss: 2.302935\n",
      "(Iteration 41501 / 61200) loss: 2.302902\n",
      "(Iteration 41601 / 61200) loss: 2.302924\n",
      "(Iteration 41701 / 61200) loss: 2.302930\n",
      "(Iteration 41801 / 61200) loss: 2.302916\n",
      "(Iteration 41901 / 61200) loss: 2.302967\n",
      "(Iteration 42001 / 61200) loss: 2.302987\n",
      "(Epoch 55 / 80) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 42101 / 61200) loss: 2.303033\n",
      "(Iteration 42201 / 61200) loss: 2.302981\n",
      "(Iteration 42301 / 61200) loss: 2.302979\n",
      "(Iteration 42401 / 61200) loss: 2.302919\n",
      "(Iteration 42501 / 61200) loss: 2.302824\n",
      "(Iteration 42601 / 61200) loss: 2.302928\n",
      "(Iteration 42701 / 61200) loss: 2.303008\n",
      "(Iteration 42801 / 61200) loss: 2.302912\n",
      "(Epoch 56 / 80) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 42901 / 61200) loss: 2.302939\n",
      "(Iteration 43001 / 61200) loss: 2.303003\n",
      "(Iteration 43101 / 61200) loss: 2.302885\n",
      "(Iteration 43201 / 61200) loss: 2.302973\n",
      "(Iteration 43301 / 61200) loss: 2.302930\n",
      "(Iteration 43401 / 61200) loss: 2.302794\n",
      "(Iteration 43501 / 61200) loss: 2.303099\n",
      "(Iteration 43601 / 61200) loss: 2.302807\n",
      "(Epoch 57 / 80) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 43701 / 61200) loss: 2.302947\n",
      "(Iteration 43801 / 61200) loss: 2.303060\n",
      "(Iteration 43901 / 61200) loss: 2.302940\n",
      "(Iteration 44001 / 61200) loss: 2.302892\n",
      "(Iteration 44101 / 61200) loss: 2.302923\n",
      "(Iteration 44201 / 61200) loss: 2.302953\n",
      "(Iteration 44301 / 61200) loss: 2.303012\n",
      "(Epoch 58 / 80) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 44401 / 61200) loss: 2.302983\n",
      "(Iteration 44501 / 61200) loss: 2.302952\n",
      "(Iteration 44601 / 61200) loss: 2.302967\n",
      "(Iteration 44701 / 61200) loss: 2.302838\n",
      "(Iteration 44801 / 61200) loss: 2.303036\n",
      "(Iteration 44901 / 61200) loss: 2.302968\n",
      "(Iteration 45001 / 61200) loss: 2.302938\n",
      "(Iteration 45101 / 61200) loss: 2.302975\n",
      "(Epoch 59 / 80) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 45201 / 61200) loss: 2.302917\n",
      "(Iteration 45301 / 61200) loss: 2.302923\n",
      "(Iteration 45401 / 61200) loss: 2.302932\n",
      "(Iteration 45501 / 61200) loss: 2.302847\n",
      "(Iteration 45601 / 61200) loss: 2.303000\n",
      "(Iteration 45701 / 61200) loss: 2.302901\n",
      "(Iteration 45801 / 61200) loss: 2.302933\n",
      "(Epoch 60 / 80) train acc: 0.093000; val_acc: 0.079000\n",
      "(Iteration 45901 / 61200) loss: 2.302904\n",
      "(Iteration 46001 / 61200) loss: 2.303017\n",
      "(Iteration 46101 / 61200) loss: 2.302946\n",
      "(Iteration 46201 / 61200) loss: 2.302913\n",
      "(Iteration 46301 / 61200) loss: 2.303021\n",
      "(Iteration 46401 / 61200) loss: 2.302963\n",
      "(Iteration 46501 / 61200) loss: 2.302872\n",
      "(Iteration 46601 / 61200) loss: 2.302966\n",
      "(Epoch 61 / 80) train acc: 0.083000; val_acc: 0.081000\n",
      "(Iteration 46701 / 61200) loss: 2.302939\n",
      "(Iteration 46801 / 61200) loss: 2.302921\n",
      "(Iteration 46901 / 61200) loss: 2.302984\n",
      "(Iteration 47001 / 61200) loss: 2.302954\n",
      "(Iteration 47101 / 61200) loss: 2.302959\n",
      "(Iteration 47201 / 61200) loss: 2.302945\n",
      "(Iteration 47301 / 61200) loss: 2.302937\n",
      "(Iteration 47401 / 61200) loss: 2.302993\n",
      "(Epoch 62 / 80) train acc: 0.104000; val_acc: 0.081000\n",
      "(Iteration 47501 / 61200) loss: 2.302977\n",
      "(Iteration 47601 / 61200) loss: 2.302841\n",
      "(Iteration 47701 / 61200) loss: 2.302949\n",
      "(Iteration 47801 / 61200) loss: 2.302886\n",
      "(Iteration 47901 / 61200) loss: 2.302845\n",
      "(Iteration 48001 / 61200) loss: 2.302926\n",
      "(Iteration 48101 / 61200) loss: 2.302892\n",
      "(Epoch 63 / 80) train acc: 0.094000; val_acc: 0.081000\n",
      "(Iteration 48201 / 61200) loss: 2.302888\n",
      "(Iteration 48301 / 61200) loss: 2.302930\n",
      "(Iteration 48401 / 61200) loss: 2.302894\n",
      "(Iteration 48501 / 61200) loss: 2.302854\n",
      "(Iteration 48601 / 61200) loss: 2.302928\n",
      "(Iteration 48701 / 61200) loss: 2.302910\n",
      "(Iteration 48801 / 61200) loss: 2.303016\n",
      "(Iteration 48901 / 61200) loss: 2.302904\n",
      "(Epoch 64 / 80) train acc: 0.090000; val_acc: 0.084000\n",
      "(Iteration 49001 / 61200) loss: 2.302998\n",
      "(Iteration 49101 / 61200) loss: 2.302909\n",
      "(Iteration 49201 / 61200) loss: 2.302894\n",
      "(Iteration 49301 / 61200) loss: 2.302999\n",
      "(Iteration 49401 / 61200) loss: 2.302966\n",
      "(Iteration 49501 / 61200) loss: 2.302933\n",
      "(Iteration 49601 / 61200) loss: 2.303035\n",
      "(Iteration 49701 / 61200) loss: 2.303069\n",
      "(Epoch 65 / 80) train acc: 0.097000; val_acc: 0.091000\n",
      "(Iteration 49801 / 61200) loss: 2.302891\n",
      "(Iteration 49901 / 61200) loss: 2.302974\n",
      "(Iteration 50001 / 61200) loss: 2.302999\n",
      "(Iteration 50101 / 61200) loss: 2.302970\n",
      "(Iteration 50201 / 61200) loss: 2.302953\n",
      "(Iteration 50301 / 61200) loss: 2.303088\n",
      "(Iteration 50401 / 61200) loss: 2.302973\n",
      "(Epoch 66 / 80) train acc: 0.120000; val_acc: 0.087000\n",
      "(Iteration 50501 / 61200) loss: 2.302903\n",
      "(Iteration 50601 / 61200) loss: 2.302949\n",
      "(Iteration 50701 / 61200) loss: 2.302917\n",
      "(Iteration 50801 / 61200) loss: 2.303010\n",
      "(Iteration 50901 / 61200) loss: 2.302952\n",
      "(Iteration 51001 / 61200) loss: 2.302946\n",
      "(Iteration 51101 / 61200) loss: 2.303020\n",
      "(Iteration 51201 / 61200) loss: 2.302963\n",
      "(Epoch 67 / 80) train acc: 0.122000; val_acc: 0.091000\n",
      "(Iteration 51301 / 61200) loss: 2.302936\n",
      "(Iteration 51401 / 61200) loss: 2.302917\n",
      "(Iteration 51501 / 61200) loss: 2.302859\n",
      "(Iteration 51601 / 61200) loss: 2.302858\n",
      "(Iteration 51701 / 61200) loss: 2.302959\n",
      "(Iteration 51801 / 61200) loss: 2.302994\n",
      "(Iteration 51901 / 61200) loss: 2.302965\n",
      "(Iteration 52001 / 61200) loss: 2.302888\n",
      "(Epoch 68 / 80) train acc: 0.107000; val_acc: 0.093000\n",
      "(Iteration 52101 / 61200) loss: 2.302998\n",
      "(Iteration 52201 / 61200) loss: 2.302905\n",
      "(Iteration 52301 / 61200) loss: 2.302913\n",
      "(Iteration 52401 / 61200) loss: 2.302882\n",
      "(Iteration 52501 / 61200) loss: 2.303013\n",
      "(Iteration 52601 / 61200) loss: 2.302974\n",
      "(Iteration 52701 / 61200) loss: 2.302993\n",
      "(Epoch 69 / 80) train acc: 0.105000; val_acc: 0.092000\n",
      "(Iteration 52801 / 61200) loss: 2.302944\n",
      "(Iteration 52901 / 61200) loss: 2.302962\n",
      "(Iteration 53001 / 61200) loss: 2.303107\n",
      "(Iteration 53101 / 61200) loss: 2.302974\n",
      "(Iteration 53201 / 61200) loss: 2.302907\n",
      "(Iteration 53301 / 61200) loss: 2.302798\n",
      "(Iteration 53401 / 61200) loss: 2.303036\n",
      "(Iteration 53501 / 61200) loss: 2.302925\n",
      "(Epoch 70 / 80) train acc: 0.109000; val_acc: 0.091000\n",
      "(Iteration 53601 / 61200) loss: 2.302917\n",
      "(Iteration 53701 / 61200) loss: 2.302909\n",
      "(Iteration 53801 / 61200) loss: 2.302967\n",
      "(Iteration 53901 / 61200) loss: 2.302973\n",
      "(Iteration 54001 / 61200) loss: 2.303026\n",
      "(Iteration 54101 / 61200) loss: 2.302972\n",
      "(Iteration 54201 / 61200) loss: 2.302922\n",
      "(Iteration 54301 / 61200) loss: 2.302895\n",
      "(Epoch 71 / 80) train acc: 0.110000; val_acc: 0.092000\n",
      "(Iteration 54401 / 61200) loss: 2.302992\n",
      "(Iteration 54501 / 61200) loss: 2.302887\n",
      "(Iteration 54601 / 61200) loss: 2.302946\n",
      "(Iteration 54701 / 61200) loss: 2.302946\n",
      "(Iteration 54801 / 61200) loss: 2.302899\n",
      "(Iteration 54901 / 61200) loss: 2.302961\n",
      "(Iteration 55001 / 61200) loss: 2.302988\n",
      "(Epoch 72 / 80) train acc: 0.104000; val_acc: 0.095000\n",
      "(Iteration 55101 / 61200) loss: 2.302936\n",
      "(Iteration 55201 / 61200) loss: 2.302970\n",
      "(Iteration 55301 / 61200) loss: 2.302946\n",
      "(Iteration 55401 / 61200) loss: 2.303096\n",
      "(Iteration 55501 / 61200) loss: 2.302950\n",
      "(Iteration 55601 / 61200) loss: 2.302961\n",
      "(Iteration 55701 / 61200) loss: 2.302926\n",
      "(Iteration 55801 / 61200) loss: 2.302936\n",
      "(Epoch 73 / 80) train acc: 0.108000; val_acc: 0.083000\n",
      "(Iteration 55901 / 61200) loss: 2.303022\n",
      "(Iteration 56001 / 61200) loss: 2.302920\n",
      "(Iteration 56101 / 61200) loss: 2.303064\n",
      "(Iteration 56201 / 61200) loss: 2.302947\n",
      "(Iteration 56301 / 61200) loss: 2.302999\n",
      "(Iteration 56401 / 61200) loss: 2.302952\n",
      "(Iteration 56501 / 61200) loss: 2.302910\n",
      "(Iteration 56601 / 61200) loss: 2.302948\n",
      "(Epoch 74 / 80) train acc: 0.102000; val_acc: 0.083000\n",
      "(Iteration 56701 / 61200) loss: 2.303007\n",
      "(Iteration 56801 / 61200) loss: 2.302915\n",
      "(Iteration 56901 / 61200) loss: 2.302937\n",
      "(Iteration 57001 / 61200) loss: 2.302846\n",
      "(Iteration 57101 / 61200) loss: 2.302831\n",
      "(Iteration 57201 / 61200) loss: 2.302875\n",
      "(Iteration 57301 / 61200) loss: 2.302977\n",
      "(Epoch 75 / 80) train acc: 0.123000; val_acc: 0.084000\n",
      "(Iteration 57401 / 61200) loss: 2.302975\n",
      "(Iteration 57501 / 61200) loss: 2.302861\n",
      "(Iteration 57601 / 61200) loss: 2.302947\n",
      "(Iteration 57701 / 61200) loss: 2.302978\n",
      "(Iteration 57801 / 61200) loss: 2.302883\n",
      "(Iteration 57901 / 61200) loss: 2.302983\n",
      "(Iteration 58001 / 61200) loss: 2.302935\n",
      "(Iteration 58101 / 61200) loss: 2.303007\n",
      "(Epoch 76 / 80) train acc: 0.102000; val_acc: 0.086000\n",
      "(Iteration 58201 / 61200) loss: 2.302875\n",
      "(Iteration 58301 / 61200) loss: 2.302967\n",
      "(Iteration 58401 / 61200) loss: 2.302925\n",
      "(Iteration 58501 / 61200) loss: 2.302909\n",
      "(Iteration 58601 / 61200) loss: 2.302862\n",
      "(Iteration 58701 / 61200) loss: 2.302905\n",
      "(Iteration 58801 / 61200) loss: 2.302924\n",
      "(Iteration 58901 / 61200) loss: 2.302911\n",
      "(Epoch 77 / 80) train acc: 0.110000; val_acc: 0.087000\n",
      "(Iteration 59001 / 61200) loss: 2.302943\n",
      "(Iteration 59101 / 61200) loss: 2.302974\n",
      "(Iteration 59201 / 61200) loss: 2.302969\n",
      "(Iteration 59301 / 61200) loss: 2.302870\n",
      "(Iteration 59401 / 61200) loss: 2.302958\n",
      "(Iteration 59501 / 61200) loss: 2.302876\n",
      "(Iteration 59601 / 61200) loss: 2.302978\n",
      "(Epoch 78 / 80) train acc: 0.089000; val_acc: 0.083000\n",
      "(Iteration 59701 / 61200) loss: 2.302969\n",
      "(Iteration 59801 / 61200) loss: 2.302980\n",
      "(Iteration 59901 / 61200) loss: 2.302935\n",
      "(Iteration 60001 / 61200) loss: 2.302929\n",
      "(Iteration 60101 / 61200) loss: 2.302904\n",
      "(Iteration 60201 / 61200) loss: 2.302990\n",
      "(Iteration 60301 / 61200) loss: 2.302867\n",
      "(Iteration 60401 / 61200) loss: 2.302922\n",
      "(Epoch 79 / 80) train acc: 0.095000; val_acc: 0.086000\n",
      "(Iteration 60501 / 61200) loss: 2.302891\n",
      "(Iteration 60601 / 61200) loss: 2.302982\n",
      "(Iteration 60701 / 61200) loss: 2.302886\n",
      "(Iteration 60801 / 61200) loss: 2.302971\n",
      "(Iteration 60901 / 61200) loss: 2.302955\n",
      "(Iteration 61001 / 61200) loss: 2.302939\n",
      "(Iteration 61101 / 61200) loss: 2.302912\n",
      "(Epoch 80 / 80) train acc: 0.106000; val_acc: 0.087000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.305474\n",
      "(Epoch 0 / 80) train acc: 0.104000; val_acc: 0.099000\n",
      "(Iteration 101 / 30560) loss: 2.305435\n",
      "(Iteration 201 / 30560) loss: 2.305402\n",
      "(Iteration 301 / 30560) loss: 2.305358\n",
      "(Epoch 1 / 80) train acc: 0.098000; val_acc: 0.096000\n",
      "(Iteration 401 / 30560) loss: 2.305314\n",
      "(Iteration 501 / 30560) loss: 2.305292\n",
      "(Iteration 601 / 30560) loss: 2.305249\n",
      "(Iteration 701 / 30560) loss: 2.305217\n",
      "(Epoch 2 / 80) train acc: 0.123000; val_acc: 0.099000\n",
      "(Iteration 801 / 30560) loss: 2.305175\n",
      "(Iteration 901 / 30560) loss: 2.305143\n",
      "(Iteration 1001 / 30560) loss: 2.305125\n",
      "(Iteration 1101 / 30560) loss: 2.305081\n",
      "(Epoch 3 / 80) train acc: 0.100000; val_acc: 0.088000\n",
      "(Iteration 1201 / 30560) loss: 2.305054\n",
      "(Iteration 1301 / 30560) loss: 2.305028\n",
      "(Iteration 1401 / 30560) loss: 2.304991\n",
      "(Iteration 1501 / 30560) loss: 2.304962\n",
      "(Epoch 4 / 80) train acc: 0.109000; val_acc: 0.089000\n",
      "(Iteration 1601 / 30560) loss: 2.304943\n",
      "(Iteration 1701 / 30560) loss: 2.304915\n",
      "(Iteration 1801 / 30560) loss: 2.304894\n",
      "(Iteration 1901 / 30560) loss: 2.304859\n",
      "(Epoch 5 / 80) train acc: 0.112000; val_acc: 0.111000\n",
      "(Iteration 2001 / 30560) loss: 2.304819\n",
      "(Iteration 2101 / 30560) loss: 2.304799\n",
      "(Iteration 2201 / 30560) loss: 2.304782\n",
      "(Epoch 6 / 80) train acc: 0.115000; val_acc: 0.104000\n",
      "(Iteration 2301 / 30560) loss: 2.304755\n",
      "(Iteration 2401 / 30560) loss: 2.304754\n",
      "(Iteration 2501 / 30560) loss: 2.304727\n",
      "(Iteration 2601 / 30560) loss: 2.304693\n",
      "(Epoch 7 / 80) train acc: 0.105000; val_acc: 0.102000\n",
      "(Iteration 2701 / 30560) loss: 2.304662\n",
      "(Iteration 2801 / 30560) loss: 2.304647\n",
      "(Iteration 2901 / 30560) loss: 2.304638\n",
      "(Iteration 3001 / 30560) loss: 2.304613\n",
      "(Epoch 8 / 80) train acc: 0.099000; val_acc: 0.106000\n",
      "(Iteration 3101 / 30560) loss: 2.304581\n",
      "(Iteration 3201 / 30560) loss: 2.304581\n",
      "(Iteration 3301 / 30560) loss: 2.304553\n",
      "(Iteration 3401 / 30560) loss: 2.304532\n",
      "(Epoch 9 / 80) train acc: 0.104000; val_acc: 0.113000\n",
      "(Iteration 3501 / 30560) loss: 2.304506\n",
      "(Iteration 3601 / 30560) loss: 2.304505\n",
      "(Iteration 3701 / 30560) loss: 2.304490\n",
      "(Iteration 3801 / 30560) loss: 2.304456\n",
      "(Epoch 10 / 80) train acc: 0.096000; val_acc: 0.108000\n",
      "(Iteration 3901 / 30560) loss: 2.304421\n",
      "(Iteration 4001 / 30560) loss: 2.304442\n",
      "(Iteration 4101 / 30560) loss: 2.304399\n",
      "(Iteration 4201 / 30560) loss: 2.304416\n",
      "(Epoch 11 / 80) train acc: 0.088000; val_acc: 0.109000\n",
      "(Iteration 4301 / 30560) loss: 2.304407\n",
      "(Iteration 4401 / 30560) loss: 2.304387\n",
      "(Iteration 4501 / 30560) loss: 2.304347\n",
      "(Epoch 12 / 80) train acc: 0.106000; val_acc: 0.118000\n",
      "(Iteration 4601 / 30560) loss: 2.304330\n",
      "(Iteration 4701 / 30560) loss: 2.304349\n",
      "(Iteration 4801 / 30560) loss: 2.304347\n",
      "(Iteration 4901 / 30560) loss: 2.304324\n",
      "(Epoch 13 / 80) train acc: 0.097000; val_acc: 0.117000\n",
      "(Iteration 5001 / 30560) loss: 2.304292\n",
      "(Iteration 5101 / 30560) loss: 2.304288\n",
      "(Iteration 5201 / 30560) loss: 2.304267\n",
      "(Iteration 5301 / 30560) loss: 2.304253\n",
      "(Epoch 14 / 80) train acc: 0.089000; val_acc: 0.104000\n",
      "(Iteration 5401 / 30560) loss: 2.304254\n",
      "(Iteration 5501 / 30560) loss: 2.304260\n",
      "(Iteration 5601 / 30560) loss: 2.304215\n",
      "(Iteration 5701 / 30560) loss: 2.304213\n",
      "(Epoch 15 / 80) train acc: 0.101000; val_acc: 0.111000\n",
      "(Iteration 5801 / 30560) loss: 2.304185\n",
      "(Iteration 5901 / 30560) loss: 2.304193\n",
      "(Iteration 6001 / 30560) loss: 2.304202\n",
      "(Iteration 6101 / 30560) loss: 2.304149\n",
      "(Epoch 16 / 80) train acc: 0.094000; val_acc: 0.106000\n",
      "(Iteration 6201 / 30560) loss: 2.304196\n",
      "(Iteration 6301 / 30560) loss: 2.304155\n",
      "(Iteration 6401 / 30560) loss: 2.304152\n",
      "(Epoch 17 / 80) train acc: 0.106000; val_acc: 0.112000\n",
      "(Iteration 6501 / 30560) loss: 2.304143\n",
      "(Iteration 6601 / 30560) loss: 2.304126\n",
      "(Iteration 6701 / 30560) loss: 2.304136\n",
      "(Iteration 6801 / 30560) loss: 2.304093\n",
      "(Epoch 18 / 80) train acc: 0.104000; val_acc: 0.119000\n",
      "(Iteration 6901 / 30560) loss: 2.304105\n",
      "(Iteration 7001 / 30560) loss: 2.304110\n",
      "(Iteration 7101 / 30560) loss: 2.304075\n",
      "(Iteration 7201 / 30560) loss: 2.304073\n",
      "(Epoch 19 / 80) train acc: 0.125000; val_acc: 0.111000\n",
      "(Iteration 7301 / 30560) loss: 2.304046\n",
      "(Iteration 7401 / 30560) loss: 2.304046\n",
      "(Iteration 7501 / 30560) loss: 2.304049\n",
      "(Iteration 7601 / 30560) loss: 2.304041\n",
      "(Epoch 20 / 80) train acc: 0.112000; val_acc: 0.120000\n",
      "(Iteration 7701 / 30560) loss: 2.304014\n",
      "(Iteration 7801 / 30560) loss: 2.304034\n",
      "(Iteration 7901 / 30560) loss: 2.304013\n",
      "(Iteration 8001 / 30560) loss: 2.304005\n",
      "(Epoch 21 / 80) train acc: 0.129000; val_acc: 0.141000\n",
      "(Iteration 8101 / 30560) loss: 2.304005\n",
      "(Iteration 8201 / 30560) loss: 2.304009\n",
      "(Iteration 8301 / 30560) loss: 2.303991\n",
      "(Iteration 8401 / 30560) loss: 2.303994\n",
      "(Epoch 22 / 80) train acc: 0.140000; val_acc: 0.139000\n",
      "(Iteration 8501 / 30560) loss: 2.303998\n",
      "(Iteration 8601 / 30560) loss: 2.303975\n",
      "(Iteration 8701 / 30560) loss: 2.303948\n",
      "(Epoch 23 / 80) train acc: 0.143000; val_acc: 0.158000\n",
      "(Iteration 8801 / 30560) loss: 2.303978\n",
      "(Iteration 8901 / 30560) loss: 2.303942\n",
      "(Iteration 9001 / 30560) loss: 2.303935\n",
      "(Iteration 9101 / 30560) loss: 2.303922\n",
      "(Epoch 24 / 80) train acc: 0.127000; val_acc: 0.145000\n",
      "(Iteration 9201 / 30560) loss: 2.303948\n",
      "(Iteration 9301 / 30560) loss: 2.303917\n",
      "(Iteration 9401 / 30560) loss: 2.303930\n",
      "(Iteration 9501 / 30560) loss: 2.303889\n",
      "(Epoch 25 / 80) train acc: 0.169000; val_acc: 0.145000\n",
      "(Iteration 9601 / 30560) loss: 2.303948\n",
      "(Iteration 9701 / 30560) loss: 2.303922\n",
      "(Iteration 9801 / 30560) loss: 2.303891\n",
      "(Iteration 9901 / 30560) loss: 2.303897\n",
      "(Epoch 26 / 80) train acc: 0.137000; val_acc: 0.163000\n",
      "(Iteration 10001 / 30560) loss: 2.303899\n",
      "(Iteration 10101 / 30560) loss: 2.303891\n",
      "(Iteration 10201 / 30560) loss: 2.303904\n",
      "(Iteration 10301 / 30560) loss: 2.303868\n",
      "(Epoch 27 / 80) train acc: 0.129000; val_acc: 0.148000\n",
      "(Iteration 10401 / 30560) loss: 2.303879\n",
      "(Iteration 10501 / 30560) loss: 2.303881\n",
      "(Iteration 10601 / 30560) loss: 2.303887\n",
      "(Epoch 28 / 80) train acc: 0.138000; val_acc: 0.154000\n",
      "(Iteration 10701 / 30560) loss: 2.303882\n",
      "(Iteration 10801 / 30560) loss: 2.303854\n",
      "(Iteration 10901 / 30560) loss: 2.303839\n",
      "(Iteration 11001 / 30560) loss: 2.303875\n",
      "(Epoch 29 / 80) train acc: 0.144000; val_acc: 0.144000\n",
      "(Iteration 11101 / 30560) loss: 2.303829\n",
      "(Iteration 11201 / 30560) loss: 2.303838\n",
      "(Iteration 11301 / 30560) loss: 2.303858\n",
      "(Iteration 11401 / 30560) loss: 2.303843\n",
      "(Epoch 30 / 80) train acc: 0.138000; val_acc: 0.140000\n",
      "(Iteration 11501 / 30560) loss: 2.303840\n",
      "(Iteration 11601 / 30560) loss: 2.303830\n",
      "(Iteration 11701 / 30560) loss: 2.303825\n",
      "(Iteration 11801 / 30560) loss: 2.303814\n",
      "(Epoch 31 / 80) train acc: 0.136000; val_acc: 0.115000\n",
      "(Iteration 11901 / 30560) loss: 2.303818\n",
      "(Iteration 12001 / 30560) loss: 2.303806\n",
      "(Iteration 12101 / 30560) loss: 2.303823\n",
      "(Iteration 12201 / 30560) loss: 2.303785\n",
      "(Epoch 32 / 80) train acc: 0.140000; val_acc: 0.120000\n",
      "(Iteration 12301 / 30560) loss: 2.303808\n",
      "(Iteration 12401 / 30560) loss: 2.303801\n",
      "(Iteration 12501 / 30560) loss: 2.303813\n",
      "(Iteration 12601 / 30560) loss: 2.303776\n",
      "(Epoch 33 / 80) train acc: 0.151000; val_acc: 0.127000\n",
      "(Iteration 12701 / 30560) loss: 2.303771\n",
      "(Iteration 12801 / 30560) loss: 2.303798\n",
      "(Iteration 12901 / 30560) loss: 2.303796\n",
      "(Epoch 34 / 80) train acc: 0.136000; val_acc: 0.130000\n",
      "(Iteration 13001 / 30560) loss: 2.303802\n",
      "(Iteration 13101 / 30560) loss: 2.303767\n",
      "(Iteration 13201 / 30560) loss: 2.303761\n",
      "(Iteration 13301 / 30560) loss: 2.303768\n",
      "(Epoch 35 / 80) train acc: 0.138000; val_acc: 0.131000\n",
      "(Iteration 13401 / 30560) loss: 2.303755\n",
      "(Iteration 13501 / 30560) loss: 2.303748\n",
      "(Iteration 13601 / 30560) loss: 2.303755\n",
      "(Iteration 13701 / 30560) loss: 2.303779\n",
      "(Epoch 36 / 80) train acc: 0.151000; val_acc: 0.132000\n",
      "(Iteration 13801 / 30560) loss: 2.303742\n",
      "(Iteration 13901 / 30560) loss: 2.303751\n",
      "(Iteration 14001 / 30560) loss: 2.303756\n",
      "(Iteration 14101 / 30560) loss: 2.303749\n",
      "(Epoch 37 / 80) train acc: 0.143000; val_acc: 0.126000\n",
      "(Iteration 14201 / 30560) loss: 2.303754\n",
      "(Iteration 14301 / 30560) loss: 2.303728\n",
      "(Iteration 14401 / 30560) loss: 2.303745\n",
      "(Iteration 14501 / 30560) loss: 2.303729\n",
      "(Epoch 38 / 80) train acc: 0.134000; val_acc: 0.120000\n",
      "(Iteration 14601 / 30560) loss: 2.303744\n",
      "(Iteration 14701 / 30560) loss: 2.303726\n",
      "(Iteration 14801 / 30560) loss: 2.303711\n",
      "(Epoch 39 / 80) train acc: 0.131000; val_acc: 0.120000\n",
      "(Iteration 14901 / 30560) loss: 2.303731\n",
      "(Iteration 15001 / 30560) loss: 2.303723\n",
      "(Iteration 15101 / 30560) loss: 2.303735\n",
      "(Iteration 15201 / 30560) loss: 2.303742\n",
      "(Epoch 40 / 80) train acc: 0.135000; val_acc: 0.117000\n",
      "(Iteration 15301 / 30560) loss: 2.303706\n",
      "(Iteration 15401 / 30560) loss: 2.303707\n",
      "(Iteration 15501 / 30560) loss: 2.303730\n",
      "(Iteration 15601 / 30560) loss: 2.303713\n",
      "(Epoch 41 / 80) train acc: 0.144000; val_acc: 0.133000\n",
      "(Iteration 15701 / 30560) loss: 2.303714\n",
      "(Iteration 15801 / 30560) loss: 2.303733\n",
      "(Iteration 15901 / 30560) loss: 2.303734\n",
      "(Iteration 16001 / 30560) loss: 2.303720\n",
      "(Epoch 42 / 80) train acc: 0.135000; val_acc: 0.106000\n",
      "(Iteration 16101 / 30560) loss: 2.303714\n",
      "(Iteration 16201 / 30560) loss: 2.303705\n",
      "(Iteration 16301 / 30560) loss: 2.303703\n",
      "(Iteration 16401 / 30560) loss: 2.303678\n",
      "(Epoch 43 / 80) train acc: 0.125000; val_acc: 0.101000\n",
      "(Iteration 16501 / 30560) loss: 2.303701\n",
      "(Iteration 16601 / 30560) loss: 2.303691\n",
      "(Iteration 16701 / 30560) loss: 2.303670\n",
      "(Iteration 16801 / 30560) loss: 2.303699\n",
      "(Epoch 44 / 80) train acc: 0.111000; val_acc: 0.095000\n",
      "(Iteration 16901 / 30560) loss: 2.303687\n",
      "(Iteration 17001 / 30560) loss: 2.303697\n",
      "(Iteration 17101 / 30560) loss: 2.303698\n",
      "(Epoch 45 / 80) train acc: 0.097000; val_acc: 0.101000\n",
      "(Iteration 17201 / 30560) loss: 2.303695\n",
      "(Iteration 17301 / 30560) loss: 2.303682\n",
      "(Iteration 17401 / 30560) loss: 2.303678\n",
      "(Iteration 17501 / 30560) loss: 2.303681\n",
      "(Epoch 46 / 80) train acc: 0.110000; val_acc: 0.095000\n",
      "(Iteration 17601 / 30560) loss: 2.303678\n",
      "(Iteration 17701 / 30560) loss: 2.303667\n",
      "(Iteration 17801 / 30560) loss: 2.303674\n",
      "(Iteration 17901 / 30560) loss: 2.303695\n",
      "(Epoch 47 / 80) train acc: 0.109000; val_acc: 0.094000\n",
      "(Iteration 18001 / 30560) loss: 2.303681\n",
      "(Iteration 18101 / 30560) loss: 2.303674\n",
      "(Iteration 18201 / 30560) loss: 2.303659\n",
      "(Iteration 18301 / 30560) loss: 2.303663\n",
      "(Epoch 48 / 80) train acc: 0.114000; val_acc: 0.092000\n",
      "(Iteration 18401 / 30560) loss: 2.303667\n",
      "(Iteration 18501 / 30560) loss: 2.303661\n",
      "(Iteration 18601 / 30560) loss: 2.303678\n",
      "(Iteration 18701 / 30560) loss: 2.303673\n",
      "(Epoch 49 / 80) train acc: 0.108000; val_acc: 0.089000\n",
      "(Iteration 18801 / 30560) loss: 2.303665\n",
      "(Iteration 18901 / 30560) loss: 2.303674\n",
      "(Iteration 19001 / 30560) loss: 2.303667\n",
      "(Epoch 50 / 80) train acc: 0.102000; val_acc: 0.090000\n",
      "(Iteration 19101 / 30560) loss: 2.303653\n",
      "(Iteration 19201 / 30560) loss: 2.303669\n",
      "(Iteration 19301 / 30560) loss: 2.303657\n",
      "(Iteration 19401 / 30560) loss: 2.303652\n",
      "(Epoch 51 / 80) train acc: 0.108000; val_acc: 0.088000\n",
      "(Iteration 19501 / 30560) loss: 2.303636\n",
      "(Iteration 19601 / 30560) loss: 2.303665\n",
      "(Iteration 19701 / 30560) loss: 2.303658\n",
      "(Iteration 19801 / 30560) loss: 2.303627\n",
      "(Epoch 52 / 80) train acc: 0.104000; val_acc: 0.089000\n",
      "(Iteration 19901 / 30560) loss: 2.303660\n",
      "(Iteration 20001 / 30560) loss: 2.303660\n",
      "(Iteration 20101 / 30560) loss: 2.303626\n",
      "(Iteration 20201 / 30560) loss: 2.303635\n",
      "(Epoch 53 / 80) train acc: 0.102000; val_acc: 0.084000\n",
      "(Iteration 20301 / 30560) loss: 2.303648\n",
      "(Iteration 20401 / 30560) loss: 2.303639\n",
      "(Iteration 20501 / 30560) loss: 2.303626\n",
      "(Iteration 20601 / 30560) loss: 2.303674\n",
      "(Epoch 54 / 80) train acc: 0.107000; val_acc: 0.090000\n",
      "(Iteration 20701 / 30560) loss: 2.303650\n",
      "(Iteration 20801 / 30560) loss: 2.303643\n",
      "(Iteration 20901 / 30560) loss: 2.303640\n",
      "(Iteration 21001 / 30560) loss: 2.303629\n",
      "(Epoch 55 / 80) train acc: 0.098000; val_acc: 0.089000\n",
      "(Iteration 21101 / 30560) loss: 2.303633\n",
      "(Iteration 21201 / 30560) loss: 2.303647\n",
      "(Iteration 21301 / 30560) loss: 2.303614\n",
      "(Epoch 56 / 80) train acc: 0.114000; val_acc: 0.089000\n",
      "(Iteration 21401 / 30560) loss: 2.303630\n",
      "(Iteration 21501 / 30560) loss: 2.303622\n",
      "(Iteration 21601 / 30560) loss: 2.303649\n",
      "(Iteration 21701 / 30560) loss: 2.303645\n",
      "(Epoch 57 / 80) train acc: 0.130000; val_acc: 0.087000\n",
      "(Iteration 21801 / 30560) loss: 2.303643\n",
      "(Iteration 21901 / 30560) loss: 2.303633\n",
      "(Iteration 22001 / 30560) loss: 2.303611\n",
      "(Iteration 22101 / 30560) loss: 2.303634\n",
      "(Epoch 58 / 80) train acc: 0.134000; val_acc: 0.085000\n",
      "(Iteration 22201 / 30560) loss: 2.303650\n",
      "(Iteration 22301 / 30560) loss: 2.303642\n",
      "(Iteration 22401 / 30560) loss: 2.303634\n",
      "(Iteration 22501 / 30560) loss: 2.303639\n",
      "(Epoch 59 / 80) train acc: 0.099000; val_acc: 0.086000\n",
      "(Iteration 22601 / 30560) loss: 2.303650\n",
      "(Iteration 22701 / 30560) loss: 2.303628\n",
      "(Iteration 22801 / 30560) loss: 2.303640\n",
      "(Iteration 22901 / 30560) loss: 2.303629\n",
      "(Epoch 60 / 80) train acc: 0.103000; val_acc: 0.084000\n",
      "(Iteration 23001 / 30560) loss: 2.303607\n",
      "(Iteration 23101 / 30560) loss: 2.303620\n",
      "(Iteration 23201 / 30560) loss: 2.303621\n",
      "(Iteration 23301 / 30560) loss: 2.303609\n",
      "(Epoch 61 / 80) train acc: 0.090000; val_acc: 0.087000\n",
      "(Iteration 23401 / 30560) loss: 2.303609\n",
      "(Iteration 23501 / 30560) loss: 2.303628\n",
      "(Iteration 23601 / 30560) loss: 2.303635\n",
      "(Epoch 62 / 80) train acc: 0.113000; val_acc: 0.084000\n",
      "(Iteration 23701 / 30560) loss: 2.303603\n",
      "(Iteration 23801 / 30560) loss: 2.303626\n",
      "(Iteration 23901 / 30560) loss: 2.303636\n",
      "(Iteration 24001 / 30560) loss: 2.303622\n",
      "(Epoch 63 / 80) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 24101 / 30560) loss: 2.303609\n",
      "(Iteration 24201 / 30560) loss: 2.303638\n",
      "(Iteration 24301 / 30560) loss: 2.303604\n",
      "(Iteration 24401 / 30560) loss: 2.303594\n",
      "(Epoch 64 / 80) train acc: 0.109000; val_acc: 0.088000\n",
      "(Iteration 24501 / 30560) loss: 2.303597\n",
      "(Iteration 24601 / 30560) loss: 2.303630\n",
      "(Iteration 24701 / 30560) loss: 2.303613\n",
      "(Iteration 24801 / 30560) loss: 2.303602\n",
      "(Epoch 65 / 80) train acc: 0.112000; val_acc: 0.091000\n",
      "(Iteration 24901 / 30560) loss: 2.303612\n",
      "(Iteration 25001 / 30560) loss: 2.303609\n",
      "(Iteration 25101 / 30560) loss: 2.303608\n",
      "(Iteration 25201 / 30560) loss: 2.303635\n",
      "(Epoch 66 / 80) train acc: 0.106000; val_acc: 0.093000\n",
      "(Iteration 25301 / 30560) loss: 2.303596\n",
      "(Iteration 25401 / 30560) loss: 2.303599\n",
      "(Iteration 25501 / 30560) loss: 2.303631\n",
      "(Epoch 67 / 80) train acc: 0.116000; val_acc: 0.092000\n",
      "(Iteration 25601 / 30560) loss: 2.303608\n",
      "(Iteration 25701 / 30560) loss: 2.303610\n",
      "(Iteration 25801 / 30560) loss: 2.303626\n",
      "(Iteration 25901 / 30560) loss: 2.303641\n",
      "(Epoch 68 / 80) train acc: 0.112000; val_acc: 0.089000\n",
      "(Iteration 26001 / 30560) loss: 2.303596\n",
      "(Iteration 26101 / 30560) loss: 2.303587\n",
      "(Iteration 26201 / 30560) loss: 2.303608\n",
      "(Iteration 26301 / 30560) loss: 2.303605\n",
      "(Epoch 69 / 80) train acc: 0.128000; val_acc: 0.087000\n",
      "(Iteration 26401 / 30560) loss: 2.303603\n",
      "(Iteration 26501 / 30560) loss: 2.303607\n",
      "(Iteration 26601 / 30560) loss: 2.303613\n",
      "(Iteration 26701 / 30560) loss: 2.303599\n",
      "(Epoch 70 / 80) train acc: 0.121000; val_acc: 0.086000\n",
      "(Iteration 26801 / 30560) loss: 2.303579\n",
      "(Iteration 26901 / 30560) loss: 2.303590\n",
      "(Iteration 27001 / 30560) loss: 2.303597\n",
      "(Iteration 27101 / 30560) loss: 2.303591\n",
      "(Epoch 71 / 80) train acc: 0.093000; val_acc: 0.092000\n",
      "(Iteration 27201 / 30560) loss: 2.303602\n",
      "(Iteration 27301 / 30560) loss: 2.303608\n",
      "(Iteration 27401 / 30560) loss: 2.303615\n",
      "(Iteration 27501 / 30560) loss: 2.303610\n",
      "(Epoch 72 / 80) train acc: 0.114000; val_acc: 0.090000\n",
      "(Iteration 27601 / 30560) loss: 2.303607\n",
      "(Iteration 27701 / 30560) loss: 2.303612\n",
      "(Iteration 27801 / 30560) loss: 2.303593\n",
      "(Epoch 73 / 80) train acc: 0.125000; val_acc: 0.084000\n",
      "(Iteration 27901 / 30560) loss: 2.303609\n",
      "(Iteration 28001 / 30560) loss: 2.303616\n",
      "(Iteration 28101 / 30560) loss: 2.303601\n",
      "(Iteration 28201 / 30560) loss: 2.303630\n",
      "(Epoch 74 / 80) train acc: 0.109000; val_acc: 0.085000\n",
      "(Iteration 28301 / 30560) loss: 2.303594\n",
      "(Iteration 28401 / 30560) loss: 2.303607\n",
      "(Iteration 28501 / 30560) loss: 2.303585\n",
      "(Iteration 28601 / 30560) loss: 2.303618\n",
      "(Epoch 75 / 80) train acc: 0.117000; val_acc: 0.087000\n",
      "(Iteration 28701 / 30560) loss: 2.303599\n",
      "(Iteration 28801 / 30560) loss: 2.303609\n",
      "(Iteration 28901 / 30560) loss: 2.303589\n",
      "(Iteration 29001 / 30560) loss: 2.303593\n",
      "(Epoch 76 / 80) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 29101 / 30560) loss: 2.303615\n",
      "(Iteration 29201 / 30560) loss: 2.303584\n",
      "(Iteration 29301 / 30560) loss: 2.303590\n",
      "(Iteration 29401 / 30560) loss: 2.303593\n",
      "(Epoch 77 / 80) train acc: 0.123000; val_acc: 0.088000\n",
      "(Iteration 29501 / 30560) loss: 2.303620\n",
      "(Iteration 29601 / 30560) loss: 2.303581\n",
      "(Iteration 29701 / 30560) loss: 2.303610\n",
      "(Epoch 78 / 80) train acc: 0.109000; val_acc: 0.086000\n",
      "(Iteration 29801 / 30560) loss: 2.303589\n",
      "(Iteration 29901 / 30560) loss: 2.303601\n",
      "(Iteration 30001 / 30560) loss: 2.303578\n",
      "(Iteration 30101 / 30560) loss: 2.303618\n",
      "(Epoch 79 / 80) train acc: 0.102000; val_acc: 0.085000\n",
      "(Iteration 30201 / 30560) loss: 2.303573\n",
      "(Iteration 30301 / 30560) loss: 2.303590\n",
      "(Iteration 30401 / 30560) loss: 2.303610\n",
      "(Iteration 30501 / 30560) loss: 2.303567\n",
      "(Epoch 80 / 80) train acc: 0.110000; val_acc: 0.087000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.304604\n",
      "(Epoch 0 / 100) train acc: 0.110000; val_acc: 0.125000\n",
      "(Iteration 101 / 76500) loss: 2.304577\n",
      "(Iteration 201 / 76500) loss: 2.304551\n",
      "(Iteration 301 / 76500) loss: 2.304517\n",
      "(Iteration 401 / 76500) loss: 2.304513\n",
      "(Iteration 501 / 76500) loss: 2.304511\n",
      "(Iteration 601 / 76500) loss: 2.304507\n",
      "(Iteration 701 / 76500) loss: 2.304483\n",
      "(Epoch 1 / 100) train acc: 0.101000; val_acc: 0.125000\n",
      "(Iteration 801 / 76500) loss: 2.304466\n",
      "(Iteration 901 / 76500) loss: 2.304455\n",
      "(Iteration 1001 / 76500) loss: 2.304424\n",
      "(Iteration 1101 / 76500) loss: 2.304367\n",
      "(Iteration 1201 / 76500) loss: 2.304373\n",
      "(Iteration 1301 / 76500) loss: 2.304384\n",
      "(Iteration 1401 / 76500) loss: 2.304364\n",
      "(Iteration 1501 / 76500) loss: 2.304330\n",
      "(Epoch 2 / 100) train acc: 0.101000; val_acc: 0.088000\n",
      "(Iteration 1601 / 76500) loss: 2.304351\n",
      "(Iteration 1701 / 76500) loss: 2.304329\n",
      "(Iteration 1801 / 76500) loss: 2.304343\n",
      "(Iteration 1901 / 76500) loss: 2.304289\n",
      "(Iteration 2001 / 76500) loss: 2.304296\n",
      "(Iteration 2101 / 76500) loss: 2.304261\n",
      "(Iteration 2201 / 76500) loss: 2.304272\n",
      "(Epoch 3 / 100) train acc: 0.081000; val_acc: 0.087000\n",
      "(Iteration 2301 / 76500) loss: 2.304159\n",
      "(Iteration 2401 / 76500) loss: 2.304256\n",
      "(Iteration 2501 / 76500) loss: 2.304226\n",
      "(Iteration 2601 / 76500) loss: 2.304194\n",
      "(Iteration 2701 / 76500) loss: 2.304254\n",
      "(Iteration 2801 / 76500) loss: 2.304168\n",
      "(Iteration 2901 / 76500) loss: 2.304211\n",
      "(Iteration 3001 / 76500) loss: 2.304134\n",
      "(Epoch 4 / 100) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 3101 / 76500) loss: 2.304155\n",
      "(Iteration 3201 / 76500) loss: 2.304109\n",
      "(Iteration 3301 / 76500) loss: 2.304085\n",
      "(Iteration 3401 / 76500) loss: 2.304070\n",
      "(Iteration 3501 / 76500) loss: 2.304050\n",
      "(Iteration 3601 / 76500) loss: 2.304009\n",
      "(Iteration 3701 / 76500) loss: 2.304060\n",
      "(Iteration 3801 / 76500) loss: 2.304040\n",
      "(Epoch 5 / 100) train acc: 0.115000; val_acc: 0.087000\n",
      "(Iteration 3901 / 76500) loss: 2.304039\n",
      "(Iteration 4001 / 76500) loss: 2.304059\n",
      "(Iteration 4101 / 76500) loss: 2.304068\n",
      "(Iteration 4201 / 76500) loss: 2.304072\n",
      "(Iteration 4301 / 76500) loss: 2.304039\n",
      "(Iteration 4401 / 76500) loss: 2.304029\n",
      "(Iteration 4501 / 76500) loss: 2.304009\n",
      "(Epoch 6 / 100) train acc: 0.110000; val_acc: 0.087000\n",
      "(Iteration 4601 / 76500) loss: 2.304038\n",
      "(Iteration 4701 / 76500) loss: 2.304062\n",
      "(Iteration 4801 / 76500) loss: 2.303971\n",
      "(Iteration 4901 / 76500) loss: 2.304003\n",
      "(Iteration 5001 / 76500) loss: 2.303941\n",
      "(Iteration 5101 / 76500) loss: 2.303925\n",
      "(Iteration 5201 / 76500) loss: 2.303916\n",
      "(Iteration 5301 / 76500) loss: 2.303902\n",
      "(Epoch 7 / 100) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 5401 / 76500) loss: 2.303905\n",
      "(Iteration 5501 / 76500) loss: 2.303947\n",
      "(Iteration 5601 / 76500) loss: 2.303954\n",
      "(Iteration 5701 / 76500) loss: 2.303897\n",
      "(Iteration 5801 / 76500) loss: 2.303904\n",
      "(Iteration 5901 / 76500) loss: 2.303912\n",
      "(Iteration 6001 / 76500) loss: 2.303905\n",
      "(Iteration 6101 / 76500) loss: 2.303934\n",
      "(Epoch 8 / 100) train acc: 0.109000; val_acc: 0.087000\n",
      "(Iteration 6201 / 76500) loss: 2.303930\n",
      "(Iteration 6301 / 76500) loss: 2.303886\n",
      "(Iteration 6401 / 76500) loss: 2.303895\n",
      "(Iteration 6501 / 76500) loss: 2.303858\n",
      "(Iteration 6601 / 76500) loss: 2.303887\n",
      "(Iteration 6701 / 76500) loss: 2.303949\n",
      "(Iteration 6801 / 76500) loss: 2.303889\n",
      "(Epoch 9 / 100) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 6901 / 76500) loss: 2.303903\n",
      "(Iteration 7001 / 76500) loss: 2.303810\n",
      "(Iteration 7101 / 76500) loss: 2.303865\n",
      "(Iteration 7201 / 76500) loss: 2.303930\n",
      "(Iteration 7301 / 76500) loss: 2.303851\n",
      "(Iteration 7401 / 76500) loss: 2.303805\n",
      "(Iteration 7501 / 76500) loss: 2.303790\n",
      "(Iteration 7601 / 76500) loss: 2.303806\n",
      "(Epoch 10 / 100) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 7701 / 76500) loss: 2.303779\n",
      "(Iteration 7801 / 76500) loss: 2.303735\n",
      "(Iteration 7901 / 76500) loss: 2.303741\n",
      "(Iteration 8001 / 76500) loss: 2.303842\n",
      "(Iteration 8101 / 76500) loss: 2.303738\n",
      "(Iteration 8201 / 76500) loss: 2.303750\n",
      "(Iteration 8301 / 76500) loss: 2.303747\n",
      "(Iteration 8401 / 76500) loss: 2.303789\n",
      "(Epoch 11 / 100) train acc: 0.112000; val_acc: 0.087000\n",
      "(Iteration 8501 / 76500) loss: 2.303767\n",
      "(Iteration 8601 / 76500) loss: 2.303764\n",
      "(Iteration 8701 / 76500) loss: 2.303743\n",
      "(Iteration 8801 / 76500) loss: 2.303747\n",
      "(Iteration 8901 / 76500) loss: 2.303798\n",
      "(Iteration 9001 / 76500) loss: 2.303814\n",
      "(Iteration 9101 / 76500) loss: 2.303789\n",
      "(Epoch 12 / 100) train acc: 0.109000; val_acc: 0.087000\n",
      "(Iteration 9201 / 76500) loss: 2.303759\n",
      "(Iteration 9301 / 76500) loss: 2.303786\n",
      "(Iteration 9401 / 76500) loss: 2.303797\n",
      "(Iteration 9501 / 76500) loss: 2.303758\n",
      "(Iteration 9601 / 76500) loss: 2.303678\n",
      "(Iteration 9701 / 76500) loss: 2.303681\n",
      "(Iteration 9801 / 76500) loss: 2.303730\n",
      "(Iteration 9901 / 76500) loss: 2.303776\n",
      "(Epoch 13 / 100) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 10001 / 76500) loss: 2.303696\n",
      "(Iteration 10101 / 76500) loss: 2.303675\n",
      "(Iteration 10201 / 76500) loss: 2.303705\n",
      "(Iteration 10301 / 76500) loss: 2.303633\n",
      "(Iteration 10401 / 76500) loss: 2.303717\n",
      "(Iteration 10501 / 76500) loss: 2.303686\n",
      "(Iteration 10601 / 76500) loss: 2.303690\n",
      "(Iteration 10701 / 76500) loss: 2.303660\n",
      "(Epoch 14 / 100) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 10801 / 76500) loss: 2.303683\n",
      "(Iteration 10901 / 76500) loss: 2.303703\n",
      "(Iteration 11001 / 76500) loss: 2.303744\n",
      "(Iteration 11101 / 76500) loss: 2.303693\n",
      "(Iteration 11201 / 76500) loss: 2.303760\n",
      "(Iteration 11301 / 76500) loss: 2.303659\n",
      "(Iteration 11401 / 76500) loss: 2.303669\n",
      "(Epoch 15 / 100) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 11501 / 76500) loss: 2.303674\n",
      "(Iteration 11601 / 76500) loss: 2.303637\n",
      "(Iteration 11701 / 76500) loss: 2.303655\n",
      "(Iteration 11801 / 76500) loss: 2.303654\n",
      "(Iteration 11901 / 76500) loss: 2.303678\n",
      "(Iteration 12001 / 76500) loss: 2.303742\n",
      "(Iteration 12101 / 76500) loss: 2.303696\n",
      "(Iteration 12201 / 76500) loss: 2.303701\n",
      "(Epoch 16 / 100) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 12301 / 76500) loss: 2.303608\n",
      "(Iteration 12401 / 76500) loss: 2.303593\n",
      "(Iteration 12501 / 76500) loss: 2.303619\n",
      "(Iteration 12601 / 76500) loss: 2.303668\n",
      "(Iteration 12701 / 76500) loss: 2.303699\n",
      "(Iteration 12801 / 76500) loss: 2.303661\n",
      "(Iteration 12901 / 76500) loss: 2.303671\n",
      "(Iteration 13001 / 76500) loss: 2.303663\n",
      "(Epoch 17 / 100) train acc: 0.118000; val_acc: 0.087000\n",
      "(Iteration 13101 / 76500) loss: 2.303665\n",
      "(Iteration 13201 / 76500) loss: 2.303559\n",
      "(Iteration 13301 / 76500) loss: 2.303642\n",
      "(Iteration 13401 / 76500) loss: 2.303558\n",
      "(Iteration 13501 / 76500) loss: 2.303719\n",
      "(Iteration 13601 / 76500) loss: 2.303643\n",
      "(Iteration 13701 / 76500) loss: 2.303679\n",
      "(Epoch 18 / 100) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 13801 / 76500) loss: 2.303586\n",
      "(Iteration 13901 / 76500) loss: 2.303669\n",
      "(Iteration 14001 / 76500) loss: 2.303625\n",
      "(Iteration 14101 / 76500) loss: 2.303583\n",
      "(Iteration 14201 / 76500) loss: 2.303603\n",
      "(Iteration 14301 / 76500) loss: 2.303592\n",
      "(Iteration 14401 / 76500) loss: 2.303648\n",
      "(Iteration 14501 / 76500) loss: 2.303669\n",
      "(Epoch 19 / 100) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 14601 / 76500) loss: 2.303624\n",
      "(Iteration 14701 / 76500) loss: 2.303639\n",
      "(Iteration 14801 / 76500) loss: 2.303615\n",
      "(Iteration 14901 / 76500) loss: 2.303633\n",
      "(Iteration 15001 / 76500) loss: 2.303545\n",
      "(Iteration 15101 / 76500) loss: 2.303644\n",
      "(Iteration 15201 / 76500) loss: 2.303655\n",
      "(Epoch 20 / 100) train acc: 0.111000; val_acc: 0.087000\n",
      "(Iteration 15301 / 76500) loss: 2.303588\n",
      "(Iteration 15401 / 76500) loss: 2.303578\n",
      "(Iteration 15501 / 76500) loss: 2.303591\n",
      "(Iteration 15601 / 76500) loss: 2.303617\n",
      "(Iteration 15701 / 76500) loss: 2.303571\n",
      "(Iteration 15801 / 76500) loss: 2.303717\n",
      "(Iteration 15901 / 76500) loss: 2.303576\n",
      "(Iteration 16001 / 76500) loss: 2.303481\n",
      "(Epoch 21 / 100) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 16101 / 76500) loss: 2.303622\n",
      "(Iteration 16201 / 76500) loss: 2.303617\n",
      "(Iteration 16301 / 76500) loss: 2.303735\n",
      "(Iteration 16401 / 76500) loss: 2.303641\n",
      "(Iteration 16501 / 76500) loss: 2.303600\n",
      "(Iteration 16601 / 76500) loss: 2.303580\n",
      "(Iteration 16701 / 76500) loss: 2.303561\n",
      "(Iteration 16801 / 76500) loss: 2.303613\n",
      "(Epoch 22 / 100) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 16901 / 76500) loss: 2.303576\n",
      "(Iteration 17001 / 76500) loss: 2.303595\n",
      "(Iteration 17101 / 76500) loss: 2.303568\n",
      "(Iteration 17201 / 76500) loss: 2.303613\n",
      "(Iteration 17301 / 76500) loss: 2.303560\n",
      "(Iteration 17401 / 76500) loss: 2.303525\n",
      "(Iteration 17501 / 76500) loss: 2.303560\n",
      "(Epoch 23 / 100) train acc: 0.091000; val_acc: 0.087000\n",
      "(Iteration 17601 / 76500) loss: 2.303500\n",
      "(Iteration 17701 / 76500) loss: 2.303553\n",
      "(Iteration 17801 / 76500) loss: 2.303538\n",
      "(Iteration 17901 / 76500) loss: 2.303539\n",
      "(Iteration 18001 / 76500) loss: 2.303549\n",
      "(Iteration 18101 / 76500) loss: 2.303631\n",
      "(Iteration 18201 / 76500) loss: 2.303641\n",
      "(Iteration 18301 / 76500) loss: 2.303571\n",
      "(Epoch 24 / 100) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 18401 / 76500) loss: 2.303474\n",
      "(Iteration 18501 / 76500) loss: 2.303506\n",
      "(Iteration 18601 / 76500) loss: 2.303593\n",
      "(Iteration 18701 / 76500) loss: 2.303627\n",
      "(Iteration 18801 / 76500) loss: 2.303539\n",
      "(Iteration 18901 / 76500) loss: 2.303585\n",
      "(Iteration 19001 / 76500) loss: 2.303466\n",
      "(Iteration 19101 / 76500) loss: 2.303594\n",
      "(Epoch 25 / 100) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 19201 / 76500) loss: 2.303568\n",
      "(Iteration 19301 / 76500) loss: 2.303564\n",
      "(Iteration 19401 / 76500) loss: 2.303517\n",
      "(Iteration 19501 / 76500) loss: 2.303648\n",
      "(Iteration 19601 / 76500) loss: 2.303609\n",
      "(Iteration 19701 / 76500) loss: 2.303541\n",
      "(Iteration 19801 / 76500) loss: 2.303555\n",
      "(Epoch 26 / 100) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 19901 / 76500) loss: 2.303674\n",
      "(Iteration 20001 / 76500) loss: 2.303496\n",
      "(Iteration 20101 / 76500) loss: 2.303590\n",
      "(Iteration 20201 / 76500) loss: 2.303577\n",
      "(Iteration 20301 / 76500) loss: 2.303569\n",
      "(Iteration 20401 / 76500) loss: 2.303617\n",
      "(Iteration 20501 / 76500) loss: 2.303551\n",
      "(Iteration 20601 / 76500) loss: 2.303653\n",
      "(Epoch 27 / 100) train acc: 0.123000; val_acc: 0.087000\n",
      "(Iteration 20701 / 76500) loss: 2.303633\n",
      "(Iteration 20801 / 76500) loss: 2.303580\n",
      "(Iteration 20901 / 76500) loss: 2.303548\n",
      "(Iteration 21001 / 76500) loss: 2.303552\n",
      "(Iteration 21101 / 76500) loss: 2.303597\n",
      "(Iteration 21201 / 76500) loss: 2.303607\n",
      "(Iteration 21301 / 76500) loss: 2.303595\n",
      "(Iteration 21401 / 76500) loss: 2.303586\n",
      "(Epoch 28 / 100) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 21501 / 76500) loss: 2.303555\n",
      "(Iteration 21601 / 76500) loss: 2.303614\n",
      "(Iteration 21701 / 76500) loss: 2.303516\n",
      "(Iteration 21801 / 76500) loss: 2.303550\n",
      "(Iteration 21901 / 76500) loss: 2.303613\n",
      "(Iteration 22001 / 76500) loss: 2.303576\n",
      "(Iteration 22101 / 76500) loss: 2.303614\n",
      "(Epoch 29 / 100) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 22201 / 76500) loss: 2.303523\n",
      "(Iteration 22301 / 76500) loss: 2.303504\n",
      "(Iteration 22401 / 76500) loss: 2.303561\n",
      "(Iteration 22501 / 76500) loss: 2.303502\n",
      "(Iteration 22601 / 76500) loss: 2.303625\n",
      "(Iteration 22701 / 76500) loss: 2.303615\n",
      "(Iteration 22801 / 76500) loss: 2.303558\n",
      "(Iteration 22901 / 76500) loss: 2.303500\n",
      "(Epoch 30 / 100) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 23001 / 76500) loss: 2.303649\n",
      "(Iteration 23101 / 76500) loss: 2.303653\n",
      "(Iteration 23201 / 76500) loss: 2.303584\n",
      "(Iteration 23301 / 76500) loss: 2.303618\n",
      "(Iteration 23401 / 76500) loss: 2.303542\n",
      "(Iteration 23501 / 76500) loss: 2.303627\n",
      "(Iteration 23601 / 76500) loss: 2.303520\n",
      "(Iteration 23701 / 76500) loss: 2.303508\n",
      "(Epoch 31 / 100) train acc: 0.117000; val_acc: 0.087000\n",
      "(Iteration 23801 / 76500) loss: 2.303521\n",
      "(Iteration 23901 / 76500) loss: 2.303556\n",
      "(Iteration 24001 / 76500) loss: 2.303580\n",
      "(Iteration 24101 / 76500) loss: 2.303573\n",
      "(Iteration 24201 / 76500) loss: 2.303694\n",
      "(Iteration 24301 / 76500) loss: 2.303608\n",
      "(Iteration 24401 / 76500) loss: 2.303585\n",
      "(Epoch 32 / 100) train acc: 0.085000; val_acc: 0.087000\n",
      "(Iteration 24501 / 76500) loss: 2.303572\n",
      "(Iteration 24601 / 76500) loss: 2.303580\n",
      "(Iteration 24701 / 76500) loss: 2.303583\n",
      "(Iteration 24801 / 76500) loss: 2.303588\n",
      "(Iteration 24901 / 76500) loss: 2.303503\n",
      "(Iteration 25001 / 76500) loss: 2.303626\n",
      "(Iteration 25101 / 76500) loss: 2.303505\n",
      "(Iteration 25201 / 76500) loss: 2.303516\n",
      "(Epoch 33 / 100) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 25301 / 76500) loss: 2.303575\n",
      "(Iteration 25401 / 76500) loss: 2.303510\n",
      "(Iteration 25501 / 76500) loss: 2.303582\n",
      "(Iteration 25601 / 76500) loss: 2.303535\n",
      "(Iteration 25701 / 76500) loss: 2.303541\n",
      "(Iteration 25801 / 76500) loss: 2.303483\n",
      "(Iteration 25901 / 76500) loss: 2.303589\n",
      "(Iteration 26001 / 76500) loss: 2.303584\n",
      "(Epoch 34 / 100) train acc: 0.117000; val_acc: 0.087000\n",
      "(Iteration 26101 / 76500) loss: 2.303545\n",
      "(Iteration 26201 / 76500) loss: 2.303552\n",
      "(Iteration 26301 / 76500) loss: 2.303586\n",
      "(Iteration 26401 / 76500) loss: 2.303459\n",
      "(Iteration 26501 / 76500) loss: 2.303547\n",
      "(Iteration 26601 / 76500) loss: 2.303626\n",
      "(Iteration 26701 / 76500) loss: 2.303565\n",
      "(Epoch 35 / 100) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 26801 / 76500) loss: 2.303622\n",
      "(Iteration 26901 / 76500) loss: 2.303636\n",
      "(Iteration 27001 / 76500) loss: 2.303636\n",
      "(Iteration 27101 / 76500) loss: 2.303542\n",
      "(Iteration 27201 / 76500) loss: 2.303583\n",
      "(Iteration 27301 / 76500) loss: 2.303530\n",
      "(Iteration 27401 / 76500) loss: 2.303549\n",
      "(Iteration 27501 / 76500) loss: 2.303514\n",
      "(Epoch 36 / 100) train acc: 0.091000; val_acc: 0.087000\n",
      "(Iteration 27601 / 76500) loss: 2.303429\n",
      "(Iteration 27701 / 76500) loss: 2.303533\n",
      "(Iteration 27801 / 76500) loss: 2.303549\n",
      "(Iteration 27901 / 76500) loss: 2.303513\n",
      "(Iteration 28001 / 76500) loss: 2.303570\n",
      "(Iteration 28101 / 76500) loss: 2.303469\n",
      "(Iteration 28201 / 76500) loss: 2.303497\n",
      "(Iteration 28301 / 76500) loss: 2.303572\n",
      "(Epoch 37 / 100) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 28401 / 76500) loss: 2.303585\n",
      "(Iteration 28501 / 76500) loss: 2.303552\n",
      "(Iteration 28601 / 76500) loss: 2.303557\n",
      "(Iteration 28701 / 76500) loss: 2.303504\n",
      "(Iteration 28801 / 76500) loss: 2.303509\n",
      "(Iteration 28901 / 76500) loss: 2.303563\n",
      "(Iteration 29001 / 76500) loss: 2.303401\n",
      "(Epoch 38 / 100) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 29101 / 76500) loss: 2.303637\n",
      "(Iteration 29201 / 76500) loss: 2.303531\n",
      "(Iteration 29301 / 76500) loss: 2.303513\n",
      "(Iteration 29401 / 76500) loss: 2.303580\n",
      "(Iteration 29501 / 76500) loss: 2.303600\n",
      "(Iteration 29601 / 76500) loss: 2.303562\n",
      "(Iteration 29701 / 76500) loss: 2.303501\n",
      "(Iteration 29801 / 76500) loss: 2.303624\n",
      "(Epoch 39 / 100) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 29901 / 76500) loss: 2.303548\n",
      "(Iteration 30001 / 76500) loss: 2.303521\n",
      "(Iteration 30101 / 76500) loss: 2.303487\n",
      "(Iteration 30201 / 76500) loss: 2.303580\n",
      "(Iteration 30301 / 76500) loss: 2.303600\n",
      "(Iteration 30401 / 76500) loss: 2.303521\n",
      "(Iteration 30501 / 76500) loss: 2.303590\n",
      "(Epoch 40 / 100) train acc: 0.118000; val_acc: 0.087000\n",
      "(Iteration 30601 / 76500) loss: 2.303521\n",
      "(Iteration 30701 / 76500) loss: 2.303417\n",
      "(Iteration 30801 / 76500) loss: 2.303579\n",
      "(Iteration 30901 / 76500) loss: 2.303554\n",
      "(Iteration 31001 / 76500) loss: 2.303543\n",
      "(Iteration 31101 / 76500) loss: 2.303497\n",
      "(Iteration 31201 / 76500) loss: 2.303483\n",
      "(Iteration 31301 / 76500) loss: 2.303540\n",
      "(Epoch 41 / 100) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 31401 / 76500) loss: 2.303479\n",
      "(Iteration 31501 / 76500) loss: 2.303555\n",
      "(Iteration 31601 / 76500) loss: 2.303556\n",
      "(Iteration 31701 / 76500) loss: 2.303544\n",
      "(Iteration 31801 / 76500) loss: 2.303531\n",
      "(Iteration 31901 / 76500) loss: 2.303603\n",
      "(Iteration 32001 / 76500) loss: 2.303503\n",
      "(Iteration 32101 / 76500) loss: 2.303591\n",
      "(Epoch 42 / 100) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 32201 / 76500) loss: 2.303539\n",
      "(Iteration 32301 / 76500) loss: 2.303620\n",
      "(Iteration 32401 / 76500) loss: 2.303593\n",
      "(Iteration 32501 / 76500) loss: 2.303630\n",
      "(Iteration 32601 / 76500) loss: 2.303501\n",
      "(Iteration 32701 / 76500) loss: 2.303620\n",
      "(Iteration 32801 / 76500) loss: 2.303528\n",
      "(Epoch 43 / 100) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 32901 / 76500) loss: 2.303478\n",
      "(Iteration 33001 / 76500) loss: 2.303613\n",
      "(Iteration 33101 / 76500) loss: 2.303523\n",
      "(Iteration 33201 / 76500) loss: 2.303491\n",
      "(Iteration 33301 / 76500) loss: 2.303630\n",
      "(Iteration 33401 / 76500) loss: 2.303490\n",
      "(Iteration 33501 / 76500) loss: 2.303563\n",
      "(Iteration 33601 / 76500) loss: 2.303532\n",
      "(Epoch 44 / 100) train acc: 0.117000; val_acc: 0.087000\n",
      "(Iteration 33701 / 76500) loss: 2.303566\n",
      "(Iteration 33801 / 76500) loss: 2.303525\n",
      "(Iteration 33901 / 76500) loss: 2.303515\n",
      "(Iteration 34001 / 76500) loss: 2.303562\n",
      "(Iteration 34101 / 76500) loss: 2.303480\n",
      "(Iteration 34201 / 76500) loss: 2.303544\n",
      "(Iteration 34301 / 76500) loss: 2.303589\n",
      "(Iteration 34401 / 76500) loss: 2.303480\n",
      "(Epoch 45 / 100) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 34501 / 76500) loss: 2.303607\n",
      "(Iteration 34601 / 76500) loss: 2.303448\n",
      "(Iteration 34701 / 76500) loss: 2.303455\n",
      "(Iteration 34801 / 76500) loss: 2.303471\n",
      "(Iteration 34901 / 76500) loss: 2.303590\n",
      "(Iteration 35001 / 76500) loss: 2.303450\n",
      "(Iteration 35101 / 76500) loss: 2.303572\n",
      "(Epoch 46 / 100) train acc: 0.082000; val_acc: 0.087000\n",
      "(Iteration 35201 / 76500) loss: 2.303644\n",
      "(Iteration 35301 / 76500) loss: 2.303545\n",
      "(Iteration 35401 / 76500) loss: 2.303526\n",
      "(Iteration 35501 / 76500) loss: 2.303535\n",
      "(Iteration 35601 / 76500) loss: 2.303460\n",
      "(Iteration 35701 / 76500) loss: 2.303508\n",
      "(Iteration 35801 / 76500) loss: 2.303577\n",
      "(Iteration 35901 / 76500) loss: 2.303512\n",
      "(Epoch 47 / 100) train acc: 0.122000; val_acc: 0.087000\n",
      "(Iteration 36001 / 76500) loss: 2.303483\n",
      "(Iteration 36101 / 76500) loss: 2.303518\n",
      "(Iteration 36201 / 76500) loss: 2.303539\n",
      "(Iteration 36301 / 76500) loss: 2.303475\n",
      "(Iteration 36401 / 76500) loss: 2.303552\n",
      "(Iteration 36501 / 76500) loss: 2.303486\n",
      "(Iteration 36601 / 76500) loss: 2.303522\n",
      "(Iteration 36701 / 76500) loss: 2.303455\n",
      "(Epoch 48 / 100) train acc: 0.113000; val_acc: 0.087000\n",
      "(Iteration 36801 / 76500) loss: 2.303442\n",
      "(Iteration 36901 / 76500) loss: 2.303573\n",
      "(Iteration 37001 / 76500) loss: 2.303580\n",
      "(Iteration 37101 / 76500) loss: 2.303468\n",
      "(Iteration 37201 / 76500) loss: 2.303550\n",
      "(Iteration 37301 / 76500) loss: 2.303554\n",
      "(Iteration 37401 / 76500) loss: 2.303565\n",
      "(Epoch 49 / 100) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 37501 / 76500) loss: 2.303564\n",
      "(Iteration 37601 / 76500) loss: 2.303467\n",
      "(Iteration 37701 / 76500) loss: 2.303558\n",
      "(Iteration 37801 / 76500) loss: 2.303530\n",
      "(Iteration 37901 / 76500) loss: 2.303569\n",
      "(Iteration 38001 / 76500) loss: 2.303477\n",
      "(Iteration 38101 / 76500) loss: 2.303508\n",
      "(Iteration 38201 / 76500) loss: 2.303526\n",
      "(Epoch 50 / 100) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 38301 / 76500) loss: 2.303481\n",
      "(Iteration 38401 / 76500) loss: 2.303499\n",
      "(Iteration 38501 / 76500) loss: 2.303616\n",
      "(Iteration 38601 / 76500) loss: 2.303586\n",
      "(Iteration 38701 / 76500) loss: 2.303539\n",
      "(Iteration 38801 / 76500) loss: 2.303561\n",
      "(Iteration 38901 / 76500) loss: 2.303510\n",
      "(Iteration 39001 / 76500) loss: 2.303488\n",
      "(Epoch 51 / 100) train acc: 0.107000; val_acc: 0.087000\n",
      "(Iteration 39101 / 76500) loss: 2.303527\n",
      "(Iteration 39201 / 76500) loss: 2.303604\n",
      "(Iteration 39301 / 76500) loss: 2.303542\n",
      "(Iteration 39401 / 76500) loss: 2.303528\n",
      "(Iteration 39501 / 76500) loss: 2.303502\n",
      "(Iteration 39601 / 76500) loss: 2.303596\n",
      "(Iteration 39701 / 76500) loss: 2.303480\n",
      "(Epoch 52 / 100) train acc: 0.092000; val_acc: 0.087000\n",
      "(Iteration 39801 / 76500) loss: 2.303577\n",
      "(Iteration 39901 / 76500) loss: 2.303563\n",
      "(Iteration 40001 / 76500) loss: 2.303533\n",
      "(Iteration 40101 / 76500) loss: 2.303602\n",
      "(Iteration 40201 / 76500) loss: 2.303598\n",
      "(Iteration 40301 / 76500) loss: 2.303590\n",
      "(Iteration 40401 / 76500) loss: 2.303564\n",
      "(Iteration 40501 / 76500) loss: 2.303546\n",
      "(Epoch 53 / 100) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 40601 / 76500) loss: 2.303530\n",
      "(Iteration 40701 / 76500) loss: 2.303557\n",
      "(Iteration 40801 / 76500) loss: 2.303550\n",
      "(Iteration 40901 / 76500) loss: 2.303537\n",
      "(Iteration 41001 / 76500) loss: 2.303593\n",
      "(Iteration 41101 / 76500) loss: 2.303589\n",
      "(Iteration 41201 / 76500) loss: 2.303480\n",
      "(Iteration 41301 / 76500) loss: 2.303524\n",
      "(Epoch 54 / 100) train acc: 0.112000; val_acc: 0.087000\n",
      "(Iteration 41401 / 76500) loss: 2.303470\n",
      "(Iteration 41501 / 76500) loss: 2.303504\n",
      "(Iteration 41601 / 76500) loss: 2.303559\n",
      "(Iteration 41701 / 76500) loss: 2.303570\n",
      "(Iteration 41801 / 76500) loss: 2.303561\n",
      "(Iteration 41901 / 76500) loss: 2.303427\n",
      "(Iteration 42001 / 76500) loss: 2.303505\n",
      "(Epoch 55 / 100) train acc: 0.092000; val_acc: 0.087000\n",
      "(Iteration 42101 / 76500) loss: 2.303490\n",
      "(Iteration 42201 / 76500) loss: 2.303631\n",
      "(Iteration 42301 / 76500) loss: 2.303537\n",
      "(Iteration 42401 / 76500) loss: 2.303484\n",
      "(Iteration 42501 / 76500) loss: 2.303575\n",
      "(Iteration 42601 / 76500) loss: 2.303549\n",
      "(Iteration 42701 / 76500) loss: 2.303450\n",
      "(Iteration 42801 / 76500) loss: 2.303588\n",
      "(Epoch 56 / 100) train acc: 0.109000; val_acc: 0.087000\n",
      "(Iteration 42901 / 76500) loss: 2.303529\n",
      "(Iteration 43001 / 76500) loss: 2.303636\n",
      "(Iteration 43101 / 76500) loss: 2.303492\n",
      "(Iteration 43201 / 76500) loss: 2.303538\n",
      "(Iteration 43301 / 76500) loss: 2.303542\n",
      "(Iteration 43401 / 76500) loss: 2.303513\n",
      "(Iteration 43501 / 76500) loss: 2.303505\n",
      "(Iteration 43601 / 76500) loss: 2.303541\n",
      "(Epoch 57 / 100) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 43701 / 76500) loss: 2.303588\n",
      "(Iteration 43801 / 76500) loss: 2.303550\n",
      "(Iteration 43901 / 76500) loss: 2.303522\n",
      "(Iteration 44001 / 76500) loss: 2.303504\n",
      "(Iteration 44101 / 76500) loss: 2.303555\n",
      "(Iteration 44201 / 76500) loss: 2.303544\n",
      "(Iteration 44301 / 76500) loss: 2.303549\n",
      "(Epoch 58 / 100) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 44401 / 76500) loss: 2.303434\n",
      "(Iteration 44501 / 76500) loss: 2.303490\n",
      "(Iteration 44601 / 76500) loss: 2.303548\n",
      "(Iteration 44701 / 76500) loss: 2.303576\n",
      "(Iteration 44801 / 76500) loss: 2.303503\n",
      "(Iteration 44901 / 76500) loss: 2.303470\n",
      "(Iteration 45001 / 76500) loss: 2.303539\n",
      "(Iteration 45101 / 76500) loss: 2.303448\n",
      "(Epoch 59 / 100) train acc: 0.087000; val_acc: 0.087000\n",
      "(Iteration 45201 / 76500) loss: 2.303544\n",
      "(Iteration 45301 / 76500) loss: 2.303450\n",
      "(Iteration 45401 / 76500) loss: 2.303546\n",
      "(Iteration 45501 / 76500) loss: 2.303475\n",
      "(Iteration 45601 / 76500) loss: 2.303603\n",
      "(Iteration 45701 / 76500) loss: 2.303535\n",
      "(Iteration 45801 / 76500) loss: 2.303593\n",
      "(Epoch 60 / 100) train acc: 0.091000; val_acc: 0.087000\n",
      "(Iteration 45901 / 76500) loss: 2.303596\n",
      "(Iteration 46001 / 76500) loss: 2.303515\n",
      "(Iteration 46101 / 76500) loss: 2.303531\n",
      "(Iteration 46201 / 76500) loss: 2.303629\n",
      "(Iteration 46301 / 76500) loss: 2.303568\n",
      "(Iteration 46401 / 76500) loss: 2.303494\n",
      "(Iteration 46501 / 76500) loss: 2.303567\n",
      "(Iteration 46601 / 76500) loss: 2.303530\n",
      "(Epoch 61 / 100) train acc: 0.092000; val_acc: 0.087000\n",
      "(Iteration 46701 / 76500) loss: 2.303538\n",
      "(Iteration 46801 / 76500) loss: 2.303536\n",
      "(Iteration 46901 / 76500) loss: 2.303582\n",
      "(Iteration 47001 / 76500) loss: 2.303508\n",
      "(Iteration 47101 / 76500) loss: 2.303540\n",
      "(Iteration 47201 / 76500) loss: 2.303521\n",
      "(Iteration 47301 / 76500) loss: 2.303480\n",
      "(Iteration 47401 / 76500) loss: 2.303506\n",
      "(Epoch 62 / 100) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 47501 / 76500) loss: 2.303569\n",
      "(Iteration 47601 / 76500) loss: 2.303582\n",
      "(Iteration 47701 / 76500) loss: 2.303445\n",
      "(Iteration 47801 / 76500) loss: 2.303491\n",
      "(Iteration 47901 / 76500) loss: 2.303490\n",
      "(Iteration 48001 / 76500) loss: 2.303460\n",
      "(Iteration 48101 / 76500) loss: 2.303519\n",
      "(Epoch 63 / 100) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 48201 / 76500) loss: 2.303593\n",
      "(Iteration 48301 / 76500) loss: 2.303535\n",
      "(Iteration 48401 / 76500) loss: 2.303512\n",
      "(Iteration 48501 / 76500) loss: 2.303458\n",
      "(Iteration 48601 / 76500) loss: 2.303604\n",
      "(Iteration 48701 / 76500) loss: 2.303476\n",
      "(Iteration 48801 / 76500) loss: 2.303557\n",
      "(Iteration 48901 / 76500) loss: 2.303463\n",
      "(Epoch 64 / 100) train acc: 0.114000; val_acc: 0.087000\n",
      "(Iteration 49001 / 76500) loss: 2.303535\n",
      "(Iteration 49101 / 76500) loss: 2.303450\n",
      "(Iteration 49201 / 76500) loss: 2.303484\n",
      "(Iteration 49301 / 76500) loss: 2.303548\n",
      "(Iteration 49401 / 76500) loss: 2.303487\n",
      "(Iteration 49501 / 76500) loss: 2.303496\n",
      "(Iteration 49601 / 76500) loss: 2.303461\n",
      "(Iteration 49701 / 76500) loss: 2.303514\n",
      "(Epoch 65 / 100) train acc: 0.112000; val_acc: 0.087000\n",
      "(Iteration 49801 / 76500) loss: 2.303546\n",
      "(Iteration 49901 / 76500) loss: 2.303512\n",
      "(Iteration 50001 / 76500) loss: 2.303554\n",
      "(Iteration 50101 / 76500) loss: 2.303541\n",
      "(Iteration 50201 / 76500) loss: 2.303580\n",
      "(Iteration 50301 / 76500) loss: 2.303600\n",
      "(Iteration 50401 / 76500) loss: 2.303608\n",
      "(Epoch 66 / 100) train acc: 0.088000; val_acc: 0.087000\n",
      "(Iteration 50501 / 76500) loss: 2.303568\n",
      "(Iteration 50601 / 76500) loss: 2.303509\n",
      "(Iteration 50701 / 76500) loss: 2.303582\n",
      "(Iteration 50801 / 76500) loss: 2.303514\n",
      "(Iteration 50901 / 76500) loss: 2.303534\n",
      "(Iteration 51001 / 76500) loss: 2.303578\n",
      "(Iteration 51101 / 76500) loss: 2.303458\n",
      "(Iteration 51201 / 76500) loss: 2.303561\n",
      "(Epoch 67 / 100) train acc: 0.086000; val_acc: 0.087000\n",
      "(Iteration 51301 / 76500) loss: 2.303554\n",
      "(Iteration 51401 / 76500) loss: 2.303503\n",
      "(Iteration 51501 / 76500) loss: 2.303507\n",
      "(Iteration 51601 / 76500) loss: 2.303491\n",
      "(Iteration 51701 / 76500) loss: 2.303444\n",
      "(Iteration 51801 / 76500) loss: 2.303528\n",
      "(Iteration 51901 / 76500) loss: 2.303534\n",
      "(Iteration 52001 / 76500) loss: 2.303545\n",
      "(Epoch 68 / 100) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 52101 / 76500) loss: 2.303551\n",
      "(Iteration 52201 / 76500) loss: 2.303489\n",
      "(Iteration 52301 / 76500) loss: 2.303490\n",
      "(Iteration 52401 / 76500) loss: 2.303546\n",
      "(Iteration 52501 / 76500) loss: 2.303523\n",
      "(Iteration 52601 / 76500) loss: 2.303574\n",
      "(Iteration 52701 / 76500) loss: 2.303513\n",
      "(Epoch 69 / 100) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 52801 / 76500) loss: 2.303565\n",
      "(Iteration 52901 / 76500) loss: 2.303511\n",
      "(Iteration 53001 / 76500) loss: 2.303434\n",
      "(Iteration 53101 / 76500) loss: 2.303477\n",
      "(Iteration 53201 / 76500) loss: 2.303587\n",
      "(Iteration 53301 / 76500) loss: 2.303545\n",
      "(Iteration 53401 / 76500) loss: 2.303537\n",
      "(Iteration 53501 / 76500) loss: 2.303579\n",
      "(Epoch 70 / 100) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 53601 / 76500) loss: 2.303496\n",
      "(Iteration 53701 / 76500) loss: 2.303447\n",
      "(Iteration 53801 / 76500) loss: 2.303570\n",
      "(Iteration 53901 / 76500) loss: 2.303592\n",
      "(Iteration 54001 / 76500) loss: 2.303427\n",
      "(Iteration 54101 / 76500) loss: 2.303500\n",
      "(Iteration 54201 / 76500) loss: 2.303602\n",
      "(Iteration 54301 / 76500) loss: 2.303468\n",
      "(Epoch 71 / 100) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 54401 / 76500) loss: 2.303590\n",
      "(Iteration 54501 / 76500) loss: 2.303549\n",
      "(Iteration 54601 / 76500) loss: 2.303493\n",
      "(Iteration 54701 / 76500) loss: 2.303461\n",
      "(Iteration 54801 / 76500) loss: 2.303460\n",
      "(Iteration 54901 / 76500) loss: 2.303528\n",
      "(Iteration 55001 / 76500) loss: 2.303515\n",
      "(Epoch 72 / 100) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 55101 / 76500) loss: 2.303497\n",
      "(Iteration 55201 / 76500) loss: 2.303367\n",
      "(Iteration 55301 / 76500) loss: 2.303532\n",
      "(Iteration 55401 / 76500) loss: 2.303476\n",
      "(Iteration 55501 / 76500) loss: 2.303451\n",
      "(Iteration 55601 / 76500) loss: 2.303582\n",
      "(Iteration 55701 / 76500) loss: 2.303514\n",
      "(Iteration 55801 / 76500) loss: 2.303586\n",
      "(Epoch 73 / 100) train acc: 0.090000; val_acc: 0.087000\n",
      "(Iteration 55901 / 76500) loss: 2.303541\n",
      "(Iteration 56001 / 76500) loss: 2.303531\n",
      "(Iteration 56101 / 76500) loss: 2.303563\n",
      "(Iteration 56201 / 76500) loss: 2.303538\n",
      "(Iteration 56301 / 76500) loss: 2.303519\n",
      "(Iteration 56401 / 76500) loss: 2.303611\n",
      "(Iteration 56501 / 76500) loss: 2.303534\n",
      "(Iteration 56601 / 76500) loss: 2.303419\n",
      "(Epoch 74 / 100) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 56701 / 76500) loss: 2.303567\n",
      "(Iteration 56801 / 76500) loss: 2.303570\n",
      "(Iteration 56901 / 76500) loss: 2.303483\n",
      "(Iteration 57001 / 76500) loss: 2.303537\n",
      "(Iteration 57101 / 76500) loss: 2.303573\n",
      "(Iteration 57201 / 76500) loss: 2.303458\n",
      "(Iteration 57301 / 76500) loss: 2.303505\n",
      "(Epoch 75 / 100) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 57401 / 76500) loss: 2.303519\n",
      "(Iteration 57501 / 76500) loss: 2.303522\n",
      "(Iteration 57601 / 76500) loss: 2.303464\n",
      "(Iteration 57701 / 76500) loss: 2.303530\n",
      "(Iteration 57801 / 76500) loss: 2.303501\n",
      "(Iteration 57901 / 76500) loss: 2.303499\n",
      "(Iteration 58001 / 76500) loss: 2.303525\n",
      "(Iteration 58101 / 76500) loss: 2.303532\n",
      "(Epoch 76 / 100) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 58201 / 76500) loss: 2.303481\n",
      "(Iteration 58301 / 76500) loss: 2.303559\n",
      "(Iteration 58401 / 76500) loss: 2.303491\n",
      "(Iteration 58501 / 76500) loss: 2.303566\n",
      "(Iteration 58601 / 76500) loss: 2.303581\n",
      "(Iteration 58701 / 76500) loss: 2.303436\n",
      "(Iteration 58801 / 76500) loss: 2.303497\n",
      "(Iteration 58901 / 76500) loss: 2.303452\n",
      "(Epoch 77 / 100) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 59001 / 76500) loss: 2.303494\n",
      "(Iteration 59101 / 76500) loss: 2.303538\n",
      "(Iteration 59201 / 76500) loss: 2.303503\n",
      "(Iteration 59301 / 76500) loss: 2.303494\n",
      "(Iteration 59401 / 76500) loss: 2.303534\n",
      "(Iteration 59501 / 76500) loss: 2.303479\n",
      "(Iteration 59601 / 76500) loss: 2.303523\n",
      "(Epoch 78 / 100) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 59701 / 76500) loss: 2.303535\n",
      "(Iteration 59801 / 76500) loss: 2.303481\n",
      "(Iteration 59901 / 76500) loss: 2.303459\n",
      "(Iteration 60001 / 76500) loss: 2.303415\n",
      "(Iteration 60101 / 76500) loss: 2.303513\n",
      "(Iteration 60201 / 76500) loss: 2.303478\n",
      "(Iteration 60301 / 76500) loss: 2.303604\n",
      "(Iteration 60401 / 76500) loss: 2.303392\n",
      "(Epoch 79 / 100) train acc: 0.123000; val_acc: 0.087000\n",
      "(Iteration 60501 / 76500) loss: 2.303529\n",
      "(Iteration 60601 / 76500) loss: 2.303403\n",
      "(Iteration 60701 / 76500) loss: 2.303553\n",
      "(Iteration 60801 / 76500) loss: 2.303551\n",
      "(Iteration 60901 / 76500) loss: 2.303519\n",
      "(Iteration 61001 / 76500) loss: 2.303451\n",
      "(Iteration 61101 / 76500) loss: 2.303550\n",
      "(Epoch 80 / 100) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 61201 / 76500) loss: 2.303560\n",
      "(Iteration 61301 / 76500) loss: 2.303472\n",
      "(Iteration 61401 / 76500) loss: 2.303454\n",
      "(Iteration 61501 / 76500) loss: 2.303546\n",
      "(Iteration 61601 / 76500) loss: 2.303418\n",
      "(Iteration 61701 / 76500) loss: 2.303522\n",
      "(Iteration 61801 / 76500) loss: 2.303475\n",
      "(Iteration 61901 / 76500) loss: 2.303516\n",
      "(Epoch 81 / 100) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 62001 / 76500) loss: 2.303540\n",
      "(Iteration 62101 / 76500) loss: 2.303529\n",
      "(Iteration 62201 / 76500) loss: 2.303532\n",
      "(Iteration 62301 / 76500) loss: 2.303571\n",
      "(Iteration 62401 / 76500) loss: 2.303537\n",
      "(Iteration 62501 / 76500) loss: 2.303470\n",
      "(Iteration 62601 / 76500) loss: 2.303525\n",
      "(Iteration 62701 / 76500) loss: 2.303492\n",
      "(Epoch 82 / 100) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 62801 / 76500) loss: 2.303465\n",
      "(Iteration 62901 / 76500) loss: 2.303473\n",
      "(Iteration 63001 / 76500) loss: 2.303547\n",
      "(Iteration 63101 / 76500) loss: 2.303568\n",
      "(Iteration 63201 / 76500) loss: 2.303483\n",
      "(Iteration 63301 / 76500) loss: 2.303495\n",
      "(Iteration 63401 / 76500) loss: 2.303498\n",
      "(Epoch 83 / 100) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 63501 / 76500) loss: 2.303517\n",
      "(Iteration 63601 / 76500) loss: 2.303543\n",
      "(Iteration 63701 / 76500) loss: 2.303478\n",
      "(Iteration 63801 / 76500) loss: 2.303542\n",
      "(Iteration 63901 / 76500) loss: 2.303577\n",
      "(Iteration 64001 / 76500) loss: 2.303425\n",
      "(Iteration 64101 / 76500) loss: 2.303555\n",
      "(Iteration 64201 / 76500) loss: 2.303539\n",
      "(Epoch 84 / 100) train acc: 0.092000; val_acc: 0.087000\n",
      "(Iteration 64301 / 76500) loss: 2.303571\n",
      "(Iteration 64401 / 76500) loss: 2.303594\n",
      "(Iteration 64501 / 76500) loss: 2.303584\n",
      "(Iteration 64601 / 76500) loss: 2.303389\n",
      "(Iteration 64701 / 76500) loss: 2.303553\n",
      "(Iteration 64801 / 76500) loss: 2.303459\n",
      "(Iteration 64901 / 76500) loss: 2.303499\n",
      "(Iteration 65001 / 76500) loss: 2.303571\n",
      "(Epoch 85 / 100) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 65101 / 76500) loss: 2.303466\n",
      "(Iteration 65201 / 76500) loss: 2.303566\n",
      "(Iteration 65301 / 76500) loss: 2.303510\n",
      "(Iteration 65401 / 76500) loss: 2.303493\n",
      "(Iteration 65501 / 76500) loss: 2.303511\n",
      "(Iteration 65601 / 76500) loss: 2.303471\n",
      "(Iteration 65701 / 76500) loss: 2.303514\n",
      "(Epoch 86 / 100) train acc: 0.116000; val_acc: 0.087000\n",
      "(Iteration 65801 / 76500) loss: 2.303526\n",
      "(Iteration 65901 / 76500) loss: 2.303503\n",
      "(Iteration 66001 / 76500) loss: 2.303546\n",
      "(Iteration 66101 / 76500) loss: 2.303463\n",
      "(Iteration 66201 / 76500) loss: 2.303478\n",
      "(Iteration 66301 / 76500) loss: 2.303471\n",
      "(Iteration 66401 / 76500) loss: 2.303602\n",
      "(Iteration 66501 / 76500) loss: 2.303564\n",
      "(Epoch 87 / 100) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 66601 / 76500) loss: 2.303546\n",
      "(Iteration 66701 / 76500) loss: 2.303576\n",
      "(Iteration 66801 / 76500) loss: 2.303502\n",
      "(Iteration 66901 / 76500) loss: 2.303545\n",
      "(Iteration 67001 / 76500) loss: 2.303608\n",
      "(Iteration 67101 / 76500) loss: 2.303456\n",
      "(Iteration 67201 / 76500) loss: 2.303495\n",
      "(Iteration 67301 / 76500) loss: 2.303474\n",
      "(Epoch 88 / 100) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 67401 / 76500) loss: 2.303515\n",
      "(Iteration 67501 / 76500) loss: 2.303508\n",
      "(Iteration 67601 / 76500) loss: 2.303519\n",
      "(Iteration 67701 / 76500) loss: 2.303588\n",
      "(Iteration 67801 / 76500) loss: 2.303573\n",
      "(Iteration 67901 / 76500) loss: 2.303530\n",
      "(Iteration 68001 / 76500) loss: 2.303541\n",
      "(Epoch 89 / 100) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 68101 / 76500) loss: 2.303575\n",
      "(Iteration 68201 / 76500) loss: 2.303522\n",
      "(Iteration 68301 / 76500) loss: 2.303516\n",
      "(Iteration 68401 / 76500) loss: 2.303516\n",
      "(Iteration 68501 / 76500) loss: 2.303500\n",
      "(Iteration 68601 / 76500) loss: 2.303470\n",
      "(Iteration 68701 / 76500) loss: 2.303527\n",
      "(Iteration 68801 / 76500) loss: 2.303559\n",
      "(Epoch 90 / 100) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 68901 / 76500) loss: 2.303525\n",
      "(Iteration 69001 / 76500) loss: 2.303530\n",
      "(Iteration 69101 / 76500) loss: 2.303485\n",
      "(Iteration 69201 / 76500) loss: 2.303586\n",
      "(Iteration 69301 / 76500) loss: 2.303502\n",
      "(Iteration 69401 / 76500) loss: 2.303450\n",
      "(Iteration 69501 / 76500) loss: 2.303514\n",
      "(Iteration 69601 / 76500) loss: 2.303539\n",
      "(Epoch 91 / 100) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 69701 / 76500) loss: 2.303500\n",
      "(Iteration 69801 / 76500) loss: 2.303506\n",
      "(Iteration 69901 / 76500) loss: 2.303495\n",
      "(Iteration 70001 / 76500) loss: 2.303463\n",
      "(Iteration 70101 / 76500) loss: 2.303508\n",
      "(Iteration 70201 / 76500) loss: 2.303618\n",
      "(Iteration 70301 / 76500) loss: 2.303583\n",
      "(Epoch 92 / 100) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 70401 / 76500) loss: 2.303547\n",
      "(Iteration 70501 / 76500) loss: 2.303559\n",
      "(Iteration 70601 / 76500) loss: 2.303454\n",
      "(Iteration 70701 / 76500) loss: 2.303551\n",
      "(Iteration 70801 / 76500) loss: 2.303520\n",
      "(Iteration 70901 / 76500) loss: 2.303520\n",
      "(Iteration 71001 / 76500) loss: 2.303478\n",
      "(Iteration 71101 / 76500) loss: 2.303526\n",
      "(Epoch 93 / 100) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 71201 / 76500) loss: 2.303577\n",
      "(Iteration 71301 / 76500) loss: 2.303566\n",
      "(Iteration 71401 / 76500) loss: 2.303515\n",
      "(Iteration 71501 / 76500) loss: 2.303572\n",
      "(Iteration 71601 / 76500) loss: 2.303554\n",
      "(Iteration 71701 / 76500) loss: 2.303516\n",
      "(Iteration 71801 / 76500) loss: 2.303479\n",
      "(Iteration 71901 / 76500) loss: 2.303528\n",
      "(Epoch 94 / 100) train acc: 0.077000; val_acc: 0.087000\n",
      "(Iteration 72001 / 76500) loss: 2.303576\n",
      "(Iteration 72101 / 76500) loss: 2.303472\n",
      "(Iteration 72201 / 76500) loss: 2.303599\n",
      "(Iteration 72301 / 76500) loss: 2.303537\n",
      "(Iteration 72401 / 76500) loss: 2.303615\n",
      "(Iteration 72501 / 76500) loss: 2.303483\n",
      "(Iteration 72601 / 76500) loss: 2.303487\n",
      "(Epoch 95 / 100) train acc: 0.086000; val_acc: 0.087000\n",
      "(Iteration 72701 / 76500) loss: 2.303442\n",
      "(Iteration 72801 / 76500) loss: 2.303504\n",
      "(Iteration 72901 / 76500) loss: 2.303511\n",
      "(Iteration 73001 / 76500) loss: 2.303457\n",
      "(Iteration 73101 / 76500) loss: 2.303508\n",
      "(Iteration 73201 / 76500) loss: 2.303456\n",
      "(Iteration 73301 / 76500) loss: 2.303461\n",
      "(Iteration 73401 / 76500) loss: 2.303504\n",
      "(Epoch 96 / 100) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 73501 / 76500) loss: 2.303580\n",
      "(Iteration 73601 / 76500) loss: 2.303509\n",
      "(Iteration 73701 / 76500) loss: 2.303586\n",
      "(Iteration 73801 / 76500) loss: 2.303492\n",
      "(Iteration 73901 / 76500) loss: 2.303609\n",
      "(Iteration 74001 / 76500) loss: 2.303466\n",
      "(Iteration 74101 / 76500) loss: 2.303546\n",
      "(Iteration 74201 / 76500) loss: 2.303503\n",
      "(Epoch 97 / 100) train acc: 0.113000; val_acc: 0.087000\n",
      "(Iteration 74301 / 76500) loss: 2.303467\n",
      "(Iteration 74401 / 76500) loss: 2.303500\n",
      "(Iteration 74501 / 76500) loss: 2.303451\n",
      "(Iteration 74601 / 76500) loss: 2.303489\n",
      "(Iteration 74701 / 76500) loss: 2.303605\n",
      "(Iteration 74801 / 76500) loss: 2.303445\n",
      "(Iteration 74901 / 76500) loss: 2.303520\n",
      "(Epoch 98 / 100) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 75001 / 76500) loss: 2.303495\n",
      "(Iteration 75101 / 76500) loss: 2.303556\n",
      "(Iteration 75201 / 76500) loss: 2.303534\n",
      "(Iteration 75301 / 76500) loss: 2.303523\n",
      "(Iteration 75401 / 76500) loss: 2.303471\n",
      "(Iteration 75501 / 76500) loss: 2.303445\n",
      "(Iteration 75601 / 76500) loss: 2.303402\n",
      "(Iteration 75701 / 76500) loss: 2.303517\n",
      "(Epoch 99 / 100) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 75801 / 76500) loss: 2.303522\n",
      "(Iteration 75901 / 76500) loss: 2.303549\n",
      "(Iteration 76001 / 76500) loss: 2.303536\n",
      "(Iteration 76101 / 76500) loss: 2.303492\n",
      "(Iteration 76201 / 76500) loss: 2.303494\n",
      "(Iteration 76301 / 76500) loss: 2.303499\n",
      "(Iteration 76401 / 76500) loss: 2.303494\n",
      "(Epoch 100 / 100) train acc: 0.093000; val_acc: 0.087000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.304588\n",
      "(Epoch 0 / 100) train acc: 0.104000; val_acc: 0.103000\n",
      "(Iteration 101 / 38200) loss: 2.304573\n",
      "(Iteration 201 / 38200) loss: 2.304552\n",
      "(Iteration 301 / 38200) loss: 2.304541\n",
      "(Epoch 1 / 100) train acc: 0.099000; val_acc: 0.102000\n",
      "(Iteration 401 / 38200) loss: 2.304507\n",
      "(Iteration 501 / 38200) loss: 2.304491\n",
      "(Iteration 601 / 38200) loss: 2.304480\n",
      "(Iteration 701 / 38200) loss: 2.304445\n",
      "(Epoch 2 / 100) train acc: 0.102000; val_acc: 0.101000\n",
      "(Iteration 801 / 38200) loss: 2.304447\n",
      "(Iteration 901 / 38200) loss: 2.304433\n",
      "(Iteration 1001 / 38200) loss: 2.304399\n",
      "(Iteration 1101 / 38200) loss: 2.304389\n",
      "(Epoch 3 / 100) train acc: 0.109000; val_acc: 0.106000\n",
      "(Iteration 1201 / 38200) loss: 2.304371\n",
      "(Iteration 1301 / 38200) loss: 2.304364\n",
      "(Iteration 1401 / 38200) loss: 2.304339\n",
      "(Iteration 1501 / 38200) loss: 2.304350\n",
      "(Epoch 4 / 100) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 1601 / 38200) loss: 2.304338\n",
      "(Iteration 1701 / 38200) loss: 2.304299\n",
      "(Iteration 1801 / 38200) loss: 2.304306\n",
      "(Iteration 1901 / 38200) loss: 2.304301\n",
      "(Epoch 5 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 2001 / 38200) loss: 2.304290\n",
      "(Iteration 2101 / 38200) loss: 2.304284\n",
      "(Iteration 2201 / 38200) loss: 2.304249\n",
      "(Epoch 6 / 100) train acc: 0.084000; val_acc: 0.070000\n",
      "(Iteration 2301 / 38200) loss: 2.304260\n",
      "(Iteration 2401 / 38200) loss: 2.304248\n",
      "(Iteration 2501 / 38200) loss: 2.304224\n",
      "(Iteration 2601 / 38200) loss: 2.304223\n",
      "(Epoch 7 / 100) train acc: 0.097000; val_acc: 0.076000\n",
      "(Iteration 2701 / 38200) loss: 2.304215\n",
      "(Iteration 2801 / 38200) loss: 2.304215\n",
      "(Iteration 2901 / 38200) loss: 2.304174\n",
      "(Iteration 3001 / 38200) loss: 2.304184\n",
      "(Epoch 8 / 100) train acc: 0.090000; val_acc: 0.072000\n",
      "(Iteration 3101 / 38200) loss: 2.304194\n",
      "(Iteration 3201 / 38200) loss: 2.304175\n",
      "(Iteration 3301 / 38200) loss: 2.304173\n",
      "(Iteration 3401 / 38200) loss: 2.304170\n",
      "(Epoch 9 / 100) train acc: 0.120000; val_acc: 0.071000\n",
      "(Iteration 3501 / 38200) loss: 2.304169\n",
      "(Iteration 3601 / 38200) loss: 2.304161\n",
      "(Iteration 3701 / 38200) loss: 2.304150\n",
      "(Iteration 3801 / 38200) loss: 2.304136\n",
      "(Epoch 10 / 100) train acc: 0.100000; val_acc: 0.074000\n",
      "(Iteration 3901 / 38200) loss: 2.304140\n",
      "(Iteration 4001 / 38200) loss: 2.304125\n",
      "(Iteration 4101 / 38200) loss: 2.304134\n",
      "(Iteration 4201 / 38200) loss: 2.304136\n",
      "(Epoch 11 / 100) train acc: 0.094000; val_acc: 0.071000\n",
      "(Iteration 4301 / 38200) loss: 2.304134\n",
      "(Iteration 4401 / 38200) loss: 2.304112\n",
      "(Iteration 4501 / 38200) loss: 2.304113\n",
      "(Epoch 12 / 100) train acc: 0.086000; val_acc: 0.073000\n",
      "(Iteration 4601 / 38200) loss: 2.304111\n",
      "(Iteration 4701 / 38200) loss: 2.304111\n",
      "(Iteration 4801 / 38200) loss: 2.304109\n",
      "(Iteration 4901 / 38200) loss: 2.304095\n",
      "(Epoch 13 / 100) train acc: 0.103000; val_acc: 0.073000\n",
      "(Iteration 5001 / 38200) loss: 2.304088\n",
      "(Iteration 5101 / 38200) loss: 2.304052\n",
      "(Iteration 5201 / 38200) loss: 2.304078\n",
      "(Iteration 5301 / 38200) loss: 2.304072\n",
      "(Epoch 14 / 100) train acc: 0.109000; val_acc: 0.067000\n",
      "(Iteration 5401 / 38200) loss: 2.304070\n",
      "(Iteration 5501 / 38200) loss: 2.304067\n",
      "(Iteration 5601 / 38200) loss: 2.304080\n",
      "(Iteration 5701 / 38200) loss: 2.304073\n",
      "(Epoch 15 / 100) train acc: 0.102000; val_acc: 0.070000\n",
      "(Iteration 5801 / 38200) loss: 2.304082\n",
      "(Iteration 5901 / 38200) loss: 2.304054\n",
      "(Iteration 6001 / 38200) loss: 2.304053\n",
      "(Iteration 6101 / 38200) loss: 2.304040\n",
      "(Epoch 16 / 100) train acc: 0.118000; val_acc: 0.070000\n",
      "(Iteration 6201 / 38200) loss: 2.304043\n",
      "(Iteration 6301 / 38200) loss: 2.304034\n",
      "(Iteration 6401 / 38200) loss: 2.304052\n",
      "(Epoch 17 / 100) train acc: 0.085000; val_acc: 0.067000\n",
      "(Iteration 6501 / 38200) loss: 2.304048\n",
      "(Iteration 6601 / 38200) loss: 2.304047\n",
      "(Iteration 6701 / 38200) loss: 2.304032\n",
      "(Iteration 6801 / 38200) loss: 2.304000\n",
      "(Epoch 18 / 100) train acc: 0.097000; val_acc: 0.067000\n",
      "(Iteration 6901 / 38200) loss: 2.304013\n",
      "(Iteration 7001 / 38200) loss: 2.304018\n",
      "(Iteration 7101 / 38200) loss: 2.304017\n",
      "(Iteration 7201 / 38200) loss: 2.304019\n",
      "(Epoch 19 / 100) train acc: 0.105000; val_acc: 0.070000\n",
      "(Iteration 7301 / 38200) loss: 2.304011\n",
      "(Iteration 7401 / 38200) loss: 2.304041\n",
      "(Iteration 7501 / 38200) loss: 2.303999\n",
      "(Iteration 7601 / 38200) loss: 2.304022\n",
      "(Epoch 20 / 100) train acc: 0.088000; val_acc: 0.067000\n",
      "(Iteration 7701 / 38200) loss: 2.303998\n",
      "(Iteration 7801 / 38200) loss: 2.304011\n",
      "(Iteration 7901 / 38200) loss: 2.303993\n",
      "(Iteration 8001 / 38200) loss: 2.304025\n",
      "(Epoch 21 / 100) train acc: 0.092000; val_acc: 0.069000\n",
      "(Iteration 8101 / 38200) loss: 2.304018\n",
      "(Iteration 8201 / 38200) loss: 2.304018\n",
      "(Iteration 8301 / 38200) loss: 2.303997\n",
      "(Iteration 8401 / 38200) loss: 2.304003\n",
      "(Epoch 22 / 100) train acc: 0.099000; val_acc: 0.067000\n",
      "(Iteration 8501 / 38200) loss: 2.303995\n",
      "(Iteration 8601 / 38200) loss: 2.304002\n",
      "(Iteration 8701 / 38200) loss: 2.303965\n",
      "(Epoch 23 / 100) train acc: 0.094000; val_acc: 0.067000\n",
      "(Iteration 8801 / 38200) loss: 2.303981\n",
      "(Iteration 8901 / 38200) loss: 2.304016\n",
      "(Iteration 9001 / 38200) loss: 2.303985\n",
      "(Iteration 9101 / 38200) loss: 2.303989\n",
      "(Epoch 24 / 100) train acc: 0.109000; val_acc: 0.071000\n",
      "(Iteration 9201 / 38200) loss: 2.303989\n",
      "(Iteration 9301 / 38200) loss: 2.303989\n",
      "(Iteration 9401 / 38200) loss: 2.303988\n",
      "(Iteration 9501 / 38200) loss: 2.303975\n",
      "(Epoch 25 / 100) train acc: 0.123000; val_acc: 0.070000\n",
      "(Iteration 9601 / 38200) loss: 2.304006\n",
      "(Iteration 9701 / 38200) loss: 2.303988\n",
      "(Iteration 9801 / 38200) loss: 2.303969\n",
      "(Iteration 9901 / 38200) loss: 2.303989\n",
      "(Epoch 26 / 100) train acc: 0.102000; val_acc: 0.070000\n",
      "(Iteration 10001 / 38200) loss: 2.303981\n",
      "(Iteration 10101 / 38200) loss: 2.303978\n",
      "(Iteration 10201 / 38200) loss: 2.304001\n",
      "(Iteration 10301 / 38200) loss: 2.303961\n",
      "(Epoch 27 / 100) train acc: 0.113000; val_acc: 0.068000\n",
      "(Iteration 10401 / 38200) loss: 2.303967\n",
      "(Iteration 10501 / 38200) loss: 2.303998\n",
      "(Iteration 10601 / 38200) loss: 2.303963\n",
      "(Epoch 28 / 100) train acc: 0.103000; val_acc: 0.067000\n",
      "(Iteration 10701 / 38200) loss: 2.303974\n",
      "(Iteration 10801 / 38200) loss: 2.303974\n",
      "(Iteration 10901 / 38200) loss: 2.303995\n",
      "(Iteration 11001 / 38200) loss: 2.303984\n",
      "(Epoch 29 / 100) train acc: 0.085000; val_acc: 0.068000\n",
      "(Iteration 11101 / 38200) loss: 2.303975\n",
      "(Iteration 11201 / 38200) loss: 2.303974\n",
      "(Iteration 11301 / 38200) loss: 2.303961\n",
      "(Iteration 11401 / 38200) loss: 2.304002\n",
      "(Epoch 30 / 100) train acc: 0.099000; val_acc: 0.067000\n",
      "(Iteration 11501 / 38200) loss: 2.303977\n",
      "(Iteration 11601 / 38200) loss: 2.303983\n",
      "(Iteration 11701 / 38200) loss: 2.303956\n",
      "(Iteration 11801 / 38200) loss: 2.303963\n",
      "(Epoch 31 / 100) train acc: 0.107000; val_acc: 0.066000\n",
      "(Iteration 11901 / 38200) loss: 2.303938\n",
      "(Iteration 12001 / 38200) loss: 2.303954\n",
      "(Iteration 12101 / 38200) loss: 2.303960\n",
      "(Iteration 12201 / 38200) loss: 2.303958\n",
      "(Epoch 32 / 100) train acc: 0.109000; val_acc: 0.067000\n",
      "(Iteration 12301 / 38200) loss: 2.303962\n",
      "(Iteration 12401 / 38200) loss: 2.303968\n",
      "(Iteration 12501 / 38200) loss: 2.303947\n",
      "(Iteration 12601 / 38200) loss: 2.303991\n",
      "(Epoch 33 / 100) train acc: 0.099000; val_acc: 0.066000\n",
      "(Iteration 12701 / 38200) loss: 2.303967\n",
      "(Iteration 12801 / 38200) loss: 2.303993\n",
      "(Iteration 12901 / 38200) loss: 2.303965\n",
      "(Epoch 34 / 100) train acc: 0.100000; val_acc: 0.066000\n",
      "(Iteration 13001 / 38200) loss: 2.303972\n",
      "(Iteration 13101 / 38200) loss: 2.303949\n",
      "(Iteration 13201 / 38200) loss: 2.303953\n",
      "(Iteration 13301 / 38200) loss: 2.303959\n",
      "(Epoch 35 / 100) train acc: 0.122000; val_acc: 0.068000\n",
      "(Iteration 13401 / 38200) loss: 2.303963\n",
      "(Iteration 13501 / 38200) loss: 2.303944\n",
      "(Iteration 13601 / 38200) loss: 2.303952\n",
      "(Iteration 13701 / 38200) loss: 2.303940\n",
      "(Epoch 36 / 100) train acc: 0.101000; val_acc: 0.068000\n",
      "(Iteration 13801 / 38200) loss: 2.303967\n",
      "(Iteration 13901 / 38200) loss: 2.303957\n",
      "(Iteration 14001 / 38200) loss: 2.303955\n",
      "(Iteration 14101 / 38200) loss: 2.303957\n",
      "(Epoch 37 / 100) train acc: 0.102000; val_acc: 0.067000\n",
      "(Iteration 14201 / 38200) loss: 2.303967\n",
      "(Iteration 14301 / 38200) loss: 2.303948\n",
      "(Iteration 14401 / 38200) loss: 2.303966\n",
      "(Iteration 14501 / 38200) loss: 2.303972\n",
      "(Epoch 38 / 100) train acc: 0.114000; val_acc: 0.067000\n",
      "(Iteration 14601 / 38200) loss: 2.303954\n",
      "(Iteration 14701 / 38200) loss: 2.303941\n",
      "(Iteration 14801 / 38200) loss: 2.303962\n",
      "(Epoch 39 / 100) train acc: 0.092000; val_acc: 0.068000\n",
      "(Iteration 14901 / 38200) loss: 2.303965\n",
      "(Iteration 15001 / 38200) loss: 2.303960\n",
      "(Iteration 15101 / 38200) loss: 2.303975\n",
      "(Iteration 15201 / 38200) loss: 2.303974\n",
      "(Epoch 40 / 100) train acc: 0.095000; val_acc: 0.067000\n",
      "(Iteration 15301 / 38200) loss: 2.303973\n",
      "(Iteration 15401 / 38200) loss: 2.303967\n",
      "(Iteration 15501 / 38200) loss: 2.303950\n",
      "(Iteration 15601 / 38200) loss: 2.303978\n",
      "(Epoch 41 / 100) train acc: 0.100000; val_acc: 0.067000\n",
      "(Iteration 15701 / 38200) loss: 2.303950\n",
      "(Iteration 15801 / 38200) loss: 2.303942\n",
      "(Iteration 15901 / 38200) loss: 2.303965\n",
      "(Iteration 16001 / 38200) loss: 2.303923\n",
      "(Epoch 42 / 100) train acc: 0.084000; val_acc: 0.069000\n",
      "(Iteration 16101 / 38200) loss: 2.303956\n",
      "(Iteration 16201 / 38200) loss: 2.303990\n",
      "(Iteration 16301 / 38200) loss: 2.303952\n",
      "(Iteration 16401 / 38200) loss: 2.303942\n",
      "(Epoch 43 / 100) train acc: 0.082000; val_acc: 0.069000\n",
      "(Iteration 16501 / 38200) loss: 2.303955\n",
      "(Iteration 16601 / 38200) loss: 2.303951\n",
      "(Iteration 16701 / 38200) loss: 2.303962\n",
      "(Iteration 16801 / 38200) loss: 2.303959\n",
      "(Epoch 44 / 100) train acc: 0.099000; val_acc: 0.069000\n",
      "(Iteration 16901 / 38200) loss: 2.303949\n",
      "(Iteration 17001 / 38200) loss: 2.303928\n",
      "(Iteration 17101 / 38200) loss: 2.303963\n",
      "(Epoch 45 / 100) train acc: 0.114000; val_acc: 0.069000\n",
      "(Iteration 17201 / 38200) loss: 2.303942\n",
      "(Iteration 17301 / 38200) loss: 2.303958\n",
      "(Iteration 17401 / 38200) loss: 2.303959\n",
      "(Iteration 17501 / 38200) loss: 2.303957\n",
      "(Epoch 46 / 100) train acc: 0.103000; val_acc: 0.069000\n",
      "(Iteration 17601 / 38200) loss: 2.303958\n",
      "(Iteration 17701 / 38200) loss: 2.303943\n",
      "(Iteration 17801 / 38200) loss: 2.303950\n",
      "(Iteration 17901 / 38200) loss: 2.303947\n",
      "(Epoch 47 / 100) train acc: 0.110000; val_acc: 0.069000\n",
      "(Iteration 18001 / 38200) loss: 2.303977\n",
      "(Iteration 18101 / 38200) loss: 2.303950\n",
      "(Iteration 18201 / 38200) loss: 2.303953\n",
      "(Iteration 18301 / 38200) loss: 2.303963\n",
      "(Epoch 48 / 100) train acc: 0.097000; val_acc: 0.069000\n",
      "(Iteration 18401 / 38200) loss: 2.303951\n",
      "(Iteration 18501 / 38200) loss: 2.303922\n",
      "(Iteration 18601 / 38200) loss: 2.303937\n",
      "(Iteration 18701 / 38200) loss: 2.303951\n",
      "(Epoch 49 / 100) train acc: 0.107000; val_acc: 0.069000\n",
      "(Iteration 18801 / 38200) loss: 2.303942\n",
      "(Iteration 18901 / 38200) loss: 2.303948\n",
      "(Iteration 19001 / 38200) loss: 2.303958\n",
      "(Epoch 50 / 100) train acc: 0.109000; val_acc: 0.069000\n",
      "(Iteration 19101 / 38200) loss: 2.303960\n",
      "(Iteration 19201 / 38200) loss: 2.303977\n",
      "(Iteration 19301 / 38200) loss: 2.303968\n",
      "(Iteration 19401 / 38200) loss: 2.303944\n",
      "(Epoch 51 / 100) train acc: 0.095000; val_acc: 0.069000\n",
      "(Iteration 19501 / 38200) loss: 2.303950\n",
      "(Iteration 19601 / 38200) loss: 2.303950\n",
      "(Iteration 19701 / 38200) loss: 2.303945\n",
      "(Iteration 19801 / 38200) loss: 2.303940\n",
      "(Epoch 52 / 100) train acc: 0.113000; val_acc: 0.069000\n",
      "(Iteration 19901 / 38200) loss: 2.303942\n",
      "(Iteration 20001 / 38200) loss: 2.303948\n",
      "(Iteration 20101 / 38200) loss: 2.303964\n",
      "(Iteration 20201 / 38200) loss: 2.303973\n",
      "(Epoch 53 / 100) train acc: 0.090000; val_acc: 0.069000\n",
      "(Iteration 20301 / 38200) loss: 2.303951\n",
      "(Iteration 20401 / 38200) loss: 2.303932\n",
      "(Iteration 20501 / 38200) loss: 2.303948\n",
      "(Iteration 20601 / 38200) loss: 2.303948\n",
      "(Epoch 54 / 100) train acc: 0.086000; val_acc: 0.069000\n",
      "(Iteration 20701 / 38200) loss: 2.303945\n",
      "(Iteration 20801 / 38200) loss: 2.303958\n",
      "(Iteration 20901 / 38200) loss: 2.303944\n",
      "(Iteration 21001 / 38200) loss: 2.303952\n",
      "(Epoch 55 / 100) train acc: 0.099000; val_acc: 0.069000\n",
      "(Iteration 21101 / 38200) loss: 2.303932\n",
      "(Iteration 21201 / 38200) loss: 2.303921\n",
      "(Iteration 21301 / 38200) loss: 2.303943\n",
      "(Epoch 56 / 100) train acc: 0.085000; val_acc: 0.069000\n",
      "(Iteration 21401 / 38200) loss: 2.303937\n",
      "(Iteration 21501 / 38200) loss: 2.303917\n",
      "(Iteration 21601 / 38200) loss: 2.303927\n",
      "(Iteration 21701 / 38200) loss: 2.303947\n",
      "(Epoch 57 / 100) train acc: 0.114000; val_acc: 0.069000\n",
      "(Iteration 21801 / 38200) loss: 2.303954\n",
      "(Iteration 21901 / 38200) loss: 2.303932\n",
      "(Iteration 22001 / 38200) loss: 2.303949\n",
      "(Iteration 22101 / 38200) loss: 2.303929\n",
      "(Epoch 58 / 100) train acc: 0.107000; val_acc: 0.069000\n",
      "(Iteration 22201 / 38200) loss: 2.303958\n",
      "(Iteration 22301 / 38200) loss: 2.303944\n",
      "(Iteration 22401 / 38200) loss: 2.303947\n",
      "(Iteration 22501 / 38200) loss: 2.303951\n",
      "(Epoch 59 / 100) train acc: 0.079000; val_acc: 0.070000\n",
      "(Iteration 22601 / 38200) loss: 2.303921\n",
      "(Iteration 22701 / 38200) loss: 2.303960\n",
      "(Iteration 22801 / 38200) loss: 2.303952\n",
      "(Iteration 22901 / 38200) loss: 2.303970\n",
      "(Epoch 60 / 100) train acc: 0.100000; val_acc: 0.070000\n",
      "(Iteration 23001 / 38200) loss: 2.303956\n",
      "(Iteration 23101 / 38200) loss: 2.303954\n",
      "(Iteration 23201 / 38200) loss: 2.303937\n",
      "(Iteration 23301 / 38200) loss: 2.303949\n",
      "(Epoch 61 / 100) train acc: 0.091000; val_acc: 0.070000\n",
      "(Iteration 23401 / 38200) loss: 2.303966\n",
      "(Iteration 23501 / 38200) loss: 2.303949\n",
      "(Iteration 23601 / 38200) loss: 2.303948\n",
      "(Epoch 62 / 100) train acc: 0.093000; val_acc: 0.069000\n",
      "(Iteration 23701 / 38200) loss: 2.303924\n",
      "(Iteration 23801 / 38200) loss: 2.303957\n",
      "(Iteration 23901 / 38200) loss: 2.303939\n",
      "(Iteration 24001 / 38200) loss: 2.303949\n",
      "(Epoch 63 / 100) train acc: 0.087000; val_acc: 0.069000\n",
      "(Iteration 24101 / 38200) loss: 2.303953\n",
      "(Iteration 24201 / 38200) loss: 2.303959\n",
      "(Iteration 24301 / 38200) loss: 2.303972\n",
      "(Iteration 24401 / 38200) loss: 2.303949\n",
      "(Epoch 64 / 100) train acc: 0.095000; val_acc: 0.070000\n",
      "(Iteration 24501 / 38200) loss: 2.303953\n",
      "(Iteration 24601 / 38200) loss: 2.303931\n",
      "(Iteration 24701 / 38200) loss: 2.303951\n",
      "(Iteration 24801 / 38200) loss: 2.303961\n",
      "(Epoch 65 / 100) train acc: 0.085000; val_acc: 0.070000\n",
      "(Iteration 24901 / 38200) loss: 2.303918\n",
      "(Iteration 25001 / 38200) loss: 2.303936\n",
      "(Iteration 25101 / 38200) loss: 2.303929\n",
      "(Iteration 25201 / 38200) loss: 2.303930\n",
      "(Epoch 66 / 100) train acc: 0.084000; val_acc: 0.070000\n",
      "(Iteration 25301 / 38200) loss: 2.303982\n",
      "(Iteration 25401 / 38200) loss: 2.303947\n",
      "(Iteration 25501 / 38200) loss: 2.303942\n",
      "(Epoch 67 / 100) train acc: 0.097000; val_acc: 0.070000\n",
      "(Iteration 25601 / 38200) loss: 2.303954\n",
      "(Iteration 25701 / 38200) loss: 2.303935\n",
      "(Iteration 25801 / 38200) loss: 2.303967\n",
      "(Iteration 25901 / 38200) loss: 2.303945\n",
      "(Epoch 68 / 100) train acc: 0.102000; val_acc: 0.070000\n",
      "(Iteration 26001 / 38200) loss: 2.303925\n",
      "(Iteration 26101 / 38200) loss: 2.303944\n",
      "(Iteration 26201 / 38200) loss: 2.303961\n",
      "(Iteration 26301 / 38200) loss: 2.303972\n",
      "(Epoch 69 / 100) train acc: 0.109000; val_acc: 0.070000\n",
      "(Iteration 26401 / 38200) loss: 2.303950\n",
      "(Iteration 26501 / 38200) loss: 2.303939\n",
      "(Iteration 26601 / 38200) loss: 2.303956\n",
      "(Iteration 26701 / 38200) loss: 2.303941\n",
      "(Epoch 70 / 100) train acc: 0.094000; val_acc: 0.070000\n",
      "(Iteration 26801 / 38200) loss: 2.303970\n",
      "(Iteration 26901 / 38200) loss: 2.303932\n",
      "(Iteration 27001 / 38200) loss: 2.303946\n",
      "(Iteration 27101 / 38200) loss: 2.303942\n",
      "(Epoch 71 / 100) train acc: 0.097000; val_acc: 0.070000\n",
      "(Iteration 27201 / 38200) loss: 2.303945\n",
      "(Iteration 27301 / 38200) loss: 2.303946\n",
      "(Iteration 27401 / 38200) loss: 2.303933\n",
      "(Iteration 27501 / 38200) loss: 2.303946\n",
      "(Epoch 72 / 100) train acc: 0.084000; val_acc: 0.070000\n",
      "(Iteration 27601 / 38200) loss: 2.303957\n",
      "(Iteration 27701 / 38200) loss: 2.303921\n",
      "(Iteration 27801 / 38200) loss: 2.303953\n",
      "(Epoch 73 / 100) train acc: 0.108000; val_acc: 0.069000\n",
      "(Iteration 27901 / 38200) loss: 2.303940\n",
      "(Iteration 28001 / 38200) loss: 2.303949\n",
      "(Iteration 28101 / 38200) loss: 2.303944\n",
      "(Iteration 28201 / 38200) loss: 2.303923\n",
      "(Epoch 74 / 100) train acc: 0.089000; val_acc: 0.070000\n",
      "(Iteration 28301 / 38200) loss: 2.303952\n",
      "(Iteration 28401 / 38200) loss: 2.303939\n",
      "(Iteration 28501 / 38200) loss: 2.303942\n",
      "(Iteration 28601 / 38200) loss: 2.303955\n",
      "(Epoch 75 / 100) train acc: 0.101000; val_acc: 0.070000\n",
      "(Iteration 28701 / 38200) loss: 2.303947\n",
      "(Iteration 28801 / 38200) loss: 2.303954\n",
      "(Iteration 28901 / 38200) loss: 2.303956\n",
      "(Iteration 29001 / 38200) loss: 2.303911\n",
      "(Epoch 76 / 100) train acc: 0.098000; val_acc: 0.070000\n",
      "(Iteration 29101 / 38200) loss: 2.303953\n",
      "(Iteration 29201 / 38200) loss: 2.303956\n",
      "(Iteration 29301 / 38200) loss: 2.303949\n",
      "(Iteration 29401 / 38200) loss: 2.303930\n",
      "(Epoch 77 / 100) train acc: 0.112000; val_acc: 0.070000\n",
      "(Iteration 29501 / 38200) loss: 2.303925\n",
      "(Iteration 29601 / 38200) loss: 2.303954\n",
      "(Iteration 29701 / 38200) loss: 2.303970\n",
      "(Epoch 78 / 100) train acc: 0.097000; val_acc: 0.070000\n",
      "(Iteration 29801 / 38200) loss: 2.303950\n",
      "(Iteration 29901 / 38200) loss: 2.303923\n",
      "(Iteration 30001 / 38200) loss: 2.303948\n",
      "(Iteration 30101 / 38200) loss: 2.303962\n",
      "(Epoch 79 / 100) train acc: 0.098000; val_acc: 0.070000\n",
      "(Iteration 30201 / 38200) loss: 2.303947\n",
      "(Iteration 30301 / 38200) loss: 2.303957\n",
      "(Iteration 30401 / 38200) loss: 2.303963\n",
      "(Iteration 30501 / 38200) loss: 2.303963\n",
      "(Epoch 80 / 100) train acc: 0.089000; val_acc: 0.070000\n",
      "(Iteration 30601 / 38200) loss: 2.303939\n",
      "(Iteration 30701 / 38200) loss: 2.303964\n",
      "(Iteration 30801 / 38200) loss: 2.303964\n",
      "(Iteration 30901 / 38200) loss: 2.303953\n",
      "(Epoch 81 / 100) train acc: 0.110000; val_acc: 0.070000\n",
      "(Iteration 31001 / 38200) loss: 2.303936\n",
      "(Iteration 31101 / 38200) loss: 2.303957\n",
      "(Iteration 31201 / 38200) loss: 2.303949\n",
      "(Iteration 31301 / 38200) loss: 2.303950\n",
      "(Epoch 82 / 100) train acc: 0.103000; val_acc: 0.069000\n",
      "(Iteration 31401 / 38200) loss: 2.303952\n",
      "(Iteration 31501 / 38200) loss: 2.303964\n",
      "(Iteration 31601 / 38200) loss: 2.303950\n",
      "(Iteration 31701 / 38200) loss: 2.303926\n",
      "(Epoch 83 / 100) train acc: 0.097000; val_acc: 0.069000\n",
      "(Iteration 31801 / 38200) loss: 2.303958\n",
      "(Iteration 31901 / 38200) loss: 2.303934\n",
      "(Iteration 32001 / 38200) loss: 2.303948\n",
      "(Epoch 84 / 100) train acc: 0.095000; val_acc: 0.069000\n",
      "(Iteration 32101 / 38200) loss: 2.303968\n",
      "(Iteration 32201 / 38200) loss: 2.303943\n",
      "(Iteration 32301 / 38200) loss: 2.303955\n",
      "(Iteration 32401 / 38200) loss: 2.303943\n",
      "(Epoch 85 / 100) train acc: 0.100000; val_acc: 0.069000\n",
      "(Iteration 32501 / 38200) loss: 2.303941\n",
      "(Iteration 32601 / 38200) loss: 2.303925\n",
      "(Iteration 32701 / 38200) loss: 2.303964\n",
      "(Iteration 32801 / 38200) loss: 2.303955\n",
      "(Epoch 86 / 100) train acc: 0.098000; val_acc: 0.069000\n",
      "(Iteration 32901 / 38200) loss: 2.303933\n",
      "(Iteration 33001 / 38200) loss: 2.303940\n",
      "(Iteration 33101 / 38200) loss: 2.303951\n",
      "(Iteration 33201 / 38200) loss: 2.303967\n",
      "(Epoch 87 / 100) train acc: 0.116000; val_acc: 0.069000\n",
      "(Iteration 33301 / 38200) loss: 2.303952\n",
      "(Iteration 33401 / 38200) loss: 2.303964\n",
      "(Iteration 33501 / 38200) loss: 2.303930\n",
      "(Iteration 33601 / 38200) loss: 2.303946\n",
      "(Epoch 88 / 100) train acc: 0.095000; val_acc: 0.069000\n",
      "(Iteration 33701 / 38200) loss: 2.303956\n",
      "(Iteration 33801 / 38200) loss: 2.303941\n",
      "(Iteration 33901 / 38200) loss: 2.303962\n",
      "(Epoch 89 / 100) train acc: 0.079000; val_acc: 0.069000\n",
      "(Iteration 34001 / 38200) loss: 2.303920\n",
      "(Iteration 34101 / 38200) loss: 2.303925\n",
      "(Iteration 34201 / 38200) loss: 2.303952\n",
      "(Iteration 34301 / 38200) loss: 2.303955\n",
      "(Epoch 90 / 100) train acc: 0.103000; val_acc: 0.069000\n",
      "(Iteration 34401 / 38200) loss: 2.303948\n",
      "(Iteration 34501 / 38200) loss: 2.303958\n",
      "(Iteration 34601 / 38200) loss: 2.303924\n",
      "(Iteration 34701 / 38200) loss: 2.303955\n",
      "(Epoch 91 / 100) train acc: 0.118000; val_acc: 0.069000\n",
      "(Iteration 34801 / 38200) loss: 2.303960\n",
      "(Iteration 34901 / 38200) loss: 2.303949\n",
      "(Iteration 35001 / 38200) loss: 2.303956\n",
      "(Iteration 35101 / 38200) loss: 2.303949\n",
      "(Epoch 92 / 100) train acc: 0.094000; val_acc: 0.069000\n",
      "(Iteration 35201 / 38200) loss: 2.303947\n",
      "(Iteration 35301 / 38200) loss: 2.303937\n",
      "(Iteration 35401 / 38200) loss: 2.303972\n",
      "(Iteration 35501 / 38200) loss: 2.303951\n",
      "(Epoch 93 / 100) train acc: 0.094000; val_acc: 0.069000\n",
      "(Iteration 35601 / 38200) loss: 2.303960\n",
      "(Iteration 35701 / 38200) loss: 2.303970\n",
      "(Iteration 35801 / 38200) loss: 2.303961\n",
      "(Iteration 35901 / 38200) loss: 2.303945\n",
      "(Epoch 94 / 100) train acc: 0.104000; val_acc: 0.069000\n",
      "(Iteration 36001 / 38200) loss: 2.303932\n",
      "(Iteration 36101 / 38200) loss: 2.303945\n",
      "(Iteration 36201 / 38200) loss: 2.303945\n",
      "(Epoch 95 / 100) train acc: 0.102000; val_acc: 0.069000\n",
      "(Iteration 36301 / 38200) loss: 2.303923\n",
      "(Iteration 36401 / 38200) loss: 2.303951\n",
      "(Iteration 36501 / 38200) loss: 2.303951\n",
      "(Iteration 36601 / 38200) loss: 2.303933\n",
      "(Epoch 96 / 100) train acc: 0.091000; val_acc: 0.069000\n",
      "(Iteration 36701 / 38200) loss: 2.303954\n",
      "(Iteration 36801 / 38200) loss: 2.303949\n",
      "(Iteration 36901 / 38200) loss: 2.303952\n",
      "(Iteration 37001 / 38200) loss: 2.303983\n",
      "(Epoch 97 / 100) train acc: 0.106000; val_acc: 0.069000\n",
      "(Iteration 37101 / 38200) loss: 2.303926\n",
      "(Iteration 37201 / 38200) loss: 2.303946\n",
      "(Iteration 37301 / 38200) loss: 2.303938\n",
      "(Iteration 37401 / 38200) loss: 2.303941\n",
      "(Epoch 98 / 100) train acc: 0.088000; val_acc: 0.069000\n",
      "(Iteration 37501 / 38200) loss: 2.303970\n",
      "(Iteration 37601 / 38200) loss: 2.303935\n",
      "(Iteration 37701 / 38200) loss: 2.303942\n",
      "(Iteration 37801 / 38200) loss: 2.303958\n",
      "(Epoch 99 / 100) train acc: 0.088000; val_acc: 0.069000\n",
      "(Iteration 37901 / 38200) loss: 2.303946\n",
      "(Iteration 38001 / 38200) loss: 2.303953\n",
      "(Iteration 38101 / 38200) loss: 2.303953\n",
      "(Epoch 100 / 100) train acc: 0.108000; val_acc: 0.069000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.304643\n",
      "(Epoch 0 / 100) train acc: 0.105000; val_acc: 0.116000\n",
      "(Iteration 101 / 76500) loss: 2.304643\n",
      "(Iteration 201 / 76500) loss: 2.304622\n",
      "(Iteration 301 / 76500) loss: 2.304607\n",
      "(Iteration 401 / 76500) loss: 2.304561\n",
      "(Iteration 501 / 76500) loss: 2.304569\n",
      "(Iteration 601 / 76500) loss: 2.304540\n",
      "(Iteration 701 / 76500) loss: 2.304502\n",
      "(Epoch 1 / 100) train acc: 0.098000; val_acc: 0.104000\n",
      "(Iteration 801 / 76500) loss: 2.304498\n",
      "(Iteration 901 / 76500) loss: 2.304476\n",
      "(Iteration 1001 / 76500) loss: 2.304479\n",
      "(Iteration 1101 / 76500) loss: 2.304443\n",
      "(Iteration 1201 / 76500) loss: 2.304413\n",
      "(Iteration 1301 / 76500) loss: 2.304403\n",
      "(Iteration 1401 / 76500) loss: 2.304400\n",
      "(Iteration 1501 / 76500) loss: 2.304363\n",
      "(Epoch 2 / 100) train acc: 0.098000; val_acc: 0.098000\n",
      "(Iteration 1601 / 76500) loss: 2.304353\n",
      "(Iteration 1701 / 76500) loss: 2.304344\n",
      "(Iteration 1801 / 76500) loss: 2.304344\n",
      "(Iteration 1901 / 76500) loss: 2.304293\n",
      "(Iteration 2001 / 76500) loss: 2.304284\n",
      "(Iteration 2101 / 76500) loss: 2.304276\n",
      "(Iteration 2201 / 76500) loss: 2.304269\n",
      "(Epoch 3 / 100) train acc: 0.074000; val_acc: 0.094000\n",
      "(Iteration 2301 / 76500) loss: 2.304228\n",
      "(Iteration 2401 / 76500) loss: 2.304211\n",
      "(Iteration 2501 / 76500) loss: 2.304195\n",
      "(Iteration 2601 / 76500) loss: 2.304218\n",
      "(Iteration 2701 / 76500) loss: 2.304163\n",
      "(Iteration 2801 / 76500) loss: 2.304178\n",
      "(Iteration 2901 / 76500) loss: 2.304137\n",
      "(Iteration 3001 / 76500) loss: 2.304125\n",
      "(Epoch 4 / 100) train acc: 0.108000; val_acc: 0.090000\n",
      "(Iteration 3101 / 76500) loss: 2.304123\n",
      "(Iteration 3201 / 76500) loss: 2.304123\n",
      "(Iteration 3301 / 76500) loss: 2.304100\n",
      "(Iteration 3401 / 76500) loss: 2.304098\n",
      "(Iteration 3501 / 76500) loss: 2.304102\n",
      "(Iteration 3601 / 76500) loss: 2.304082\n",
      "(Iteration 3701 / 76500) loss: 2.304098\n",
      "(Iteration 3801 / 76500) loss: 2.304053\n",
      "(Epoch 5 / 100) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 3901 / 76500) loss: 2.304033\n",
      "(Iteration 4001 / 76500) loss: 2.304027\n",
      "(Iteration 4101 / 76500) loss: 2.304024\n",
      "(Iteration 4201 / 76500) loss: 2.303984\n",
      "(Iteration 4301 / 76500) loss: 2.304006\n",
      "(Iteration 4401 / 76500) loss: 2.303994\n",
      "(Iteration 4501 / 76500) loss: 2.303987\n",
      "(Epoch 6 / 100) train acc: 0.090000; val_acc: 0.087000\n",
      "(Iteration 4601 / 76500) loss: 2.303994\n",
      "(Iteration 4701 / 76500) loss: 2.303969\n",
      "(Iteration 4801 / 76500) loss: 2.303973\n",
      "(Iteration 4901 / 76500) loss: 2.303927\n",
      "(Iteration 5001 / 76500) loss: 2.303871\n",
      "(Iteration 5101 / 76500) loss: 2.303881\n",
      "(Iteration 5201 / 76500) loss: 2.303860\n",
      "(Iteration 5301 / 76500) loss: 2.303889\n",
      "(Epoch 7 / 100) train acc: 0.125000; val_acc: 0.082000\n",
      "(Iteration 5401 / 76500) loss: 2.303919\n",
      "(Iteration 5501 / 76500) loss: 2.303871\n",
      "(Iteration 5601 / 76500) loss: 2.303841\n",
      "(Iteration 5701 / 76500) loss: 2.303823\n",
      "(Iteration 5801 / 76500) loss: 2.303851\n",
      "(Iteration 5901 / 76500) loss: 2.303863\n",
      "(Iteration 6001 / 76500) loss: 2.303780\n",
      "(Iteration 6101 / 76500) loss: 2.303787\n",
      "(Epoch 8 / 100) train acc: 0.116000; val_acc: 0.082000\n",
      "(Iteration 6201 / 76500) loss: 2.303747\n",
      "(Iteration 6301 / 76500) loss: 2.303815\n",
      "(Iteration 6401 / 76500) loss: 2.303870\n",
      "(Iteration 6501 / 76500) loss: 2.303794\n",
      "(Iteration 6601 / 76500) loss: 2.303750\n",
      "(Iteration 6701 / 76500) loss: 2.303754\n",
      "(Iteration 6801 / 76500) loss: 2.303667\n",
      "(Epoch 9 / 100) train acc: 0.106000; val_acc: 0.081000\n",
      "(Iteration 6901 / 76500) loss: 2.303777\n",
      "(Iteration 7001 / 76500) loss: 2.303747\n",
      "(Iteration 7101 / 76500) loss: 2.303683\n",
      "(Iteration 7201 / 76500) loss: 2.303692\n",
      "(Iteration 7301 / 76500) loss: 2.303729\n",
      "(Iteration 7401 / 76500) loss: 2.303706\n",
      "(Iteration 7501 / 76500) loss: 2.303764\n",
      "(Iteration 7601 / 76500) loss: 2.303696\n",
      "(Epoch 10 / 100) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 7701 / 76500) loss: 2.303739\n",
      "(Iteration 7801 / 76500) loss: 2.303733\n",
      "(Iteration 7901 / 76500) loss: 2.303666\n",
      "(Iteration 8001 / 76500) loss: 2.303661\n",
      "(Iteration 8101 / 76500) loss: 2.303654\n",
      "(Iteration 8201 / 76500) loss: 2.303707\n",
      "(Iteration 8301 / 76500) loss: 2.303526\n",
      "(Iteration 8401 / 76500) loss: 2.303642\n",
      "(Epoch 11 / 100) train acc: 0.112000; val_acc: 0.079000\n",
      "(Iteration 8501 / 76500) loss: 2.303619\n",
      "(Iteration 8601 / 76500) loss: 2.303683\n",
      "(Iteration 8701 / 76500) loss: 2.303618\n",
      "(Iteration 8801 / 76500) loss: 2.303684\n",
      "(Iteration 8901 / 76500) loss: 2.303596\n",
      "(Iteration 9001 / 76500) loss: 2.303618\n",
      "(Iteration 9101 / 76500) loss: 2.303599\n",
      "(Epoch 12 / 100) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 9201 / 76500) loss: 2.303575\n",
      "(Iteration 9301 / 76500) loss: 2.303607\n",
      "(Iteration 9401 / 76500) loss: 2.303576\n",
      "(Iteration 9501 / 76500) loss: 2.303532\n",
      "(Iteration 9601 / 76500) loss: 2.303510\n",
      "(Iteration 9701 / 76500) loss: 2.303577\n",
      "(Iteration 9801 / 76500) loss: 2.303620\n",
      "(Iteration 9901 / 76500) loss: 2.303620\n",
      "(Epoch 13 / 100) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 10001 / 76500) loss: 2.303668\n",
      "(Iteration 10101 / 76500) loss: 2.303601\n",
      "(Iteration 10201 / 76500) loss: 2.303608\n",
      "(Iteration 10301 / 76500) loss: 2.303507\n",
      "(Iteration 10401 / 76500) loss: 2.303550\n",
      "(Iteration 10501 / 76500) loss: 2.303506\n",
      "(Iteration 10601 / 76500) loss: 2.303522\n",
      "(Iteration 10701 / 76500) loss: 2.303501\n",
      "(Epoch 14 / 100) train acc: 0.078000; val_acc: 0.079000\n",
      "(Iteration 10801 / 76500) loss: 2.303530\n",
      "(Iteration 10901 / 76500) loss: 2.303541\n",
      "(Iteration 11001 / 76500) loss: 2.303485\n",
      "(Iteration 11101 / 76500) loss: 2.303502\n",
      "(Iteration 11201 / 76500) loss: 2.303451\n",
      "(Iteration 11301 / 76500) loss: 2.303516\n",
      "(Iteration 11401 / 76500) loss: 2.303478\n",
      "(Epoch 15 / 100) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 11501 / 76500) loss: 2.303477\n",
      "(Iteration 11601 / 76500) loss: 2.303433\n",
      "(Iteration 11701 / 76500) loss: 2.303541\n",
      "(Iteration 11801 / 76500) loss: 2.303489\n",
      "(Iteration 11901 / 76500) loss: 2.303451\n",
      "(Iteration 12001 / 76500) loss: 2.303384\n",
      "(Iteration 12101 / 76500) loss: 2.303446\n",
      "(Iteration 12201 / 76500) loss: 2.303430\n",
      "(Epoch 16 / 100) train acc: 0.093000; val_acc: 0.079000\n",
      "(Iteration 12301 / 76500) loss: 2.303471\n",
      "(Iteration 12401 / 76500) loss: 2.303415\n",
      "(Iteration 12501 / 76500) loss: 2.303443\n",
      "(Iteration 12601 / 76500) loss: 2.303396\n",
      "(Iteration 12701 / 76500) loss: 2.303455\n",
      "(Iteration 12801 / 76500) loss: 2.303398\n",
      "(Iteration 12901 / 76500) loss: 2.303420\n",
      "(Iteration 13001 / 76500) loss: 2.303409\n",
      "(Epoch 17 / 100) train acc: 0.118000; val_acc: 0.079000\n",
      "(Iteration 13101 / 76500) loss: 2.303266\n",
      "(Iteration 13201 / 76500) loss: 2.303412\n",
      "(Iteration 13301 / 76500) loss: 2.303439\n",
      "(Iteration 13401 / 76500) loss: 2.303406\n",
      "(Iteration 13501 / 76500) loss: 2.303444\n",
      "(Iteration 13601 / 76500) loss: 2.303439\n",
      "(Iteration 13701 / 76500) loss: 2.303498\n",
      "(Epoch 18 / 100) train acc: 0.094000; val_acc: 0.075000\n",
      "(Iteration 13801 / 76500) loss: 2.303479\n",
      "(Iteration 13901 / 76500) loss: 2.303481\n",
      "(Iteration 14001 / 76500) loss: 2.303406\n",
      "(Iteration 14101 / 76500) loss: 2.303359\n",
      "(Iteration 14201 / 76500) loss: 2.303319\n",
      "(Iteration 14301 / 76500) loss: 2.303504\n",
      "(Iteration 14401 / 76500) loss: 2.303358\n",
      "(Iteration 14501 / 76500) loss: 2.303420\n",
      "(Epoch 19 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 14601 / 76500) loss: 2.303316\n",
      "(Iteration 14701 / 76500) loss: 2.303337\n",
      "(Iteration 14801 / 76500) loss: 2.303462\n",
      "(Iteration 14901 / 76500) loss: 2.303277\n",
      "(Iteration 15001 / 76500) loss: 2.303309\n",
      "(Iteration 15101 / 76500) loss: 2.303396\n",
      "(Iteration 15201 / 76500) loss: 2.303386\n",
      "(Epoch 20 / 100) train acc: 0.106000; val_acc: 0.073000\n",
      "(Iteration 15301 / 76500) loss: 2.303348\n",
      "(Iteration 15401 / 76500) loss: 2.303239\n",
      "(Iteration 15501 / 76500) loss: 2.303368\n",
      "(Iteration 15601 / 76500) loss: 2.303353\n",
      "(Iteration 15701 / 76500) loss: 2.303305\n",
      "(Iteration 15801 / 76500) loss: 2.303377\n",
      "(Iteration 15901 / 76500) loss: 2.303328\n",
      "(Iteration 16001 / 76500) loss: 2.303268\n",
      "(Epoch 21 / 100) train acc: 0.098000; val_acc: 0.082000\n",
      "(Iteration 16101 / 76500) loss: 2.303273\n",
      "(Iteration 16201 / 76500) loss: 2.303326\n",
      "(Iteration 16301 / 76500) loss: 2.303439\n",
      "(Iteration 16401 / 76500) loss: 2.303362\n",
      "(Iteration 16501 / 76500) loss: 2.303364\n",
      "(Iteration 16601 / 76500) loss: 2.303238\n",
      "(Iteration 16701 / 76500) loss: 2.303317\n",
      "(Iteration 16801 / 76500) loss: 2.303320\n",
      "(Epoch 22 / 100) train acc: 0.099000; val_acc: 0.072000\n",
      "(Iteration 16901 / 76500) loss: 2.303330\n",
      "(Iteration 17001 / 76500) loss: 2.303329\n",
      "(Iteration 17101 / 76500) loss: 2.303427\n",
      "(Iteration 17201 / 76500) loss: 2.303343\n",
      "(Iteration 17301 / 76500) loss: 2.303256\n",
      "(Iteration 17401 / 76500) loss: 2.303202\n",
      "(Iteration 17501 / 76500) loss: 2.303218\n",
      "(Epoch 23 / 100) train acc: 0.119000; val_acc: 0.073000\n",
      "(Iteration 17601 / 76500) loss: 2.303281\n",
      "(Iteration 17701 / 76500) loss: 2.303266\n",
      "(Iteration 17801 / 76500) loss: 2.303295\n",
      "(Iteration 17901 / 76500) loss: 2.303281\n",
      "(Iteration 18001 / 76500) loss: 2.303371\n",
      "(Iteration 18101 / 76500) loss: 2.303336\n",
      "(Iteration 18201 / 76500) loss: 2.303229\n",
      "(Iteration 18301 / 76500) loss: 2.303328\n",
      "(Epoch 24 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 18401 / 76500) loss: 2.303251\n",
      "(Iteration 18501 / 76500) loss: 2.303237\n",
      "(Iteration 18601 / 76500) loss: 2.303266\n",
      "(Iteration 18701 / 76500) loss: 2.303269\n",
      "(Iteration 18801 / 76500) loss: 2.303196\n",
      "(Iteration 18901 / 76500) loss: 2.303325\n",
      "(Iteration 19001 / 76500) loss: 2.303280\n",
      "(Iteration 19101 / 76500) loss: 2.303240\n",
      "(Epoch 25 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 19201 / 76500) loss: 2.303331\n",
      "(Iteration 19301 / 76500) loss: 2.303230\n",
      "(Iteration 19401 / 76500) loss: 2.303297\n",
      "(Iteration 19501 / 76500) loss: 2.303236\n",
      "(Iteration 19601 / 76500) loss: 2.303229\n",
      "(Iteration 19701 / 76500) loss: 2.303287\n",
      "(Iteration 19801 / 76500) loss: 2.303227\n",
      "(Epoch 26 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 19901 / 76500) loss: 2.303302\n",
      "(Iteration 20001 / 76500) loss: 2.303202\n",
      "(Iteration 20101 / 76500) loss: 2.303233\n",
      "(Iteration 20201 / 76500) loss: 2.303337\n",
      "(Iteration 20301 / 76500) loss: 2.303210\n",
      "(Iteration 20401 / 76500) loss: 2.303244\n",
      "(Iteration 20501 / 76500) loss: 2.303315\n",
      "(Iteration 20601 / 76500) loss: 2.303269\n",
      "(Epoch 27 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 20701 / 76500) loss: 2.303261\n",
      "(Iteration 20801 / 76500) loss: 2.303261\n",
      "(Iteration 20901 / 76500) loss: 2.303178\n",
      "(Iteration 21001 / 76500) loss: 2.303121\n",
      "(Iteration 21101 / 76500) loss: 2.303294\n",
      "(Iteration 21201 / 76500) loss: 2.303238\n",
      "(Iteration 21301 / 76500) loss: 2.303234\n",
      "(Iteration 21401 / 76500) loss: 2.303273\n",
      "(Epoch 28 / 100) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 21501 / 76500) loss: 2.303211\n",
      "(Iteration 21601 / 76500) loss: 2.303221\n",
      "(Iteration 21701 / 76500) loss: 2.303184\n",
      "(Iteration 21801 / 76500) loss: 2.303223\n",
      "(Iteration 21901 / 76500) loss: 2.303171\n",
      "(Iteration 22001 / 76500) loss: 2.303241\n",
      "(Iteration 22101 / 76500) loss: 2.303207\n",
      "(Epoch 29 / 100) train acc: 0.084000; val_acc: 0.079000\n",
      "(Iteration 22201 / 76500) loss: 2.303143\n",
      "(Iteration 22301 / 76500) loss: 2.303213\n",
      "(Iteration 22401 / 76500) loss: 2.303225\n",
      "(Iteration 22501 / 76500) loss: 2.303178\n",
      "(Iteration 22601 / 76500) loss: 2.303254\n",
      "(Iteration 22701 / 76500) loss: 2.303162\n",
      "(Iteration 22801 / 76500) loss: 2.303114\n",
      "(Iteration 22901 / 76500) loss: 2.303254\n",
      "(Epoch 30 / 100) train acc: 0.078000; val_acc: 0.078000\n",
      "(Iteration 23001 / 76500) loss: 2.303266\n",
      "(Iteration 23101 / 76500) loss: 2.303211\n",
      "(Iteration 23201 / 76500) loss: 2.303162\n",
      "(Iteration 23301 / 76500) loss: 2.303198\n",
      "(Iteration 23401 / 76500) loss: 2.303126\n",
      "(Iteration 23501 / 76500) loss: 2.303217\n",
      "(Iteration 23601 / 76500) loss: 2.303116\n",
      "(Iteration 23701 / 76500) loss: 2.303197\n",
      "(Epoch 31 / 100) train acc: 0.101000; val_acc: 0.077000\n",
      "(Iteration 23801 / 76500) loss: 2.303146\n",
      "(Iteration 23901 / 76500) loss: 2.303165\n",
      "(Iteration 24001 / 76500) loss: 2.303214\n",
      "(Iteration 24101 / 76500) loss: 2.303195\n",
      "(Iteration 24201 / 76500) loss: 2.303143\n",
      "(Iteration 24301 / 76500) loss: 2.303188\n",
      "(Iteration 24401 / 76500) loss: 2.303184\n",
      "(Epoch 32 / 100) train acc: 0.123000; val_acc: 0.077000\n",
      "(Iteration 24501 / 76500) loss: 2.303099\n",
      "(Iteration 24601 / 76500) loss: 2.303183\n",
      "(Iteration 24701 / 76500) loss: 2.303213\n",
      "(Iteration 24801 / 76500) loss: 2.303134\n",
      "(Iteration 24901 / 76500) loss: 2.303115\n",
      "(Iteration 25001 / 76500) loss: 2.303215\n",
      "(Iteration 25101 / 76500) loss: 2.303183\n",
      "(Iteration 25201 / 76500) loss: 2.303183\n",
      "(Epoch 33 / 100) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 25301 / 76500) loss: 2.303077\n",
      "(Iteration 25401 / 76500) loss: 2.303249\n",
      "(Iteration 25501 / 76500) loss: 2.303197\n",
      "(Iteration 25601 / 76500) loss: 2.303132\n",
      "(Iteration 25701 / 76500) loss: 2.303203\n",
      "(Iteration 25801 / 76500) loss: 2.303160\n",
      "(Iteration 25901 / 76500) loss: 2.303229\n",
      "(Iteration 26001 / 76500) loss: 2.303122\n",
      "(Epoch 34 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 26101 / 76500) loss: 2.303235\n",
      "(Iteration 26201 / 76500) loss: 2.303250\n",
      "(Iteration 26301 / 76500) loss: 2.303162\n",
      "(Iteration 26401 / 76500) loss: 2.303251\n",
      "(Iteration 26501 / 76500) loss: 2.303110\n",
      "(Iteration 26601 / 76500) loss: 2.303136\n",
      "(Iteration 26701 / 76500) loss: 2.303158\n",
      "(Epoch 35 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 26801 / 76500) loss: 2.303242\n",
      "(Iteration 26901 / 76500) loss: 2.303210\n",
      "(Iteration 27001 / 76500) loss: 2.303113\n",
      "(Iteration 27101 / 76500) loss: 2.302996\n",
      "(Iteration 27201 / 76500) loss: 2.303000\n",
      "(Iteration 27301 / 76500) loss: 2.303225\n",
      "(Iteration 27401 / 76500) loss: 2.303137\n",
      "(Iteration 27501 / 76500) loss: 2.303062\n",
      "(Epoch 36 / 100) train acc: 0.088000; val_acc: 0.073000\n",
      "(Iteration 27601 / 76500) loss: 2.303073\n",
      "(Iteration 27701 / 76500) loss: 2.303122\n",
      "(Iteration 27801 / 76500) loss: 2.303159\n",
      "(Iteration 27901 / 76500) loss: 2.303084\n",
      "(Iteration 28001 / 76500) loss: 2.303218\n",
      "(Iteration 28101 / 76500) loss: 2.303162\n",
      "(Iteration 28201 / 76500) loss: 2.303188\n",
      "(Iteration 28301 / 76500) loss: 2.303186\n",
      "(Epoch 37 / 100) train acc: 0.081000; val_acc: 0.077000\n",
      "(Iteration 28401 / 76500) loss: 2.303207\n",
      "(Iteration 28501 / 76500) loss: 2.303091\n",
      "(Iteration 28601 / 76500) loss: 2.303010\n",
      "(Iteration 28701 / 76500) loss: 2.303267\n",
      "(Iteration 28801 / 76500) loss: 2.303111\n",
      "(Iteration 28901 / 76500) loss: 2.303243\n",
      "(Iteration 29001 / 76500) loss: 2.303087\n",
      "(Epoch 38 / 100) train acc: 0.103000; val_acc: 0.076000\n",
      "(Iteration 29101 / 76500) loss: 2.303198\n",
      "(Iteration 29201 / 76500) loss: 2.303107\n",
      "(Iteration 29301 / 76500) loss: 2.303134\n",
      "(Iteration 29401 / 76500) loss: 2.303080\n",
      "(Iteration 29501 / 76500) loss: 2.303105\n",
      "(Iteration 29601 / 76500) loss: 2.303049\n",
      "(Iteration 29701 / 76500) loss: 2.303208\n",
      "(Iteration 29801 / 76500) loss: 2.303154\n",
      "(Epoch 39 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 29901 / 76500) loss: 2.303128\n",
      "(Iteration 30001 / 76500) loss: 2.303133\n",
      "(Iteration 30101 / 76500) loss: 2.303127\n",
      "(Iteration 30201 / 76500) loss: 2.303078\n",
      "(Iteration 30301 / 76500) loss: 2.303154\n",
      "(Iteration 30401 / 76500) loss: 2.303065\n",
      "(Iteration 30501 / 76500) loss: 2.303084\n",
      "(Epoch 40 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 30601 / 76500) loss: 2.303190\n",
      "(Iteration 30701 / 76500) loss: 2.303215\n",
      "(Iteration 30801 / 76500) loss: 2.303028\n",
      "(Iteration 30901 / 76500) loss: 2.303154\n",
      "(Iteration 31001 / 76500) loss: 2.303147\n",
      "(Iteration 31101 / 76500) loss: 2.303070\n",
      "(Iteration 31201 / 76500) loss: 2.303154\n",
      "(Iteration 31301 / 76500) loss: 2.303176\n",
      "(Epoch 41 / 100) train acc: 0.097000; val_acc: 0.075000\n",
      "(Iteration 31401 / 76500) loss: 2.303152\n",
      "(Iteration 31501 / 76500) loss: 2.303037\n",
      "(Iteration 31601 / 76500) loss: 2.303230\n",
      "(Iteration 31701 / 76500) loss: 2.303099\n",
      "(Iteration 31801 / 76500) loss: 2.303072\n",
      "(Iteration 31901 / 76500) loss: 2.303052\n",
      "(Iteration 32001 / 76500) loss: 2.303202\n",
      "(Iteration 32101 / 76500) loss: 2.303069\n",
      "(Epoch 42 / 100) train acc: 0.110000; val_acc: 0.074000\n",
      "(Iteration 32201 / 76500) loss: 2.303079\n",
      "(Iteration 32301 / 76500) loss: 2.303236\n",
      "(Iteration 32401 / 76500) loss: 2.303065\n",
      "(Iteration 32501 / 76500) loss: 2.303111\n",
      "(Iteration 32601 / 76500) loss: 2.303089\n",
      "(Iteration 32701 / 76500) loss: 2.303181\n",
      "(Iteration 32801 / 76500) loss: 2.303146\n",
      "(Epoch 43 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 32901 / 76500) loss: 2.303036\n",
      "(Iteration 33001 / 76500) loss: 2.303089\n",
      "(Iteration 33101 / 76500) loss: 2.303191\n",
      "(Iteration 33201 / 76500) loss: 2.303100\n",
      "(Iteration 33301 / 76500) loss: 2.303074\n",
      "(Iteration 33401 / 76500) loss: 2.303041\n",
      "(Iteration 33501 / 76500) loss: 2.303086\n",
      "(Iteration 33601 / 76500) loss: 2.303121\n",
      "(Epoch 44 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 33701 / 76500) loss: 2.303167\n",
      "(Iteration 33801 / 76500) loss: 2.303063\n",
      "(Iteration 33901 / 76500) loss: 2.303090\n",
      "(Iteration 34001 / 76500) loss: 2.303058\n",
      "(Iteration 34101 / 76500) loss: 2.303035\n",
      "(Iteration 34201 / 76500) loss: 2.303096\n",
      "(Iteration 34301 / 76500) loss: 2.303041\n",
      "(Iteration 34401 / 76500) loss: 2.303138\n",
      "(Epoch 45 / 100) train acc: 0.112000; val_acc: 0.078000\n",
      "(Iteration 34501 / 76500) loss: 2.303242\n",
      "(Iteration 34601 / 76500) loss: 2.303090\n",
      "(Iteration 34701 / 76500) loss: 2.303142\n",
      "(Iteration 34801 / 76500) loss: 2.303067\n",
      "(Iteration 34901 / 76500) loss: 2.303197\n",
      "(Iteration 35001 / 76500) loss: 2.303143\n",
      "(Iteration 35101 / 76500) loss: 2.303089\n",
      "(Epoch 46 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 35201 / 76500) loss: 2.303001\n",
      "(Iteration 35301 / 76500) loss: 2.303096\n",
      "(Iteration 35401 / 76500) loss: 2.303085\n",
      "(Iteration 35501 / 76500) loss: 2.303098\n",
      "(Iteration 35601 / 76500) loss: 2.303076\n",
      "(Iteration 35701 / 76500) loss: 2.303183\n",
      "(Iteration 35801 / 76500) loss: 2.303191\n",
      "(Iteration 35901 / 76500) loss: 2.303042\n",
      "(Epoch 47 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 36001 / 76500) loss: 2.303012\n",
      "(Iteration 36101 / 76500) loss: 2.303145\n",
      "(Iteration 36201 / 76500) loss: 2.303020\n",
      "(Iteration 36301 / 76500) loss: 2.303137\n",
      "(Iteration 36401 / 76500) loss: 2.303162\n",
      "(Iteration 36501 / 76500) loss: 2.303045\n",
      "(Iteration 36601 / 76500) loss: 2.303151\n",
      "(Iteration 36701 / 76500) loss: 2.303176\n",
      "(Epoch 48 / 100) train acc: 0.094000; val_acc: 0.077000\n",
      "(Iteration 36801 / 76500) loss: 2.303089\n",
      "(Iteration 36901 / 76500) loss: 2.303094\n",
      "(Iteration 37001 / 76500) loss: 2.303144\n",
      "(Iteration 37101 / 76500) loss: 2.303045\n",
      "(Iteration 37201 / 76500) loss: 2.303148\n",
      "(Iteration 37301 / 76500) loss: 2.303090\n",
      "(Iteration 37401 / 76500) loss: 2.303101\n",
      "(Epoch 49 / 100) train acc: 0.086000; val_acc: 0.077000\n",
      "(Iteration 37501 / 76500) loss: 2.303169\n",
      "(Iteration 37601 / 76500) loss: 2.303178\n",
      "(Iteration 37701 / 76500) loss: 2.303056\n",
      "(Iteration 37801 / 76500) loss: 2.303093\n",
      "(Iteration 37901 / 76500) loss: 2.303190\n",
      "(Iteration 38001 / 76500) loss: 2.303144\n",
      "(Iteration 38101 / 76500) loss: 2.303045\n",
      "(Iteration 38201 / 76500) loss: 2.303106\n",
      "(Epoch 50 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 38301 / 76500) loss: 2.303121\n",
      "(Iteration 38401 / 76500) loss: 2.302966\n",
      "(Iteration 38501 / 76500) loss: 2.303077\n",
      "(Iteration 38601 / 76500) loss: 2.303089\n",
      "(Iteration 38701 / 76500) loss: 2.303046\n",
      "(Iteration 38801 / 76500) loss: 2.303065\n",
      "(Iteration 38901 / 76500) loss: 2.303191\n",
      "(Iteration 39001 / 76500) loss: 2.303168\n",
      "(Epoch 51 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 39101 / 76500) loss: 2.303004\n",
      "(Iteration 39201 / 76500) loss: 2.303057\n",
      "(Iteration 39301 / 76500) loss: 2.303073\n",
      "(Iteration 39401 / 76500) loss: 2.303051\n",
      "(Iteration 39501 / 76500) loss: 2.303022\n",
      "(Iteration 39601 / 76500) loss: 2.303140\n",
      "(Iteration 39701 / 76500) loss: 2.303166\n",
      "(Epoch 52 / 100) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 39801 / 76500) loss: 2.302977\n",
      "(Iteration 39901 / 76500) loss: 2.303007\n",
      "(Iteration 40001 / 76500) loss: 2.303044\n",
      "(Iteration 40101 / 76500) loss: 2.303076\n",
      "(Iteration 40201 / 76500) loss: 2.303056\n",
      "(Iteration 40301 / 76500) loss: 2.303142\n",
      "(Iteration 40401 / 76500) loss: 2.303046\n",
      "(Iteration 40501 / 76500) loss: 2.303061\n",
      "(Epoch 53 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 40601 / 76500) loss: 2.303089\n",
      "(Iteration 40701 / 76500) loss: 2.303091\n",
      "(Iteration 40801 / 76500) loss: 2.303024\n",
      "(Iteration 40901 / 76500) loss: 2.303020\n",
      "(Iteration 41001 / 76500) loss: 2.303076\n",
      "(Iteration 41101 / 76500) loss: 2.303071\n",
      "(Iteration 41201 / 76500) loss: 2.303047\n",
      "(Iteration 41301 / 76500) loss: 2.303090\n",
      "(Epoch 54 / 100) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 41401 / 76500) loss: 2.303135\n",
      "(Iteration 41501 / 76500) loss: 2.303043\n",
      "(Iteration 41601 / 76500) loss: 2.303154\n",
      "(Iteration 41701 / 76500) loss: 2.303075\n",
      "(Iteration 41801 / 76500) loss: 2.303064\n",
      "(Iteration 41901 / 76500) loss: 2.303187\n",
      "(Iteration 42001 / 76500) loss: 2.303067\n",
      "(Epoch 55 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 42101 / 76500) loss: 2.303055\n",
      "(Iteration 42201 / 76500) loss: 2.303116\n",
      "(Iteration 42301 / 76500) loss: 2.303068\n",
      "(Iteration 42401 / 76500) loss: 2.303106\n",
      "(Iteration 42501 / 76500) loss: 2.303040\n",
      "(Iteration 42601 / 76500) loss: 2.303140\n",
      "(Iteration 42701 / 76500) loss: 2.302998\n",
      "(Iteration 42801 / 76500) loss: 2.303037\n",
      "(Epoch 56 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 42901 / 76500) loss: 2.303101\n",
      "(Iteration 43001 / 76500) loss: 2.302987\n",
      "(Iteration 43101 / 76500) loss: 2.302976\n",
      "(Iteration 43201 / 76500) loss: 2.303122\n",
      "(Iteration 43301 / 76500) loss: 2.302954\n",
      "(Iteration 43401 / 76500) loss: 2.303097\n",
      "(Iteration 43501 / 76500) loss: 2.303068\n",
      "(Iteration 43601 / 76500) loss: 2.303066\n",
      "(Epoch 57 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 43701 / 76500) loss: 2.302993\n",
      "(Iteration 43801 / 76500) loss: 2.303086\n",
      "(Iteration 43901 / 76500) loss: 2.303050\n",
      "(Iteration 44001 / 76500) loss: 2.303046\n",
      "(Iteration 44101 / 76500) loss: 2.303040\n",
      "(Iteration 44201 / 76500) loss: 2.302987\n",
      "(Iteration 44301 / 76500) loss: 2.303118\n",
      "(Epoch 58 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 44401 / 76500) loss: 2.303118\n",
      "(Iteration 44501 / 76500) loss: 2.303040\n",
      "(Iteration 44601 / 76500) loss: 2.303103\n",
      "(Iteration 44701 / 76500) loss: 2.303115\n",
      "(Iteration 44801 / 76500) loss: 2.303059\n",
      "(Iteration 44901 / 76500) loss: 2.303028\n",
      "(Iteration 45001 / 76500) loss: 2.303026\n",
      "(Iteration 45101 / 76500) loss: 2.303054\n",
      "(Epoch 59 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 45201 / 76500) loss: 2.303059\n",
      "(Iteration 45301 / 76500) loss: 2.302965\n",
      "(Iteration 45401 / 76500) loss: 2.303168\n",
      "(Iteration 45501 / 76500) loss: 2.303030\n",
      "(Iteration 45601 / 76500) loss: 2.303029\n",
      "(Iteration 45701 / 76500) loss: 2.302984\n",
      "(Iteration 45801 / 76500) loss: 2.303059\n",
      "(Epoch 60 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 45901 / 76500) loss: 2.303010\n",
      "(Iteration 46001 / 76500) loss: 2.303140\n",
      "(Iteration 46101 / 76500) loss: 2.303049\n",
      "(Iteration 46201 / 76500) loss: 2.303082\n",
      "(Iteration 46301 / 76500) loss: 2.303001\n",
      "(Iteration 46401 / 76500) loss: 2.303078\n",
      "(Iteration 46501 / 76500) loss: 2.303027\n",
      "(Iteration 46601 / 76500) loss: 2.302975\n",
      "(Epoch 61 / 100) train acc: 0.086000; val_acc: 0.078000\n",
      "(Iteration 46701 / 76500) loss: 2.302968\n",
      "(Iteration 46801 / 76500) loss: 2.303056\n",
      "(Iteration 46901 / 76500) loss: 2.303058\n",
      "(Iteration 47001 / 76500) loss: 2.303039\n",
      "(Iteration 47101 / 76500) loss: 2.302965\n",
      "(Iteration 47201 / 76500) loss: 2.303014\n",
      "(Iteration 47301 / 76500) loss: 2.303113\n",
      "(Iteration 47401 / 76500) loss: 2.303061\n",
      "(Epoch 62 / 100) train acc: 0.131000; val_acc: 0.078000\n",
      "(Iteration 47501 / 76500) loss: 2.303090\n",
      "(Iteration 47601 / 76500) loss: 2.303107\n",
      "(Iteration 47701 / 76500) loss: 2.303152\n",
      "(Iteration 47801 / 76500) loss: 2.303021\n",
      "(Iteration 47901 / 76500) loss: 2.303072\n",
      "(Iteration 48001 / 76500) loss: 2.303146\n",
      "(Iteration 48101 / 76500) loss: 2.302969\n",
      "(Epoch 63 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 48201 / 76500) loss: 2.302977\n",
      "(Iteration 48301 / 76500) loss: 2.303084\n",
      "(Iteration 48401 / 76500) loss: 2.303093\n",
      "(Iteration 48501 / 76500) loss: 2.303153\n",
      "(Iteration 48601 / 76500) loss: 2.303083\n",
      "(Iteration 48701 / 76500) loss: 2.303098\n",
      "(Iteration 48801 / 76500) loss: 2.303023\n",
      "(Iteration 48901 / 76500) loss: 2.303054\n",
      "(Epoch 64 / 100) train acc: 0.084000; val_acc: 0.078000\n",
      "(Iteration 49001 / 76500) loss: 2.303113\n",
      "(Iteration 49101 / 76500) loss: 2.302955\n",
      "(Iteration 49201 / 76500) loss: 2.303020\n",
      "(Iteration 49301 / 76500) loss: 2.303006\n",
      "(Iteration 49401 / 76500) loss: 2.303218\n",
      "(Iteration 49501 / 76500) loss: 2.303077\n",
      "(Iteration 49601 / 76500) loss: 2.303109\n",
      "(Iteration 49701 / 76500) loss: 2.303020\n",
      "(Epoch 65 / 100) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 49801 / 76500) loss: 2.303098\n",
      "(Iteration 49901 / 76500) loss: 2.303113\n",
      "(Iteration 50001 / 76500) loss: 2.303045\n",
      "(Iteration 50101 / 76500) loss: 2.302978\n",
      "(Iteration 50201 / 76500) loss: 2.303049\n",
      "(Iteration 50301 / 76500) loss: 2.303026\n",
      "(Iteration 50401 / 76500) loss: 2.303022\n",
      "(Epoch 66 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 50501 / 76500) loss: 2.303101\n",
      "(Iteration 50601 / 76500) loss: 2.303109\n",
      "(Iteration 50701 / 76500) loss: 2.302993\n",
      "(Iteration 50801 / 76500) loss: 2.303092\n",
      "(Iteration 50901 / 76500) loss: 2.303057\n",
      "(Iteration 51001 / 76500) loss: 2.303082\n",
      "(Iteration 51101 / 76500) loss: 2.303095\n",
      "(Iteration 51201 / 76500) loss: 2.302972\n",
      "(Epoch 67 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 51301 / 76500) loss: 2.303076\n",
      "(Iteration 51401 / 76500) loss: 2.303168\n",
      "(Iteration 51501 / 76500) loss: 2.303045\n",
      "(Iteration 51601 / 76500) loss: 2.303030\n",
      "(Iteration 51701 / 76500) loss: 2.303095\n",
      "(Iteration 51801 / 76500) loss: 2.303065\n",
      "(Iteration 51901 / 76500) loss: 2.303055\n",
      "(Iteration 52001 / 76500) loss: 2.303150\n",
      "(Epoch 68 / 100) train acc: 0.114000; val_acc: 0.078000\n",
      "(Iteration 52101 / 76500) loss: 2.302950\n",
      "(Iteration 52201 / 76500) loss: 2.303103\n",
      "(Iteration 52301 / 76500) loss: 2.303119\n",
      "(Iteration 52401 / 76500) loss: 2.303188\n",
      "(Iteration 52501 / 76500) loss: 2.303012\n",
      "(Iteration 52601 / 76500) loss: 2.303115\n",
      "(Iteration 52701 / 76500) loss: 2.303036\n",
      "(Epoch 69 / 100) train acc: 0.118000; val_acc: 0.078000\n",
      "(Iteration 52801 / 76500) loss: 2.303106\n",
      "(Iteration 52901 / 76500) loss: 2.302886\n",
      "(Iteration 53001 / 76500) loss: 2.303010\n",
      "(Iteration 53101 / 76500) loss: 2.303015\n",
      "(Iteration 53201 / 76500) loss: 2.303060\n",
      "(Iteration 53301 / 76500) loss: 2.302947\n",
      "(Iteration 53401 / 76500) loss: 2.303032\n",
      "(Iteration 53501 / 76500) loss: 2.303058\n",
      "(Epoch 70 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 53601 / 76500) loss: 2.303034\n",
      "(Iteration 53701 / 76500) loss: 2.303132\n",
      "(Iteration 53801 / 76500) loss: 2.303062\n",
      "(Iteration 53901 / 76500) loss: 2.303031\n",
      "(Iteration 54001 / 76500) loss: 2.303102\n",
      "(Iteration 54101 / 76500) loss: 2.303086\n",
      "(Iteration 54201 / 76500) loss: 2.303053\n",
      "(Iteration 54301 / 76500) loss: 2.302980\n",
      "(Epoch 71 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 54401 / 76500) loss: 2.303105\n",
      "(Iteration 54501 / 76500) loss: 2.303067\n",
      "(Iteration 54601 / 76500) loss: 2.303066\n",
      "(Iteration 54701 / 76500) loss: 2.302978\n",
      "(Iteration 54801 / 76500) loss: 2.302946\n",
      "(Iteration 54901 / 76500) loss: 2.302983\n",
      "(Iteration 55001 / 76500) loss: 2.303138\n",
      "(Epoch 72 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 55101 / 76500) loss: 2.303008\n",
      "(Iteration 55201 / 76500) loss: 2.302970\n",
      "(Iteration 55301 / 76500) loss: 2.303047\n",
      "(Iteration 55401 / 76500) loss: 2.303007\n",
      "(Iteration 55501 / 76500) loss: 2.303026\n",
      "(Iteration 55601 / 76500) loss: 2.303089\n",
      "(Iteration 55701 / 76500) loss: 2.303072\n",
      "(Iteration 55801 / 76500) loss: 2.302968\n",
      "(Epoch 73 / 100) train acc: 0.115000; val_acc: 0.078000\n",
      "(Iteration 55901 / 76500) loss: 2.303049\n",
      "(Iteration 56001 / 76500) loss: 2.303086\n",
      "(Iteration 56101 / 76500) loss: 2.303052\n",
      "(Iteration 56201 / 76500) loss: 2.303109\n",
      "(Iteration 56301 / 76500) loss: 2.303003\n",
      "(Iteration 56401 / 76500) loss: 2.303048\n",
      "(Iteration 56501 / 76500) loss: 2.303107\n",
      "(Iteration 56601 / 76500) loss: 2.303097\n",
      "(Epoch 74 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 56701 / 76500) loss: 2.303024\n",
      "(Iteration 56801 / 76500) loss: 2.302939\n",
      "(Iteration 56901 / 76500) loss: 2.302994\n",
      "(Iteration 57001 / 76500) loss: 2.302958\n",
      "(Iteration 57101 / 76500) loss: 2.303016\n",
      "(Iteration 57201 / 76500) loss: 2.303203\n",
      "(Iteration 57301 / 76500) loss: 2.302972\n",
      "(Epoch 75 / 100) train acc: 0.111000; val_acc: 0.078000\n",
      "(Iteration 57401 / 76500) loss: 2.303034\n",
      "(Iteration 57501 / 76500) loss: 2.303076\n",
      "(Iteration 57601 / 76500) loss: 2.303031\n",
      "(Iteration 57701 / 76500) loss: 2.303061\n",
      "(Iteration 57801 / 76500) loss: 2.303100\n",
      "(Iteration 57901 / 76500) loss: 2.302990\n",
      "(Iteration 58001 / 76500) loss: 2.303007\n",
      "(Iteration 58101 / 76500) loss: 2.302966\n",
      "(Epoch 76 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 58201 / 76500) loss: 2.303080\n",
      "(Iteration 58301 / 76500) loss: 2.302963\n",
      "(Iteration 58401 / 76500) loss: 2.302951\n",
      "(Iteration 58501 / 76500) loss: 2.303072\n",
      "(Iteration 58601 / 76500) loss: 2.303165\n",
      "(Iteration 58701 / 76500) loss: 2.302958\n",
      "(Iteration 58801 / 76500) loss: 2.303060\n",
      "(Iteration 58901 / 76500) loss: 2.303045\n",
      "(Epoch 77 / 100) train acc: 0.117000; val_acc: 0.078000\n",
      "(Iteration 59001 / 76500) loss: 2.303138\n",
      "(Iteration 59101 / 76500) loss: 2.303007\n",
      "(Iteration 59201 / 76500) loss: 2.303150\n",
      "(Iteration 59301 / 76500) loss: 2.303057\n",
      "(Iteration 59401 / 76500) loss: 2.302939\n",
      "(Iteration 59501 / 76500) loss: 2.303016\n",
      "(Iteration 59601 / 76500) loss: 2.303004\n",
      "(Epoch 78 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 59701 / 76500) loss: 2.303020\n",
      "(Iteration 59801 / 76500) loss: 2.303041\n",
      "(Iteration 59901 / 76500) loss: 2.303118\n",
      "(Iteration 60001 / 76500) loss: 2.303028\n",
      "(Iteration 60101 / 76500) loss: 2.302915\n",
      "(Iteration 60201 / 76500) loss: 2.303071\n",
      "(Iteration 60301 / 76500) loss: 2.303077\n",
      "(Iteration 60401 / 76500) loss: 2.303061\n",
      "(Epoch 79 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 60501 / 76500) loss: 2.303016\n",
      "(Iteration 60601 / 76500) loss: 2.303072\n",
      "(Iteration 60701 / 76500) loss: 2.303070\n",
      "(Iteration 60801 / 76500) loss: 2.303070\n",
      "(Iteration 60901 / 76500) loss: 2.303095\n",
      "(Iteration 61001 / 76500) loss: 2.303091\n",
      "(Iteration 61101 / 76500) loss: 2.303081\n",
      "(Epoch 80 / 100) train acc: 0.114000; val_acc: 0.078000\n",
      "(Iteration 61201 / 76500) loss: 2.303080\n",
      "(Iteration 61301 / 76500) loss: 2.302953\n",
      "(Iteration 61401 / 76500) loss: 2.303069\n",
      "(Iteration 61501 / 76500) loss: 2.302935\n",
      "(Iteration 61601 / 76500) loss: 2.303103\n",
      "(Iteration 61701 / 76500) loss: 2.302970\n",
      "(Iteration 61801 / 76500) loss: 2.303056\n",
      "(Iteration 61901 / 76500) loss: 2.303036\n",
      "(Epoch 81 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 62001 / 76500) loss: 2.303075\n",
      "(Iteration 62101 / 76500) loss: 2.303044\n",
      "(Iteration 62201 / 76500) loss: 2.303041\n",
      "(Iteration 62301 / 76500) loss: 2.303060\n",
      "(Iteration 62401 / 76500) loss: 2.302994\n",
      "(Iteration 62501 / 76500) loss: 2.303066\n",
      "(Iteration 62601 / 76500) loss: 2.303150\n",
      "(Iteration 62701 / 76500) loss: 2.303030\n",
      "(Epoch 82 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 62801 / 76500) loss: 2.303075\n",
      "(Iteration 62901 / 76500) loss: 2.303041\n",
      "(Iteration 63001 / 76500) loss: 2.303045\n",
      "(Iteration 63101 / 76500) loss: 2.302983\n",
      "(Iteration 63201 / 76500) loss: 2.303045\n",
      "(Iteration 63301 / 76500) loss: 2.303127\n",
      "(Iteration 63401 / 76500) loss: 2.303181\n",
      "(Epoch 83 / 100) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 63501 / 76500) loss: 2.303008\n",
      "(Iteration 63601 / 76500) loss: 2.303037\n",
      "(Iteration 63701 / 76500) loss: 2.303126\n",
      "(Iteration 63801 / 76500) loss: 2.303084\n",
      "(Iteration 63901 / 76500) loss: 2.302996\n",
      "(Iteration 64001 / 76500) loss: 2.303121\n",
      "(Iteration 64101 / 76500) loss: 2.303050\n",
      "(Iteration 64201 / 76500) loss: 2.303117\n",
      "(Epoch 84 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 64301 / 76500) loss: 2.303077\n",
      "(Iteration 64401 / 76500) loss: 2.303075\n",
      "(Iteration 64501 / 76500) loss: 2.303014\n",
      "(Iteration 64601 / 76500) loss: 2.302940\n",
      "(Iteration 64701 / 76500) loss: 2.303000\n",
      "(Iteration 64801 / 76500) loss: 2.302997\n",
      "(Iteration 64901 / 76500) loss: 2.303027\n",
      "(Iteration 65001 / 76500) loss: 2.302984\n",
      "(Epoch 85 / 100) train acc: 0.074000; val_acc: 0.078000\n",
      "(Iteration 65101 / 76500) loss: 2.303092\n",
      "(Iteration 65201 / 76500) loss: 2.303010\n",
      "(Iteration 65301 / 76500) loss: 2.303086\n",
      "(Iteration 65401 / 76500) loss: 2.303126\n",
      "(Iteration 65501 / 76500) loss: 2.303102\n",
      "(Iteration 65601 / 76500) loss: 2.303043\n",
      "(Iteration 65701 / 76500) loss: 2.303023\n",
      "(Epoch 86 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 65801 / 76500) loss: 2.303045\n",
      "(Iteration 65901 / 76500) loss: 2.303085\n",
      "(Iteration 66001 / 76500) loss: 2.303125\n",
      "(Iteration 66101 / 76500) loss: 2.303030\n",
      "(Iteration 66201 / 76500) loss: 2.303112\n",
      "(Iteration 66301 / 76500) loss: 2.302962\n",
      "(Iteration 66401 / 76500) loss: 2.303059\n",
      "(Iteration 66501 / 76500) loss: 2.303156\n",
      "(Epoch 87 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 66601 / 76500) loss: 2.303118\n",
      "(Iteration 66701 / 76500) loss: 2.303070\n",
      "(Iteration 66801 / 76500) loss: 2.303081\n",
      "(Iteration 66901 / 76500) loss: 2.303116\n",
      "(Iteration 67001 / 76500) loss: 2.303041\n",
      "(Iteration 67101 / 76500) loss: 2.302980\n",
      "(Iteration 67201 / 76500) loss: 2.303056\n",
      "(Iteration 67301 / 76500) loss: 2.303097\n",
      "(Epoch 88 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 67401 / 76500) loss: 2.303033\n",
      "(Iteration 67501 / 76500) loss: 2.303012\n",
      "(Iteration 67601 / 76500) loss: 2.303051\n",
      "(Iteration 67701 / 76500) loss: 2.302958\n",
      "(Iteration 67801 / 76500) loss: 2.303059\n",
      "(Iteration 67901 / 76500) loss: 2.303032\n",
      "(Iteration 68001 / 76500) loss: 2.303035\n",
      "(Epoch 89 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 68101 / 76500) loss: 2.302931\n",
      "(Iteration 68201 / 76500) loss: 2.303092\n",
      "(Iteration 68301 / 76500) loss: 2.303049\n",
      "(Iteration 68401 / 76500) loss: 2.303106\n",
      "(Iteration 68501 / 76500) loss: 2.302915\n",
      "(Iteration 68601 / 76500) loss: 2.302960\n",
      "(Iteration 68701 / 76500) loss: 2.302927\n",
      "(Iteration 68801 / 76500) loss: 2.303116\n",
      "(Epoch 90 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 68901 / 76500) loss: 2.303096\n",
      "(Iteration 69001 / 76500) loss: 2.303058\n",
      "(Iteration 69101 / 76500) loss: 2.303099\n",
      "(Iteration 69201 / 76500) loss: 2.302967\n",
      "(Iteration 69301 / 76500) loss: 2.303019\n",
      "(Iteration 69401 / 76500) loss: 2.303054\n",
      "(Iteration 69501 / 76500) loss: 2.302987\n",
      "(Iteration 69601 / 76500) loss: 2.303097\n",
      "(Epoch 91 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 69701 / 76500) loss: 2.302924\n",
      "(Iteration 69801 / 76500) loss: 2.303038\n",
      "(Iteration 69901 / 76500) loss: 2.303003\n",
      "(Iteration 70001 / 76500) loss: 2.302964\n",
      "(Iteration 70101 / 76500) loss: 2.303118\n",
      "(Iteration 70201 / 76500) loss: 2.303006\n",
      "(Iteration 70301 / 76500) loss: 2.303035\n",
      "(Epoch 92 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 70401 / 76500) loss: 2.303070\n",
      "(Iteration 70501 / 76500) loss: 2.302891\n",
      "(Iteration 70601 / 76500) loss: 2.303091\n",
      "(Iteration 70701 / 76500) loss: 2.303081\n",
      "(Iteration 70801 / 76500) loss: 2.303017\n",
      "(Iteration 70901 / 76500) loss: 2.302959\n",
      "(Iteration 71001 / 76500) loss: 2.303021\n",
      "(Iteration 71101 / 76500) loss: 2.303122\n",
      "(Epoch 93 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 71201 / 76500) loss: 2.303100\n",
      "(Iteration 71301 / 76500) loss: 2.303099\n",
      "(Iteration 71401 / 76500) loss: 2.303049\n",
      "(Iteration 71501 / 76500) loss: 2.303128\n",
      "(Iteration 71601 / 76500) loss: 2.303153\n",
      "(Iteration 71701 / 76500) loss: 2.303050\n",
      "(Iteration 71801 / 76500) loss: 2.303004\n",
      "(Iteration 71901 / 76500) loss: 2.302963\n",
      "(Epoch 94 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 72001 / 76500) loss: 2.303021\n",
      "(Iteration 72101 / 76500) loss: 2.302978\n",
      "(Iteration 72201 / 76500) loss: 2.302981\n",
      "(Iteration 72301 / 76500) loss: 2.302986\n",
      "(Iteration 72401 / 76500) loss: 2.303009\n",
      "(Iteration 72501 / 76500) loss: 2.303103\n",
      "(Iteration 72601 / 76500) loss: 2.303066\n",
      "(Epoch 95 / 100) train acc: 0.111000; val_acc: 0.078000\n",
      "(Iteration 72701 / 76500) loss: 2.303001\n",
      "(Iteration 72801 / 76500) loss: 2.302918\n",
      "(Iteration 72901 / 76500) loss: 2.303061\n",
      "(Iteration 73001 / 76500) loss: 2.302961\n",
      "(Iteration 73101 / 76500) loss: 2.303020\n",
      "(Iteration 73201 / 76500) loss: 2.302976\n",
      "(Iteration 73301 / 76500) loss: 2.302969\n",
      "(Iteration 73401 / 76500) loss: 2.302990\n",
      "(Epoch 96 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 73501 / 76500) loss: 2.303066\n",
      "(Iteration 73601 / 76500) loss: 2.302958\n",
      "(Iteration 73701 / 76500) loss: 2.303118\n",
      "(Iteration 73801 / 76500) loss: 2.303089\n",
      "(Iteration 73901 / 76500) loss: 2.303010\n",
      "(Iteration 74001 / 76500) loss: 2.303134\n",
      "(Iteration 74101 / 76500) loss: 2.303052\n",
      "(Iteration 74201 / 76500) loss: 2.303085\n",
      "(Epoch 97 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 74301 / 76500) loss: 2.303027\n",
      "(Iteration 74401 / 76500) loss: 2.303034\n",
      "(Iteration 74501 / 76500) loss: 2.303081\n",
      "(Iteration 74601 / 76500) loss: 2.303128\n",
      "(Iteration 74701 / 76500) loss: 2.303040\n",
      "(Iteration 74801 / 76500) loss: 2.303080\n",
      "(Iteration 74901 / 76500) loss: 2.302968\n",
      "(Epoch 98 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 75001 / 76500) loss: 2.303171\n",
      "(Iteration 75101 / 76500) loss: 2.303043\n",
      "(Iteration 75201 / 76500) loss: 2.303115\n",
      "(Iteration 75301 / 76500) loss: 2.303018\n",
      "(Iteration 75401 / 76500) loss: 2.303000\n",
      "(Iteration 75501 / 76500) loss: 2.303027\n",
      "(Iteration 75601 / 76500) loss: 2.303143\n",
      "(Iteration 75701 / 76500) loss: 2.302899\n",
      "(Epoch 99 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 75801 / 76500) loss: 2.303049\n",
      "(Iteration 75901 / 76500) loss: 2.303019\n",
      "(Iteration 76001 / 76500) loss: 2.303100\n",
      "(Iteration 76101 / 76500) loss: 2.303132\n",
      "(Iteration 76201 / 76500) loss: 2.303020\n",
      "(Iteration 76301 / 76500) loss: 2.303107\n",
      "(Iteration 76401 / 76500) loss: 2.303069\n",
      "(Epoch 100 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.304648\n",
      "(Epoch 0 / 100) train acc: 0.089000; val_acc: 0.091000\n",
      "(Iteration 101 / 38200) loss: 2.304618\n",
      "(Iteration 201 / 38200) loss: 2.304601\n",
      "(Iteration 301 / 38200) loss: 2.304590\n",
      "(Epoch 1 / 100) train acc: 0.085000; val_acc: 0.074000\n",
      "(Iteration 401 / 38200) loss: 2.304559\n",
      "(Iteration 501 / 38200) loss: 2.304555\n",
      "(Iteration 601 / 38200) loss: 2.304521\n",
      "(Iteration 701 / 38200) loss: 2.304501\n",
      "(Epoch 2 / 100) train acc: 0.125000; val_acc: 0.098000\n",
      "(Iteration 801 / 38200) loss: 2.304487\n",
      "(Iteration 901 / 38200) loss: 2.304479\n",
      "(Iteration 1001 / 38200) loss: 2.304450\n",
      "(Iteration 1101 / 38200) loss: 2.304464\n",
      "(Epoch 3 / 100) train acc: 0.103000; val_acc: 0.085000\n",
      "(Iteration 1201 / 38200) loss: 2.304438\n",
      "(Iteration 1301 / 38200) loss: 2.304381\n",
      "(Iteration 1401 / 38200) loss: 2.304385\n",
      "(Iteration 1501 / 38200) loss: 2.304361\n",
      "(Epoch 4 / 100) train acc: 0.097000; val_acc: 0.080000\n",
      "(Iteration 1601 / 38200) loss: 2.304350\n",
      "(Iteration 1701 / 38200) loss: 2.304341\n",
      "(Iteration 1801 / 38200) loss: 2.304331\n",
      "(Iteration 1901 / 38200) loss: 2.304317\n",
      "(Epoch 5 / 100) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 2001 / 38200) loss: 2.304316\n",
      "(Iteration 2101 / 38200) loss: 2.304291\n",
      "(Iteration 2201 / 38200) loss: 2.304270\n",
      "(Epoch 6 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 2301 / 38200) loss: 2.304251\n",
      "(Iteration 2401 / 38200) loss: 2.304245\n",
      "(Iteration 2501 / 38200) loss: 2.304219\n",
      "(Iteration 2601 / 38200) loss: 2.304231\n",
      "(Epoch 7 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 2701 / 38200) loss: 2.304234\n",
      "(Iteration 2801 / 38200) loss: 2.304178\n",
      "(Iteration 2901 / 38200) loss: 2.304173\n",
      "(Iteration 3001 / 38200) loss: 2.304178\n",
      "(Epoch 8 / 100) train acc: 0.123000; val_acc: 0.078000\n",
      "(Iteration 3101 / 38200) loss: 2.304189\n",
      "(Iteration 3201 / 38200) loss: 2.304135\n",
      "(Iteration 3301 / 38200) loss: 2.304143\n",
      "(Iteration 3401 / 38200) loss: 2.304129\n",
      "(Epoch 9 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 3501 / 38200) loss: 2.304113\n",
      "(Iteration 3601 / 38200) loss: 2.304125\n",
      "(Iteration 3701 / 38200) loss: 2.304097\n",
      "(Iteration 3801 / 38200) loss: 2.304123\n",
      "(Epoch 10 / 100) train acc: 0.084000; val_acc: 0.078000\n",
      "(Iteration 3901 / 38200) loss: 2.304106\n",
      "(Iteration 4001 / 38200) loss: 2.304079\n",
      "(Iteration 4101 / 38200) loss: 2.304068\n",
      "(Iteration 4201 / 38200) loss: 2.304051\n",
      "(Epoch 11 / 100) train acc: 0.111000; val_acc: 0.078000\n",
      "(Iteration 4301 / 38200) loss: 2.304037\n",
      "(Iteration 4401 / 38200) loss: 2.304018\n",
      "(Iteration 4501 / 38200) loss: 2.304051\n",
      "(Epoch 12 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 4601 / 38200) loss: 2.303993\n",
      "(Iteration 4701 / 38200) loss: 2.304006\n",
      "(Iteration 4801 / 38200) loss: 2.304014\n",
      "(Iteration 4901 / 38200) loss: 2.304011\n",
      "(Epoch 13 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 5001 / 38200) loss: 2.303990\n",
      "(Iteration 5101 / 38200) loss: 2.303972\n",
      "(Iteration 5201 / 38200) loss: 2.303978\n",
      "(Iteration 5301 / 38200) loss: 2.303986\n",
      "(Epoch 14 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 5401 / 38200) loss: 2.303965\n",
      "(Iteration 5501 / 38200) loss: 2.303934\n",
      "(Iteration 5601 / 38200) loss: 2.303959\n",
      "(Iteration 5701 / 38200) loss: 2.303947\n",
      "(Epoch 15 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 5801 / 38200) loss: 2.303969\n",
      "(Iteration 5901 / 38200) loss: 2.303954\n",
      "(Iteration 6001 / 38200) loss: 2.303940\n",
      "(Iteration 6101 / 38200) loss: 2.303936\n",
      "(Epoch 16 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 6201 / 38200) loss: 2.303934\n",
      "(Iteration 6301 / 38200) loss: 2.303920\n",
      "(Iteration 6401 / 38200) loss: 2.303912\n",
      "(Epoch 17 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 6501 / 38200) loss: 2.303891\n",
      "(Iteration 6601 / 38200) loss: 2.303883\n",
      "(Iteration 6701 / 38200) loss: 2.303919\n",
      "(Iteration 6801 / 38200) loss: 2.303913\n",
      "(Epoch 18 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 6901 / 38200) loss: 2.303837\n",
      "(Iteration 7001 / 38200) loss: 2.303861\n",
      "(Iteration 7101 / 38200) loss: 2.303860\n",
      "(Iteration 7201 / 38200) loss: 2.303859\n",
      "(Epoch 19 / 100) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 7301 / 38200) loss: 2.303859\n",
      "(Iteration 7401 / 38200) loss: 2.303855\n",
      "(Iteration 7501 / 38200) loss: 2.303833\n",
      "(Iteration 7601 / 38200) loss: 2.303829\n",
      "(Epoch 20 / 100) train acc: 0.080000; val_acc: 0.078000\n",
      "(Iteration 7701 / 38200) loss: 2.303823\n",
      "(Iteration 7801 / 38200) loss: 2.303817\n",
      "(Iteration 7901 / 38200) loss: 2.303843\n",
      "(Iteration 8001 / 38200) loss: 2.303823\n",
      "(Epoch 21 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 8101 / 38200) loss: 2.303820\n",
      "(Iteration 8201 / 38200) loss: 2.303837\n",
      "(Iteration 8301 / 38200) loss: 2.303802\n",
      "(Iteration 8401 / 38200) loss: 2.303813\n",
      "(Epoch 22 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 8501 / 38200) loss: 2.303819\n",
      "(Iteration 8601 / 38200) loss: 2.303799\n",
      "(Iteration 8701 / 38200) loss: 2.303773\n",
      "(Epoch 23 / 100) train acc: 0.121000; val_acc: 0.078000\n",
      "(Iteration 8801 / 38200) loss: 2.303798\n",
      "(Iteration 8901 / 38200) loss: 2.303819\n",
      "(Iteration 9001 / 38200) loss: 2.303753\n",
      "(Iteration 9101 / 38200) loss: 2.303789\n",
      "(Epoch 24 / 100) train acc: 0.116000; val_acc: 0.078000\n",
      "(Iteration 9201 / 38200) loss: 2.303768\n",
      "(Iteration 9301 / 38200) loss: 2.303805\n",
      "(Iteration 9401 / 38200) loss: 2.303776\n",
      "(Iteration 9501 / 38200) loss: 2.303732\n",
      "(Epoch 25 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 9601 / 38200) loss: 2.303741\n",
      "(Iteration 9701 / 38200) loss: 2.303781\n",
      "(Iteration 9801 / 38200) loss: 2.303754\n",
      "(Iteration 9901 / 38200) loss: 2.303756\n",
      "(Epoch 26 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 10001 / 38200) loss: 2.303763\n",
      "(Iteration 10101 / 38200) loss: 2.303753\n",
      "(Iteration 10201 / 38200) loss: 2.303787\n",
      "(Iteration 10301 / 38200) loss: 2.303747\n",
      "(Epoch 27 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 10401 / 38200) loss: 2.303773\n",
      "(Iteration 10501 / 38200) loss: 2.303730\n",
      "(Iteration 10601 / 38200) loss: 2.303723\n",
      "(Epoch 28 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 10701 / 38200) loss: 2.303743\n",
      "(Iteration 10801 / 38200) loss: 2.303702\n",
      "(Iteration 10901 / 38200) loss: 2.303696\n",
      "(Iteration 11001 / 38200) loss: 2.303722\n",
      "(Epoch 29 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 11101 / 38200) loss: 2.303712\n",
      "(Iteration 11201 / 38200) loss: 2.303692\n",
      "(Iteration 11301 / 38200) loss: 2.303733\n",
      "(Iteration 11401 / 38200) loss: 2.303734\n",
      "(Epoch 30 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 11501 / 38200) loss: 2.303700\n",
      "(Iteration 11601 / 38200) loss: 2.303705\n",
      "(Iteration 11701 / 38200) loss: 2.303728\n",
      "(Iteration 11801 / 38200) loss: 2.303701\n",
      "(Epoch 31 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 11901 / 38200) loss: 2.303689\n",
      "(Iteration 12001 / 38200) loss: 2.303721\n",
      "(Iteration 12101 / 38200) loss: 2.303647\n",
      "(Iteration 12201 / 38200) loss: 2.303730\n",
      "(Epoch 32 / 100) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 12301 / 38200) loss: 2.303718\n",
      "(Iteration 12401 / 38200) loss: 2.303681\n",
      "(Iteration 12501 / 38200) loss: 2.303647\n",
      "(Iteration 12601 / 38200) loss: 2.303662\n",
      "(Epoch 33 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 12701 / 38200) loss: 2.303630\n",
      "(Iteration 12801 / 38200) loss: 2.303680\n",
      "(Iteration 12901 / 38200) loss: 2.303693\n",
      "(Epoch 34 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 13001 / 38200) loss: 2.303731\n",
      "(Iteration 13101 / 38200) loss: 2.303694\n",
      "(Iteration 13201 / 38200) loss: 2.303666\n",
      "(Iteration 13301 / 38200) loss: 2.303662\n",
      "(Epoch 35 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 13401 / 38200) loss: 2.303678\n",
      "(Iteration 13501 / 38200) loss: 2.303671\n",
      "(Iteration 13601 / 38200) loss: 2.303644\n",
      "(Iteration 13701 / 38200) loss: 2.303656\n",
      "(Epoch 36 / 100) train acc: 0.121000; val_acc: 0.078000\n",
      "(Iteration 13801 / 38200) loss: 2.303675\n",
      "(Iteration 13901 / 38200) loss: 2.303686\n",
      "(Iteration 14001 / 38200) loss: 2.303633\n",
      "(Iteration 14101 / 38200) loss: 2.303657\n",
      "(Epoch 37 / 100) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 14201 / 38200) loss: 2.303665\n",
      "(Iteration 14301 / 38200) loss: 2.303665\n",
      "(Iteration 14401 / 38200) loss: 2.303639\n",
      "(Iteration 14501 / 38200) loss: 2.303646\n",
      "(Epoch 38 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 14601 / 38200) loss: 2.303671\n",
      "(Iteration 14701 / 38200) loss: 2.303640\n",
      "(Iteration 14801 / 38200) loss: 2.303674\n",
      "(Epoch 39 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 14901 / 38200) loss: 2.303648\n",
      "(Iteration 15001 / 38200) loss: 2.303610\n",
      "(Iteration 15101 / 38200) loss: 2.303619\n",
      "(Iteration 15201 / 38200) loss: 2.303640\n",
      "(Epoch 40 / 100) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 15301 / 38200) loss: 2.303647\n",
      "(Iteration 15401 / 38200) loss: 2.303644\n",
      "(Iteration 15501 / 38200) loss: 2.303634\n",
      "(Iteration 15601 / 38200) loss: 2.303624\n",
      "(Epoch 41 / 100) train acc: 0.122000; val_acc: 0.078000\n",
      "(Iteration 15701 / 38200) loss: 2.303653\n",
      "(Iteration 15801 / 38200) loss: 2.303602\n",
      "(Iteration 15901 / 38200) loss: 2.303654\n",
      "(Iteration 16001 / 38200) loss: 2.303619\n",
      "(Epoch 42 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 16101 / 38200) loss: 2.303641\n",
      "(Iteration 16201 / 38200) loss: 2.303637\n",
      "(Iteration 16301 / 38200) loss: 2.303605\n",
      "(Iteration 16401 / 38200) loss: 2.303639\n",
      "(Epoch 43 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 16501 / 38200) loss: 2.303651\n",
      "(Iteration 16601 / 38200) loss: 2.303635\n",
      "(Iteration 16701 / 38200) loss: 2.303622\n",
      "(Iteration 16801 / 38200) loss: 2.303627\n",
      "(Epoch 44 / 100) train acc: 0.114000; val_acc: 0.078000\n",
      "(Iteration 16901 / 38200) loss: 2.303611\n",
      "(Iteration 17001 / 38200) loss: 2.303633\n",
      "(Iteration 17101 / 38200) loss: 2.303610\n",
      "(Epoch 45 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 17201 / 38200) loss: 2.303628\n",
      "(Iteration 17301 / 38200) loss: 2.303624\n",
      "(Iteration 17401 / 38200) loss: 2.303618\n",
      "(Iteration 17501 / 38200) loss: 2.303625\n",
      "(Epoch 46 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 17601 / 38200) loss: 2.303586\n",
      "(Iteration 17701 / 38200) loss: 2.303611\n",
      "(Iteration 17801 / 38200) loss: 2.303584\n",
      "(Iteration 17901 / 38200) loss: 2.303599\n",
      "(Epoch 47 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 18001 / 38200) loss: 2.303610\n",
      "(Iteration 18101 / 38200) loss: 2.303584\n",
      "(Iteration 18201 / 38200) loss: 2.303618\n",
      "(Iteration 18301 / 38200) loss: 2.303587\n",
      "(Epoch 48 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 18401 / 38200) loss: 2.303576\n",
      "(Iteration 18501 / 38200) loss: 2.303588\n",
      "(Iteration 18601 / 38200) loss: 2.303598\n",
      "(Iteration 18701 / 38200) loss: 2.303578\n",
      "(Epoch 49 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 18801 / 38200) loss: 2.303608\n",
      "(Iteration 18901 / 38200) loss: 2.303588\n",
      "(Iteration 19001 / 38200) loss: 2.303603\n",
      "(Epoch 50 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 19101 / 38200) loss: 2.303566\n",
      "(Iteration 19201 / 38200) loss: 2.303590\n",
      "(Iteration 19301 / 38200) loss: 2.303564\n",
      "(Iteration 19401 / 38200) loss: 2.303640\n",
      "(Epoch 51 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 19501 / 38200) loss: 2.303552\n",
      "(Iteration 19601 / 38200) loss: 2.303577\n",
      "(Iteration 19701 / 38200) loss: 2.303558\n",
      "(Iteration 19801 / 38200) loss: 2.303568\n",
      "(Epoch 52 / 100) train acc: 0.120000; val_acc: 0.078000\n",
      "(Iteration 19901 / 38200) loss: 2.303587\n",
      "(Iteration 20001 / 38200) loss: 2.303559\n",
      "(Iteration 20101 / 38200) loss: 2.303609\n",
      "(Iteration 20201 / 38200) loss: 2.303610\n",
      "(Epoch 53 / 100) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 20301 / 38200) loss: 2.303580\n",
      "(Iteration 20401 / 38200) loss: 2.303576\n",
      "(Iteration 20501 / 38200) loss: 2.303631\n",
      "(Iteration 20601 / 38200) loss: 2.303609\n",
      "(Epoch 54 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 20701 / 38200) loss: 2.303588\n",
      "(Iteration 20801 / 38200) loss: 2.303594\n",
      "(Iteration 20901 / 38200) loss: 2.303540\n",
      "(Iteration 21001 / 38200) loss: 2.303551\n",
      "(Epoch 55 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 21101 / 38200) loss: 2.303595\n",
      "(Iteration 21201 / 38200) loss: 2.303596\n",
      "(Iteration 21301 / 38200) loss: 2.303591\n",
      "(Epoch 56 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 21401 / 38200) loss: 2.303590\n",
      "(Iteration 21501 / 38200) loss: 2.303587\n",
      "(Iteration 21601 / 38200) loss: 2.303596\n",
      "(Iteration 21701 / 38200) loss: 2.303550\n",
      "(Epoch 57 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 21801 / 38200) loss: 2.303620\n",
      "(Iteration 21901 / 38200) loss: 2.303612\n",
      "(Iteration 22001 / 38200) loss: 2.303618\n",
      "(Iteration 22101 / 38200) loss: 2.303569\n",
      "(Epoch 58 / 100) train acc: 0.117000; val_acc: 0.078000\n",
      "(Iteration 22201 / 38200) loss: 2.303594\n",
      "(Iteration 22301 / 38200) loss: 2.303576\n",
      "(Iteration 22401 / 38200) loss: 2.303551\n",
      "(Iteration 22501 / 38200) loss: 2.303602\n",
      "(Epoch 59 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 22601 / 38200) loss: 2.303577\n",
      "(Iteration 22701 / 38200) loss: 2.303567\n",
      "(Iteration 22801 / 38200) loss: 2.303583\n",
      "(Iteration 22901 / 38200) loss: 2.303585\n",
      "(Epoch 60 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 23001 / 38200) loss: 2.303548\n",
      "(Iteration 23101 / 38200) loss: 2.303611\n",
      "(Iteration 23201 / 38200) loss: 2.303560\n",
      "(Iteration 23301 / 38200) loss: 2.303542\n",
      "(Epoch 61 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 23401 / 38200) loss: 2.303543\n",
      "(Iteration 23501 / 38200) loss: 2.303595\n",
      "(Iteration 23601 / 38200) loss: 2.303565\n",
      "(Epoch 62 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 23701 / 38200) loss: 2.303533\n",
      "(Iteration 23801 / 38200) loss: 2.303569\n",
      "(Iteration 23901 / 38200) loss: 2.303610\n",
      "(Iteration 24001 / 38200) loss: 2.303591\n",
      "(Epoch 63 / 100) train acc: 0.115000; val_acc: 0.078000\n",
      "(Iteration 24101 / 38200) loss: 2.303526\n",
      "(Iteration 24201 / 38200) loss: 2.303551\n",
      "(Iteration 24301 / 38200) loss: 2.303534\n",
      "(Iteration 24401 / 38200) loss: 2.303554\n",
      "(Epoch 64 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 24501 / 38200) loss: 2.303580\n",
      "(Iteration 24601 / 38200) loss: 2.303579\n",
      "(Iteration 24701 / 38200) loss: 2.303542\n",
      "(Iteration 24801 / 38200) loss: 2.303561\n",
      "(Epoch 65 / 100) train acc: 0.123000; val_acc: 0.078000\n",
      "(Iteration 24901 / 38200) loss: 2.303567\n",
      "(Iteration 25001 / 38200) loss: 2.303527\n",
      "(Iteration 25101 / 38200) loss: 2.303536\n",
      "(Iteration 25201 / 38200) loss: 2.303541\n",
      "(Epoch 66 / 100) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 25301 / 38200) loss: 2.303566\n",
      "(Iteration 25401 / 38200) loss: 2.303555\n",
      "(Iteration 25501 / 38200) loss: 2.303558\n",
      "(Epoch 67 / 100) train acc: 0.116000; val_acc: 0.078000\n",
      "(Iteration 25601 / 38200) loss: 2.303536\n",
      "(Iteration 25701 / 38200) loss: 2.303526\n",
      "(Iteration 25801 / 38200) loss: 2.303570\n",
      "(Iteration 25901 / 38200) loss: 2.303533\n",
      "(Epoch 68 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 26001 / 38200) loss: 2.303576\n",
      "(Iteration 26101 / 38200) loss: 2.303560\n",
      "(Iteration 26201 / 38200) loss: 2.303554\n",
      "(Iteration 26301 / 38200) loss: 2.303563\n",
      "(Epoch 69 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 26401 / 38200) loss: 2.303572\n",
      "(Iteration 26501 / 38200) loss: 2.303616\n",
      "(Iteration 26601 / 38200) loss: 2.303609\n",
      "(Iteration 26701 / 38200) loss: 2.303561\n",
      "(Epoch 70 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 26801 / 38200) loss: 2.303529\n",
      "(Iteration 26901 / 38200) loss: 2.303562\n",
      "(Iteration 27001 / 38200) loss: 2.303536\n",
      "(Iteration 27101 / 38200) loss: 2.303566\n",
      "(Epoch 71 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 27201 / 38200) loss: 2.303591\n",
      "(Iteration 27301 / 38200) loss: 2.303522\n",
      "(Iteration 27401 / 38200) loss: 2.303537\n",
      "(Iteration 27501 / 38200) loss: 2.303570\n",
      "(Epoch 72 / 100) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 27601 / 38200) loss: 2.303573\n",
      "(Iteration 27701 / 38200) loss: 2.303551\n",
      "(Iteration 27801 / 38200) loss: 2.303528\n",
      "(Epoch 73 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 27901 / 38200) loss: 2.303582\n",
      "(Iteration 28001 / 38200) loss: 2.303552\n",
      "(Iteration 28101 / 38200) loss: 2.303519\n",
      "(Iteration 28201 / 38200) loss: 2.303528\n",
      "(Epoch 74 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 28301 / 38200) loss: 2.303567\n",
      "(Iteration 28401 / 38200) loss: 2.303554\n",
      "(Iteration 28501 / 38200) loss: 2.303562\n",
      "(Iteration 28601 / 38200) loss: 2.303549\n",
      "(Epoch 75 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 28701 / 38200) loss: 2.303539\n",
      "(Iteration 28801 / 38200) loss: 2.303581\n",
      "(Iteration 28901 / 38200) loss: 2.303561\n",
      "(Iteration 29001 / 38200) loss: 2.303595\n",
      "(Epoch 76 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 29101 / 38200) loss: 2.303553\n",
      "(Iteration 29201 / 38200) loss: 2.303526\n",
      "(Iteration 29301 / 38200) loss: 2.303575\n",
      "(Iteration 29401 / 38200) loss: 2.303521\n",
      "(Epoch 77 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 29501 / 38200) loss: 2.303573\n",
      "(Iteration 29601 / 38200) loss: 2.303547\n",
      "(Iteration 29701 / 38200) loss: 2.303534\n",
      "(Epoch 78 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 29801 / 38200) loss: 2.303567\n",
      "(Iteration 29901 / 38200) loss: 2.303544\n",
      "(Iteration 30001 / 38200) loss: 2.303546\n",
      "(Iteration 30101 / 38200) loss: 2.303564\n",
      "(Epoch 79 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 30201 / 38200) loss: 2.303564\n",
      "(Iteration 30301 / 38200) loss: 2.303569\n",
      "(Iteration 30401 / 38200) loss: 2.303540\n",
      "(Iteration 30501 / 38200) loss: 2.303546\n",
      "(Epoch 80 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 30601 / 38200) loss: 2.303568\n",
      "(Iteration 30701 / 38200) loss: 2.303534\n",
      "(Iteration 30801 / 38200) loss: 2.303558\n",
      "(Iteration 30901 / 38200) loss: 2.303543\n",
      "(Epoch 81 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 31001 / 38200) loss: 2.303560\n",
      "(Iteration 31101 / 38200) loss: 2.303538\n",
      "(Iteration 31201 / 38200) loss: 2.303534\n",
      "(Iteration 31301 / 38200) loss: 2.303518\n",
      "(Epoch 82 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 31401 / 38200) loss: 2.303549\n",
      "(Iteration 31501 / 38200) loss: 2.303548\n",
      "(Iteration 31601 / 38200) loss: 2.303521\n",
      "(Iteration 31701 / 38200) loss: 2.303543\n",
      "(Epoch 83 / 100) train acc: 0.111000; val_acc: 0.078000\n",
      "(Iteration 31801 / 38200) loss: 2.303570\n",
      "(Iteration 31901 / 38200) loss: 2.303556\n",
      "(Iteration 32001 / 38200) loss: 2.303531\n",
      "(Epoch 84 / 100) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 32101 / 38200) loss: 2.303534\n",
      "(Iteration 32201 / 38200) loss: 2.303560\n",
      "(Iteration 32301 / 38200) loss: 2.303584\n",
      "(Iteration 32401 / 38200) loss: 2.303548\n",
      "(Epoch 85 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 32501 / 38200) loss: 2.303546\n",
      "(Iteration 32601 / 38200) loss: 2.303590\n",
      "(Iteration 32701 / 38200) loss: 2.303556\n",
      "(Iteration 32801 / 38200) loss: 2.303546\n",
      "(Epoch 86 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 32901 / 38200) loss: 2.303567\n",
      "(Iteration 33001 / 38200) loss: 2.303533\n",
      "(Iteration 33101 / 38200) loss: 2.303531\n",
      "(Iteration 33201 / 38200) loss: 2.303553\n",
      "(Epoch 87 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 33301 / 38200) loss: 2.303534\n",
      "(Iteration 33401 / 38200) loss: 2.303553\n",
      "(Iteration 33501 / 38200) loss: 2.303542\n",
      "(Iteration 33601 / 38200) loss: 2.303498\n",
      "(Epoch 88 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 33701 / 38200) loss: 2.303540\n",
      "(Iteration 33801 / 38200) loss: 2.303593\n",
      "(Iteration 33901 / 38200) loss: 2.303540\n",
      "(Epoch 89 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 34001 / 38200) loss: 2.303578\n",
      "(Iteration 34101 / 38200) loss: 2.303528\n",
      "(Iteration 34201 / 38200) loss: 2.303506\n",
      "(Iteration 34301 / 38200) loss: 2.303568\n",
      "(Epoch 90 / 100) train acc: 0.082000; val_acc: 0.078000\n",
      "(Iteration 34401 / 38200) loss: 2.303509\n",
      "(Iteration 34501 / 38200) loss: 2.303527\n",
      "(Iteration 34601 / 38200) loss: 2.303532\n",
      "(Iteration 34701 / 38200) loss: 2.303545\n",
      "(Epoch 91 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 34801 / 38200) loss: 2.303585\n",
      "(Iteration 34901 / 38200) loss: 2.303477\n",
      "(Iteration 35001 / 38200) loss: 2.303548\n",
      "(Iteration 35101 / 38200) loss: 2.303523\n",
      "(Epoch 92 / 100) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 35201 / 38200) loss: 2.303556\n",
      "(Iteration 35301 / 38200) loss: 2.303555\n",
      "(Iteration 35401 / 38200) loss: 2.303550\n",
      "(Iteration 35501 / 38200) loss: 2.303540\n",
      "(Epoch 93 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 35601 / 38200) loss: 2.303548\n",
      "(Iteration 35701 / 38200) loss: 2.303542\n",
      "(Iteration 35801 / 38200) loss: 2.303540\n",
      "(Iteration 35901 / 38200) loss: 2.303510\n",
      "(Epoch 94 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 36001 / 38200) loss: 2.303569\n",
      "(Iteration 36101 / 38200) loss: 2.303492\n",
      "(Iteration 36201 / 38200) loss: 2.303535\n",
      "(Epoch 95 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 36301 / 38200) loss: 2.303542\n",
      "(Iteration 36401 / 38200) loss: 2.303549\n",
      "(Iteration 36501 / 38200) loss: 2.303548\n",
      "(Iteration 36601 / 38200) loss: 2.303570\n",
      "(Epoch 96 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 36701 / 38200) loss: 2.303555\n",
      "(Iteration 36801 / 38200) loss: 2.303541\n",
      "(Iteration 36901 / 38200) loss: 2.303583\n",
      "(Iteration 37001 / 38200) loss: 2.303556\n",
      "(Epoch 97 / 100) train acc: 0.114000; val_acc: 0.078000\n",
      "(Iteration 37101 / 38200) loss: 2.303567\n",
      "(Iteration 37201 / 38200) loss: 2.303550\n",
      "(Iteration 37301 / 38200) loss: 2.303558\n",
      "(Iteration 37401 / 38200) loss: 2.303553\n",
      "(Epoch 98 / 100) train acc: 0.127000; val_acc: 0.078000\n",
      "(Iteration 37501 / 38200) loss: 2.303556\n",
      "(Iteration 37601 / 38200) loss: 2.303555\n",
      "(Iteration 37701 / 38200) loss: 2.303590\n",
      "(Iteration 37801 / 38200) loss: 2.303574\n",
      "(Epoch 99 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 37901 / 38200) loss: 2.303501\n",
      "(Iteration 38001 / 38200) loss: 2.303539\n",
      "(Iteration 38101 / 38200) loss: 2.303560\n",
      "(Epoch 100 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.305412\n",
      "(Epoch 0 / 100) train acc: 0.068000; val_acc: 0.075000\n",
      "(Iteration 101 / 76500) loss: 2.305361\n",
      "(Iteration 201 / 76500) loss: 2.305317\n",
      "(Iteration 301 / 76500) loss: 2.305303\n",
      "(Iteration 401 / 76500) loss: 2.305249\n",
      "(Iteration 501 / 76500) loss: 2.305221\n",
      "(Iteration 601 / 76500) loss: 2.305176\n",
      "(Iteration 701 / 76500) loss: 2.305147\n",
      "(Epoch 1 / 100) train acc: 0.097000; val_acc: 0.096000\n",
      "(Iteration 801 / 76500) loss: 2.305120\n",
      "(Iteration 901 / 76500) loss: 2.305085\n",
      "(Iteration 1001 / 76500) loss: 2.305077\n",
      "(Iteration 1101 / 76500) loss: 2.305018\n",
      "(Iteration 1201 / 76500) loss: 2.304986\n",
      "(Iteration 1301 / 76500) loss: 2.304956\n",
      "(Iteration 1401 / 76500) loss: 2.304915\n",
      "(Iteration 1501 / 76500) loss: 2.304907\n",
      "(Epoch 2 / 100) train acc: 0.094000; val_acc: 0.105000\n",
      "(Iteration 1601 / 76500) loss: 2.304856\n",
      "(Iteration 1701 / 76500) loss: 2.304833\n",
      "(Iteration 1801 / 76500) loss: 2.304812\n",
      "(Iteration 1901 / 76500) loss: 2.304789\n",
      "(Iteration 2001 / 76500) loss: 2.304755\n",
      "(Iteration 2101 / 76500) loss: 2.304781\n",
      "(Iteration 2201 / 76500) loss: 2.304709\n",
      "(Epoch 3 / 100) train acc: 0.099000; val_acc: 0.110000\n",
      "(Iteration 2301 / 76500) loss: 2.304714\n",
      "(Iteration 2401 / 76500) loss: 2.304675\n",
      "(Iteration 2501 / 76500) loss: 2.304659\n",
      "(Iteration 2601 / 76500) loss: 2.304658\n",
      "(Iteration 2701 / 76500) loss: 2.304585\n",
      "(Iteration 2801 / 76500) loss: 2.304602\n",
      "(Iteration 2901 / 76500) loss: 2.304553\n",
      "(Iteration 3001 / 76500) loss: 2.304540\n",
      "(Epoch 4 / 100) train acc: 0.124000; val_acc: 0.107000\n",
      "(Iteration 3101 / 76500) loss: 2.304523\n",
      "(Iteration 3201 / 76500) loss: 2.304509\n",
      "(Iteration 3301 / 76500) loss: 2.304513\n",
      "(Iteration 3401 / 76500) loss: 2.304481\n",
      "(Iteration 3501 / 76500) loss: 2.304449\n",
      "(Iteration 3601 / 76500) loss: 2.304427\n",
      "(Iteration 3701 / 76500) loss: 2.304446\n",
      "(Iteration 3801 / 76500) loss: 2.304437\n",
      "(Epoch 5 / 100) train acc: 0.107000; val_acc: 0.113000\n",
      "(Iteration 3901 / 76500) loss: 2.304365\n",
      "(Iteration 4001 / 76500) loss: 2.304362\n",
      "(Iteration 4101 / 76500) loss: 2.304374\n",
      "(Iteration 4201 / 76500) loss: 2.304338\n",
      "(Iteration 4301 / 76500) loss: 2.304322\n",
      "(Iteration 4401 / 76500) loss: 2.304295\n",
      "(Iteration 4501 / 76500) loss: 2.304306\n",
      "(Epoch 6 / 100) train acc: 0.109000; val_acc: 0.108000\n",
      "(Iteration 4601 / 76500) loss: 2.304303\n",
      "(Iteration 4701 / 76500) loss: 2.304296\n",
      "(Iteration 4801 / 76500) loss: 2.304282\n",
      "(Iteration 4901 / 76500) loss: 2.304198\n",
      "(Iteration 5001 / 76500) loss: 2.304224\n",
      "(Iteration 5101 / 76500) loss: 2.304234\n",
      "(Iteration 5201 / 76500) loss: 2.304208\n",
      "(Iteration 5301 / 76500) loss: 2.304195\n",
      "(Epoch 7 / 100) train acc: 0.104000; val_acc: 0.107000\n",
      "(Iteration 5401 / 76500) loss: 2.304240\n",
      "(Iteration 5501 / 76500) loss: 2.304152\n",
      "(Iteration 5601 / 76500) loss: 2.304208\n",
      "(Iteration 5701 / 76500) loss: 2.304181\n",
      "(Iteration 5801 / 76500) loss: 2.304173\n",
      "(Iteration 5901 / 76500) loss: 2.304146\n",
      "(Iteration 6001 / 76500) loss: 2.304148\n",
      "(Iteration 6101 / 76500) loss: 2.304084\n",
      "(Epoch 8 / 100) train acc: 0.113000; val_acc: 0.099000\n",
      "(Iteration 6201 / 76500) loss: 2.304081\n",
      "(Iteration 6301 / 76500) loss: 2.304106\n",
      "(Iteration 6401 / 76500) loss: 2.304109\n",
      "(Iteration 6501 / 76500) loss: 2.304075\n",
      "(Iteration 6601 / 76500) loss: 2.304103\n",
      "(Iteration 6701 / 76500) loss: 2.304082\n",
      "(Iteration 6801 / 76500) loss: 2.304108\n",
      "(Epoch 9 / 100) train acc: 0.112000; val_acc: 0.079000\n",
      "(Iteration 6901 / 76500) loss: 2.304057\n",
      "(Iteration 7001 / 76500) loss: 2.304044\n",
      "(Iteration 7101 / 76500) loss: 2.304033\n",
      "(Iteration 7201 / 76500) loss: 2.304040\n",
      "(Iteration 7301 / 76500) loss: 2.304009\n",
      "(Iteration 7401 / 76500) loss: 2.304017\n",
      "(Iteration 7501 / 76500) loss: 2.303994\n",
      "(Iteration 7601 / 76500) loss: 2.303967\n",
      "(Epoch 10 / 100) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 7701 / 76500) loss: 2.304001\n",
      "(Iteration 7801 / 76500) loss: 2.304000\n",
      "(Iteration 7901 / 76500) loss: 2.303962\n",
      "(Iteration 8001 / 76500) loss: 2.303972\n",
      "(Iteration 8101 / 76500) loss: 2.303956\n",
      "(Iteration 8201 / 76500) loss: 2.303958\n",
      "(Iteration 8301 / 76500) loss: 2.303920\n",
      "(Iteration 8401 / 76500) loss: 2.303948\n",
      "(Epoch 11 / 100) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 8501 / 76500) loss: 2.303925\n",
      "(Iteration 8601 / 76500) loss: 2.303939\n",
      "(Iteration 8701 / 76500) loss: 2.303947\n",
      "(Iteration 8801 / 76500) loss: 2.303923\n",
      "(Iteration 8901 / 76500) loss: 2.303915\n",
      "(Iteration 9001 / 76500) loss: 2.303877\n",
      "(Iteration 9101 / 76500) loss: 2.303884\n",
      "(Epoch 12 / 100) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 9201 / 76500) loss: 2.303876\n",
      "(Iteration 9301 / 76500) loss: 2.303885\n",
      "(Iteration 9401 / 76500) loss: 2.303855\n",
      "(Iteration 9501 / 76500) loss: 2.303927\n",
      "(Iteration 9601 / 76500) loss: 2.303902\n",
      "(Iteration 9701 / 76500) loss: 2.303876\n",
      "(Iteration 9801 / 76500) loss: 2.303873\n",
      "(Iteration 9901 / 76500) loss: 2.303834\n",
      "(Epoch 13 / 100) train acc: 0.089000; val_acc: 0.079000\n",
      "(Iteration 10001 / 76500) loss: 2.303863\n",
      "(Iteration 10101 / 76500) loss: 2.303864\n",
      "(Iteration 10201 / 76500) loss: 2.303874\n",
      "(Iteration 10301 / 76500) loss: 2.303816\n",
      "(Iteration 10401 / 76500) loss: 2.303858\n",
      "(Iteration 10501 / 76500) loss: 2.303793\n",
      "(Iteration 10601 / 76500) loss: 2.303794\n",
      "(Iteration 10701 / 76500) loss: 2.303797\n",
      "(Epoch 14 / 100) train acc: 0.084000; val_acc: 0.079000\n",
      "(Iteration 10801 / 76500) loss: 2.303819\n",
      "(Iteration 10901 / 76500) loss: 2.303801\n",
      "(Iteration 11001 / 76500) loss: 2.303810\n",
      "(Iteration 11101 / 76500) loss: 2.303770\n",
      "(Iteration 11201 / 76500) loss: 2.303793\n",
      "(Iteration 11301 / 76500) loss: 2.303847\n",
      "(Iteration 11401 / 76500) loss: 2.303782\n",
      "(Epoch 15 / 100) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 11501 / 76500) loss: 2.303799\n",
      "(Iteration 11601 / 76500) loss: 2.303773\n",
      "(Iteration 11701 / 76500) loss: 2.303748\n",
      "(Iteration 11801 / 76500) loss: 2.303788\n",
      "(Iteration 11901 / 76500) loss: 2.303774\n",
      "(Iteration 12001 / 76500) loss: 2.303744\n",
      "(Iteration 12101 / 76500) loss: 2.303776\n",
      "(Iteration 12201 / 76500) loss: 2.303771\n",
      "(Epoch 16 / 100) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 12301 / 76500) loss: 2.303743\n",
      "(Iteration 12401 / 76500) loss: 2.303766\n",
      "(Iteration 12501 / 76500) loss: 2.303765\n",
      "(Iteration 12601 / 76500) loss: 2.303780\n",
      "(Iteration 12701 / 76500) loss: 2.303729\n",
      "(Iteration 12801 / 76500) loss: 2.303730\n",
      "(Iteration 12901 / 76500) loss: 2.303773\n",
      "(Iteration 13001 / 76500) loss: 2.303735\n",
      "(Epoch 17 / 100) train acc: 0.092000; val_acc: 0.079000\n",
      "(Iteration 13101 / 76500) loss: 2.303755\n",
      "(Iteration 13201 / 76500) loss: 2.303700\n",
      "(Iteration 13301 / 76500) loss: 2.303741\n",
      "(Iteration 13401 / 76500) loss: 2.303698\n",
      "(Iteration 13501 / 76500) loss: 2.303746\n",
      "(Iteration 13601 / 76500) loss: 2.303748\n",
      "(Iteration 13701 / 76500) loss: 2.303752\n",
      "(Epoch 18 / 100) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 13801 / 76500) loss: 2.303662\n",
      "(Iteration 13901 / 76500) loss: 2.303738\n",
      "(Iteration 14001 / 76500) loss: 2.303768\n",
      "(Iteration 14101 / 76500) loss: 2.303686\n",
      "(Iteration 14201 / 76500) loss: 2.303687\n",
      "(Iteration 14301 / 76500) loss: 2.303698\n",
      "(Iteration 14401 / 76500) loss: 2.303727\n",
      "(Iteration 14501 / 76500) loss: 2.303736\n",
      "(Epoch 19 / 100) train acc: 0.109000; val_acc: 0.079000\n",
      "(Iteration 14601 / 76500) loss: 2.303722\n",
      "(Iteration 14701 / 76500) loss: 2.303683\n",
      "(Iteration 14801 / 76500) loss: 2.303652\n",
      "(Iteration 14901 / 76500) loss: 2.303700\n",
      "(Iteration 15001 / 76500) loss: 2.303670\n",
      "(Iteration 15101 / 76500) loss: 2.303704\n",
      "(Iteration 15201 / 76500) loss: 2.303699\n",
      "(Epoch 20 / 100) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 15301 / 76500) loss: 2.303666\n",
      "(Iteration 15401 / 76500) loss: 2.303697\n",
      "(Iteration 15501 / 76500) loss: 2.303715\n",
      "(Iteration 15601 / 76500) loss: 2.303711\n",
      "(Iteration 15701 / 76500) loss: 2.303621\n",
      "(Iteration 15801 / 76500) loss: 2.303684\n",
      "(Iteration 15901 / 76500) loss: 2.303664\n",
      "(Iteration 16001 / 76500) loss: 2.303636\n",
      "(Epoch 21 / 100) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 16101 / 76500) loss: 2.303687\n",
      "(Iteration 16201 / 76500) loss: 2.303668\n",
      "(Iteration 16301 / 76500) loss: 2.303648\n",
      "(Iteration 16401 / 76500) loss: 2.303624\n",
      "(Iteration 16501 / 76500) loss: 2.303685\n",
      "(Iteration 16601 / 76500) loss: 2.303644\n",
      "(Iteration 16701 / 76500) loss: 2.303621\n",
      "(Iteration 16801 / 76500) loss: 2.303654\n",
      "(Epoch 22 / 100) train acc: 0.082000; val_acc: 0.079000\n",
      "(Iteration 16901 / 76500) loss: 2.303614\n",
      "(Iteration 17001 / 76500) loss: 2.303628\n",
      "(Iteration 17101 / 76500) loss: 2.303670\n",
      "(Iteration 17201 / 76500) loss: 2.303656\n",
      "(Iteration 17301 / 76500) loss: 2.303731\n",
      "(Iteration 17401 / 76500) loss: 2.303620\n",
      "(Iteration 17501 / 76500) loss: 2.303627\n",
      "(Epoch 23 / 100) train acc: 0.085000; val_acc: 0.079000\n",
      "(Iteration 17601 / 76500) loss: 2.303645\n",
      "(Iteration 17701 / 76500) loss: 2.303599\n",
      "(Iteration 17801 / 76500) loss: 2.303641\n",
      "(Iteration 17901 / 76500) loss: 2.303655\n",
      "(Iteration 18001 / 76500) loss: 2.303645\n",
      "(Iteration 18101 / 76500) loss: 2.303661\n",
      "(Iteration 18201 / 76500) loss: 2.303659\n",
      "(Iteration 18301 / 76500) loss: 2.303646\n",
      "(Epoch 24 / 100) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 18401 / 76500) loss: 2.303691\n",
      "(Iteration 18501 / 76500) loss: 2.303639\n",
      "(Iteration 18601 / 76500) loss: 2.303597\n",
      "(Iteration 18701 / 76500) loss: 2.303662\n",
      "(Iteration 18801 / 76500) loss: 2.303623\n",
      "(Iteration 18901 / 76500) loss: 2.303604\n",
      "(Iteration 19001 / 76500) loss: 2.303624\n",
      "(Iteration 19101 / 76500) loss: 2.303624\n",
      "(Epoch 25 / 100) train acc: 0.090000; val_acc: 0.079000\n",
      "(Iteration 19201 / 76500) loss: 2.303667\n",
      "(Iteration 19301 / 76500) loss: 2.303650\n",
      "(Iteration 19401 / 76500) loss: 2.303586\n",
      "(Iteration 19501 / 76500) loss: 2.303634\n",
      "(Iteration 19601 / 76500) loss: 2.303623\n",
      "(Iteration 19701 / 76500) loss: 2.303602\n",
      "(Iteration 19801 / 76500) loss: 2.303632\n",
      "(Epoch 26 / 100) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 19901 / 76500) loss: 2.303614\n",
      "(Iteration 20001 / 76500) loss: 2.303590\n",
      "(Iteration 20101 / 76500) loss: 2.303595\n",
      "(Iteration 20201 / 76500) loss: 2.303598\n",
      "(Iteration 20301 / 76500) loss: 2.303652\n",
      "(Iteration 20401 / 76500) loss: 2.303606\n",
      "(Iteration 20501 / 76500) loss: 2.303669\n",
      "(Iteration 20601 / 76500) loss: 2.303663\n",
      "(Epoch 27 / 100) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 20701 / 76500) loss: 2.303575\n",
      "(Iteration 20801 / 76500) loss: 2.303622\n",
      "(Iteration 20901 / 76500) loss: 2.303614\n",
      "(Iteration 21001 / 76500) loss: 2.303574\n",
      "(Iteration 21101 / 76500) loss: 2.303636\n",
      "(Iteration 21201 / 76500) loss: 2.303594\n",
      "(Iteration 21301 / 76500) loss: 2.303638\n",
      "(Iteration 21401 / 76500) loss: 2.303653\n",
      "(Epoch 28 / 100) train acc: 0.117000; val_acc: 0.079000\n",
      "(Iteration 21501 / 76500) loss: 2.303643\n",
      "(Iteration 21601 / 76500) loss: 2.303610\n",
      "(Iteration 21701 / 76500) loss: 2.303569\n",
      "(Iteration 21801 / 76500) loss: 2.303627\n",
      "(Iteration 21901 / 76500) loss: 2.303607\n",
      "(Iteration 22001 / 76500) loss: 2.303618\n",
      "(Iteration 22101 / 76500) loss: 2.303599\n",
      "(Epoch 29 / 100) train acc: 0.109000; val_acc: 0.079000\n",
      "(Iteration 22201 / 76500) loss: 2.303529\n",
      "(Iteration 22301 / 76500) loss: 2.303657\n",
      "(Iteration 22401 / 76500) loss: 2.303577\n",
      "(Iteration 22501 / 76500) loss: 2.303593\n",
      "(Iteration 22601 / 76500) loss: 2.303608\n",
      "(Iteration 22701 / 76500) loss: 2.303561\n",
      "(Iteration 22801 / 76500) loss: 2.303594\n",
      "(Iteration 22901 / 76500) loss: 2.303588\n",
      "(Epoch 30 / 100) train acc: 0.089000; val_acc: 0.079000\n",
      "(Iteration 23001 / 76500) loss: 2.303554\n",
      "(Iteration 23101 / 76500) loss: 2.303620\n",
      "(Iteration 23201 / 76500) loss: 2.303564\n",
      "(Iteration 23301 / 76500) loss: 2.303595\n",
      "(Iteration 23401 / 76500) loss: 2.303597\n",
      "(Iteration 23501 / 76500) loss: 2.303546\n",
      "(Iteration 23601 / 76500) loss: 2.303619\n",
      "(Iteration 23701 / 76500) loss: 2.303575\n",
      "(Epoch 31 / 100) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 23801 / 76500) loss: 2.303595\n",
      "(Iteration 23901 / 76500) loss: 2.303580\n",
      "(Iteration 24001 / 76500) loss: 2.303554\n",
      "(Iteration 24101 / 76500) loss: 2.303569\n",
      "(Iteration 24201 / 76500) loss: 2.303594\n",
      "(Iteration 24301 / 76500) loss: 2.303586\n",
      "(Iteration 24401 / 76500) loss: 2.303565\n",
      "(Epoch 32 / 100) train acc: 0.114000; val_acc: 0.079000\n",
      "(Iteration 24501 / 76500) loss: 2.303590\n",
      "(Iteration 24601 / 76500) loss: 2.303620\n",
      "(Iteration 24701 / 76500) loss: 2.303625\n",
      "(Iteration 24801 / 76500) loss: 2.303567\n",
      "(Iteration 24901 / 76500) loss: 2.303605\n",
      "(Iteration 25001 / 76500) loss: 2.303583\n",
      "(Iteration 25101 / 76500) loss: 2.303603\n",
      "(Iteration 25201 / 76500) loss: 2.303563\n",
      "(Epoch 33 / 100) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 25301 / 76500) loss: 2.303548\n",
      "(Iteration 25401 / 76500) loss: 2.303600\n",
      "(Iteration 25501 / 76500) loss: 2.303574\n",
      "(Iteration 25601 / 76500) loss: 2.303603\n",
      "(Iteration 25701 / 76500) loss: 2.303562\n",
      "(Iteration 25801 / 76500) loss: 2.303589\n",
      "(Iteration 25901 / 76500) loss: 2.303550\n",
      "(Iteration 26001 / 76500) loss: 2.303562\n",
      "(Epoch 34 / 100) train acc: 0.093000; val_acc: 0.079000\n",
      "(Iteration 26101 / 76500) loss: 2.303554\n",
      "(Iteration 26201 / 76500) loss: 2.303608\n",
      "(Iteration 26301 / 76500) loss: 2.303529\n",
      "(Iteration 26401 / 76500) loss: 2.303570\n",
      "(Iteration 26501 / 76500) loss: 2.303591\n",
      "(Iteration 26601 / 76500) loss: 2.303582\n",
      "(Iteration 26701 / 76500) loss: 2.303594\n",
      "(Epoch 35 / 100) train acc: 0.109000; val_acc: 0.079000\n",
      "(Iteration 26801 / 76500) loss: 2.303611\n",
      "(Iteration 26901 / 76500) loss: 2.303560\n",
      "(Iteration 27001 / 76500) loss: 2.303586\n",
      "(Iteration 27101 / 76500) loss: 2.303591\n",
      "(Iteration 27201 / 76500) loss: 2.303590\n",
      "(Iteration 27301 / 76500) loss: 2.303592\n",
      "(Iteration 27401 / 76500) loss: 2.303569\n",
      "(Iteration 27501 / 76500) loss: 2.303531\n",
      "(Epoch 36 / 100) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 27601 / 76500) loss: 2.303569\n",
      "(Iteration 27701 / 76500) loss: 2.303557\n",
      "(Iteration 27801 / 76500) loss: 2.303561\n",
      "(Iteration 27901 / 76500) loss: 2.303572\n",
      "(Iteration 28001 / 76500) loss: 2.303541\n",
      "(Iteration 28101 / 76500) loss: 2.303550\n",
      "(Iteration 28201 / 76500) loss: 2.303577\n",
      "(Iteration 28301 / 76500) loss: 2.303619\n",
      "(Epoch 37 / 100) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 28401 / 76500) loss: 2.303584\n",
      "(Iteration 28501 / 76500) loss: 2.303488\n",
      "(Iteration 28601 / 76500) loss: 2.303575\n",
      "(Iteration 28701 / 76500) loss: 2.303575\n",
      "(Iteration 28801 / 76500) loss: 2.303568\n",
      "(Iteration 28901 / 76500) loss: 2.303577\n",
      "(Iteration 29001 / 76500) loss: 2.303583\n",
      "(Epoch 38 / 100) train acc: 0.114000; val_acc: 0.079000\n",
      "(Iteration 29101 / 76500) loss: 2.303570\n",
      "(Iteration 29201 / 76500) loss: 2.303602\n",
      "(Iteration 29301 / 76500) loss: 2.303599\n",
      "(Iteration 29401 / 76500) loss: 2.303511\n",
      "(Iteration 29501 / 76500) loss: 2.303582\n",
      "(Iteration 29601 / 76500) loss: 2.303553\n",
      "(Iteration 29701 / 76500) loss: 2.303548\n",
      "(Iteration 29801 / 76500) loss: 2.303615\n",
      "(Epoch 39 / 100) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 29901 / 76500) loss: 2.303605\n",
      "(Iteration 30001 / 76500) loss: 2.303578\n",
      "(Iteration 30101 / 76500) loss: 2.303539\n",
      "(Iteration 30201 / 76500) loss: 2.303571\n",
      "(Iteration 30301 / 76500) loss: 2.303591\n",
      "(Iteration 30401 / 76500) loss: 2.303561\n",
      "(Iteration 30501 / 76500) loss: 2.303561\n",
      "(Epoch 40 / 100) train acc: 0.083000; val_acc: 0.079000\n",
      "(Iteration 30601 / 76500) loss: 2.303570\n",
      "(Iteration 30701 / 76500) loss: 2.303580\n",
      "(Iteration 30801 / 76500) loss: 2.303509\n",
      "(Iteration 30901 / 76500) loss: 2.303625\n",
      "(Iteration 31001 / 76500) loss: 2.303585\n",
      "(Iteration 31101 / 76500) loss: 2.303571\n",
      "(Iteration 31201 / 76500) loss: 2.303488\n",
      "(Iteration 31301 / 76500) loss: 2.303537\n",
      "(Epoch 41 / 100) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 31401 / 76500) loss: 2.303603\n",
      "(Iteration 31501 / 76500) loss: 2.303497\n",
      "(Iteration 31601 / 76500) loss: 2.303531\n",
      "(Iteration 31701 / 76500) loss: 2.303558\n",
      "(Iteration 31801 / 76500) loss: 2.303490\n",
      "(Iteration 31901 / 76500) loss: 2.303540\n",
      "(Iteration 32001 / 76500) loss: 2.303564\n",
      "(Iteration 32101 / 76500) loss: 2.303591\n",
      "(Epoch 42 / 100) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 32201 / 76500) loss: 2.303560\n",
      "(Iteration 32301 / 76500) loss: 2.303535\n",
      "(Iteration 32401 / 76500) loss: 2.303563\n",
      "(Iteration 32501 / 76500) loss: 2.303577\n",
      "(Iteration 32601 / 76500) loss: 2.303535\n",
      "(Iteration 32701 / 76500) loss: 2.303560\n",
      "(Iteration 32801 / 76500) loss: 2.303562\n",
      "(Epoch 43 / 100) train acc: 0.093000; val_acc: 0.079000\n",
      "(Iteration 32901 / 76500) loss: 2.303559\n",
      "(Iteration 33001 / 76500) loss: 2.303511\n",
      "(Iteration 33101 / 76500) loss: 2.303547\n",
      "(Iteration 33201 / 76500) loss: 2.303579\n",
      "(Iteration 33301 / 76500) loss: 2.303597\n",
      "(Iteration 33401 / 76500) loss: 2.303528\n",
      "(Iteration 33501 / 76500) loss: 2.303556\n",
      "(Iteration 33601 / 76500) loss: 2.303568\n",
      "(Epoch 44 / 100) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 33701 / 76500) loss: 2.303594\n",
      "(Iteration 33801 / 76500) loss: 2.303557\n",
      "(Iteration 33901 / 76500) loss: 2.303545\n",
      "(Iteration 34001 / 76500) loss: 2.303590\n",
      "(Iteration 34101 / 76500) loss: 2.303587\n",
      "(Iteration 34201 / 76500) loss: 2.303562\n",
      "(Iteration 34301 / 76500) loss: 2.303575\n",
      "(Iteration 34401 / 76500) loss: 2.303529\n",
      "(Epoch 45 / 100) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 34501 / 76500) loss: 2.303565\n",
      "(Iteration 34601 / 76500) loss: 2.303542\n",
      "(Iteration 34701 / 76500) loss: 2.303556\n",
      "(Iteration 34801 / 76500) loss: 2.303579\n",
      "(Iteration 34901 / 76500) loss: 2.303629\n",
      "(Iteration 35001 / 76500) loss: 2.303612\n",
      "(Iteration 35101 / 76500) loss: 2.303575\n",
      "(Epoch 46 / 100) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 35201 / 76500) loss: 2.303573\n",
      "(Iteration 35301 / 76500) loss: 2.303590\n",
      "(Iteration 35401 / 76500) loss: 2.303576\n",
      "(Iteration 35501 / 76500) loss: 2.303547\n",
      "(Iteration 35601 / 76500) loss: 2.303534\n",
      "(Iteration 35701 / 76500) loss: 2.303508\n",
      "(Iteration 35801 / 76500) loss: 2.303514\n",
      "(Iteration 35901 / 76500) loss: 2.303526\n",
      "(Epoch 47 / 100) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 36001 / 76500) loss: 2.303562\n",
      "(Iteration 36101 / 76500) loss: 2.303505\n",
      "(Iteration 36201 / 76500) loss: 2.303583\n",
      "(Iteration 36301 / 76500) loss: 2.303557\n",
      "(Iteration 36401 / 76500) loss: 2.303520\n",
      "(Iteration 36501 / 76500) loss: 2.303550\n",
      "(Iteration 36601 / 76500) loss: 2.303574\n",
      "(Iteration 36701 / 76500) loss: 2.303613\n",
      "(Epoch 48 / 100) train acc: 0.085000; val_acc: 0.079000\n",
      "(Iteration 36801 / 76500) loss: 2.303552\n",
      "(Iteration 36901 / 76500) loss: 2.303614\n",
      "(Iteration 37001 / 76500) loss: 2.303532\n",
      "(Iteration 37101 / 76500) loss: 2.303612\n",
      "(Iteration 37201 / 76500) loss: 2.303533\n",
      "(Iteration 37301 / 76500) loss: 2.303560\n",
      "(Iteration 37401 / 76500) loss: 2.303573\n",
      "(Epoch 49 / 100) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 37501 / 76500) loss: 2.303559\n",
      "(Iteration 37601 / 76500) loss: 2.303553\n",
      "(Iteration 37701 / 76500) loss: 2.303524\n",
      "(Iteration 37801 / 76500) loss: 2.303561\n",
      "(Iteration 37901 / 76500) loss: 2.303589\n",
      "(Iteration 38001 / 76500) loss: 2.303598\n",
      "(Iteration 38101 / 76500) loss: 2.303579\n",
      "(Iteration 38201 / 76500) loss: 2.303546\n",
      "(Epoch 50 / 100) train acc: 0.085000; val_acc: 0.079000\n",
      "(Iteration 38301 / 76500) loss: 2.303567\n",
      "(Iteration 38401 / 76500) loss: 2.303518\n",
      "(Iteration 38501 / 76500) loss: 2.303570\n",
      "(Iteration 38601 / 76500) loss: 2.303557\n",
      "(Iteration 38701 / 76500) loss: 2.303526\n",
      "(Iteration 38801 / 76500) loss: 2.303570\n",
      "(Iteration 38901 / 76500) loss: 2.303553\n",
      "(Iteration 39001 / 76500) loss: 2.303554\n",
      "(Epoch 51 / 100) train acc: 0.087000; val_acc: 0.079000\n",
      "(Iteration 39101 / 76500) loss: 2.303557\n",
      "(Iteration 39201 / 76500) loss: 2.303510\n",
      "(Iteration 39301 / 76500) loss: 2.303480\n",
      "(Iteration 39401 / 76500) loss: 2.303559\n",
      "(Iteration 39501 / 76500) loss: 2.303549\n",
      "(Iteration 39601 / 76500) loss: 2.303550\n",
      "(Iteration 39701 / 76500) loss: 2.303554\n",
      "(Epoch 52 / 100) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 39801 / 76500) loss: 2.303533\n",
      "(Iteration 39901 / 76500) loss: 2.303596\n",
      "(Iteration 40001 / 76500) loss: 2.303555\n",
      "(Iteration 40101 / 76500) loss: 2.303582\n",
      "(Iteration 40201 / 76500) loss: 2.303563\n",
      "(Iteration 40301 / 76500) loss: 2.303538\n",
      "(Iteration 40401 / 76500) loss: 2.303567\n",
      "(Iteration 40501 / 76500) loss: 2.303548\n",
      "(Epoch 53 / 100) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 40601 / 76500) loss: 2.303537\n",
      "(Iteration 40701 / 76500) loss: 2.303547\n",
      "(Iteration 40801 / 76500) loss: 2.303546\n",
      "(Iteration 40901 / 76500) loss: 2.303533\n",
      "(Iteration 41001 / 76500) loss: 2.303488\n",
      "(Iteration 41101 / 76500) loss: 2.303561\n",
      "(Iteration 41201 / 76500) loss: 2.303554\n",
      "(Iteration 41301 / 76500) loss: 2.303558\n",
      "(Epoch 54 / 100) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 41401 / 76500) loss: 2.303528\n",
      "(Iteration 41501 / 76500) loss: 2.303514\n",
      "(Iteration 41601 / 76500) loss: 2.303602\n",
      "(Iteration 41701 / 76500) loss: 2.303581\n",
      "(Iteration 41801 / 76500) loss: 2.303561\n",
      "(Iteration 41901 / 76500) loss: 2.303514\n",
      "(Iteration 42001 / 76500) loss: 2.303524\n",
      "(Epoch 55 / 100) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 42101 / 76500) loss: 2.303638\n",
      "(Iteration 42201 / 76500) loss: 2.303559\n",
      "(Iteration 42301 / 76500) loss: 2.303536\n",
      "(Iteration 42401 / 76500) loss: 2.303600\n",
      "(Iteration 42501 / 76500) loss: 2.303523\n",
      "(Iteration 42601 / 76500) loss: 2.303547\n",
      "(Iteration 42701 / 76500) loss: 2.303569\n",
      "(Iteration 42801 / 76500) loss: 2.303541\n",
      "(Epoch 56 / 100) train acc: 0.110000; val_acc: 0.079000\n",
      "(Iteration 42901 / 76500) loss: 2.303516\n",
      "(Iteration 43001 / 76500) loss: 2.303516\n",
      "(Iteration 43101 / 76500) loss: 2.303593\n",
      "(Iteration 43201 / 76500) loss: 2.303539\n",
      "(Iteration 43301 / 76500) loss: 2.303513\n",
      "(Iteration 43401 / 76500) loss: 2.303511\n",
      "(Iteration 43501 / 76500) loss: 2.303591\n",
      "(Iteration 43601 / 76500) loss: 2.303541\n",
      "(Epoch 57 / 100) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 43701 / 76500) loss: 2.303569\n",
      "(Iteration 43801 / 76500) loss: 2.303613\n",
      "(Iteration 43901 / 76500) loss: 2.303560\n",
      "(Iteration 44001 / 76500) loss: 2.303550\n",
      "(Iteration 44101 / 76500) loss: 2.303528\n",
      "(Iteration 44201 / 76500) loss: 2.303517\n",
      "(Iteration 44301 / 76500) loss: 2.303578\n",
      "(Epoch 58 / 100) train acc: 0.109000; val_acc: 0.079000\n",
      "(Iteration 44401 / 76500) loss: 2.303546\n",
      "(Iteration 44501 / 76500) loss: 2.303553\n",
      "(Iteration 44601 / 76500) loss: 2.303533\n",
      "(Iteration 44701 / 76500) loss: 2.303526\n",
      "(Iteration 44801 / 76500) loss: 2.303541\n",
      "(Iteration 44901 / 76500) loss: 2.303513\n",
      "(Iteration 45001 / 76500) loss: 2.303597\n",
      "(Iteration 45101 / 76500) loss: 2.303581\n",
      "(Epoch 59 / 100) train acc: 0.091000; val_acc: 0.079000\n",
      "(Iteration 45201 / 76500) loss: 2.303585\n",
      "(Iteration 45301 / 76500) loss: 2.303574\n",
      "(Iteration 45401 / 76500) loss: 2.303556\n",
      "(Iteration 45501 / 76500) loss: 2.303507\n",
      "(Iteration 45601 / 76500) loss: 2.303513\n",
      "(Iteration 45701 / 76500) loss: 2.303560\n",
      "(Iteration 45801 / 76500) loss: 2.303542\n",
      "(Epoch 60 / 100) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 45901 / 76500) loss: 2.303553\n",
      "(Iteration 46001 / 76500) loss: 2.303572\n",
      "(Iteration 46101 / 76500) loss: 2.303598\n",
      "(Iteration 46201 / 76500) loss: 2.303544\n",
      "(Iteration 46301 / 76500) loss: 2.303614\n",
      "(Iteration 46401 / 76500) loss: 2.303634\n",
      "(Iteration 46501 / 76500) loss: 2.303566\n",
      "(Iteration 46601 / 76500) loss: 2.303510\n",
      "(Epoch 61 / 100) train acc: 0.089000; val_acc: 0.079000\n",
      "(Iteration 46701 / 76500) loss: 2.303522\n",
      "(Iteration 46801 / 76500) loss: 2.303597\n",
      "(Iteration 46901 / 76500) loss: 2.303546\n",
      "(Iteration 47001 / 76500) loss: 2.303564\n",
      "(Iteration 47101 / 76500) loss: 2.303536\n",
      "(Iteration 47201 / 76500) loss: 2.303573\n",
      "(Iteration 47301 / 76500) loss: 2.303546\n",
      "(Iteration 47401 / 76500) loss: 2.303544\n",
      "(Epoch 62 / 100) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 47501 / 76500) loss: 2.303521\n",
      "(Iteration 47601 / 76500) loss: 2.303535\n",
      "(Iteration 47701 / 76500) loss: 2.303561\n",
      "(Iteration 47801 / 76500) loss: 2.303566\n",
      "(Iteration 47901 / 76500) loss: 2.303551\n",
      "(Iteration 48001 / 76500) loss: 2.303548\n",
      "(Iteration 48101 / 76500) loss: 2.303590\n",
      "(Epoch 63 / 100) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 48201 / 76500) loss: 2.303580\n",
      "(Iteration 48301 / 76500) loss: 2.303490\n",
      "(Iteration 48401 / 76500) loss: 2.303551\n",
      "(Iteration 48501 / 76500) loss: 2.303522\n",
      "(Iteration 48601 / 76500) loss: 2.303546\n",
      "(Iteration 48701 / 76500) loss: 2.303537\n",
      "(Iteration 48801 / 76500) loss: 2.303581\n",
      "(Iteration 48901 / 76500) loss: 2.303544\n",
      "(Epoch 64 / 100) train acc: 0.092000; val_acc: 0.079000\n",
      "(Iteration 49001 / 76500) loss: 2.303561\n",
      "(Iteration 49101 / 76500) loss: 2.303542\n",
      "(Iteration 49201 / 76500) loss: 2.303534\n",
      "(Iteration 49301 / 76500) loss: 2.303564\n",
      "(Iteration 49401 / 76500) loss: 2.303599\n",
      "(Iteration 49501 / 76500) loss: 2.303560\n",
      "(Iteration 49601 / 76500) loss: 2.303572\n",
      "(Iteration 49701 / 76500) loss: 2.303569\n",
      "(Epoch 65 / 100) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 49801 / 76500) loss: 2.303557\n",
      "(Iteration 49901 / 76500) loss: 2.303548\n",
      "(Iteration 50001 / 76500) loss: 2.303559\n",
      "(Iteration 50101 / 76500) loss: 2.303541\n",
      "(Iteration 50201 / 76500) loss: 2.303574\n",
      "(Iteration 50301 / 76500) loss: 2.303540\n",
      "(Iteration 50401 / 76500) loss: 2.303505\n",
      "(Epoch 66 / 100) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 50501 / 76500) loss: 2.303561\n",
      "(Iteration 50601 / 76500) loss: 2.303551\n",
      "(Iteration 50701 / 76500) loss: 2.303570\n",
      "(Iteration 50801 / 76500) loss: 2.303534\n",
      "(Iteration 50901 / 76500) loss: 2.303579\n",
      "(Iteration 51001 / 76500) loss: 2.303554\n",
      "(Iteration 51101 / 76500) loss: 2.303553\n",
      "(Iteration 51201 / 76500) loss: 2.303570\n",
      "(Epoch 67 / 100) train acc: 0.112000; val_acc: 0.079000\n",
      "(Iteration 51301 / 76500) loss: 2.303528\n",
      "(Iteration 51401 / 76500) loss: 2.303503\n",
      "(Iteration 51501 / 76500) loss: 2.303539\n",
      "(Iteration 51601 / 76500) loss: 2.303575\n",
      "(Iteration 51701 / 76500) loss: 2.303486\n",
      "(Iteration 51801 / 76500) loss: 2.303576\n",
      "(Iteration 51901 / 76500) loss: 2.303524\n",
      "(Iteration 52001 / 76500) loss: 2.303588\n",
      "(Epoch 68 / 100) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 52101 / 76500) loss: 2.303556\n",
      "(Iteration 52201 / 76500) loss: 2.303569\n",
      "(Iteration 52301 / 76500) loss: 2.303563\n",
      "(Iteration 52401 / 76500) loss: 2.303564\n",
      "(Iteration 52501 / 76500) loss: 2.303527\n",
      "(Iteration 52601 / 76500) loss: 2.303546\n",
      "(Iteration 52701 / 76500) loss: 2.303564\n",
      "(Epoch 69 / 100) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 52801 / 76500) loss: 2.303497\n",
      "(Iteration 52901 / 76500) loss: 2.303568\n",
      "(Iteration 53001 / 76500) loss: 2.303540\n",
      "(Iteration 53101 / 76500) loss: 2.303519\n",
      "(Iteration 53201 / 76500) loss: 2.303552\n",
      "(Iteration 53301 / 76500) loss: 2.303588\n",
      "(Iteration 53401 / 76500) loss: 2.303522\n",
      "(Iteration 53501 / 76500) loss: 2.303499\n",
      "(Epoch 70 / 100) train acc: 0.090000; val_acc: 0.079000\n",
      "(Iteration 53601 / 76500) loss: 2.303544\n",
      "(Iteration 53701 / 76500) loss: 2.303520\n",
      "(Iteration 53801 / 76500) loss: 2.303515\n",
      "(Iteration 53901 / 76500) loss: 2.303544\n",
      "(Iteration 54001 / 76500) loss: 2.303560\n",
      "(Iteration 54101 / 76500) loss: 2.303552\n",
      "(Iteration 54201 / 76500) loss: 2.303524\n",
      "(Iteration 54301 / 76500) loss: 2.303552\n",
      "(Epoch 71 / 100) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 54401 / 76500) loss: 2.303598\n",
      "(Iteration 54501 / 76500) loss: 2.303592\n",
      "(Iteration 54601 / 76500) loss: 2.303511\n",
      "(Iteration 54701 / 76500) loss: 2.303569\n",
      "(Iteration 54801 / 76500) loss: 2.303490\n",
      "(Iteration 54901 / 76500) loss: 2.303529\n",
      "(Iteration 55001 / 76500) loss: 2.303580\n",
      "(Epoch 72 / 100) train acc: 0.087000; val_acc: 0.079000\n",
      "(Iteration 55101 / 76500) loss: 2.303559\n",
      "(Iteration 55201 / 76500) loss: 2.303501\n",
      "(Iteration 55301 / 76500) loss: 2.303528\n",
      "(Iteration 55401 / 76500) loss: 2.303587\n",
      "(Iteration 55501 / 76500) loss: 2.303516\n",
      "(Iteration 55601 / 76500) loss: 2.303590\n",
      "(Iteration 55701 / 76500) loss: 2.303576\n",
      "(Iteration 55801 / 76500) loss: 2.303547\n",
      "(Epoch 73 / 100) train acc: 0.087000; val_acc: 0.079000\n",
      "(Iteration 55901 / 76500) loss: 2.303525\n",
      "(Iteration 56001 / 76500) loss: 2.303550\n",
      "(Iteration 56101 / 76500) loss: 2.303497\n",
      "(Iteration 56201 / 76500) loss: 2.303552\n",
      "(Iteration 56301 / 76500) loss: 2.303578\n",
      "(Iteration 56401 / 76500) loss: 2.303556\n",
      "(Iteration 56501 / 76500) loss: 2.303512\n",
      "(Iteration 56601 / 76500) loss: 2.303558\n",
      "(Epoch 74 / 100) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 56701 / 76500) loss: 2.303568\n",
      "(Iteration 56801 / 76500) loss: 2.303515\n",
      "(Iteration 56901 / 76500) loss: 2.303573\n",
      "(Iteration 57001 / 76500) loss: 2.303531\n",
      "(Iteration 57101 / 76500) loss: 2.303528\n",
      "(Iteration 57201 / 76500) loss: 2.303508\n",
      "(Iteration 57301 / 76500) loss: 2.303546\n",
      "(Epoch 75 / 100) train acc: 0.093000; val_acc: 0.079000\n",
      "(Iteration 57401 / 76500) loss: 2.303539\n",
      "(Iteration 57501 / 76500) loss: 2.303545\n",
      "(Iteration 57601 / 76500) loss: 2.303525\n",
      "(Iteration 57701 / 76500) loss: 2.303550\n",
      "(Iteration 57801 / 76500) loss: 2.303542\n",
      "(Iteration 57901 / 76500) loss: 2.303476\n",
      "(Iteration 58001 / 76500) loss: 2.303599\n",
      "(Iteration 58101 / 76500) loss: 2.303535\n",
      "(Epoch 76 / 100) train acc: 0.113000; val_acc: 0.079000\n",
      "(Iteration 58201 / 76500) loss: 2.303552\n",
      "(Iteration 58301 / 76500) loss: 2.303572\n",
      "(Iteration 58401 / 76500) loss: 2.303568\n",
      "(Iteration 58501 / 76500) loss: 2.303551\n",
      "(Iteration 58601 / 76500) loss: 2.303563\n",
      "(Iteration 58701 / 76500) loss: 2.303532\n",
      "(Iteration 58801 / 76500) loss: 2.303591\n",
      "(Iteration 58901 / 76500) loss: 2.303556\n",
      "(Epoch 77 / 100) train acc: 0.122000; val_acc: 0.079000\n",
      "(Iteration 59001 / 76500) loss: 2.303562\n",
      "(Iteration 59101 / 76500) loss: 2.303572\n",
      "(Iteration 59201 / 76500) loss: 2.303536\n",
      "(Iteration 59301 / 76500) loss: 2.303515\n",
      "(Iteration 59401 / 76500) loss: 2.303522\n",
      "(Iteration 59501 / 76500) loss: 2.303559\n",
      "(Iteration 59601 / 76500) loss: 2.303541\n",
      "(Epoch 78 / 100) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 59701 / 76500) loss: 2.303577\n",
      "(Iteration 59801 / 76500) loss: 2.303514\n",
      "(Iteration 59901 / 76500) loss: 2.303541\n",
      "(Iteration 60001 / 76500) loss: 2.303563\n",
      "(Iteration 60101 / 76500) loss: 2.303498\n",
      "(Iteration 60201 / 76500) loss: 2.303552\n",
      "(Iteration 60301 / 76500) loss: 2.303615\n",
      "(Iteration 60401 / 76500) loss: 2.303585\n",
      "(Epoch 79 / 100) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 60501 / 76500) loss: 2.303569\n",
      "(Iteration 60601 / 76500) loss: 2.303492\n",
      "(Iteration 60701 / 76500) loss: 2.303584\n",
      "(Iteration 60801 / 76500) loss: 2.303614\n",
      "(Iteration 60901 / 76500) loss: 2.303553\n",
      "(Iteration 61001 / 76500) loss: 2.303619\n",
      "(Iteration 61101 / 76500) loss: 2.303544\n",
      "(Epoch 80 / 100) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 61201 / 76500) loss: 2.303536\n",
      "(Iteration 61301 / 76500) loss: 2.303546\n",
      "(Iteration 61401 / 76500) loss: 2.303602\n",
      "(Iteration 61501 / 76500) loss: 2.303577\n",
      "(Iteration 61601 / 76500) loss: 2.303520\n",
      "(Iteration 61701 / 76500) loss: 2.303557\n",
      "(Iteration 61801 / 76500) loss: 2.303568\n",
      "(Iteration 61901 / 76500) loss: 2.303532\n",
      "(Epoch 81 / 100) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 62001 / 76500) loss: 2.303528\n",
      "(Iteration 62101 / 76500) loss: 2.303555\n",
      "(Iteration 62201 / 76500) loss: 2.303579\n",
      "(Iteration 62301 / 76500) loss: 2.303536\n",
      "(Iteration 62401 / 76500) loss: 2.303508\n",
      "(Iteration 62501 / 76500) loss: 2.303538\n",
      "(Iteration 62601 / 76500) loss: 2.303592\n",
      "(Iteration 62701 / 76500) loss: 2.303567\n",
      "(Epoch 82 / 100) train acc: 0.113000; val_acc: 0.079000\n",
      "(Iteration 62801 / 76500) loss: 2.303558\n",
      "(Iteration 62901 / 76500) loss: 2.303572\n",
      "(Iteration 63001 / 76500) loss: 2.303547\n",
      "(Iteration 63101 / 76500) loss: 2.303535\n",
      "(Iteration 63201 / 76500) loss: 2.303576\n",
      "(Iteration 63301 / 76500) loss: 2.303544\n",
      "(Iteration 63401 / 76500) loss: 2.303522\n",
      "(Epoch 83 / 100) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 63501 / 76500) loss: 2.303551\n",
      "(Iteration 63601 / 76500) loss: 2.303559\n",
      "(Iteration 63701 / 76500) loss: 2.303535\n",
      "(Iteration 63801 / 76500) loss: 2.303542\n",
      "(Iteration 63901 / 76500) loss: 2.303540\n",
      "(Iteration 64001 / 76500) loss: 2.303549\n",
      "(Iteration 64101 / 76500) loss: 2.303512\n",
      "(Iteration 64201 / 76500) loss: 2.303564\n",
      "(Epoch 84 / 100) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 64301 / 76500) loss: 2.303572\n",
      "(Iteration 64401 / 76500) loss: 2.303504\n",
      "(Iteration 64501 / 76500) loss: 2.303572\n",
      "(Iteration 64601 / 76500) loss: 2.303592\n",
      "(Iteration 64701 / 76500) loss: 2.303548\n",
      "(Iteration 64801 / 76500) loss: 2.303604\n",
      "(Iteration 64901 / 76500) loss: 2.303585\n",
      "(Iteration 65001 / 76500) loss: 2.303504\n",
      "(Epoch 85 / 100) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 65101 / 76500) loss: 2.303534\n",
      "(Iteration 65201 / 76500) loss: 2.303571\n",
      "(Iteration 65301 / 76500) loss: 2.303477\n",
      "(Iteration 65401 / 76500) loss: 2.303541\n",
      "(Iteration 65501 / 76500) loss: 2.303572\n",
      "(Iteration 65601 / 76500) loss: 2.303559\n",
      "(Iteration 65701 / 76500) loss: 2.303528\n",
      "(Epoch 86 / 100) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 65801 / 76500) loss: 2.303561\n",
      "(Iteration 65901 / 76500) loss: 2.303555\n",
      "(Iteration 66001 / 76500) loss: 2.303508\n",
      "(Iteration 66101 / 76500) loss: 2.303486\n",
      "(Iteration 66201 / 76500) loss: 2.303575\n",
      "(Iteration 66301 / 76500) loss: 2.303540\n",
      "(Iteration 66401 / 76500) loss: 2.303577\n",
      "(Iteration 66501 / 76500) loss: 2.303543\n",
      "(Epoch 87 / 100) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 66601 / 76500) loss: 2.303545\n",
      "(Iteration 66701 / 76500) loss: 2.303572\n",
      "(Iteration 66801 / 76500) loss: 2.303581\n",
      "(Iteration 66901 / 76500) loss: 2.303588\n",
      "(Iteration 67001 / 76500) loss: 2.303578\n",
      "(Iteration 67101 / 76500) loss: 2.303571\n",
      "(Iteration 67201 / 76500) loss: 2.303567\n",
      "(Iteration 67301 / 76500) loss: 2.303571\n",
      "(Epoch 88 / 100) train acc: 0.093000; val_acc: 0.079000\n",
      "(Iteration 67401 / 76500) loss: 2.303528\n",
      "(Iteration 67501 / 76500) loss: 2.303530\n",
      "(Iteration 67601 / 76500) loss: 2.303599\n",
      "(Iteration 67701 / 76500) loss: 2.303544\n",
      "(Iteration 67801 / 76500) loss: 2.303557\n",
      "(Iteration 67901 / 76500) loss: 2.303515\n",
      "(Iteration 68001 / 76500) loss: 2.303611\n",
      "(Epoch 89 / 100) train acc: 0.086000; val_acc: 0.079000\n",
      "(Iteration 68101 / 76500) loss: 2.303546\n",
      "(Iteration 68201 / 76500) loss: 2.303478\n",
      "(Iteration 68301 / 76500) loss: 2.303558\n",
      "(Iteration 68401 / 76500) loss: 2.303591\n",
      "(Iteration 68501 / 76500) loss: 2.303575\n",
      "(Iteration 68601 / 76500) loss: 2.303610\n",
      "(Iteration 68701 / 76500) loss: 2.303505\n",
      "(Iteration 68801 / 76500) loss: 2.303468\n",
      "(Epoch 90 / 100) train acc: 0.082000; val_acc: 0.079000\n",
      "(Iteration 68901 / 76500) loss: 2.303532\n",
      "(Iteration 69001 / 76500) loss: 2.303546\n",
      "(Iteration 69101 / 76500) loss: 2.303569\n",
      "(Iteration 69201 / 76500) loss: 2.303560\n",
      "(Iteration 69301 / 76500) loss: 2.303594\n",
      "(Iteration 69401 / 76500) loss: 2.303481\n",
      "(Iteration 69501 / 76500) loss: 2.303522\n",
      "(Iteration 69601 / 76500) loss: 2.303584\n",
      "(Epoch 91 / 100) train acc: 0.109000; val_acc: 0.079000\n",
      "(Iteration 69701 / 76500) loss: 2.303577\n",
      "(Iteration 69801 / 76500) loss: 2.303519\n",
      "(Iteration 69901 / 76500) loss: 2.303512\n",
      "(Iteration 70001 / 76500) loss: 2.303511\n",
      "(Iteration 70101 / 76500) loss: 2.303553\n",
      "(Iteration 70201 / 76500) loss: 2.303535\n",
      "(Iteration 70301 / 76500) loss: 2.303527\n",
      "(Epoch 92 / 100) train acc: 0.116000; val_acc: 0.079000\n",
      "(Iteration 70401 / 76500) loss: 2.303563\n",
      "(Iteration 70501 / 76500) loss: 2.303573\n",
      "(Iteration 70601 / 76500) loss: 2.303488\n",
      "(Iteration 70701 / 76500) loss: 2.303530\n",
      "(Iteration 70801 / 76500) loss: 2.303547\n",
      "(Iteration 70901 / 76500) loss: 2.303585\n",
      "(Iteration 71001 / 76500) loss: 2.303571\n",
      "(Iteration 71101 / 76500) loss: 2.303553\n",
      "(Epoch 93 / 100) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 71201 / 76500) loss: 2.303546\n",
      "(Iteration 71301 / 76500) loss: 2.303499\n",
      "(Iteration 71401 / 76500) loss: 2.303548\n",
      "(Iteration 71501 / 76500) loss: 2.303602\n",
      "(Iteration 71601 / 76500) loss: 2.303614\n",
      "(Iteration 71701 / 76500) loss: 2.303494\n",
      "(Iteration 71801 / 76500) loss: 2.303496\n",
      "(Iteration 71901 / 76500) loss: 2.303536\n",
      "(Epoch 94 / 100) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 72001 / 76500) loss: 2.303544\n",
      "(Iteration 72101 / 76500) loss: 2.303501\n",
      "(Iteration 72201 / 76500) loss: 2.303578\n",
      "(Iteration 72301 / 76500) loss: 2.303564\n",
      "(Iteration 72401 / 76500) loss: 2.303531\n",
      "(Iteration 72501 / 76500) loss: 2.303586\n",
      "(Iteration 72601 / 76500) loss: 2.303557\n",
      "(Epoch 95 / 100) train acc: 0.085000; val_acc: 0.079000\n",
      "(Iteration 72701 / 76500) loss: 2.303593\n",
      "(Iteration 72801 / 76500) loss: 2.303532\n",
      "(Iteration 72901 / 76500) loss: 2.303570\n",
      "(Iteration 73001 / 76500) loss: 2.303519\n",
      "(Iteration 73101 / 76500) loss: 2.303505\n",
      "(Iteration 73201 / 76500) loss: 2.303579\n",
      "(Iteration 73301 / 76500) loss: 2.303558\n",
      "(Iteration 73401 / 76500) loss: 2.303515\n",
      "(Epoch 96 / 100) train acc: 0.086000; val_acc: 0.079000\n",
      "(Iteration 73501 / 76500) loss: 2.303535\n",
      "(Iteration 73601 / 76500) loss: 2.303529\n",
      "(Iteration 73701 / 76500) loss: 2.303583\n",
      "(Iteration 73801 / 76500) loss: 2.303543\n",
      "(Iteration 73901 / 76500) loss: 2.303547\n",
      "(Iteration 74001 / 76500) loss: 2.303612\n",
      "(Iteration 74101 / 76500) loss: 2.303548\n",
      "(Iteration 74201 / 76500) loss: 2.303523\n",
      "(Epoch 97 / 100) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 74301 / 76500) loss: 2.303538\n",
      "(Iteration 74401 / 76500) loss: 2.303639\n",
      "(Iteration 74501 / 76500) loss: 2.303515\n",
      "(Iteration 74601 / 76500) loss: 2.303513\n",
      "(Iteration 74701 / 76500) loss: 2.303501\n",
      "(Iteration 74801 / 76500) loss: 2.303574\n",
      "(Iteration 74901 / 76500) loss: 2.303577\n",
      "(Epoch 98 / 100) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 75001 / 76500) loss: 2.303555\n",
      "(Iteration 75101 / 76500) loss: 2.303565\n",
      "(Iteration 75201 / 76500) loss: 2.303583\n",
      "(Iteration 75301 / 76500) loss: 2.303544\n",
      "(Iteration 75401 / 76500) loss: 2.303545\n",
      "(Iteration 75501 / 76500) loss: 2.303536\n",
      "(Iteration 75601 / 76500) loss: 2.303548\n",
      "(Iteration 75701 / 76500) loss: 2.303552\n",
      "(Epoch 99 / 100) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 75801 / 76500) loss: 2.303550\n",
      "(Iteration 75901 / 76500) loss: 2.303586\n",
      "(Iteration 76001 / 76500) loss: 2.303553\n",
      "(Iteration 76101 / 76500) loss: 2.303580\n",
      "(Iteration 76201 / 76500) loss: 2.303545\n",
      "(Iteration 76301 / 76500) loss: 2.303561\n",
      "(Iteration 76401 / 76500) loss: 2.303530\n",
      "(Epoch 100 / 100) train acc: 0.081000; val_acc: 0.079000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.305488\n",
      "(Epoch 0 / 100) train acc: 0.086000; val_acc: 0.088000\n",
      "(Iteration 101 / 38200) loss: 2.305436\n",
      "(Iteration 201 / 38200) loss: 2.305388\n",
      "(Iteration 301 / 38200) loss: 2.305362\n",
      "(Epoch 1 / 100) train acc: 0.111000; val_acc: 0.085000\n",
      "(Iteration 401 / 38200) loss: 2.305320\n",
      "(Iteration 501 / 38200) loss: 2.305286\n",
      "(Iteration 601 / 38200) loss: 2.305246\n",
      "(Iteration 701 / 38200) loss: 2.305209\n",
      "(Epoch 2 / 100) train acc: 0.105000; val_acc: 0.093000\n",
      "(Iteration 801 / 38200) loss: 2.305182\n",
      "(Iteration 901 / 38200) loss: 2.305163\n",
      "(Iteration 1001 / 38200) loss: 2.305133\n",
      "(Iteration 1101 / 38200) loss: 2.305101\n",
      "(Epoch 3 / 100) train acc: 0.103000; val_acc: 0.090000\n",
      "(Iteration 1201 / 38200) loss: 2.305059\n",
      "(Iteration 1301 / 38200) loss: 2.305055\n",
      "(Iteration 1401 / 38200) loss: 2.305012\n",
      "(Iteration 1501 / 38200) loss: 2.304989\n",
      "(Epoch 4 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 1601 / 38200) loss: 2.304980\n",
      "(Iteration 1701 / 38200) loss: 2.304953\n",
      "(Iteration 1801 / 38200) loss: 2.304927\n",
      "(Iteration 1901 / 38200) loss: 2.304906\n",
      "(Epoch 5 / 100) train acc: 0.084000; val_acc: 0.086000\n",
      "(Iteration 2001 / 38200) loss: 2.304899\n",
      "(Iteration 2101 / 38200) loss: 2.304856\n",
      "(Iteration 2201 / 38200) loss: 2.304845\n",
      "(Epoch 6 / 100) train acc: 0.078000; val_acc: 0.082000\n",
      "(Iteration 2301 / 38200) loss: 2.304849\n",
      "(Iteration 2401 / 38200) loss: 2.304827\n",
      "(Iteration 2501 / 38200) loss: 2.304814\n",
      "(Iteration 2601 / 38200) loss: 2.304776\n",
      "(Epoch 7 / 100) train acc: 0.076000; val_acc: 0.087000\n",
      "(Iteration 2701 / 38200) loss: 2.304783\n",
      "(Iteration 2801 / 38200) loss: 2.304754\n",
      "(Iteration 2901 / 38200) loss: 2.304739\n",
      "(Iteration 3001 / 38200) loss: 2.304720\n",
      "(Epoch 8 / 100) train acc: 0.090000; val_acc: 0.096000\n",
      "(Iteration 3101 / 38200) loss: 2.304713\n",
      "(Iteration 3201 / 38200) loss: 2.304709\n",
      "(Iteration 3301 / 38200) loss: 2.304688\n",
      "(Iteration 3401 / 38200) loss: 2.304684\n",
      "(Epoch 9 / 100) train acc: 0.099000; val_acc: 0.090000\n",
      "(Iteration 3501 / 38200) loss: 2.304657\n",
      "(Iteration 3601 / 38200) loss: 2.304644\n",
      "(Iteration 3701 / 38200) loss: 2.304626\n",
      "(Iteration 3801 / 38200) loss: 2.304630\n",
      "(Epoch 10 / 100) train acc: 0.089000; val_acc: 0.086000\n",
      "(Iteration 3901 / 38200) loss: 2.304641\n",
      "(Iteration 4001 / 38200) loss: 2.304599\n",
      "(Iteration 4101 / 38200) loss: 2.304601\n",
      "(Iteration 4201 / 38200) loss: 2.304576\n",
      "(Epoch 11 / 100) train acc: 0.099000; val_acc: 0.082000\n",
      "(Iteration 4301 / 38200) loss: 2.304576\n",
      "(Iteration 4401 / 38200) loss: 2.304562\n",
      "(Iteration 4501 / 38200) loss: 2.304564\n",
      "(Epoch 12 / 100) train acc: 0.080000; val_acc: 0.085000\n",
      "(Iteration 4601 / 38200) loss: 2.304554\n",
      "(Iteration 4701 / 38200) loss: 2.304554\n",
      "(Iteration 4801 / 38200) loss: 2.304517\n",
      "(Iteration 4901 / 38200) loss: 2.304515\n",
      "(Epoch 13 / 100) train acc: 0.100000; val_acc: 0.084000\n",
      "(Iteration 5001 / 38200) loss: 2.304542\n",
      "(Iteration 5101 / 38200) loss: 2.304517\n",
      "(Iteration 5201 / 38200) loss: 2.304495\n",
      "(Iteration 5301 / 38200) loss: 2.304526\n",
      "(Epoch 14 / 100) train acc: 0.095000; val_acc: 0.085000\n",
      "(Iteration 5401 / 38200) loss: 2.304496\n",
      "(Iteration 5501 / 38200) loss: 2.304485\n",
      "(Iteration 5601 / 38200) loss: 2.304492\n",
      "(Iteration 5701 / 38200) loss: 2.304476\n",
      "(Epoch 15 / 100) train acc: 0.092000; val_acc: 0.086000\n",
      "(Iteration 5801 / 38200) loss: 2.304468\n",
      "(Iteration 5901 / 38200) loss: 2.304471\n",
      "(Iteration 6001 / 38200) loss: 2.304459\n",
      "(Iteration 6101 / 38200) loss: 2.304448\n",
      "(Epoch 16 / 100) train acc: 0.092000; val_acc: 0.095000\n",
      "(Iteration 6201 / 38200) loss: 2.304456\n",
      "(Iteration 6301 / 38200) loss: 2.304443\n",
      "(Iteration 6401 / 38200) loss: 2.304425\n",
      "(Epoch 17 / 100) train acc: 0.083000; val_acc: 0.090000\n",
      "(Iteration 6501 / 38200) loss: 2.304431\n",
      "(Iteration 6601 / 38200) loss: 2.304418\n",
      "(Iteration 6701 / 38200) loss: 2.304417\n",
      "(Iteration 6801 / 38200) loss: 2.304416\n",
      "(Epoch 18 / 100) train acc: 0.091000; val_acc: 0.095000\n",
      "(Iteration 6901 / 38200) loss: 2.304399\n",
      "(Iteration 7001 / 38200) loss: 2.304414\n",
      "(Iteration 7101 / 38200) loss: 2.304403\n",
      "(Iteration 7201 / 38200) loss: 2.304398\n",
      "(Epoch 19 / 100) train acc: 0.105000; val_acc: 0.093000\n",
      "(Iteration 7301 / 38200) loss: 2.304415\n",
      "(Iteration 7401 / 38200) loss: 2.304410\n",
      "(Iteration 7501 / 38200) loss: 2.304415\n",
      "(Iteration 7601 / 38200) loss: 2.304400\n",
      "(Epoch 20 / 100) train acc: 0.100000; val_acc: 0.094000\n",
      "(Iteration 7701 / 38200) loss: 2.304400\n",
      "(Iteration 7801 / 38200) loss: 2.304376\n",
      "(Iteration 7901 / 38200) loss: 2.304367\n",
      "(Iteration 8001 / 38200) loss: 2.304376\n",
      "(Epoch 21 / 100) train acc: 0.118000; val_acc: 0.096000\n",
      "(Iteration 8101 / 38200) loss: 2.304372\n",
      "(Iteration 8201 / 38200) loss: 2.304360\n",
      "(Iteration 8301 / 38200) loss: 2.304379\n",
      "(Iteration 8401 / 38200) loss: 2.304369\n",
      "(Epoch 22 / 100) train acc: 0.117000; val_acc: 0.094000\n",
      "(Iteration 8501 / 38200) loss: 2.304358\n",
      "(Iteration 8601 / 38200) loss: 2.304372\n",
      "(Iteration 8701 / 38200) loss: 2.304376\n",
      "(Epoch 23 / 100) train acc: 0.096000; val_acc: 0.096000\n",
      "(Iteration 8801 / 38200) loss: 2.304356\n",
      "(Iteration 8901 / 38200) loss: 2.304364\n",
      "(Iteration 9001 / 38200) loss: 2.304348\n",
      "(Iteration 9101 / 38200) loss: 2.304353\n",
      "(Epoch 24 / 100) train acc: 0.089000; val_acc: 0.094000\n",
      "(Iteration 9201 / 38200) loss: 2.304349\n",
      "(Iteration 9301 / 38200) loss: 2.304349\n",
      "(Iteration 9401 / 38200) loss: 2.304333\n",
      "(Iteration 9501 / 38200) loss: 2.304339\n",
      "(Epoch 25 / 100) train acc: 0.098000; val_acc: 0.092000\n",
      "(Iteration 9601 / 38200) loss: 2.304355\n",
      "(Iteration 9701 / 38200) loss: 2.304328\n",
      "(Iteration 9801 / 38200) loss: 2.304345\n",
      "(Iteration 9901 / 38200) loss: 2.304331\n",
      "(Epoch 26 / 100) train acc: 0.097000; val_acc: 0.093000\n",
      "(Iteration 10001 / 38200) loss: 2.304334\n",
      "(Iteration 10101 / 38200) loss: 2.304317\n",
      "(Iteration 10201 / 38200) loss: 2.304339\n",
      "(Iteration 10301 / 38200) loss: 2.304324\n",
      "(Epoch 27 / 100) train acc: 0.108000; val_acc: 0.094000\n",
      "(Iteration 10401 / 38200) loss: 2.304349\n",
      "(Iteration 10501 / 38200) loss: 2.304323\n",
      "(Iteration 10601 / 38200) loss: 2.304325\n",
      "(Epoch 28 / 100) train acc: 0.114000; val_acc: 0.096000\n",
      "(Iteration 10701 / 38200) loss: 2.304335\n",
      "(Iteration 10801 / 38200) loss: 2.304321\n",
      "(Iteration 10901 / 38200) loss: 2.304340\n",
      "(Iteration 11001 / 38200) loss: 2.304318\n",
      "(Epoch 29 / 100) train acc: 0.095000; val_acc: 0.096000\n",
      "(Iteration 11101 / 38200) loss: 2.304314\n",
      "(Iteration 11201 / 38200) loss: 2.304305\n",
      "(Iteration 11301 / 38200) loss: 2.304305\n",
      "(Iteration 11401 / 38200) loss: 2.304303\n",
      "(Epoch 30 / 100) train acc: 0.097000; val_acc: 0.093000\n",
      "(Iteration 11501 / 38200) loss: 2.304317\n",
      "(Iteration 11601 / 38200) loss: 2.304296\n",
      "(Iteration 11701 / 38200) loss: 2.304300\n",
      "(Iteration 11801 / 38200) loss: 2.304318\n",
      "(Epoch 31 / 100) train acc: 0.090000; val_acc: 0.093000\n",
      "(Iteration 11901 / 38200) loss: 2.304309\n",
      "(Iteration 12001 / 38200) loss: 2.304309\n",
      "(Iteration 12101 / 38200) loss: 2.304310\n",
      "(Iteration 12201 / 38200) loss: 2.304287\n",
      "(Epoch 32 / 100) train acc: 0.085000; val_acc: 0.093000\n",
      "(Iteration 12301 / 38200) loss: 2.304306\n",
      "(Iteration 12401 / 38200) loss: 2.304333\n",
      "(Iteration 12501 / 38200) loss: 2.304308\n",
      "(Iteration 12601 / 38200) loss: 2.304300\n",
      "(Epoch 33 / 100) train acc: 0.099000; val_acc: 0.094000\n",
      "(Iteration 12701 / 38200) loss: 2.304299\n",
      "(Iteration 12801 / 38200) loss: 2.304290\n",
      "(Iteration 12901 / 38200) loss: 2.304281\n",
      "(Epoch 34 / 100) train acc: 0.081000; val_acc: 0.090000\n",
      "(Iteration 13001 / 38200) loss: 2.304304\n",
      "(Iteration 13101 / 38200) loss: 2.304306\n",
      "(Iteration 13201 / 38200) loss: 2.304300\n",
      "(Iteration 13301 / 38200) loss: 2.304284\n",
      "(Epoch 35 / 100) train acc: 0.094000; val_acc: 0.094000\n",
      "(Iteration 13401 / 38200) loss: 2.304297\n",
      "(Iteration 13501 / 38200) loss: 2.304304\n",
      "(Iteration 13601 / 38200) loss: 2.304287\n",
      "(Iteration 13701 / 38200) loss: 2.304286\n",
      "(Epoch 36 / 100) train acc: 0.105000; val_acc: 0.089000\n",
      "(Iteration 13801 / 38200) loss: 2.304307\n",
      "(Iteration 13901 / 38200) loss: 2.304303\n",
      "(Iteration 14001 / 38200) loss: 2.304306\n",
      "(Iteration 14101 / 38200) loss: 2.304319\n",
      "(Epoch 37 / 100) train acc: 0.092000; val_acc: 0.090000\n",
      "(Iteration 14201 / 38200) loss: 2.304301\n",
      "(Iteration 14301 / 38200) loss: 2.304299\n",
      "(Iteration 14401 / 38200) loss: 2.304292\n",
      "(Iteration 14501 / 38200) loss: 2.304309\n",
      "(Epoch 38 / 100) train acc: 0.095000; val_acc: 0.092000\n",
      "(Iteration 14601 / 38200) loss: 2.304290\n",
      "(Iteration 14701 / 38200) loss: 2.304293\n",
      "(Iteration 14801 / 38200) loss: 2.304299\n",
      "(Epoch 39 / 100) train acc: 0.102000; val_acc: 0.091000\n",
      "(Iteration 14901 / 38200) loss: 2.304297\n",
      "(Iteration 15001 / 38200) loss: 2.304291\n",
      "(Iteration 15101 / 38200) loss: 2.304296\n",
      "(Iteration 15201 / 38200) loss: 2.304280\n",
      "(Epoch 40 / 100) train acc: 0.108000; val_acc: 0.091000\n",
      "(Iteration 15301 / 38200) loss: 2.304294\n",
      "(Iteration 15401 / 38200) loss: 2.304287\n",
      "(Iteration 15501 / 38200) loss: 2.304292\n",
      "(Iteration 15601 / 38200) loss: 2.304295\n",
      "(Epoch 41 / 100) train acc: 0.091000; val_acc: 0.090000\n",
      "(Iteration 15701 / 38200) loss: 2.304299\n",
      "(Iteration 15801 / 38200) loss: 2.304281\n",
      "(Iteration 15901 / 38200) loss: 2.304292\n",
      "(Iteration 16001 / 38200) loss: 2.304303\n",
      "(Epoch 42 / 100) train acc: 0.113000; val_acc: 0.094000\n",
      "(Iteration 16101 / 38200) loss: 2.304285\n",
      "(Iteration 16201 / 38200) loss: 2.304285\n",
      "(Iteration 16301 / 38200) loss: 2.304297\n",
      "(Iteration 16401 / 38200) loss: 2.304300\n",
      "(Epoch 43 / 100) train acc: 0.101000; val_acc: 0.093000\n",
      "(Iteration 16501 / 38200) loss: 2.304287\n",
      "(Iteration 16601 / 38200) loss: 2.304284\n",
      "(Iteration 16701 / 38200) loss: 2.304275\n",
      "(Iteration 16801 / 38200) loss: 2.304289\n",
      "(Epoch 44 / 100) train acc: 0.108000; val_acc: 0.093000\n",
      "(Iteration 16901 / 38200) loss: 2.304293\n",
      "(Iteration 17001 / 38200) loss: 2.304272\n",
      "(Iteration 17101 / 38200) loss: 2.304276\n",
      "(Epoch 45 / 100) train acc: 0.103000; val_acc: 0.095000\n",
      "(Iteration 17201 / 38200) loss: 2.304277\n",
      "(Iteration 17301 / 38200) loss: 2.304292\n",
      "(Iteration 17401 / 38200) loss: 2.304298\n",
      "(Iteration 17501 / 38200) loss: 2.304285\n",
      "(Epoch 46 / 100) train acc: 0.091000; val_acc: 0.094000\n",
      "(Iteration 17601 / 38200) loss: 2.304280\n",
      "(Iteration 17701 / 38200) loss: 2.304296\n",
      "(Iteration 17801 / 38200) loss: 2.304284\n",
      "(Iteration 17901 / 38200) loss: 2.304267\n",
      "(Epoch 47 / 100) train acc: 0.108000; val_acc: 0.094000\n",
      "(Iteration 18001 / 38200) loss: 2.304274\n",
      "(Iteration 18101 / 38200) loss: 2.304280\n",
      "(Iteration 18201 / 38200) loss: 2.304276\n",
      "(Iteration 18301 / 38200) loss: 2.304285\n",
      "(Epoch 48 / 100) train acc: 0.099000; val_acc: 0.095000\n",
      "(Iteration 18401 / 38200) loss: 2.304275\n",
      "(Iteration 18501 / 38200) loss: 2.304302\n",
      "(Iteration 18601 / 38200) loss: 2.304286\n",
      "(Iteration 18701 / 38200) loss: 2.304283\n",
      "(Epoch 49 / 100) train acc: 0.106000; val_acc: 0.094000\n",
      "(Iteration 18801 / 38200) loss: 2.304277\n",
      "(Iteration 18901 / 38200) loss: 2.304281\n",
      "(Iteration 19001 / 38200) loss: 2.304304\n",
      "(Epoch 50 / 100) train acc: 0.097000; val_acc: 0.094000\n",
      "(Iteration 19101 / 38200) loss: 2.304280\n",
      "(Iteration 19201 / 38200) loss: 2.304278\n",
      "(Iteration 19301 / 38200) loss: 2.304270\n",
      "(Iteration 19401 / 38200) loss: 2.304284\n",
      "(Epoch 51 / 100) train acc: 0.101000; val_acc: 0.095000\n",
      "(Iteration 19501 / 38200) loss: 2.304285\n",
      "(Iteration 19601 / 38200) loss: 2.304291\n",
      "(Iteration 19701 / 38200) loss: 2.304282\n",
      "(Iteration 19801 / 38200) loss: 2.304282\n",
      "(Epoch 52 / 100) train acc: 0.110000; val_acc: 0.094000\n",
      "(Iteration 19901 / 38200) loss: 2.304283\n",
      "(Iteration 20001 / 38200) loss: 2.304278\n",
      "(Iteration 20101 / 38200) loss: 2.304298\n",
      "(Iteration 20201 / 38200) loss: 2.304263\n",
      "(Epoch 53 / 100) train acc: 0.099000; val_acc: 0.096000\n",
      "(Iteration 20301 / 38200) loss: 2.304275\n",
      "(Iteration 20401 / 38200) loss: 2.304286\n",
      "(Iteration 20501 / 38200) loss: 2.304283\n",
      "(Iteration 20601 / 38200) loss: 2.304261\n",
      "(Epoch 54 / 100) train acc: 0.091000; val_acc: 0.094000\n",
      "(Iteration 20701 / 38200) loss: 2.304267\n",
      "(Iteration 20801 / 38200) loss: 2.304295\n",
      "(Iteration 20901 / 38200) loss: 2.304292\n",
      "(Iteration 21001 / 38200) loss: 2.304296\n",
      "(Epoch 55 / 100) train acc: 0.087000; val_acc: 0.094000\n",
      "(Iteration 21101 / 38200) loss: 2.304267\n",
      "(Iteration 21201 / 38200) loss: 2.304279\n",
      "(Iteration 21301 / 38200) loss: 2.304264\n",
      "(Epoch 56 / 100) train acc: 0.104000; val_acc: 0.094000\n",
      "(Iteration 21401 / 38200) loss: 2.304269\n",
      "(Iteration 21501 / 38200) loss: 2.304268\n",
      "(Iteration 21601 / 38200) loss: 2.304275\n",
      "(Iteration 21701 / 38200) loss: 2.304293\n",
      "(Epoch 57 / 100) train acc: 0.097000; val_acc: 0.094000\n",
      "(Iteration 21801 / 38200) loss: 2.304276\n",
      "(Iteration 21901 / 38200) loss: 2.304282\n",
      "(Iteration 22001 / 38200) loss: 2.304285\n",
      "(Iteration 22101 / 38200) loss: 2.304281\n",
      "(Epoch 58 / 100) train acc: 0.108000; val_acc: 0.094000\n",
      "(Iteration 22201 / 38200) loss: 2.304285\n",
      "(Iteration 22301 / 38200) loss: 2.304277\n",
      "(Iteration 22401 / 38200) loss: 2.304267\n",
      "(Iteration 22501 / 38200) loss: 2.304285\n",
      "(Epoch 59 / 100) train acc: 0.117000; val_acc: 0.094000\n",
      "(Iteration 22601 / 38200) loss: 2.304262\n",
      "(Iteration 22701 / 38200) loss: 2.304267\n",
      "(Iteration 22801 / 38200) loss: 2.304284\n",
      "(Iteration 22901 / 38200) loss: 2.304267\n",
      "(Epoch 60 / 100) train acc: 0.074000; val_acc: 0.094000\n",
      "(Iteration 23001 / 38200) loss: 2.304284\n",
      "(Iteration 23101 / 38200) loss: 2.304268\n",
      "(Iteration 23201 / 38200) loss: 2.304282\n",
      "(Iteration 23301 / 38200) loss: 2.304280\n",
      "(Epoch 61 / 100) train acc: 0.109000; val_acc: 0.094000\n",
      "(Iteration 23401 / 38200) loss: 2.304275\n",
      "(Iteration 23501 / 38200) loss: 2.304279\n",
      "(Iteration 23601 / 38200) loss: 2.304272\n",
      "(Epoch 62 / 100) train acc: 0.114000; val_acc: 0.094000\n",
      "(Iteration 23701 / 38200) loss: 2.304274\n",
      "(Iteration 23801 / 38200) loss: 2.304278\n",
      "(Iteration 23901 / 38200) loss: 2.304276\n",
      "(Iteration 24001 / 38200) loss: 2.304291\n",
      "(Epoch 63 / 100) train acc: 0.100000; val_acc: 0.094000\n",
      "(Iteration 24101 / 38200) loss: 2.304271\n",
      "(Iteration 24201 / 38200) loss: 2.304274\n",
      "(Iteration 24301 / 38200) loss: 2.304283\n",
      "(Iteration 24401 / 38200) loss: 2.304268\n",
      "(Epoch 64 / 100) train acc: 0.103000; val_acc: 0.094000\n",
      "(Iteration 24501 / 38200) loss: 2.304277\n",
      "(Iteration 24601 / 38200) loss: 2.304276\n",
      "(Iteration 24701 / 38200) loss: 2.304265\n",
      "(Iteration 24801 / 38200) loss: 2.304271\n",
      "(Epoch 65 / 100) train acc: 0.088000; val_acc: 0.094000\n",
      "(Iteration 24901 / 38200) loss: 2.304282\n",
      "(Iteration 25001 / 38200) loss: 2.304276\n",
      "(Iteration 25101 / 38200) loss: 2.304281\n",
      "(Iteration 25201 / 38200) loss: 2.304273\n",
      "(Epoch 66 / 100) train acc: 0.103000; val_acc: 0.094000\n",
      "(Iteration 25301 / 38200) loss: 2.304282\n",
      "(Iteration 25401 / 38200) loss: 2.304270\n",
      "(Iteration 25501 / 38200) loss: 2.304269\n",
      "(Epoch 67 / 100) train acc: 0.099000; val_acc: 0.094000\n",
      "(Iteration 25601 / 38200) loss: 2.304290\n",
      "(Iteration 25701 / 38200) loss: 2.304293\n",
      "(Iteration 25801 / 38200) loss: 2.304284\n",
      "(Iteration 25901 / 38200) loss: 2.304278\n",
      "(Epoch 68 / 100) train acc: 0.117000; val_acc: 0.094000\n",
      "(Iteration 26001 / 38200) loss: 2.304271\n",
      "(Iteration 26101 / 38200) loss: 2.304261\n",
      "(Iteration 26201 / 38200) loss: 2.304277\n",
      "(Iteration 26301 / 38200) loss: 2.304272\n",
      "(Epoch 69 / 100) train acc: 0.100000; val_acc: 0.094000\n",
      "(Iteration 26401 / 38200) loss: 2.304276\n",
      "(Iteration 26501 / 38200) loss: 2.304268\n",
      "(Iteration 26601 / 38200) loss: 2.304272\n",
      "(Iteration 26701 / 38200) loss: 2.304271\n",
      "(Epoch 70 / 100) train acc: 0.108000; val_acc: 0.094000\n",
      "(Iteration 26801 / 38200) loss: 2.304275\n",
      "(Iteration 26901 / 38200) loss: 2.304285\n",
      "(Iteration 27001 / 38200) loss: 2.304278\n",
      "(Iteration 27101 / 38200) loss: 2.304264\n",
      "(Epoch 71 / 100) train acc: 0.092000; val_acc: 0.094000\n",
      "(Iteration 27201 / 38200) loss: 2.304265\n",
      "(Iteration 27301 / 38200) loss: 2.304272\n",
      "(Iteration 27401 / 38200) loss: 2.304289\n",
      "(Iteration 27501 / 38200) loss: 2.304283\n",
      "(Epoch 72 / 100) train acc: 0.096000; val_acc: 0.094000\n",
      "(Iteration 27601 / 38200) loss: 2.304281\n",
      "(Iteration 27701 / 38200) loss: 2.304265\n",
      "(Iteration 27801 / 38200) loss: 2.304292\n",
      "(Epoch 73 / 100) train acc: 0.124000; val_acc: 0.094000\n",
      "(Iteration 27901 / 38200) loss: 2.304271\n",
      "(Iteration 28001 / 38200) loss: 2.304258\n",
      "(Iteration 28101 / 38200) loss: 2.304279\n",
      "(Iteration 28201 / 38200) loss: 2.304261\n",
      "(Epoch 74 / 100) train acc: 0.088000; val_acc: 0.094000\n",
      "(Iteration 28301 / 38200) loss: 2.304247\n",
      "(Iteration 28401 / 38200) loss: 2.304280\n",
      "(Iteration 28501 / 38200) loss: 2.304255\n",
      "(Iteration 28601 / 38200) loss: 2.304282\n",
      "(Epoch 75 / 100) train acc: 0.116000; val_acc: 0.094000\n",
      "(Iteration 28701 / 38200) loss: 2.304284\n",
      "(Iteration 28801 / 38200) loss: 2.304264\n",
      "(Iteration 28901 / 38200) loss: 2.304273\n",
      "(Iteration 29001 / 38200) loss: 2.304266\n",
      "(Epoch 76 / 100) train acc: 0.122000; val_acc: 0.094000\n",
      "(Iteration 29101 / 38200) loss: 2.304278\n",
      "(Iteration 29201 / 38200) loss: 2.304282\n",
      "(Iteration 29301 / 38200) loss: 2.304289\n",
      "(Iteration 29401 / 38200) loss: 2.304284\n",
      "(Epoch 77 / 100) train acc: 0.111000; val_acc: 0.094000\n",
      "(Iteration 29501 / 38200) loss: 2.304269\n",
      "(Iteration 29601 / 38200) loss: 2.304279\n",
      "(Iteration 29701 / 38200) loss: 2.304300\n",
      "(Epoch 78 / 100) train acc: 0.105000; val_acc: 0.094000\n",
      "(Iteration 29801 / 38200) loss: 2.304305\n",
      "(Iteration 29901 / 38200) loss: 2.304274\n",
      "(Iteration 30001 / 38200) loss: 2.304282\n",
      "(Iteration 30101 / 38200) loss: 2.304295\n",
      "(Epoch 79 / 100) train acc: 0.104000; val_acc: 0.094000\n",
      "(Iteration 30201 / 38200) loss: 2.304289\n",
      "(Iteration 30301 / 38200) loss: 2.304273\n",
      "(Iteration 30401 / 38200) loss: 2.304281\n",
      "(Iteration 30501 / 38200) loss: 2.304284\n",
      "(Epoch 80 / 100) train acc: 0.085000; val_acc: 0.094000\n",
      "(Iteration 30601 / 38200) loss: 2.304267\n",
      "(Iteration 30701 / 38200) loss: 2.304274\n",
      "(Iteration 30801 / 38200) loss: 2.304283\n",
      "(Iteration 30901 / 38200) loss: 2.304283\n",
      "(Epoch 81 / 100) train acc: 0.110000; val_acc: 0.094000\n",
      "(Iteration 31001 / 38200) loss: 2.304280\n",
      "(Iteration 31101 / 38200) loss: 2.304276\n",
      "(Iteration 31201 / 38200) loss: 2.304269\n",
      "(Iteration 31301 / 38200) loss: 2.304283\n",
      "(Epoch 82 / 100) train acc: 0.101000; val_acc: 0.094000\n",
      "(Iteration 31401 / 38200) loss: 2.304249\n",
      "(Iteration 31501 / 38200) loss: 2.304278\n",
      "(Iteration 31601 / 38200) loss: 2.304260\n",
      "(Iteration 31701 / 38200) loss: 2.304291\n",
      "(Epoch 83 / 100) train acc: 0.104000; val_acc: 0.094000\n",
      "(Iteration 31801 / 38200) loss: 2.304255\n",
      "(Iteration 31901 / 38200) loss: 2.304292\n",
      "(Iteration 32001 / 38200) loss: 2.304272\n",
      "(Epoch 84 / 100) train acc: 0.109000; val_acc: 0.094000\n",
      "(Iteration 32101 / 38200) loss: 2.304267\n",
      "(Iteration 32201 / 38200) loss: 2.304293\n",
      "(Iteration 32301 / 38200) loss: 2.304278\n",
      "(Iteration 32401 / 38200) loss: 2.304268\n",
      "(Epoch 85 / 100) train acc: 0.085000; val_acc: 0.094000\n",
      "(Iteration 32501 / 38200) loss: 2.304280\n",
      "(Iteration 32601 / 38200) loss: 2.304283\n",
      "(Iteration 32701 / 38200) loss: 2.304285\n",
      "(Iteration 32801 / 38200) loss: 2.304278\n",
      "(Epoch 86 / 100) train acc: 0.109000; val_acc: 0.094000\n",
      "(Iteration 32901 / 38200) loss: 2.304274\n",
      "(Iteration 33001 / 38200) loss: 2.304286\n",
      "(Iteration 33101 / 38200) loss: 2.304261\n",
      "(Iteration 33201 / 38200) loss: 2.304274\n",
      "(Epoch 87 / 100) train acc: 0.088000; val_acc: 0.094000\n",
      "(Iteration 33301 / 38200) loss: 2.304285\n",
      "(Iteration 33401 / 38200) loss: 2.304277\n",
      "(Iteration 33501 / 38200) loss: 2.304287\n",
      "(Iteration 33601 / 38200) loss: 2.304256\n",
      "(Epoch 88 / 100) train acc: 0.092000; val_acc: 0.094000\n",
      "(Iteration 33701 / 38200) loss: 2.304276\n",
      "(Iteration 33801 / 38200) loss: 2.304281\n",
      "(Iteration 33901 / 38200) loss: 2.304278\n",
      "(Epoch 89 / 100) train acc: 0.102000; val_acc: 0.094000\n",
      "(Iteration 34001 / 38200) loss: 2.304268\n",
      "(Iteration 34101 / 38200) loss: 2.304287\n",
      "(Iteration 34201 / 38200) loss: 2.304277\n",
      "(Iteration 34301 / 38200) loss: 2.304275\n",
      "(Epoch 90 / 100) train acc: 0.094000; val_acc: 0.094000\n",
      "(Iteration 34401 / 38200) loss: 2.304280\n",
      "(Iteration 34501 / 38200) loss: 2.304287\n",
      "(Iteration 34601 / 38200) loss: 2.304286\n",
      "(Iteration 34701 / 38200) loss: 2.304244\n",
      "(Epoch 91 / 100) train acc: 0.108000; val_acc: 0.094000\n",
      "(Iteration 34801 / 38200) loss: 2.304289\n",
      "(Iteration 34901 / 38200) loss: 2.304280\n",
      "(Iteration 35001 / 38200) loss: 2.304265\n",
      "(Iteration 35101 / 38200) loss: 2.304287\n",
      "(Epoch 92 / 100) train acc: 0.099000; val_acc: 0.094000\n",
      "(Iteration 35201 / 38200) loss: 2.304269\n",
      "(Iteration 35301 / 38200) loss: 2.304279\n",
      "(Iteration 35401 / 38200) loss: 2.304276\n",
      "(Iteration 35501 / 38200) loss: 2.304276\n",
      "(Epoch 93 / 100) train acc: 0.096000; val_acc: 0.094000\n",
      "(Iteration 35601 / 38200) loss: 2.304278\n",
      "(Iteration 35701 / 38200) loss: 2.304297\n",
      "(Iteration 35801 / 38200) loss: 2.304284\n",
      "(Iteration 35901 / 38200) loss: 2.304260\n",
      "(Epoch 94 / 100) train acc: 0.093000; val_acc: 0.094000\n",
      "(Iteration 36001 / 38200) loss: 2.304277\n",
      "(Iteration 36101 / 38200) loss: 2.304266\n",
      "(Iteration 36201 / 38200) loss: 2.304290\n",
      "(Epoch 95 / 100) train acc: 0.098000; val_acc: 0.094000\n",
      "(Iteration 36301 / 38200) loss: 2.304276\n",
      "(Iteration 36401 / 38200) loss: 2.304286\n",
      "(Iteration 36501 / 38200) loss: 2.304266\n",
      "(Iteration 36601 / 38200) loss: 2.304272\n",
      "(Epoch 96 / 100) train acc: 0.098000; val_acc: 0.094000\n",
      "(Iteration 36701 / 38200) loss: 2.304259\n",
      "(Iteration 36801 / 38200) loss: 2.304269\n",
      "(Iteration 36901 / 38200) loss: 2.304277\n",
      "(Iteration 37001 / 38200) loss: 2.304263\n",
      "(Epoch 97 / 100) train acc: 0.098000; val_acc: 0.094000\n",
      "(Iteration 37101 / 38200) loss: 2.304295\n",
      "(Iteration 37201 / 38200) loss: 2.304258\n",
      "(Iteration 37301 / 38200) loss: 2.304281\n",
      "(Iteration 37401 / 38200) loss: 2.304266\n",
      "(Epoch 98 / 100) train acc: 0.095000; val_acc: 0.094000\n",
      "(Iteration 37501 / 38200) loss: 2.304264\n",
      "(Iteration 37601 / 38200) loss: 2.304263\n",
      "(Iteration 37701 / 38200) loss: 2.304270\n",
      "(Iteration 37801 / 38200) loss: 2.304262\n",
      "(Epoch 99 / 100) train acc: 0.096000; val_acc: 0.094000\n",
      "(Iteration 37901 / 38200) loss: 2.304277\n",
      "(Iteration 38001 / 38200) loss: 2.304275\n",
      "(Iteration 38101 / 38200) loss: 2.304296\n",
      "(Epoch 100 / 100) train acc: 0.097000; val_acc: 0.094000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.305425\n",
      "(Epoch 0 / 100) train acc: 0.119000; val_acc: 0.118000\n",
      "(Iteration 101 / 76500) loss: 2.305402\n",
      "(Iteration 201 / 76500) loss: 2.305372\n",
      "(Iteration 301 / 76500) loss: 2.305299\n",
      "(Iteration 401 / 76500) loss: 2.305286\n",
      "(Iteration 501 / 76500) loss: 2.305247\n",
      "(Iteration 601 / 76500) loss: 2.305189\n",
      "(Iteration 701 / 76500) loss: 2.305144\n",
      "(Epoch 1 / 100) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 801 / 76500) loss: 2.305093\n",
      "(Iteration 901 / 76500) loss: 2.305085\n",
      "(Iteration 1001 / 76500) loss: 2.305058\n",
      "(Iteration 1101 / 76500) loss: 2.305036\n",
      "(Iteration 1201 / 76500) loss: 2.305027\n",
      "(Iteration 1301 / 76500) loss: 2.304961\n",
      "(Iteration 1401 / 76500) loss: 2.304961\n",
      "(Iteration 1501 / 76500) loss: 2.304908\n",
      "(Epoch 2 / 100) train acc: 0.098000; val_acc: 0.099000\n",
      "(Iteration 1601 / 76500) loss: 2.304860\n",
      "(Iteration 1701 / 76500) loss: 2.304838\n",
      "(Iteration 1801 / 76500) loss: 2.304842\n",
      "(Iteration 1901 / 76500) loss: 2.304766\n",
      "(Iteration 2001 / 76500) loss: 2.304739\n",
      "(Iteration 2101 / 76500) loss: 2.304726\n",
      "(Iteration 2201 / 76500) loss: 2.304711\n",
      "(Epoch 3 / 100) train acc: 0.133000; val_acc: 0.144000\n",
      "(Iteration 2301 / 76500) loss: 2.304660\n",
      "(Iteration 2401 / 76500) loss: 2.304673\n",
      "(Iteration 2501 / 76500) loss: 2.304627\n",
      "(Iteration 2601 / 76500) loss: 2.304575\n",
      "(Iteration 2701 / 76500) loss: 2.304559\n",
      "(Iteration 2801 / 76500) loss: 2.304544\n",
      "(Iteration 2901 / 76500) loss: 2.304584\n",
      "(Iteration 3001 / 76500) loss: 2.304520\n",
      "(Epoch 4 / 100) train acc: 0.112000; val_acc: 0.143000\n",
      "(Iteration 3101 / 76500) loss: 2.304473\n",
      "(Iteration 3201 / 76500) loss: 2.304448\n",
      "(Iteration 3301 / 76500) loss: 2.304434\n",
      "(Iteration 3401 / 76500) loss: 2.304435\n",
      "(Iteration 3501 / 76500) loss: 2.304451\n",
      "(Iteration 3601 / 76500) loss: 2.304389\n",
      "(Iteration 3701 / 76500) loss: 2.304398\n",
      "(Iteration 3801 / 76500) loss: 2.304317\n",
      "(Epoch 5 / 100) train acc: 0.095000; val_acc: 0.109000\n",
      "(Iteration 3901 / 76500) loss: 2.304314\n",
      "(Iteration 4001 / 76500) loss: 2.304267\n",
      "(Iteration 4101 / 76500) loss: 2.304274\n",
      "(Iteration 4201 / 76500) loss: 2.304242\n",
      "(Iteration 4301 / 76500) loss: 2.304263\n",
      "(Iteration 4401 / 76500) loss: 2.304224\n",
      "(Iteration 4501 / 76500) loss: 2.304235\n",
      "(Epoch 6 / 100) train acc: 0.083000; val_acc: 0.102000\n",
      "(Iteration 4601 / 76500) loss: 2.304167\n",
      "(Iteration 4701 / 76500) loss: 2.304170\n",
      "(Iteration 4801 / 76500) loss: 2.304202\n",
      "(Iteration 4901 / 76500) loss: 2.304210\n",
      "(Iteration 5001 / 76500) loss: 2.304132\n",
      "(Iteration 5101 / 76500) loss: 2.304066\n",
      "(Iteration 5201 / 76500) loss: 2.304088\n",
      "(Iteration 5301 / 76500) loss: 2.304088\n",
      "(Epoch 7 / 100) train acc: 0.082000; val_acc: 0.102000\n",
      "(Iteration 5401 / 76500) loss: 2.304007\n",
      "(Iteration 5501 / 76500) loss: 2.304040\n",
      "(Iteration 5601 / 76500) loss: 2.304002\n",
      "(Iteration 5701 / 76500) loss: 2.304035\n",
      "(Iteration 5801 / 76500) loss: 2.303988\n",
      "(Iteration 5901 / 76500) loss: 2.304005\n",
      "(Iteration 6001 / 76500) loss: 2.304018\n",
      "(Iteration 6101 / 76500) loss: 2.303994\n",
      "(Epoch 8 / 100) train acc: 0.104000; val_acc: 0.101000\n",
      "(Iteration 6201 / 76500) loss: 2.303952\n",
      "(Iteration 6301 / 76500) loss: 2.303953\n",
      "(Iteration 6401 / 76500) loss: 2.303895\n",
      "(Iteration 6501 / 76500) loss: 2.303902\n",
      "(Iteration 6601 / 76500) loss: 2.303940\n",
      "(Iteration 6701 / 76500) loss: 2.303824\n",
      "(Iteration 6801 / 76500) loss: 2.303868\n",
      "(Epoch 9 / 100) train acc: 0.115000; val_acc: 0.103000\n",
      "(Iteration 6901 / 76500) loss: 2.303862\n",
      "(Iteration 7001 / 76500) loss: 2.303824\n",
      "(Iteration 7101 / 76500) loss: 2.303861\n",
      "(Iteration 7201 / 76500) loss: 2.303862\n",
      "(Iteration 7301 / 76500) loss: 2.303808\n",
      "(Iteration 7401 / 76500) loss: 2.303861\n",
      "(Iteration 7501 / 76500) loss: 2.303847\n",
      "(Iteration 7601 / 76500) loss: 2.303790\n",
      "(Epoch 10 / 100) train acc: 0.103000; val_acc: 0.102000\n",
      "(Iteration 7701 / 76500) loss: 2.303740\n",
      "(Iteration 7801 / 76500) loss: 2.303838\n",
      "(Iteration 7901 / 76500) loss: 2.303714\n",
      "(Iteration 8001 / 76500) loss: 2.303710\n",
      "(Iteration 8101 / 76500) loss: 2.303722\n",
      "(Iteration 8201 / 76500) loss: 2.303703\n",
      "(Iteration 8301 / 76500) loss: 2.303756\n",
      "(Iteration 8401 / 76500) loss: 2.303716\n",
      "(Epoch 11 / 100) train acc: 0.101000; val_acc: 0.102000\n",
      "(Iteration 8501 / 76500) loss: 2.303701\n",
      "(Iteration 8601 / 76500) loss: 2.303645\n",
      "(Iteration 8701 / 76500) loss: 2.303636\n",
      "(Iteration 8801 / 76500) loss: 2.303596\n",
      "(Iteration 8901 / 76500) loss: 2.303677\n",
      "(Iteration 9001 / 76500) loss: 2.303707\n",
      "(Iteration 9101 / 76500) loss: 2.303693\n",
      "(Epoch 12 / 100) train acc: 0.111000; val_acc: 0.102000\n",
      "(Iteration 9201 / 76500) loss: 2.303641\n",
      "(Iteration 9301 / 76500) loss: 2.303617\n",
      "(Iteration 9401 / 76500) loss: 2.303609\n",
      "(Iteration 9501 / 76500) loss: 2.303568\n",
      "(Iteration 9601 / 76500) loss: 2.303571\n",
      "(Iteration 9701 / 76500) loss: 2.303572\n",
      "(Iteration 9801 / 76500) loss: 2.303672\n",
      "(Iteration 9901 / 76500) loss: 2.303613\n",
      "(Epoch 13 / 100) train acc: 0.107000; val_acc: 0.095000\n",
      "(Iteration 10001 / 76500) loss: 2.303533\n",
      "(Iteration 10101 / 76500) loss: 2.303535\n",
      "(Iteration 10201 / 76500) loss: 2.303594\n",
      "(Iteration 10301 / 76500) loss: 2.303509\n",
      "(Iteration 10401 / 76500) loss: 2.303536\n",
      "(Iteration 10501 / 76500) loss: 2.303552\n",
      "(Iteration 10601 / 76500) loss: 2.303592\n",
      "(Iteration 10701 / 76500) loss: 2.303530\n",
      "(Epoch 14 / 100) train acc: 0.095000; val_acc: 0.101000\n",
      "(Iteration 10801 / 76500) loss: 2.303537\n",
      "(Iteration 10901 / 76500) loss: 2.303503\n",
      "(Iteration 11001 / 76500) loss: 2.303488\n",
      "(Iteration 11101 / 76500) loss: 2.303516\n",
      "(Iteration 11201 / 76500) loss: 2.303510\n",
      "(Iteration 11301 / 76500) loss: 2.303503\n",
      "(Iteration 11401 / 76500) loss: 2.303494\n",
      "(Epoch 15 / 100) train acc: 0.115000; val_acc: 0.080000\n",
      "(Iteration 11501 / 76500) loss: 2.303474\n",
      "(Iteration 11601 / 76500) loss: 2.303475\n",
      "(Iteration 11701 / 76500) loss: 2.303516\n",
      "(Iteration 11801 / 76500) loss: 2.303469\n",
      "(Iteration 11901 / 76500) loss: 2.303515\n",
      "(Iteration 12001 / 76500) loss: 2.303520\n",
      "(Iteration 12101 / 76500) loss: 2.303382\n",
      "(Iteration 12201 / 76500) loss: 2.303481\n",
      "(Epoch 16 / 100) train acc: 0.097000; val_acc: 0.077000\n",
      "(Iteration 12301 / 76500) loss: 2.303407\n",
      "(Iteration 12401 / 76500) loss: 2.303391\n",
      "(Iteration 12501 / 76500) loss: 2.303456\n",
      "(Iteration 12601 / 76500) loss: 2.303378\n",
      "(Iteration 12701 / 76500) loss: 2.303409\n",
      "(Iteration 12801 / 76500) loss: 2.303388\n",
      "(Iteration 12901 / 76500) loss: 2.303434\n",
      "(Iteration 13001 / 76500) loss: 2.303366\n",
      "(Epoch 17 / 100) train acc: 0.117000; val_acc: 0.103000\n",
      "(Iteration 13101 / 76500) loss: 2.303351\n",
      "(Iteration 13201 / 76500) loss: 2.303351\n",
      "(Iteration 13301 / 76500) loss: 2.303352\n",
      "(Iteration 13401 / 76500) loss: 2.303342\n",
      "(Iteration 13501 / 76500) loss: 2.303413\n",
      "(Iteration 13601 / 76500) loss: 2.303381\n",
      "(Iteration 13701 / 76500) loss: 2.303363\n",
      "(Epoch 18 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 13801 / 76500) loss: 2.303358\n",
      "(Iteration 13901 / 76500) loss: 2.303354\n",
      "(Iteration 14001 / 76500) loss: 2.303328\n",
      "(Iteration 14101 / 76500) loss: 2.303356\n",
      "(Iteration 14201 / 76500) loss: 2.303362\n",
      "(Iteration 14301 / 76500) loss: 2.303335\n",
      "(Iteration 14401 / 76500) loss: 2.303323\n",
      "(Iteration 14501 / 76500) loss: 2.303351\n",
      "(Epoch 19 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 14601 / 76500) loss: 2.303315\n",
      "(Iteration 14701 / 76500) loss: 2.303313\n",
      "(Iteration 14801 / 76500) loss: 2.303304\n",
      "(Iteration 14901 / 76500) loss: 2.303299\n",
      "(Iteration 15001 / 76500) loss: 2.303315\n",
      "(Iteration 15101 / 76500) loss: 2.303332\n",
      "(Iteration 15201 / 76500) loss: 2.303288\n",
      "(Epoch 20 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 15301 / 76500) loss: 2.303306\n",
      "(Iteration 15401 / 76500) loss: 2.303280\n",
      "(Iteration 15501 / 76500) loss: 2.303324\n",
      "(Iteration 15601 / 76500) loss: 2.303178\n",
      "(Iteration 15701 / 76500) loss: 2.303332\n",
      "(Iteration 15801 / 76500) loss: 2.303316\n",
      "(Iteration 15901 / 76500) loss: 2.303303\n",
      "(Iteration 16001 / 76500) loss: 2.303268\n",
      "(Epoch 21 / 100) train acc: 0.105000; val_acc: 0.106000\n",
      "(Iteration 16101 / 76500) loss: 2.303306\n",
      "(Iteration 16201 / 76500) loss: 2.303290\n",
      "(Iteration 16301 / 76500) loss: 2.303259\n",
      "(Iteration 16401 / 76500) loss: 2.303283\n",
      "(Iteration 16501 / 76500) loss: 2.303238\n",
      "(Iteration 16601 / 76500) loss: 2.303331\n",
      "(Iteration 16701 / 76500) loss: 2.303230\n",
      "(Iteration 16801 / 76500) loss: 2.303281\n",
      "(Epoch 22 / 100) train acc: 0.128000; val_acc: 0.106000\n",
      "(Iteration 16901 / 76500) loss: 2.303176\n",
      "(Iteration 17001 / 76500) loss: 2.303279\n",
      "(Iteration 17101 / 76500) loss: 2.303261\n",
      "(Iteration 17201 / 76500) loss: 2.303268\n",
      "(Iteration 17301 / 76500) loss: 2.303175\n",
      "(Iteration 17401 / 76500) loss: 2.303255\n",
      "(Iteration 17501 / 76500) loss: 2.303248\n",
      "(Epoch 23 / 100) train acc: 0.110000; val_acc: 0.084000\n",
      "(Iteration 17601 / 76500) loss: 2.303281\n",
      "(Iteration 17701 / 76500) loss: 2.303216\n",
      "(Iteration 17801 / 76500) loss: 2.303153\n",
      "(Iteration 17901 / 76500) loss: 2.303256\n",
      "(Iteration 18001 / 76500) loss: 2.303209\n",
      "(Iteration 18101 / 76500) loss: 2.303310\n",
      "(Iteration 18201 / 76500) loss: 2.303210\n",
      "(Iteration 18301 / 76500) loss: 2.303203\n",
      "(Epoch 24 / 100) train acc: 0.083000; val_acc: 0.107000\n",
      "(Iteration 18401 / 76500) loss: 2.303288\n",
      "(Iteration 18501 / 76500) loss: 2.303229\n",
      "(Iteration 18601 / 76500) loss: 2.303207\n",
      "(Iteration 18701 / 76500) loss: 2.303210\n",
      "(Iteration 18801 / 76500) loss: 2.303223\n",
      "(Iteration 18901 / 76500) loss: 2.303159\n",
      "(Iteration 19001 / 76500) loss: 2.303183\n",
      "(Iteration 19101 / 76500) loss: 2.303144\n",
      "(Epoch 25 / 100) train acc: 0.090000; val_acc: 0.107000\n",
      "(Iteration 19201 / 76500) loss: 2.303180\n",
      "(Iteration 19301 / 76500) loss: 2.303212\n",
      "(Iteration 19401 / 76500) loss: 2.303171\n",
      "(Iteration 19501 / 76500) loss: 2.303218\n",
      "(Iteration 19601 / 76500) loss: 2.303216\n",
      "(Iteration 19701 / 76500) loss: 2.303173\n",
      "(Iteration 19801 / 76500) loss: 2.303188\n",
      "(Epoch 26 / 100) train acc: 0.125000; val_acc: 0.098000\n",
      "(Iteration 19901 / 76500) loss: 2.303118\n",
      "(Iteration 20001 / 76500) loss: 2.303136\n",
      "(Iteration 20101 / 76500) loss: 2.303103\n",
      "(Iteration 20201 / 76500) loss: 2.303092\n",
      "(Iteration 20301 / 76500) loss: 2.303150\n",
      "(Iteration 20401 / 76500) loss: 2.303108\n",
      "(Iteration 20501 / 76500) loss: 2.303168\n",
      "(Iteration 20601 / 76500) loss: 2.303149\n",
      "(Epoch 27 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 20701 / 76500) loss: 2.303126\n",
      "(Iteration 20801 / 76500) loss: 2.303085\n",
      "(Iteration 20901 / 76500) loss: 2.303180\n",
      "(Iteration 21001 / 76500) loss: 2.303110\n",
      "(Iteration 21101 / 76500) loss: 2.303136\n",
      "(Iteration 21201 / 76500) loss: 2.303060\n",
      "(Iteration 21301 / 76500) loss: 2.303078\n",
      "(Iteration 21401 / 76500) loss: 2.303157\n",
      "(Epoch 28 / 100) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 21501 / 76500) loss: 2.303135\n",
      "(Iteration 21601 / 76500) loss: 2.303130\n",
      "(Iteration 21701 / 76500) loss: 2.303128\n",
      "(Iteration 21801 / 76500) loss: 2.303145\n",
      "(Iteration 21901 / 76500) loss: 2.303234\n",
      "(Iteration 22001 / 76500) loss: 2.303147\n",
      "(Iteration 22101 / 76500) loss: 2.303030\n",
      "(Epoch 29 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 22201 / 76500) loss: 2.303100\n",
      "(Iteration 22301 / 76500) loss: 2.303143\n",
      "(Iteration 22401 / 76500) loss: 2.303157\n",
      "(Iteration 22501 / 76500) loss: 2.303141\n",
      "(Iteration 22601 / 76500) loss: 2.303116\n",
      "(Iteration 22701 / 76500) loss: 2.303190\n",
      "(Iteration 22801 / 76500) loss: 2.303174\n",
      "(Iteration 22901 / 76500) loss: 2.303108\n",
      "(Epoch 30 / 100) train acc: 0.105000; val_acc: 0.100000\n",
      "(Iteration 23001 / 76500) loss: 2.303094\n",
      "(Iteration 23101 / 76500) loss: 2.303115\n",
      "(Iteration 23201 / 76500) loss: 2.303110\n",
      "(Iteration 23301 / 76500) loss: 2.303098\n",
      "(Iteration 23401 / 76500) loss: 2.303105\n",
      "(Iteration 23501 / 76500) loss: 2.303085\n",
      "(Iteration 23601 / 76500) loss: 2.303067\n",
      "(Iteration 23701 / 76500) loss: 2.303143\n",
      "(Epoch 31 / 100) train acc: 0.111000; val_acc: 0.095000\n",
      "(Iteration 23801 / 76500) loss: 2.303087\n",
      "(Iteration 23901 / 76500) loss: 2.303123\n",
      "(Iteration 24001 / 76500) loss: 2.303158\n",
      "(Iteration 24101 / 76500) loss: 2.303015\n",
      "(Iteration 24201 / 76500) loss: 2.303165\n",
      "(Iteration 24301 / 76500) loss: 2.303126\n",
      "(Iteration 24401 / 76500) loss: 2.303133\n",
      "(Epoch 32 / 100) train acc: 0.108000; val_acc: 0.100000\n",
      "(Iteration 24501 / 76500) loss: 2.303082\n",
      "(Iteration 24601 / 76500) loss: 2.303101\n",
      "(Iteration 24701 / 76500) loss: 2.303060\n",
      "(Iteration 24801 / 76500) loss: 2.303105\n",
      "(Iteration 24901 / 76500) loss: 2.303041\n",
      "(Iteration 25001 / 76500) loss: 2.303108\n",
      "(Iteration 25101 / 76500) loss: 2.303081\n",
      "(Iteration 25201 / 76500) loss: 2.303026\n",
      "(Epoch 33 / 100) train acc: 0.108000; val_acc: 0.100000\n",
      "(Iteration 25301 / 76500) loss: 2.303098\n",
      "(Iteration 25401 / 76500) loss: 2.303023\n",
      "(Iteration 25501 / 76500) loss: 2.303147\n",
      "(Iteration 25601 / 76500) loss: 2.303024\n",
      "(Iteration 25701 / 76500) loss: 2.303047\n",
      "(Iteration 25801 / 76500) loss: 2.303032\n",
      "(Iteration 25901 / 76500) loss: 2.303114\n",
      "(Iteration 26001 / 76500) loss: 2.303076\n",
      "(Epoch 34 / 100) train acc: 0.111000; val_acc: 0.100000\n",
      "(Iteration 26101 / 76500) loss: 2.303059\n",
      "(Iteration 26201 / 76500) loss: 2.303107\n",
      "(Iteration 26301 / 76500) loss: 2.303121\n",
      "(Iteration 26401 / 76500) loss: 2.303087\n",
      "(Iteration 26501 / 76500) loss: 2.303071\n",
      "(Iteration 26601 / 76500) loss: 2.302988\n",
      "(Iteration 26701 / 76500) loss: 2.303080\n",
      "(Epoch 35 / 100) train acc: 0.101000; val_acc: 0.084000\n",
      "(Iteration 26801 / 76500) loss: 2.303131\n",
      "(Iteration 26901 / 76500) loss: 2.303004\n",
      "(Iteration 27001 / 76500) loss: 2.302980\n",
      "(Iteration 27101 / 76500) loss: 2.303052\n",
      "(Iteration 27201 / 76500) loss: 2.303026\n",
      "(Iteration 27301 / 76500) loss: 2.303082\n",
      "(Iteration 27401 / 76500) loss: 2.303046\n",
      "(Iteration 27501 / 76500) loss: 2.303084\n",
      "(Epoch 36 / 100) train acc: 0.088000; val_acc: 0.103000\n",
      "(Iteration 27601 / 76500) loss: 2.303097\n",
      "(Iteration 27701 / 76500) loss: 2.303011\n",
      "(Iteration 27801 / 76500) loss: 2.303038\n",
      "(Iteration 27901 / 76500) loss: 2.303107\n",
      "(Iteration 28001 / 76500) loss: 2.303147\n",
      "(Iteration 28101 / 76500) loss: 2.302989\n",
      "(Iteration 28201 / 76500) loss: 2.303028\n",
      "(Iteration 28301 / 76500) loss: 2.303049\n",
      "(Epoch 37 / 100) train acc: 0.107000; val_acc: 0.107000\n",
      "(Iteration 28401 / 76500) loss: 2.303033\n",
      "(Iteration 28501 / 76500) loss: 2.303078\n",
      "(Iteration 28601 / 76500) loss: 2.303038\n",
      "(Iteration 28701 / 76500) loss: 2.303101\n",
      "(Iteration 28801 / 76500) loss: 2.303082\n",
      "(Iteration 28901 / 76500) loss: 2.302991\n",
      "(Iteration 29001 / 76500) loss: 2.303040\n",
      "(Epoch 38 / 100) train acc: 0.105000; val_acc: 0.107000\n",
      "(Iteration 29101 / 76500) loss: 2.303067\n",
      "(Iteration 29201 / 76500) loss: 2.303044\n",
      "(Iteration 29301 / 76500) loss: 2.303025\n",
      "(Iteration 29401 / 76500) loss: 2.303010\n",
      "(Iteration 29501 / 76500) loss: 2.303015\n",
      "(Iteration 29601 / 76500) loss: 2.303057\n",
      "(Iteration 29701 / 76500) loss: 2.303026\n",
      "(Iteration 29801 / 76500) loss: 2.303006\n",
      "(Epoch 39 / 100) train acc: 0.096000; val_acc: 0.105000\n",
      "(Iteration 29901 / 76500) loss: 2.303048\n",
      "(Iteration 30001 / 76500) loss: 2.303008\n",
      "(Iteration 30101 / 76500) loss: 2.303023\n",
      "(Iteration 30201 / 76500) loss: 2.303052\n",
      "(Iteration 30301 / 76500) loss: 2.303053\n",
      "(Iteration 30401 / 76500) loss: 2.303018\n",
      "(Iteration 30501 / 76500) loss: 2.302971\n",
      "(Epoch 40 / 100) train acc: 0.092000; val_acc: 0.105000\n",
      "(Iteration 30601 / 76500) loss: 2.303075\n",
      "(Iteration 30701 / 76500) loss: 2.303005\n",
      "(Iteration 30801 / 76500) loss: 2.303024\n",
      "(Iteration 30901 / 76500) loss: 2.303052\n",
      "(Iteration 31001 / 76500) loss: 2.302928\n",
      "(Iteration 31101 / 76500) loss: 2.302976\n",
      "(Iteration 31201 / 76500) loss: 2.303070\n",
      "(Iteration 31301 / 76500) loss: 2.303106\n",
      "(Epoch 41 / 100) train acc: 0.116000; val_acc: 0.106000\n",
      "(Iteration 31401 / 76500) loss: 2.303036\n",
      "(Iteration 31501 / 76500) loss: 2.303013\n",
      "(Iteration 31601 / 76500) loss: 2.302999\n",
      "(Iteration 31701 / 76500) loss: 2.303069\n",
      "(Iteration 31801 / 76500) loss: 2.303023\n",
      "(Iteration 31901 / 76500) loss: 2.303039\n",
      "(Iteration 32001 / 76500) loss: 2.303022\n",
      "(Iteration 32101 / 76500) loss: 2.303002\n",
      "(Epoch 42 / 100) train acc: 0.112000; val_acc: 0.103000\n",
      "(Iteration 32201 / 76500) loss: 2.303091\n",
      "(Iteration 32301 / 76500) loss: 2.303019\n",
      "(Iteration 32401 / 76500) loss: 2.303041\n",
      "(Iteration 32501 / 76500) loss: 2.302941\n",
      "(Iteration 32601 / 76500) loss: 2.303026\n",
      "(Iteration 32701 / 76500) loss: 2.303063\n",
      "(Iteration 32801 / 76500) loss: 2.303097\n",
      "(Epoch 43 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 32901 / 76500) loss: 2.302982\n",
      "(Iteration 33001 / 76500) loss: 2.303051\n",
      "(Iteration 33101 / 76500) loss: 2.302975\n",
      "(Iteration 33201 / 76500) loss: 2.303041\n",
      "(Iteration 33301 / 76500) loss: 2.302968\n",
      "(Iteration 33401 / 76500) loss: 2.303003\n",
      "(Iteration 33501 / 76500) loss: 2.302965\n",
      "(Iteration 33601 / 76500) loss: 2.303081\n",
      "(Epoch 44 / 100) train acc: 0.114000; val_acc: 0.078000\n",
      "(Iteration 33701 / 76500) loss: 2.303023\n",
      "(Iteration 33801 / 76500) loss: 2.302999\n",
      "(Iteration 33901 / 76500) loss: 2.303056\n",
      "(Iteration 34001 / 76500) loss: 2.302945\n",
      "(Iteration 34101 / 76500) loss: 2.303010\n",
      "(Iteration 34201 / 76500) loss: 2.302963\n",
      "(Iteration 34301 / 76500) loss: 2.302979\n",
      "(Iteration 34401 / 76500) loss: 2.303024\n",
      "(Epoch 45 / 100) train acc: 0.121000; val_acc: 0.104000\n",
      "(Iteration 34501 / 76500) loss: 2.303057\n",
      "(Iteration 34601 / 76500) loss: 2.303046\n",
      "(Iteration 34701 / 76500) loss: 2.303024\n",
      "(Iteration 34801 / 76500) loss: 2.302949\n",
      "(Iteration 34901 / 76500) loss: 2.303052\n",
      "(Iteration 35001 / 76500) loss: 2.302907\n",
      "(Iteration 35101 / 76500) loss: 2.302987\n",
      "(Epoch 46 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 35201 / 76500) loss: 2.302881\n",
      "(Iteration 35301 / 76500) loss: 2.303082\n",
      "(Iteration 35401 / 76500) loss: 2.302957\n",
      "(Iteration 35501 / 76500) loss: 2.302929\n",
      "(Iteration 35601 / 76500) loss: 2.302926\n",
      "(Iteration 35701 / 76500) loss: 2.302911\n",
      "(Iteration 35801 / 76500) loss: 2.302994\n",
      "(Iteration 35901 / 76500) loss: 2.302944\n",
      "(Epoch 47 / 100) train acc: 0.124000; val_acc: 0.078000\n",
      "(Iteration 36001 / 76500) loss: 2.302980\n",
      "(Iteration 36101 / 76500) loss: 2.302982\n",
      "(Iteration 36201 / 76500) loss: 2.303015\n",
      "(Iteration 36301 / 76500) loss: 2.303022\n",
      "(Iteration 36401 / 76500) loss: 2.302995\n",
      "(Iteration 36501 / 76500) loss: 2.303001\n",
      "(Iteration 36601 / 76500) loss: 2.303036\n",
      "(Iteration 36701 / 76500) loss: 2.302901\n",
      "(Epoch 48 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 36801 / 76500) loss: 2.302903\n",
      "(Iteration 36901 / 76500) loss: 2.303039\n",
      "(Iteration 37001 / 76500) loss: 2.302955\n",
      "(Iteration 37101 / 76500) loss: 2.303071\n",
      "(Iteration 37201 / 76500) loss: 2.303084\n",
      "(Iteration 37301 / 76500) loss: 2.303014\n",
      "(Iteration 37401 / 76500) loss: 2.303030\n",
      "(Epoch 49 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 37501 / 76500) loss: 2.302936\n",
      "(Iteration 37601 / 76500) loss: 2.302999\n",
      "(Iteration 37701 / 76500) loss: 2.302889\n",
      "(Iteration 37801 / 76500) loss: 2.302936\n",
      "(Iteration 37901 / 76500) loss: 2.302990\n",
      "(Iteration 38001 / 76500) loss: 2.303057\n",
      "(Iteration 38101 / 76500) loss: 2.303057\n",
      "(Iteration 38201 / 76500) loss: 2.303020\n",
      "(Epoch 50 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 38301 / 76500) loss: 2.302951\n",
      "(Iteration 38401 / 76500) loss: 2.302955\n",
      "(Iteration 38501 / 76500) loss: 2.302945\n",
      "(Iteration 38601 / 76500) loss: 2.303053\n",
      "(Iteration 38701 / 76500) loss: 2.302983\n",
      "(Iteration 38801 / 76500) loss: 2.302989\n",
      "(Iteration 38901 / 76500) loss: 2.302989\n",
      "(Iteration 39001 / 76500) loss: 2.302999\n",
      "(Epoch 51 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 39101 / 76500) loss: 2.303009\n",
      "(Iteration 39201 / 76500) loss: 2.303007\n",
      "(Iteration 39301 / 76500) loss: 2.302942\n",
      "(Iteration 39401 / 76500) loss: 2.303036\n",
      "(Iteration 39501 / 76500) loss: 2.303005\n",
      "(Iteration 39601 / 76500) loss: 2.302985\n",
      "(Iteration 39701 / 76500) loss: 2.302920\n",
      "(Epoch 52 / 100) train acc: 0.115000; val_acc: 0.078000\n",
      "(Iteration 39801 / 76500) loss: 2.302884\n",
      "(Iteration 39901 / 76500) loss: 2.302897\n",
      "(Iteration 40001 / 76500) loss: 2.302956\n",
      "(Iteration 40101 / 76500) loss: 2.302950\n",
      "(Iteration 40201 / 76500) loss: 2.302986\n",
      "(Iteration 40301 / 76500) loss: 2.302919\n",
      "(Iteration 40401 / 76500) loss: 2.302962\n",
      "(Iteration 40501 / 76500) loss: 2.302937\n",
      "(Epoch 53 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 40601 / 76500) loss: 2.302961\n",
      "(Iteration 40701 / 76500) loss: 2.302977\n",
      "(Iteration 40801 / 76500) loss: 2.302930\n",
      "(Iteration 40901 / 76500) loss: 2.303012\n",
      "(Iteration 41001 / 76500) loss: 2.302947\n",
      "(Iteration 41101 / 76500) loss: 2.302913\n",
      "(Iteration 41201 / 76500) loss: 2.303097\n",
      "(Iteration 41301 / 76500) loss: 2.302949\n",
      "(Epoch 54 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 41401 / 76500) loss: 2.302954\n",
      "(Iteration 41501 / 76500) loss: 2.302965\n",
      "(Iteration 41601 / 76500) loss: 2.302923\n",
      "(Iteration 41701 / 76500) loss: 2.302982\n",
      "(Iteration 41801 / 76500) loss: 2.303012\n",
      "(Iteration 41901 / 76500) loss: 2.303030\n",
      "(Iteration 42001 / 76500) loss: 2.302994\n",
      "(Epoch 55 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 42101 / 76500) loss: 2.302946\n",
      "(Iteration 42201 / 76500) loss: 2.302997\n",
      "(Iteration 42301 / 76500) loss: 2.302968\n",
      "(Iteration 42401 / 76500) loss: 2.303033\n",
      "(Iteration 42501 / 76500) loss: 2.302943\n",
      "(Iteration 42601 / 76500) loss: 2.302987\n",
      "(Iteration 42701 / 76500) loss: 2.302979\n",
      "(Iteration 42801 / 76500) loss: 2.303007\n",
      "(Epoch 56 / 100) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 42901 / 76500) loss: 2.302989\n",
      "(Iteration 43001 / 76500) loss: 2.302921\n",
      "(Iteration 43101 / 76500) loss: 2.302951\n",
      "(Iteration 43201 / 76500) loss: 2.302943\n",
      "(Iteration 43301 / 76500) loss: 2.302895\n",
      "(Iteration 43401 / 76500) loss: 2.302938\n",
      "(Iteration 43501 / 76500) loss: 2.302940\n",
      "(Iteration 43601 / 76500) loss: 2.302952\n",
      "(Epoch 57 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 43701 / 76500) loss: 2.303062\n",
      "(Iteration 43801 / 76500) loss: 2.302953\n",
      "(Iteration 43901 / 76500) loss: 2.302947\n",
      "(Iteration 44001 / 76500) loss: 2.303005\n",
      "(Iteration 44101 / 76500) loss: 2.302971\n",
      "(Iteration 44201 / 76500) loss: 2.303081\n",
      "(Iteration 44301 / 76500) loss: 2.302963\n",
      "(Epoch 58 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 44401 / 76500) loss: 2.302960\n",
      "(Iteration 44501 / 76500) loss: 2.302991\n",
      "(Iteration 44601 / 76500) loss: 2.302940\n",
      "(Iteration 44701 / 76500) loss: 2.302913\n",
      "(Iteration 44801 / 76500) loss: 2.302963\n",
      "(Iteration 44901 / 76500) loss: 2.302971\n",
      "(Iteration 45001 / 76500) loss: 2.302908\n",
      "(Iteration 45101 / 76500) loss: 2.302907\n",
      "(Epoch 59 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 45201 / 76500) loss: 2.302992\n",
      "(Iteration 45301 / 76500) loss: 2.302998\n",
      "(Iteration 45401 / 76500) loss: 2.303036\n",
      "(Iteration 45501 / 76500) loss: 2.303025\n",
      "(Iteration 45601 / 76500) loss: 2.302930\n",
      "(Iteration 45701 / 76500) loss: 2.302999\n",
      "(Iteration 45801 / 76500) loss: 2.302992\n",
      "(Epoch 60 / 100) train acc: 0.079000; val_acc: 0.078000\n",
      "(Iteration 45901 / 76500) loss: 2.302986\n",
      "(Iteration 46001 / 76500) loss: 2.302930\n",
      "(Iteration 46101 / 76500) loss: 2.303026\n",
      "(Iteration 46201 / 76500) loss: 2.302928\n",
      "(Iteration 46301 / 76500) loss: 2.303040\n",
      "(Iteration 46401 / 76500) loss: 2.302920\n",
      "(Iteration 46501 / 76500) loss: 2.302966\n",
      "(Iteration 46601 / 76500) loss: 2.302931\n",
      "(Epoch 61 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 46701 / 76500) loss: 2.303046\n",
      "(Iteration 46801 / 76500) loss: 2.302971\n",
      "(Iteration 46901 / 76500) loss: 2.303002\n",
      "(Iteration 47001 / 76500) loss: 2.302983\n",
      "(Iteration 47101 / 76500) loss: 2.302885\n",
      "(Iteration 47201 / 76500) loss: 2.303061\n",
      "(Iteration 47301 / 76500) loss: 2.303086\n",
      "(Iteration 47401 / 76500) loss: 2.303034\n",
      "(Epoch 62 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 47501 / 76500) loss: 2.302909\n",
      "(Iteration 47601 / 76500) loss: 2.302943\n",
      "(Iteration 47701 / 76500) loss: 2.302953\n",
      "(Iteration 47801 / 76500) loss: 2.303029\n",
      "(Iteration 47901 / 76500) loss: 2.302980\n",
      "(Iteration 48001 / 76500) loss: 2.302966\n",
      "(Iteration 48101 / 76500) loss: 2.302970\n",
      "(Epoch 63 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 48201 / 76500) loss: 2.302898\n",
      "(Iteration 48301 / 76500) loss: 2.302917\n",
      "(Iteration 48401 / 76500) loss: 2.302973\n",
      "(Iteration 48501 / 76500) loss: 2.302919\n",
      "(Iteration 48601 / 76500) loss: 2.302962\n",
      "(Iteration 48701 / 76500) loss: 2.302992\n",
      "(Iteration 48801 / 76500) loss: 2.302871\n",
      "(Iteration 48901 / 76500) loss: 2.302919\n",
      "(Epoch 64 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 49001 / 76500) loss: 2.302938\n",
      "(Iteration 49101 / 76500) loss: 2.302886\n",
      "(Iteration 49201 / 76500) loss: 2.302935\n",
      "(Iteration 49301 / 76500) loss: 2.302930\n",
      "(Iteration 49401 / 76500) loss: 2.302954\n",
      "(Iteration 49501 / 76500) loss: 2.302899\n",
      "(Iteration 49601 / 76500) loss: 2.303003\n",
      "(Iteration 49701 / 76500) loss: 2.302976\n",
      "(Epoch 65 / 100) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 49801 / 76500) loss: 2.302865\n",
      "(Iteration 49901 / 76500) loss: 2.302942\n",
      "(Iteration 50001 / 76500) loss: 2.302988\n",
      "(Iteration 50101 / 76500) loss: 2.302867\n",
      "(Iteration 50201 / 76500) loss: 2.303010\n",
      "(Iteration 50301 / 76500) loss: 2.302903\n",
      "(Iteration 50401 / 76500) loss: 2.302896\n",
      "(Epoch 66 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 50501 / 76500) loss: 2.303007\n",
      "(Iteration 50601 / 76500) loss: 2.302995\n",
      "(Iteration 50701 / 76500) loss: 2.302946\n",
      "(Iteration 50801 / 76500) loss: 2.303097\n",
      "(Iteration 50901 / 76500) loss: 2.302902\n",
      "(Iteration 51001 / 76500) loss: 2.303013\n",
      "(Iteration 51101 / 76500) loss: 2.302935\n",
      "(Iteration 51201 / 76500) loss: 2.302972\n",
      "(Epoch 67 / 100) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 51301 / 76500) loss: 2.302945\n",
      "(Iteration 51401 / 76500) loss: 2.302914\n",
      "(Iteration 51501 / 76500) loss: 2.302966\n",
      "(Iteration 51601 / 76500) loss: 2.302845\n",
      "(Iteration 51701 / 76500) loss: 2.302962\n",
      "(Iteration 51801 / 76500) loss: 2.302904\n",
      "(Iteration 51901 / 76500) loss: 2.302923\n",
      "(Iteration 52001 / 76500) loss: 2.302937\n",
      "(Epoch 68 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 52101 / 76500) loss: 2.302893\n",
      "(Iteration 52201 / 76500) loss: 2.302919\n",
      "(Iteration 52301 / 76500) loss: 2.303029\n",
      "(Iteration 52401 / 76500) loss: 2.303004\n",
      "(Iteration 52501 / 76500) loss: 2.302877\n",
      "(Iteration 52601 / 76500) loss: 2.302891\n",
      "(Iteration 52701 / 76500) loss: 2.302869\n",
      "(Epoch 69 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 52801 / 76500) loss: 2.302888\n",
      "(Iteration 52901 / 76500) loss: 2.302952\n",
      "(Iteration 53001 / 76500) loss: 2.302893\n",
      "(Iteration 53101 / 76500) loss: 2.303011\n",
      "(Iteration 53201 / 76500) loss: 2.302885\n",
      "(Iteration 53301 / 76500) loss: 2.302902\n",
      "(Iteration 53401 / 76500) loss: 2.302994\n",
      "(Iteration 53501 / 76500) loss: 2.302966\n",
      "(Epoch 70 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 53601 / 76500) loss: 2.302915\n",
      "(Iteration 53701 / 76500) loss: 2.302924\n",
      "(Iteration 53801 / 76500) loss: 2.302965\n",
      "(Iteration 53901 / 76500) loss: 2.302911\n",
      "(Iteration 54001 / 76500) loss: 2.302940\n",
      "(Iteration 54101 / 76500) loss: 2.302900\n",
      "(Iteration 54201 / 76500) loss: 2.302946\n",
      "(Iteration 54301 / 76500) loss: 2.302945\n",
      "(Epoch 71 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 54401 / 76500) loss: 2.302938\n",
      "(Iteration 54501 / 76500) loss: 2.302969\n",
      "(Iteration 54601 / 76500) loss: 2.302946\n",
      "(Iteration 54701 / 76500) loss: 2.302959\n",
      "(Iteration 54801 / 76500) loss: 2.302840\n",
      "(Iteration 54901 / 76500) loss: 2.303004\n",
      "(Iteration 55001 / 76500) loss: 2.302919\n",
      "(Epoch 72 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 55101 / 76500) loss: 2.302899\n",
      "(Iteration 55201 / 76500) loss: 2.302917\n",
      "(Iteration 55301 / 76500) loss: 2.302947\n",
      "(Iteration 55401 / 76500) loss: 2.302947\n",
      "(Iteration 55501 / 76500) loss: 2.302822\n",
      "(Iteration 55601 / 76500) loss: 2.302936\n",
      "(Iteration 55701 / 76500) loss: 2.302893\n",
      "(Iteration 55801 / 76500) loss: 2.302958\n",
      "(Epoch 73 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 55901 / 76500) loss: 2.302927\n",
      "(Iteration 56001 / 76500) loss: 2.303028\n",
      "(Iteration 56101 / 76500) loss: 2.302999\n",
      "(Iteration 56201 / 76500) loss: 2.302900\n",
      "(Iteration 56301 / 76500) loss: 2.302946\n",
      "(Iteration 56401 / 76500) loss: 2.303035\n",
      "(Iteration 56501 / 76500) loss: 2.302913\n",
      "(Iteration 56601 / 76500) loss: 2.302882\n",
      "(Epoch 74 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 56701 / 76500) loss: 2.302908\n",
      "(Iteration 56801 / 76500) loss: 2.302949\n",
      "(Iteration 56901 / 76500) loss: 2.302930\n",
      "(Iteration 57001 / 76500) loss: 2.302989\n",
      "(Iteration 57101 / 76500) loss: 2.302942\n",
      "(Iteration 57201 / 76500) loss: 2.302971\n",
      "(Iteration 57301 / 76500) loss: 2.302995\n",
      "(Epoch 75 / 100) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 57401 / 76500) loss: 2.302908\n",
      "(Iteration 57501 / 76500) loss: 2.302883\n",
      "(Iteration 57601 / 76500) loss: 2.302882\n",
      "(Iteration 57701 / 76500) loss: 2.302940\n",
      "(Iteration 57801 / 76500) loss: 2.303030\n",
      "(Iteration 57901 / 76500) loss: 2.302913\n",
      "(Iteration 58001 / 76500) loss: 2.302947\n",
      "(Iteration 58101 / 76500) loss: 2.302945\n",
      "(Epoch 76 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 58201 / 76500) loss: 2.302900\n",
      "(Iteration 58301 / 76500) loss: 2.302971\n",
      "(Iteration 58401 / 76500) loss: 2.302976\n",
      "(Iteration 58501 / 76500) loss: 2.302937\n",
      "(Iteration 58601 / 76500) loss: 2.302942\n",
      "(Iteration 58701 / 76500) loss: 2.302930\n",
      "(Iteration 58801 / 76500) loss: 2.302945\n",
      "(Iteration 58901 / 76500) loss: 2.302912\n",
      "(Epoch 77 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 59001 / 76500) loss: 2.302809\n",
      "(Iteration 59101 / 76500) loss: 2.302917\n",
      "(Iteration 59201 / 76500) loss: 2.302909\n",
      "(Iteration 59301 / 76500) loss: 2.302878\n",
      "(Iteration 59401 / 76500) loss: 2.302891\n",
      "(Iteration 59501 / 76500) loss: 2.303007\n",
      "(Iteration 59601 / 76500) loss: 2.302996\n",
      "(Epoch 78 / 100) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 59701 / 76500) loss: 2.303026\n",
      "(Iteration 59801 / 76500) loss: 2.302997\n",
      "(Iteration 59901 / 76500) loss: 2.303003\n",
      "(Iteration 60001 / 76500) loss: 2.302919\n",
      "(Iteration 60101 / 76500) loss: 2.302953\n",
      "(Iteration 60201 / 76500) loss: 2.302880\n",
      "(Iteration 60301 / 76500) loss: 2.302886\n",
      "(Iteration 60401 / 76500) loss: 2.302848\n",
      "(Epoch 79 / 100) train acc: 0.079000; val_acc: 0.078000\n",
      "(Iteration 60501 / 76500) loss: 2.302933\n",
      "(Iteration 60601 / 76500) loss: 2.302873\n",
      "(Iteration 60701 / 76500) loss: 2.302968\n",
      "(Iteration 60801 / 76500) loss: 2.302849\n",
      "(Iteration 60901 / 76500) loss: 2.302994\n",
      "(Iteration 61001 / 76500) loss: 2.302890\n",
      "(Iteration 61101 / 76500) loss: 2.302816\n",
      "(Epoch 80 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 61201 / 76500) loss: 2.302914\n",
      "(Iteration 61301 / 76500) loss: 2.302950\n",
      "(Iteration 61401 / 76500) loss: 2.302871\n",
      "(Iteration 61501 / 76500) loss: 2.302990\n",
      "(Iteration 61601 / 76500) loss: 2.302954\n",
      "(Iteration 61701 / 76500) loss: 2.302910\n",
      "(Iteration 61801 / 76500) loss: 2.302860\n",
      "(Iteration 61901 / 76500) loss: 2.303009\n",
      "(Epoch 81 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 62001 / 76500) loss: 2.302976\n",
      "(Iteration 62101 / 76500) loss: 2.302860\n",
      "(Iteration 62201 / 76500) loss: 2.302960\n",
      "(Iteration 62301 / 76500) loss: 2.302807\n",
      "(Iteration 62401 / 76500) loss: 2.302900\n",
      "(Iteration 62501 / 76500) loss: 2.302953\n",
      "(Iteration 62601 / 76500) loss: 2.303003\n",
      "(Iteration 62701 / 76500) loss: 2.302988\n",
      "(Epoch 82 / 100) train acc: 0.081000; val_acc: 0.078000\n",
      "(Iteration 62801 / 76500) loss: 2.303035\n",
      "(Iteration 62901 / 76500) loss: 2.302918\n",
      "(Iteration 63001 / 76500) loss: 2.302971\n",
      "(Iteration 63101 / 76500) loss: 2.302884\n",
      "(Iteration 63201 / 76500) loss: 2.302862\n",
      "(Iteration 63301 / 76500) loss: 2.302915\n",
      "(Iteration 63401 / 76500) loss: 2.302995\n",
      "(Epoch 83 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 63501 / 76500) loss: 2.302894\n",
      "(Iteration 63601 / 76500) loss: 2.302847\n",
      "(Iteration 63701 / 76500) loss: 2.302910\n",
      "(Iteration 63801 / 76500) loss: 2.302923\n",
      "(Iteration 63901 / 76500) loss: 2.302952\n",
      "(Iteration 64001 / 76500) loss: 2.302903\n",
      "(Iteration 64101 / 76500) loss: 2.302948\n",
      "(Iteration 64201 / 76500) loss: 2.302926\n",
      "(Epoch 84 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 64301 / 76500) loss: 2.302900\n",
      "(Iteration 64401 / 76500) loss: 2.302925\n",
      "(Iteration 64501 / 76500) loss: 2.302980\n",
      "(Iteration 64601 / 76500) loss: 2.302952\n",
      "(Iteration 64701 / 76500) loss: 2.302940\n",
      "(Iteration 64801 / 76500) loss: 2.302956\n",
      "(Iteration 64901 / 76500) loss: 2.302978\n",
      "(Iteration 65001 / 76500) loss: 2.303046\n",
      "(Epoch 85 / 100) train acc: 0.118000; val_acc: 0.078000\n",
      "(Iteration 65101 / 76500) loss: 2.302862\n",
      "(Iteration 65201 / 76500) loss: 2.302909\n",
      "(Iteration 65301 / 76500) loss: 2.302917\n",
      "(Iteration 65401 / 76500) loss: 2.302874\n",
      "(Iteration 65501 / 76500) loss: 2.302913\n",
      "(Iteration 65601 / 76500) loss: 2.302869\n",
      "(Iteration 65701 / 76500) loss: 2.302844\n",
      "(Epoch 86 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 65801 / 76500) loss: 2.302944\n",
      "(Iteration 65901 / 76500) loss: 2.302963\n",
      "(Iteration 66001 / 76500) loss: 2.302919\n",
      "(Iteration 66101 / 76500) loss: 2.302906\n",
      "(Iteration 66201 / 76500) loss: 2.302918\n",
      "(Iteration 66301 / 76500) loss: 2.302860\n",
      "(Iteration 66401 / 76500) loss: 2.302859\n",
      "(Iteration 66501 / 76500) loss: 2.302981\n",
      "(Epoch 87 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 66601 / 76500) loss: 2.302908\n",
      "(Iteration 66701 / 76500) loss: 2.302941\n",
      "(Iteration 66801 / 76500) loss: 2.302945\n",
      "(Iteration 66901 / 76500) loss: 2.302943\n",
      "(Iteration 67001 / 76500) loss: 2.302874\n",
      "(Iteration 67101 / 76500) loss: 2.302955\n",
      "(Iteration 67201 / 76500) loss: 2.302918\n",
      "(Iteration 67301 / 76500) loss: 2.302844\n",
      "(Epoch 88 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 67401 / 76500) loss: 2.302839\n",
      "(Iteration 67501 / 76500) loss: 2.302971\n",
      "(Iteration 67601 / 76500) loss: 2.302872\n",
      "(Iteration 67701 / 76500) loss: 2.302898\n",
      "(Iteration 67801 / 76500) loss: 2.302956\n",
      "(Iteration 67901 / 76500) loss: 2.302988\n",
      "(Iteration 68001 / 76500) loss: 2.302900\n",
      "(Epoch 89 / 100) train acc: 0.124000; val_acc: 0.078000\n",
      "(Iteration 68101 / 76500) loss: 2.302972\n",
      "(Iteration 68201 / 76500) loss: 2.302868\n",
      "(Iteration 68301 / 76500) loss: 2.303012\n",
      "(Iteration 68401 / 76500) loss: 2.302867\n",
      "(Iteration 68501 / 76500) loss: 2.302920\n",
      "(Iteration 68601 / 76500) loss: 2.302956\n",
      "(Iteration 68701 / 76500) loss: 2.302945\n",
      "(Iteration 68801 / 76500) loss: 2.302904\n",
      "(Epoch 90 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 68901 / 76500) loss: 2.302996\n",
      "(Iteration 69001 / 76500) loss: 2.302877\n",
      "(Iteration 69101 / 76500) loss: 2.302937\n",
      "(Iteration 69201 / 76500) loss: 2.302954\n",
      "(Iteration 69301 / 76500) loss: 2.302973\n",
      "(Iteration 69401 / 76500) loss: 2.302937\n",
      "(Iteration 69501 / 76500) loss: 2.302961\n",
      "(Iteration 69601 / 76500) loss: 2.302985\n",
      "(Epoch 91 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 69701 / 76500) loss: 2.302948\n",
      "(Iteration 69801 / 76500) loss: 2.302911\n",
      "(Iteration 69901 / 76500) loss: 2.302943\n",
      "(Iteration 70001 / 76500) loss: 2.302864\n",
      "(Iteration 70101 / 76500) loss: 2.302871\n",
      "(Iteration 70201 / 76500) loss: 2.302960\n",
      "(Iteration 70301 / 76500) loss: 2.302915\n",
      "(Epoch 92 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 70401 / 76500) loss: 2.302906\n",
      "(Iteration 70501 / 76500) loss: 2.302989\n",
      "(Iteration 70601 / 76500) loss: 2.303035\n",
      "(Iteration 70701 / 76500) loss: 2.302935\n",
      "(Iteration 70801 / 76500) loss: 2.302901\n",
      "(Iteration 70901 / 76500) loss: 2.302888\n",
      "(Iteration 71001 / 76500) loss: 2.302930\n",
      "(Iteration 71101 / 76500) loss: 2.302833\n",
      "(Epoch 93 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 71201 / 76500) loss: 2.302957\n",
      "(Iteration 71301 / 76500) loss: 2.302884\n",
      "(Iteration 71401 / 76500) loss: 2.302928\n",
      "(Iteration 71501 / 76500) loss: 2.302914\n",
      "(Iteration 71601 / 76500) loss: 2.302883\n",
      "(Iteration 71701 / 76500) loss: 2.302955\n",
      "(Iteration 71801 / 76500) loss: 2.302993\n",
      "(Iteration 71901 / 76500) loss: 2.302808\n",
      "(Epoch 94 / 100) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 72001 / 76500) loss: 2.302858\n",
      "(Iteration 72101 / 76500) loss: 2.302879\n",
      "(Iteration 72201 / 76500) loss: 2.302787\n",
      "(Iteration 72301 / 76500) loss: 2.302859\n",
      "(Iteration 72401 / 76500) loss: 2.302918\n",
      "(Iteration 72501 / 76500) loss: 2.302833\n",
      "(Iteration 72601 / 76500) loss: 2.302884\n",
      "(Epoch 95 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 72701 / 76500) loss: 2.302956\n",
      "(Iteration 72801 / 76500) loss: 2.302826\n",
      "(Iteration 72901 / 76500) loss: 2.302905\n",
      "(Iteration 73001 / 76500) loss: 2.302965\n",
      "(Iteration 73101 / 76500) loss: 2.302893\n",
      "(Iteration 73201 / 76500) loss: 2.302940\n",
      "(Iteration 73301 / 76500) loss: 2.302913\n",
      "(Iteration 73401 / 76500) loss: 2.302968\n",
      "(Epoch 96 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 73501 / 76500) loss: 2.302879\n",
      "(Iteration 73601 / 76500) loss: 2.302940\n",
      "(Iteration 73701 / 76500) loss: 2.302938\n",
      "(Iteration 73801 / 76500) loss: 2.302920\n",
      "(Iteration 73901 / 76500) loss: 2.302907\n",
      "(Iteration 74001 / 76500) loss: 2.302982\n",
      "(Iteration 74101 / 76500) loss: 2.303005\n",
      "(Iteration 74201 / 76500) loss: 2.302957\n",
      "(Epoch 97 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 74301 / 76500) loss: 2.302971\n",
      "(Iteration 74401 / 76500) loss: 2.302955\n",
      "(Iteration 74501 / 76500) loss: 2.302886\n",
      "(Iteration 74601 / 76500) loss: 2.302976\n",
      "(Iteration 74701 / 76500) loss: 2.302862\n",
      "(Iteration 74801 / 76500) loss: 2.302887\n",
      "(Iteration 74901 / 76500) loss: 2.302944\n",
      "(Epoch 98 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 75001 / 76500) loss: 2.302917\n",
      "(Iteration 75101 / 76500) loss: 2.302901\n",
      "(Iteration 75201 / 76500) loss: 2.302944\n",
      "(Iteration 75301 / 76500) loss: 2.302995\n",
      "(Iteration 75401 / 76500) loss: 2.302965\n",
      "(Iteration 75501 / 76500) loss: 2.302920\n",
      "(Iteration 75601 / 76500) loss: 2.302935\n",
      "(Iteration 75701 / 76500) loss: 2.303038\n",
      "(Epoch 99 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 75801 / 76500) loss: 2.302989\n",
      "(Iteration 75901 / 76500) loss: 2.302887\n",
      "(Iteration 76001 / 76500) loss: 2.302979\n",
      "(Iteration 76101 / 76500) loss: 2.302960\n",
      "(Iteration 76201 / 76500) loss: 2.302954\n",
      "(Iteration 76301 / 76500) loss: 2.302944\n",
      "(Iteration 76401 / 76500) loss: 2.302976\n",
      "(Epoch 100 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "Training with parameters: {'hidden_size': 50, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.305478\n",
      "(Epoch 0 / 100) train acc: 0.085000; val_acc: 0.080000\n",
      "(Iteration 101 / 38200) loss: 2.305437\n",
      "(Iteration 201 / 38200) loss: 2.305390\n",
      "(Iteration 301 / 38200) loss: 2.305346\n",
      "(Epoch 1 / 100) train acc: 0.093000; val_acc: 0.068000\n",
      "(Iteration 401 / 38200) loss: 2.305309\n",
      "(Iteration 501 / 38200) loss: 2.305287\n",
      "(Iteration 601 / 38200) loss: 2.305253\n",
      "(Iteration 701 / 38200) loss: 2.305206\n",
      "(Epoch 2 / 100) train acc: 0.095000; val_acc: 0.067000\n",
      "(Iteration 801 / 38200) loss: 2.305174\n",
      "(Iteration 901 / 38200) loss: 2.305137\n",
      "(Iteration 1001 / 38200) loss: 2.305097\n",
      "(Iteration 1101 / 38200) loss: 2.305067\n",
      "(Epoch 3 / 100) train acc: 0.099000; val_acc: 0.065000\n",
      "(Iteration 1201 / 38200) loss: 2.305038\n",
      "(Iteration 1301 / 38200) loss: 2.305018\n",
      "(Iteration 1401 / 38200) loss: 2.304997\n",
      "(Iteration 1501 / 38200) loss: 2.304966\n",
      "(Epoch 4 / 100) train acc: 0.099000; val_acc: 0.073000\n",
      "(Iteration 1601 / 38200) loss: 2.304923\n",
      "(Iteration 1701 / 38200) loss: 2.304913\n",
      "(Iteration 1801 / 38200) loss: 2.304885\n",
      "(Iteration 1901 / 38200) loss: 2.304870\n",
      "(Epoch 5 / 100) train acc: 0.089000; val_acc: 0.075000\n",
      "(Iteration 2001 / 38200) loss: 2.304836\n",
      "(Iteration 2101 / 38200) loss: 2.304822\n",
      "(Iteration 2201 / 38200) loss: 2.304744\n",
      "(Epoch 6 / 100) train acc: 0.097000; val_acc: 0.085000\n",
      "(Iteration 2301 / 38200) loss: 2.304759\n",
      "(Iteration 2401 / 38200) loss: 2.304750\n",
      "(Iteration 2501 / 38200) loss: 2.304699\n",
      "(Iteration 2601 / 38200) loss: 2.304695\n",
      "(Epoch 7 / 100) train acc: 0.110000; val_acc: 0.088000\n",
      "(Iteration 2701 / 38200) loss: 2.304674\n",
      "(Iteration 2801 / 38200) loss: 2.304648\n",
      "(Iteration 2901 / 38200) loss: 2.304631\n",
      "(Iteration 3001 / 38200) loss: 2.304597\n",
      "(Epoch 8 / 100) train acc: 0.085000; val_acc: 0.107000\n",
      "(Iteration 3101 / 38200) loss: 2.304585\n",
      "(Iteration 3201 / 38200) loss: 2.304566\n",
      "(Iteration 3301 / 38200) loss: 2.304574\n",
      "(Iteration 3401 / 38200) loss: 2.304550\n",
      "(Epoch 9 / 100) train acc: 0.126000; val_acc: 0.109000\n",
      "(Iteration 3501 / 38200) loss: 2.304544\n",
      "(Iteration 3601 / 38200) loss: 2.304504\n",
      "(Iteration 3701 / 38200) loss: 2.304508\n",
      "(Iteration 3801 / 38200) loss: 2.304482\n",
      "(Epoch 10 / 100) train acc: 0.118000; val_acc: 0.099000\n",
      "(Iteration 3901 / 38200) loss: 2.304473\n",
      "(Iteration 4001 / 38200) loss: 2.304433\n",
      "(Iteration 4101 / 38200) loss: 2.304414\n",
      "(Iteration 4201 / 38200) loss: 2.304381\n",
      "(Epoch 11 / 100) train acc: 0.093000; val_acc: 0.095000\n",
      "(Iteration 4301 / 38200) loss: 2.304383\n",
      "(Iteration 4401 / 38200) loss: 2.304364\n",
      "(Iteration 4501 / 38200) loss: 2.304369\n",
      "(Epoch 12 / 100) train acc: 0.090000; val_acc: 0.098000\n",
      "(Iteration 4601 / 38200) loss: 2.304329\n",
      "(Iteration 4701 / 38200) loss: 2.304355\n",
      "(Iteration 4801 / 38200) loss: 2.304320\n",
      "(Iteration 4901 / 38200) loss: 2.304321\n",
      "(Epoch 13 / 100) train acc: 0.095000; val_acc: 0.097000\n",
      "(Iteration 5001 / 38200) loss: 2.304319\n",
      "(Iteration 5101 / 38200) loss: 2.304279\n",
      "(Iteration 5201 / 38200) loss: 2.304285\n",
      "(Iteration 5301 / 38200) loss: 2.304271\n",
      "(Epoch 14 / 100) train acc: 0.104000; val_acc: 0.098000\n",
      "(Iteration 5401 / 38200) loss: 2.304239\n",
      "(Iteration 5501 / 38200) loss: 2.304235\n",
      "(Iteration 5601 / 38200) loss: 2.304224\n",
      "(Iteration 5701 / 38200) loss: 2.304214\n",
      "(Epoch 15 / 100) train acc: 0.106000; val_acc: 0.102000\n",
      "(Iteration 5801 / 38200) loss: 2.304198\n",
      "(Iteration 5901 / 38200) loss: 2.304213\n",
      "(Iteration 6001 / 38200) loss: 2.304207\n",
      "(Iteration 6101 / 38200) loss: 2.304160\n",
      "(Epoch 16 / 100) train acc: 0.109000; val_acc: 0.081000\n",
      "(Iteration 6201 / 38200) loss: 2.304178\n",
      "(Iteration 6301 / 38200) loss: 2.304152\n",
      "(Iteration 6401 / 38200) loss: 2.304108\n",
      "(Epoch 17 / 100) train acc: 0.101000; val_acc: 0.080000\n",
      "(Iteration 6501 / 38200) loss: 2.304172\n",
      "(Iteration 6601 / 38200) loss: 2.304142\n",
      "(Iteration 6701 / 38200) loss: 2.304101\n",
      "(Iteration 6801 / 38200) loss: 2.304121\n",
      "(Epoch 18 / 100) train acc: 0.116000; val_acc: 0.081000\n",
      "(Iteration 6901 / 38200) loss: 2.304099\n",
      "(Iteration 7001 / 38200) loss: 2.304095\n",
      "(Iteration 7101 / 38200) loss: 2.304095\n",
      "(Iteration 7201 / 38200) loss: 2.304074\n",
      "(Epoch 19 / 100) train acc: 0.100000; val_acc: 0.084000\n",
      "(Iteration 7301 / 38200) loss: 2.304070\n",
      "(Iteration 7401 / 38200) loss: 2.304053\n",
      "(Iteration 7501 / 38200) loss: 2.304069\n",
      "(Iteration 7601 / 38200) loss: 2.304042\n",
      "(Epoch 20 / 100) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 7701 / 38200) loss: 2.304036\n",
      "(Iteration 7801 / 38200) loss: 2.304055\n",
      "(Iteration 7901 / 38200) loss: 2.304014\n",
      "(Iteration 8001 / 38200) loss: 2.303996\n",
      "(Epoch 21 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 8101 / 38200) loss: 2.304012\n",
      "(Iteration 8201 / 38200) loss: 2.303998\n",
      "(Iteration 8301 / 38200) loss: 2.304013\n",
      "(Iteration 8401 / 38200) loss: 2.303977\n",
      "(Epoch 22 / 100) train acc: 0.096000; val_acc: 0.072000\n",
      "(Iteration 8501 / 38200) loss: 2.303966\n",
      "(Iteration 8601 / 38200) loss: 2.303972\n",
      "(Iteration 8701 / 38200) loss: 2.303966\n",
      "(Epoch 23 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 8801 / 38200) loss: 2.303977\n",
      "(Iteration 8901 / 38200) loss: 2.303935\n",
      "(Iteration 9001 / 38200) loss: 2.303928\n",
      "(Iteration 9101 / 38200) loss: 2.303930\n",
      "(Epoch 24 / 100) train acc: 0.103000; val_acc: 0.082000\n",
      "(Iteration 9201 / 38200) loss: 2.303925\n",
      "(Iteration 9301 / 38200) loss: 2.303916\n",
      "(Iteration 9401 / 38200) loss: 2.303941\n",
      "(Iteration 9501 / 38200) loss: 2.303904\n",
      "(Epoch 25 / 100) train acc: 0.117000; val_acc: 0.089000\n",
      "(Iteration 9601 / 38200) loss: 2.303926\n",
      "(Iteration 9701 / 38200) loss: 2.303941\n",
      "(Iteration 9801 / 38200) loss: 2.303893\n",
      "(Iteration 9901 / 38200) loss: 2.303935\n",
      "(Epoch 26 / 100) train acc: 0.103000; val_acc: 0.084000\n",
      "(Iteration 10001 / 38200) loss: 2.303894\n",
      "(Iteration 10101 / 38200) loss: 2.303887\n",
      "(Iteration 10201 / 38200) loss: 2.303903\n",
      "(Iteration 10301 / 38200) loss: 2.303892\n",
      "(Epoch 27 / 100) train acc: 0.111000; val_acc: 0.085000\n",
      "(Iteration 10401 / 38200) loss: 2.303905\n",
      "(Iteration 10501 / 38200) loss: 2.303860\n",
      "(Iteration 10601 / 38200) loss: 2.303866\n",
      "(Epoch 28 / 100) train acc: 0.122000; val_acc: 0.087000\n",
      "(Iteration 10701 / 38200) loss: 2.303840\n",
      "(Iteration 10801 / 38200) loss: 2.303845\n",
      "(Iteration 10901 / 38200) loss: 2.303813\n",
      "(Iteration 11001 / 38200) loss: 2.303833\n",
      "(Epoch 29 / 100) train acc: 0.104000; val_acc: 0.072000\n",
      "(Iteration 11101 / 38200) loss: 2.303835\n",
      "(Iteration 11201 / 38200) loss: 2.303849\n",
      "(Iteration 11301 / 38200) loss: 2.303816\n",
      "(Iteration 11401 / 38200) loss: 2.303832\n",
      "(Epoch 30 / 100) train acc: 0.100000; val_acc: 0.083000\n",
      "(Iteration 11501 / 38200) loss: 2.303812\n",
      "(Iteration 11601 / 38200) loss: 2.303792\n",
      "(Iteration 11701 / 38200) loss: 2.303813\n",
      "(Iteration 11801 / 38200) loss: 2.303796\n",
      "(Epoch 31 / 100) train acc: 0.123000; val_acc: 0.087000\n",
      "(Iteration 11901 / 38200) loss: 2.303796\n",
      "(Iteration 12001 / 38200) loss: 2.303807\n",
      "(Iteration 12101 / 38200) loss: 2.303780\n",
      "(Iteration 12201 / 38200) loss: 2.303792\n",
      "(Epoch 32 / 100) train acc: 0.119000; val_acc: 0.099000\n",
      "(Iteration 12301 / 38200) loss: 2.303811\n",
      "(Iteration 12401 / 38200) loss: 2.303813\n",
      "(Iteration 12501 / 38200) loss: 2.303782\n",
      "(Iteration 12601 / 38200) loss: 2.303813\n",
      "(Epoch 33 / 100) train acc: 0.116000; val_acc: 0.095000\n",
      "(Iteration 12701 / 38200) loss: 2.303767\n",
      "(Iteration 12801 / 38200) loss: 2.303781\n",
      "(Iteration 12901 / 38200) loss: 2.303779\n",
      "(Epoch 34 / 100) train acc: 0.106000; val_acc: 0.101000\n",
      "(Iteration 13001 / 38200) loss: 2.303784\n",
      "(Iteration 13101 / 38200) loss: 2.303754\n",
      "(Iteration 13201 / 38200) loss: 2.303760\n",
      "(Iteration 13301 / 38200) loss: 2.303754\n",
      "(Epoch 35 / 100) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 13401 / 38200) loss: 2.303775\n",
      "(Iteration 13501 / 38200) loss: 2.303782\n",
      "(Iteration 13601 / 38200) loss: 2.303769\n",
      "(Iteration 13701 / 38200) loss: 2.303779\n",
      "(Epoch 36 / 100) train acc: 0.098000; val_acc: 0.085000\n",
      "(Iteration 13801 / 38200) loss: 2.303754\n",
      "(Iteration 13901 / 38200) loss: 2.303754\n",
      "(Iteration 14001 / 38200) loss: 2.303719\n",
      "(Iteration 14101 / 38200) loss: 2.303752\n",
      "(Epoch 37 / 100) train acc: 0.116000; val_acc: 0.080000\n",
      "(Iteration 14201 / 38200) loss: 2.303781\n",
      "(Iteration 14301 / 38200) loss: 2.303764\n",
      "(Iteration 14401 / 38200) loss: 2.303731\n",
      "(Iteration 14501 / 38200) loss: 2.303748\n",
      "(Epoch 38 / 100) train acc: 0.113000; val_acc: 0.083000\n",
      "(Iteration 14601 / 38200) loss: 2.303769\n",
      "(Iteration 14701 / 38200) loss: 2.303719\n",
      "(Iteration 14801 / 38200) loss: 2.303758\n",
      "(Epoch 39 / 100) train acc: 0.118000; val_acc: 0.082000\n",
      "(Iteration 14901 / 38200) loss: 2.303725\n",
      "(Iteration 15001 / 38200) loss: 2.303734\n",
      "(Iteration 15101 / 38200) loss: 2.303737\n",
      "(Iteration 15201 / 38200) loss: 2.303725\n",
      "(Epoch 40 / 100) train acc: 0.110000; val_acc: 0.081000\n",
      "(Iteration 15301 / 38200) loss: 2.303688\n",
      "(Iteration 15401 / 38200) loss: 2.303710\n",
      "(Iteration 15501 / 38200) loss: 2.303746\n",
      "(Iteration 15601 / 38200) loss: 2.303738\n",
      "(Epoch 41 / 100) train acc: 0.116000; val_acc: 0.080000\n",
      "(Iteration 15701 / 38200) loss: 2.303724\n",
      "(Iteration 15801 / 38200) loss: 2.303686\n",
      "(Iteration 15901 / 38200) loss: 2.303699\n",
      "(Iteration 16001 / 38200) loss: 2.303725\n",
      "(Epoch 42 / 100) train acc: 0.105000; val_acc: 0.080000\n",
      "(Iteration 16101 / 38200) loss: 2.303718\n",
      "(Iteration 16201 / 38200) loss: 2.303704\n",
      "(Iteration 16301 / 38200) loss: 2.303717\n",
      "(Iteration 16401 / 38200) loss: 2.303685\n",
      "(Epoch 43 / 100) train acc: 0.088000; val_acc: 0.079000\n",
      "(Iteration 16501 / 38200) loss: 2.303713\n",
      "(Iteration 16601 / 38200) loss: 2.303689\n",
      "(Iteration 16701 / 38200) loss: 2.303672\n",
      "(Iteration 16801 / 38200) loss: 2.303691\n",
      "(Epoch 44 / 100) train acc: 0.095000; val_acc: 0.080000\n",
      "(Iteration 16901 / 38200) loss: 2.303720\n",
      "(Iteration 17001 / 38200) loss: 2.303689\n",
      "(Iteration 17101 / 38200) loss: 2.303674\n",
      "(Epoch 45 / 100) train acc: 0.096000; val_acc: 0.080000\n",
      "(Iteration 17201 / 38200) loss: 2.303682\n",
      "(Iteration 17301 / 38200) loss: 2.303635\n",
      "(Iteration 17401 / 38200) loss: 2.303699\n",
      "(Iteration 17501 / 38200) loss: 2.303691\n",
      "(Epoch 46 / 100) train acc: 0.108000; val_acc: 0.080000\n",
      "(Iteration 17601 / 38200) loss: 2.303642\n",
      "(Iteration 17701 / 38200) loss: 2.303695\n",
      "(Iteration 17801 / 38200) loss: 2.303703\n",
      "(Iteration 17901 / 38200) loss: 2.303638\n",
      "(Epoch 47 / 100) train acc: 0.114000; val_acc: 0.080000\n",
      "(Iteration 18001 / 38200) loss: 2.303706\n",
      "(Iteration 18101 / 38200) loss: 2.303662\n",
      "(Iteration 18201 / 38200) loss: 2.303651\n",
      "(Iteration 18301 / 38200) loss: 2.303677\n",
      "(Epoch 48 / 100) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 18401 / 38200) loss: 2.303676\n",
      "(Iteration 18501 / 38200) loss: 2.303677\n",
      "(Iteration 18601 / 38200) loss: 2.303646\n",
      "(Iteration 18701 / 38200) loss: 2.303658\n",
      "(Epoch 49 / 100) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 18801 / 38200) loss: 2.303642\n",
      "(Iteration 18901 / 38200) loss: 2.303709\n",
      "(Iteration 19001 / 38200) loss: 2.303637\n",
      "(Epoch 50 / 100) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 19101 / 38200) loss: 2.303654\n",
      "(Iteration 19201 / 38200) loss: 2.303691\n",
      "(Iteration 19301 / 38200) loss: 2.303671\n",
      "(Iteration 19401 / 38200) loss: 2.303629\n",
      "(Epoch 51 / 100) train acc: 0.086000; val_acc: 0.079000\n",
      "(Iteration 19501 / 38200) loss: 2.303652\n",
      "(Iteration 19601 / 38200) loss: 2.303609\n",
      "(Iteration 19701 / 38200) loss: 2.303625\n",
      "(Iteration 19801 / 38200) loss: 2.303595\n",
      "(Epoch 52 / 100) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 19901 / 38200) loss: 2.303657\n",
      "(Iteration 20001 / 38200) loss: 2.303662\n",
      "(Iteration 20101 / 38200) loss: 2.303624\n",
      "(Iteration 20201 / 38200) loss: 2.303661\n",
      "(Epoch 53 / 100) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 20301 / 38200) loss: 2.303689\n",
      "(Iteration 20401 / 38200) loss: 2.303643\n",
      "(Iteration 20501 / 38200) loss: 2.303637\n",
      "(Iteration 20601 / 38200) loss: 2.303651\n",
      "(Epoch 54 / 100) train acc: 0.084000; val_acc: 0.079000\n",
      "(Iteration 20701 / 38200) loss: 2.303643\n",
      "(Iteration 20801 / 38200) loss: 2.303639\n",
      "(Iteration 20901 / 38200) loss: 2.303641\n",
      "(Iteration 21001 / 38200) loss: 2.303647\n",
      "(Epoch 55 / 100) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 21101 / 38200) loss: 2.303647\n",
      "(Iteration 21201 / 38200) loss: 2.303631\n",
      "(Iteration 21301 / 38200) loss: 2.303618\n",
      "(Epoch 56 / 100) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 21401 / 38200) loss: 2.303659\n",
      "(Iteration 21501 / 38200) loss: 2.303644\n",
      "(Iteration 21601 / 38200) loss: 2.303670\n",
      "(Iteration 21701 / 38200) loss: 2.303668\n",
      "(Epoch 57 / 100) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 21801 / 38200) loss: 2.303636\n",
      "(Iteration 21901 / 38200) loss: 2.303625\n",
      "(Iteration 22001 / 38200) loss: 2.303617\n",
      "(Iteration 22101 / 38200) loss: 2.303640\n",
      "(Epoch 58 / 100) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 22201 / 38200) loss: 2.303608\n",
      "(Iteration 22301 / 38200) loss: 2.303614\n",
      "(Iteration 22401 / 38200) loss: 2.303622\n",
      "(Iteration 22501 / 38200) loss: 2.303634\n",
      "(Epoch 59 / 100) train acc: 0.092000; val_acc: 0.079000\n",
      "(Iteration 22601 / 38200) loss: 2.303619\n",
      "(Iteration 22701 / 38200) loss: 2.303628\n",
      "(Iteration 22801 / 38200) loss: 2.303646\n",
      "(Iteration 22901 / 38200) loss: 2.303641\n",
      "(Epoch 60 / 100) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 23001 / 38200) loss: 2.303602\n",
      "(Iteration 23101 / 38200) loss: 2.303635\n",
      "(Iteration 23201 / 38200) loss: 2.303607\n",
      "(Iteration 23301 / 38200) loss: 2.303626\n",
      "(Epoch 61 / 100) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 23401 / 38200) loss: 2.303622\n",
      "(Iteration 23501 / 38200) loss: 2.303656\n",
      "(Iteration 23601 / 38200) loss: 2.303610\n",
      "(Epoch 62 / 100) train acc: 0.088000; val_acc: 0.079000\n",
      "(Iteration 23701 / 38200) loss: 2.303620\n",
      "(Iteration 23801 / 38200) loss: 2.303613\n",
      "(Iteration 23901 / 38200) loss: 2.303656\n",
      "(Iteration 24001 / 38200) loss: 2.303609\n",
      "(Epoch 63 / 100) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 24101 / 38200) loss: 2.303609\n",
      "(Iteration 24201 / 38200) loss: 2.303590\n",
      "(Iteration 24301 / 38200) loss: 2.303592\n",
      "(Iteration 24401 / 38200) loss: 2.303630\n",
      "(Epoch 64 / 100) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 24501 / 38200) loss: 2.303614\n",
      "(Iteration 24601 / 38200) loss: 2.303567\n",
      "(Iteration 24701 / 38200) loss: 2.303620\n",
      "(Iteration 24801 / 38200) loss: 2.303605\n",
      "(Epoch 65 / 100) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 24901 / 38200) loss: 2.303608\n",
      "(Iteration 25001 / 38200) loss: 2.303594\n",
      "(Iteration 25101 / 38200) loss: 2.303615\n",
      "(Iteration 25201 / 38200) loss: 2.303548\n",
      "(Epoch 66 / 100) train acc: 0.112000; val_acc: 0.079000\n",
      "(Iteration 25301 / 38200) loss: 2.303599\n",
      "(Iteration 25401 / 38200) loss: 2.303604\n",
      "(Iteration 25501 / 38200) loss: 2.303610\n",
      "(Epoch 67 / 100) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 25601 / 38200) loss: 2.303588\n",
      "(Iteration 25701 / 38200) loss: 2.303559\n",
      "(Iteration 25801 / 38200) loss: 2.303618\n",
      "(Iteration 25901 / 38200) loss: 2.303581\n",
      "(Epoch 68 / 100) train acc: 0.090000; val_acc: 0.079000\n",
      "(Iteration 26001 / 38200) loss: 2.303613\n",
      "(Iteration 26101 / 38200) loss: 2.303630\n",
      "(Iteration 26201 / 38200) loss: 2.303612\n",
      "(Iteration 26301 / 38200) loss: 2.303605\n",
      "(Epoch 69 / 100) train acc: 0.118000; val_acc: 0.079000\n",
      "(Iteration 26401 / 38200) loss: 2.303634\n",
      "(Iteration 26501 / 38200) loss: 2.303581\n",
      "(Iteration 26601 / 38200) loss: 2.303609\n",
      "(Iteration 26701 / 38200) loss: 2.303601\n",
      "(Epoch 70 / 100) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 26801 / 38200) loss: 2.303616\n",
      "(Iteration 26901 / 38200) loss: 2.303577\n",
      "(Iteration 27001 / 38200) loss: 2.303569\n",
      "(Iteration 27101 / 38200) loss: 2.303576\n",
      "(Epoch 71 / 100) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 27201 / 38200) loss: 2.303581\n",
      "(Iteration 27301 / 38200) loss: 2.303599\n",
      "(Iteration 27401 / 38200) loss: 2.303592\n",
      "(Iteration 27501 / 38200) loss: 2.303575\n",
      "(Epoch 72 / 100) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 27601 / 38200) loss: 2.303601\n",
      "(Iteration 27701 / 38200) loss: 2.303555\n",
      "(Iteration 27801 / 38200) loss: 2.303631\n",
      "(Epoch 73 / 100) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 27901 / 38200) loss: 2.303621\n",
      "(Iteration 28001 / 38200) loss: 2.303552\n",
      "(Iteration 28101 / 38200) loss: 2.303600\n",
      "(Iteration 28201 / 38200) loss: 2.303627\n",
      "(Epoch 74 / 100) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 28301 / 38200) loss: 2.303613\n",
      "(Iteration 28401 / 38200) loss: 2.303562\n",
      "(Iteration 28501 / 38200) loss: 2.303619\n",
      "(Iteration 28601 / 38200) loss: 2.303595\n",
      "(Epoch 75 / 100) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 28701 / 38200) loss: 2.303606\n",
      "(Iteration 28801 / 38200) loss: 2.303588\n",
      "(Iteration 28901 / 38200) loss: 2.303613\n",
      "(Iteration 29001 / 38200) loss: 2.303561\n",
      "(Epoch 76 / 100) train acc: 0.115000; val_acc: 0.079000\n",
      "(Iteration 29101 / 38200) loss: 2.303573\n",
      "(Iteration 29201 / 38200) loss: 2.303627\n",
      "(Iteration 29301 / 38200) loss: 2.303578\n",
      "(Iteration 29401 / 38200) loss: 2.303604\n",
      "(Epoch 77 / 100) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 29501 / 38200) loss: 2.303609\n",
      "(Iteration 29601 / 38200) loss: 2.303603\n",
      "(Iteration 29701 / 38200) loss: 2.303566\n",
      "(Epoch 78 / 100) train acc: 0.087000; val_acc: 0.079000\n",
      "(Iteration 29801 / 38200) loss: 2.303556\n",
      "(Iteration 29901 / 38200) loss: 2.303588\n",
      "(Iteration 30001 / 38200) loss: 2.303604\n",
      "(Iteration 30101 / 38200) loss: 2.303540\n",
      "(Epoch 79 / 100) train acc: 0.116000; val_acc: 0.079000\n",
      "(Iteration 30201 / 38200) loss: 2.303591\n",
      "(Iteration 30301 / 38200) loss: 2.303600\n",
      "(Iteration 30401 / 38200) loss: 2.303589\n",
      "(Iteration 30501 / 38200) loss: 2.303624\n",
      "(Epoch 80 / 100) train acc: 0.114000; val_acc: 0.079000\n",
      "(Iteration 30601 / 38200) loss: 2.303594\n",
      "(Iteration 30701 / 38200) loss: 2.303606\n",
      "(Iteration 30801 / 38200) loss: 2.303593\n",
      "(Iteration 30901 / 38200) loss: 2.303609\n",
      "(Epoch 81 / 100) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 31001 / 38200) loss: 2.303608\n",
      "(Iteration 31101 / 38200) loss: 2.303574\n",
      "(Iteration 31201 / 38200) loss: 2.303585\n",
      "(Iteration 31301 / 38200) loss: 2.303603\n",
      "(Epoch 82 / 100) train acc: 0.110000; val_acc: 0.079000\n",
      "(Iteration 31401 / 38200) loss: 2.303616\n",
      "(Iteration 31501 / 38200) loss: 2.303620\n",
      "(Iteration 31601 / 38200) loss: 2.303582\n",
      "(Iteration 31701 / 38200) loss: 2.303586\n",
      "(Epoch 83 / 100) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 31801 / 38200) loss: 2.303573\n",
      "(Iteration 31901 / 38200) loss: 2.303620\n",
      "(Iteration 32001 / 38200) loss: 2.303562\n",
      "(Epoch 84 / 100) train acc: 0.085000; val_acc: 0.079000\n",
      "(Iteration 32101 / 38200) loss: 2.303612\n",
      "(Iteration 32201 / 38200) loss: 2.303579\n",
      "(Iteration 32301 / 38200) loss: 2.303587\n",
      "(Iteration 32401 / 38200) loss: 2.303659\n",
      "(Epoch 85 / 100) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 32501 / 38200) loss: 2.303581\n",
      "(Iteration 32601 / 38200) loss: 2.303630\n",
      "(Iteration 32701 / 38200) loss: 2.303617\n",
      "(Iteration 32801 / 38200) loss: 2.303610\n",
      "(Epoch 86 / 100) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 32901 / 38200) loss: 2.303573\n",
      "(Iteration 33001 / 38200) loss: 2.303567\n",
      "(Iteration 33101 / 38200) loss: 2.303608\n",
      "(Iteration 33201 / 38200) loss: 2.303612\n",
      "(Epoch 87 / 100) train acc: 0.110000; val_acc: 0.079000\n",
      "(Iteration 33301 / 38200) loss: 2.303578\n",
      "(Iteration 33401 / 38200) loss: 2.303552\n",
      "(Iteration 33501 / 38200) loss: 2.303571\n",
      "(Iteration 33601 / 38200) loss: 2.303601\n",
      "(Epoch 88 / 100) train acc: 0.091000; val_acc: 0.079000\n",
      "(Iteration 33701 / 38200) loss: 2.303596\n",
      "(Iteration 33801 / 38200) loss: 2.303606\n",
      "(Iteration 33901 / 38200) loss: 2.303557\n",
      "(Epoch 89 / 100) train acc: 0.120000; val_acc: 0.079000\n",
      "(Iteration 34001 / 38200) loss: 2.303564\n",
      "(Iteration 34101 / 38200) loss: 2.303586\n",
      "(Iteration 34201 / 38200) loss: 2.303576\n",
      "(Iteration 34301 / 38200) loss: 2.303589\n",
      "(Epoch 90 / 100) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 34401 / 38200) loss: 2.303596\n",
      "(Iteration 34501 / 38200) loss: 2.303588\n",
      "(Iteration 34601 / 38200) loss: 2.303611\n",
      "(Iteration 34701 / 38200) loss: 2.303568\n",
      "(Epoch 91 / 100) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 34801 / 38200) loss: 2.303583\n",
      "(Iteration 34901 / 38200) loss: 2.303599\n",
      "(Iteration 35001 / 38200) loss: 2.303612\n",
      "(Iteration 35101 / 38200) loss: 2.303576\n",
      "(Epoch 92 / 100) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 35201 / 38200) loss: 2.303603\n",
      "(Iteration 35301 / 38200) loss: 2.303551\n",
      "(Iteration 35401 / 38200) loss: 2.303575\n",
      "(Iteration 35501 / 38200) loss: 2.303603\n",
      "(Epoch 93 / 100) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 35601 / 38200) loss: 2.303578\n",
      "(Iteration 35701 / 38200) loss: 2.303585\n",
      "(Iteration 35801 / 38200) loss: 2.303571\n",
      "(Iteration 35901 / 38200) loss: 2.303618\n",
      "(Epoch 94 / 100) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 36001 / 38200) loss: 2.303602\n",
      "(Iteration 36101 / 38200) loss: 2.303588\n",
      "(Iteration 36201 / 38200) loss: 2.303595\n",
      "(Epoch 95 / 100) train acc: 0.118000; val_acc: 0.079000\n",
      "(Iteration 36301 / 38200) loss: 2.303594\n",
      "(Iteration 36401 / 38200) loss: 2.303604\n",
      "(Iteration 36501 / 38200) loss: 2.303562\n",
      "(Iteration 36601 / 38200) loss: 2.303567\n",
      "(Epoch 96 / 100) train acc: 0.083000; val_acc: 0.079000\n",
      "(Iteration 36701 / 38200) loss: 2.303550\n",
      "(Iteration 36801 / 38200) loss: 2.303557\n",
      "(Iteration 36901 / 38200) loss: 2.303583\n",
      "(Iteration 37001 / 38200) loss: 2.303595\n",
      "(Epoch 97 / 100) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 37101 / 38200) loss: 2.303569\n",
      "(Iteration 37201 / 38200) loss: 2.303581\n",
      "(Iteration 37301 / 38200) loss: 2.303569\n",
      "(Iteration 37401 / 38200) loss: 2.303603\n",
      "(Epoch 98 / 100) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 37501 / 38200) loss: 2.303527\n",
      "(Iteration 37601 / 38200) loss: 2.303552\n",
      "(Iteration 37701 / 38200) loss: 2.303556\n",
      "(Iteration 37801 / 38200) loss: 2.303592\n",
      "(Epoch 99 / 100) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 37901 / 38200) loss: 2.303589\n",
      "(Iteration 38001 / 38200) loss: 2.303552\n",
      "(Iteration 38101 / 38200) loss: 2.303584\n",
      "(Epoch 100 / 100) train acc: 0.103000; val_acc: 0.079000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.306671\n",
      "(Epoch 0 / 80) train acc: 0.112000; val_acc: 0.099000\n",
      "(Iteration 101 / 61200) loss: 2.306662\n",
      "(Iteration 201 / 61200) loss: 2.306669\n",
      "(Iteration 301 / 61200) loss: 2.306660\n",
      "(Iteration 401 / 61200) loss: 2.306679\n",
      "(Iteration 501 / 61200) loss: 2.306663\n",
      "(Iteration 601 / 61200) loss: 2.306673\n",
      "(Iteration 701 / 61200) loss: 2.306676\n",
      "(Epoch 1 / 80) train acc: 0.085000; val_acc: 0.099000\n",
      "(Iteration 801 / 61200) loss: 2.306681\n",
      "(Iteration 901 / 61200) loss: 2.306672\n",
      "(Iteration 1001 / 61200) loss: 2.306651\n",
      "(Iteration 1101 / 61200) loss: 2.306683\n",
      "(Iteration 1201 / 61200) loss: 2.306677\n",
      "(Iteration 1301 / 61200) loss: 2.306659\n",
      "(Iteration 1401 / 61200) loss: 2.306647\n",
      "(Iteration 1501 / 61200) loss: 2.306670\n",
      "(Epoch 2 / 80) train acc: 0.085000; val_acc: 0.099000\n",
      "(Iteration 1601 / 61200) loss: 2.306670\n",
      "(Iteration 1701 / 61200) loss: 2.306674\n",
      "(Iteration 1801 / 61200) loss: 2.306678\n",
      "(Iteration 1901 / 61200) loss: 2.306689\n",
      "(Iteration 2001 / 61200) loss: 2.306677\n",
      "(Iteration 2101 / 61200) loss: 2.306655\n",
      "(Iteration 2201 / 61200) loss: 2.306659\n",
      "(Epoch 3 / 80) train acc: 0.099000; val_acc: 0.099000\n",
      "(Iteration 2301 / 61200) loss: 2.306671\n",
      "(Iteration 2401 / 61200) loss: 2.306674\n",
      "(Iteration 2501 / 61200) loss: 2.306655\n",
      "(Iteration 2601 / 61200) loss: 2.306651\n",
      "(Iteration 2701 / 61200) loss: 2.306665\n",
      "(Iteration 2801 / 61200) loss: 2.306678\n",
      "(Iteration 2901 / 61200) loss: 2.306672\n",
      "(Iteration 3001 / 61200) loss: 2.306670\n",
      "(Epoch 4 / 80) train acc: 0.100000; val_acc: 0.099000\n",
      "(Iteration 3101 / 61200) loss: 2.306663\n",
      "(Iteration 3201 / 61200) loss: 2.306671\n",
      "(Iteration 3301 / 61200) loss: 2.306690\n",
      "(Iteration 3401 / 61200) loss: 2.306666\n",
      "(Iteration 3501 / 61200) loss: 2.306686\n",
      "(Iteration 3601 / 61200) loss: 2.306650\n",
      "(Iteration 3701 / 61200) loss: 2.306666\n",
      "(Iteration 3801 / 61200) loss: 2.306646\n",
      "(Epoch 5 / 80) train acc: 0.085000; val_acc: 0.099000\n",
      "(Iteration 3901 / 61200) loss: 2.306677\n",
      "(Iteration 4001 / 61200) loss: 2.306645\n",
      "(Iteration 4101 / 61200) loss: 2.306660\n",
      "(Iteration 4201 / 61200) loss: 2.306675\n",
      "(Iteration 4301 / 61200) loss: 2.306660\n",
      "(Iteration 4401 / 61200) loss: 2.306669\n",
      "(Iteration 4501 / 61200) loss: 2.306674\n",
      "(Epoch 6 / 80) train acc: 0.095000; val_acc: 0.098000\n",
      "(Iteration 4601 / 61200) loss: 2.306658\n",
      "(Iteration 4701 / 61200) loss: 2.306658\n",
      "(Iteration 4801 / 61200) loss: 2.306683\n",
      "(Iteration 4901 / 61200) loss: 2.306646\n",
      "(Iteration 5001 / 61200) loss: 2.306676\n",
      "(Iteration 5101 / 61200) loss: 2.306675\n",
      "(Iteration 5201 / 61200) loss: 2.306685\n",
      "(Iteration 5301 / 61200) loss: 2.306641\n",
      "(Epoch 7 / 80) train acc: 0.088000; val_acc: 0.098000\n",
      "(Iteration 5401 / 61200) loss: 2.306659\n",
      "(Iteration 5501 / 61200) loss: 2.306654\n",
      "(Iteration 5601 / 61200) loss: 2.306651\n",
      "(Iteration 5701 / 61200) loss: 2.306676\n",
      "(Iteration 5801 / 61200) loss: 2.306662\n",
      "(Iteration 5901 / 61200) loss: 2.306666\n",
      "(Iteration 6001 / 61200) loss: 2.306654\n",
      "(Iteration 6101 / 61200) loss: 2.306673\n",
      "(Epoch 8 / 80) train acc: 0.101000; val_acc: 0.099000\n",
      "(Iteration 6201 / 61200) loss: 2.306662\n",
      "(Iteration 6301 / 61200) loss: 2.306660\n",
      "(Iteration 6401 / 61200) loss: 2.306654\n",
      "(Iteration 6501 / 61200) loss: 2.306660\n",
      "(Iteration 6601 / 61200) loss: 2.306648\n",
      "(Iteration 6701 / 61200) loss: 2.306659\n",
      "(Iteration 6801 / 61200) loss: 2.306663\n",
      "(Epoch 9 / 80) train acc: 0.109000; val_acc: 0.099000\n",
      "(Iteration 6901 / 61200) loss: 2.306678\n",
      "(Iteration 7001 / 61200) loss: 2.306656\n",
      "(Iteration 7101 / 61200) loss: 2.306689\n",
      "(Iteration 7201 / 61200) loss: 2.306673\n",
      "(Iteration 7301 / 61200) loss: 2.306663\n",
      "(Iteration 7401 / 61200) loss: 2.306668\n",
      "(Iteration 7501 / 61200) loss: 2.306667\n",
      "(Iteration 7601 / 61200) loss: 2.306654\n",
      "(Epoch 10 / 80) train acc: 0.095000; val_acc: 0.099000\n",
      "(Iteration 7701 / 61200) loss: 2.306658\n",
      "(Iteration 7801 / 61200) loss: 2.306655\n",
      "(Iteration 7901 / 61200) loss: 2.306676\n",
      "(Iteration 8001 / 61200) loss: 2.306675\n",
      "(Iteration 8101 / 61200) loss: 2.306650\n",
      "(Iteration 8201 / 61200) loss: 2.306662\n",
      "(Iteration 8301 / 61200) loss: 2.306656\n",
      "(Iteration 8401 / 61200) loss: 2.306665\n",
      "(Epoch 11 / 80) train acc: 0.106000; val_acc: 0.099000\n",
      "(Iteration 8501 / 61200) loss: 2.306672\n",
      "(Iteration 8601 / 61200) loss: 2.306668\n",
      "(Iteration 8701 / 61200) loss: 2.306662\n",
      "(Iteration 8801 / 61200) loss: 2.306664\n",
      "(Iteration 8901 / 61200) loss: 2.306660\n",
      "(Iteration 9001 / 61200) loss: 2.306676\n",
      "(Iteration 9101 / 61200) loss: 2.306678\n",
      "(Epoch 12 / 80) train acc: 0.116000; val_acc: 0.099000\n",
      "(Iteration 9201 / 61200) loss: 2.306669\n",
      "(Iteration 9301 / 61200) loss: 2.306665\n",
      "(Iteration 9401 / 61200) loss: 2.306671\n",
      "(Iteration 9501 / 61200) loss: 2.306655\n",
      "(Iteration 9601 / 61200) loss: 2.306652\n",
      "(Iteration 9701 / 61200) loss: 2.306673\n",
      "(Iteration 9801 / 61200) loss: 2.306678\n",
      "(Iteration 9901 / 61200) loss: 2.306656\n",
      "(Epoch 13 / 80) train acc: 0.082000; val_acc: 0.099000\n",
      "(Iteration 10001 / 61200) loss: 2.306654\n",
      "(Iteration 10101 / 61200) loss: 2.306674\n",
      "(Iteration 10201 / 61200) loss: 2.306653\n",
      "(Iteration 10301 / 61200) loss: 2.306674\n",
      "(Iteration 10401 / 61200) loss: 2.306664\n",
      "(Iteration 10501 / 61200) loss: 2.306655\n",
      "(Iteration 10601 / 61200) loss: 2.306664\n",
      "(Iteration 10701 / 61200) loss: 2.306655\n",
      "(Epoch 14 / 80) train acc: 0.113000; val_acc: 0.099000\n",
      "(Iteration 10801 / 61200) loss: 2.306674\n",
      "(Iteration 10901 / 61200) loss: 2.306661\n",
      "(Iteration 11001 / 61200) loss: 2.306665\n",
      "(Iteration 11101 / 61200) loss: 2.306650\n",
      "(Iteration 11201 / 61200) loss: 2.306649\n",
      "(Iteration 11301 / 61200) loss: 2.306650\n",
      "(Iteration 11401 / 61200) loss: 2.306672\n",
      "(Epoch 15 / 80) train acc: 0.103000; val_acc: 0.099000\n",
      "(Iteration 11501 / 61200) loss: 2.306657\n",
      "(Iteration 11601 / 61200) loss: 2.306683\n",
      "(Iteration 11701 / 61200) loss: 2.306672\n",
      "(Iteration 11801 / 61200) loss: 2.306671\n",
      "(Iteration 11901 / 61200) loss: 2.306686\n",
      "(Iteration 12001 / 61200) loss: 2.306653\n",
      "(Iteration 12101 / 61200) loss: 2.306672\n",
      "(Iteration 12201 / 61200) loss: 2.306667\n",
      "(Epoch 16 / 80) train acc: 0.091000; val_acc: 0.099000\n",
      "(Iteration 12301 / 61200) loss: 2.306675\n",
      "(Iteration 12401 / 61200) loss: 2.306662\n",
      "(Iteration 12501 / 61200) loss: 2.306663\n",
      "(Iteration 12601 / 61200) loss: 2.306681\n",
      "(Iteration 12701 / 61200) loss: 2.306675\n",
      "(Iteration 12801 / 61200) loss: 2.306673\n",
      "(Iteration 12901 / 61200) loss: 2.306682\n",
      "(Iteration 13001 / 61200) loss: 2.306675\n",
      "(Epoch 17 / 80) train acc: 0.099000; val_acc: 0.099000\n",
      "(Iteration 13101 / 61200) loss: 2.306662\n",
      "(Iteration 13201 / 61200) loss: 2.306681\n",
      "(Iteration 13301 / 61200) loss: 2.306681\n",
      "(Iteration 13401 / 61200) loss: 2.306682\n",
      "(Iteration 13501 / 61200) loss: 2.306652\n",
      "(Iteration 13601 / 61200) loss: 2.306643\n",
      "(Iteration 13701 / 61200) loss: 2.306673\n",
      "(Epoch 18 / 80) train acc: 0.095000; val_acc: 0.099000\n",
      "(Iteration 13801 / 61200) loss: 2.306683\n",
      "(Iteration 13901 / 61200) loss: 2.306673\n",
      "(Iteration 14001 / 61200) loss: 2.306674\n",
      "(Iteration 14101 / 61200) loss: 2.306664\n",
      "(Iteration 14201 / 61200) loss: 2.306661\n",
      "(Iteration 14301 / 61200) loss: 2.306676\n",
      "(Iteration 14401 / 61200) loss: 2.306662\n",
      "(Iteration 14501 / 61200) loss: 2.306655\n",
      "(Epoch 19 / 80) train acc: 0.093000; val_acc: 0.099000\n",
      "(Iteration 14601 / 61200) loss: 2.306660\n",
      "(Iteration 14701 / 61200) loss: 2.306665\n",
      "(Iteration 14801 / 61200) loss: 2.306665\n",
      "(Iteration 14901 / 61200) loss: 2.306663\n",
      "(Iteration 15001 / 61200) loss: 2.306673\n",
      "(Iteration 15101 / 61200) loss: 2.306664\n",
      "(Iteration 15201 / 61200) loss: 2.306663\n",
      "(Epoch 20 / 80) train acc: 0.090000; val_acc: 0.099000\n",
      "(Iteration 15301 / 61200) loss: 2.306660\n",
      "(Iteration 15401 / 61200) loss: 2.306665\n",
      "(Iteration 15501 / 61200) loss: 2.306658\n",
      "(Iteration 15601 / 61200) loss: 2.306662\n",
      "(Iteration 15701 / 61200) loss: 2.306656\n",
      "(Iteration 15801 / 61200) loss: 2.306659\n",
      "(Iteration 15901 / 61200) loss: 2.306671\n",
      "(Iteration 16001 / 61200) loss: 2.306683\n",
      "(Epoch 21 / 80) train acc: 0.096000; val_acc: 0.099000\n",
      "(Iteration 16101 / 61200) loss: 2.306677\n",
      "(Iteration 16201 / 61200) loss: 2.306670\n",
      "(Iteration 16301 / 61200) loss: 2.306655\n",
      "(Iteration 16401 / 61200) loss: 2.306668\n",
      "(Iteration 16501 / 61200) loss: 2.306665\n",
      "(Iteration 16601 / 61200) loss: 2.306670\n",
      "(Iteration 16701 / 61200) loss: 2.306662\n",
      "(Iteration 16801 / 61200) loss: 2.306657\n",
      "(Epoch 22 / 80) train acc: 0.100000; val_acc: 0.099000\n",
      "(Iteration 16901 / 61200) loss: 2.306677\n",
      "(Iteration 17001 / 61200) loss: 2.306670\n",
      "(Iteration 17101 / 61200) loss: 2.306653\n",
      "(Iteration 17201 / 61200) loss: 2.306653\n",
      "(Iteration 17301 / 61200) loss: 2.306689\n",
      "(Iteration 17401 / 61200) loss: 2.306667\n",
      "(Iteration 17501 / 61200) loss: 2.306678\n",
      "(Epoch 23 / 80) train acc: 0.098000; val_acc: 0.099000\n",
      "(Iteration 17601 / 61200) loss: 2.306674\n",
      "(Iteration 17701 / 61200) loss: 2.306662\n",
      "(Iteration 17801 / 61200) loss: 2.306673\n",
      "(Iteration 17901 / 61200) loss: 2.306662\n",
      "(Iteration 18001 / 61200) loss: 2.306654\n",
      "(Iteration 18101 / 61200) loss: 2.306672\n",
      "(Iteration 18201 / 61200) loss: 2.306657\n",
      "(Iteration 18301 / 61200) loss: 2.306679\n",
      "(Epoch 24 / 80) train acc: 0.111000; val_acc: 0.099000\n",
      "(Iteration 18401 / 61200) loss: 2.306678\n",
      "(Iteration 18501 / 61200) loss: 2.306673\n",
      "(Iteration 18601 / 61200) loss: 2.306658\n",
      "(Iteration 18701 / 61200) loss: 2.306672\n",
      "(Iteration 18801 / 61200) loss: 2.306667\n",
      "(Iteration 18901 / 61200) loss: 2.306667\n",
      "(Iteration 19001 / 61200) loss: 2.306679\n",
      "(Iteration 19101 / 61200) loss: 2.306661\n",
      "(Epoch 25 / 80) train acc: 0.109000; val_acc: 0.099000\n",
      "(Iteration 19201 / 61200) loss: 2.306687\n",
      "(Iteration 19301 / 61200) loss: 2.306659\n",
      "(Iteration 19401 / 61200) loss: 2.306673\n",
      "(Iteration 19501 / 61200) loss: 2.306680\n",
      "(Iteration 19601 / 61200) loss: 2.306669\n",
      "(Iteration 19701 / 61200) loss: 2.306656\n",
      "(Iteration 19801 / 61200) loss: 2.306657\n",
      "(Epoch 26 / 80) train acc: 0.109000; val_acc: 0.099000\n",
      "(Iteration 19901 / 61200) loss: 2.306683\n",
      "(Iteration 20001 / 61200) loss: 2.306662\n",
      "(Iteration 20101 / 61200) loss: 2.306658\n",
      "(Iteration 20201 / 61200) loss: 2.306653\n",
      "(Iteration 20301 / 61200) loss: 2.306667\n",
      "(Iteration 20401 / 61200) loss: 2.306667\n",
      "(Iteration 20501 / 61200) loss: 2.306646\n",
      "(Iteration 20601 / 61200) loss: 2.306670\n",
      "(Epoch 27 / 80) train acc: 0.102000; val_acc: 0.099000\n",
      "(Iteration 20701 / 61200) loss: 2.306672\n",
      "(Iteration 20801 / 61200) loss: 2.306655\n",
      "(Iteration 20901 / 61200) loss: 2.306681\n",
      "(Iteration 21001 / 61200) loss: 2.306667\n",
      "(Iteration 21101 / 61200) loss: 2.306674\n",
      "(Iteration 21201 / 61200) loss: 2.306665\n",
      "(Iteration 21301 / 61200) loss: 2.306681\n",
      "(Iteration 21401 / 61200) loss: 2.306660\n",
      "(Epoch 28 / 80) train acc: 0.085000; val_acc: 0.099000\n",
      "(Iteration 21501 / 61200) loss: 2.306674\n",
      "(Iteration 21601 / 61200) loss: 2.306657\n",
      "(Iteration 21701 / 61200) loss: 2.306660\n",
      "(Iteration 21801 / 61200) loss: 2.306665\n",
      "(Iteration 21901 / 61200) loss: 2.306682\n",
      "(Iteration 22001 / 61200) loss: 2.306688\n",
      "(Iteration 22101 / 61200) loss: 2.306664\n",
      "(Epoch 29 / 80) train acc: 0.097000; val_acc: 0.099000\n",
      "(Iteration 22201 / 61200) loss: 2.306666\n",
      "(Iteration 22301 / 61200) loss: 2.306667\n",
      "(Iteration 22401 / 61200) loss: 2.306664\n",
      "(Iteration 22501 / 61200) loss: 2.306656\n",
      "(Iteration 22601 / 61200) loss: 2.306659\n",
      "(Iteration 22701 / 61200) loss: 2.306662\n",
      "(Iteration 22801 / 61200) loss: 2.306691\n",
      "(Iteration 22901 / 61200) loss: 2.306681\n",
      "(Epoch 30 / 80) train acc: 0.100000; val_acc: 0.099000\n",
      "(Iteration 23001 / 61200) loss: 2.306681\n",
      "(Iteration 23101 / 61200) loss: 2.306671\n",
      "(Iteration 23201 / 61200) loss: 2.306678\n",
      "(Iteration 23301 / 61200) loss: 2.306665\n",
      "(Iteration 23401 / 61200) loss: 2.306675\n",
      "(Iteration 23501 / 61200) loss: 2.306674\n",
      "(Iteration 23601 / 61200) loss: 2.306659\n",
      "(Iteration 23701 / 61200) loss: 2.306652\n",
      "(Epoch 31 / 80) train acc: 0.092000; val_acc: 0.099000\n",
      "(Iteration 23801 / 61200) loss: 2.306663\n",
      "(Iteration 23901 / 61200) loss: 2.306672\n",
      "(Iteration 24001 / 61200) loss: 2.306678\n",
      "(Iteration 24101 / 61200) loss: 2.306663\n",
      "(Iteration 24201 / 61200) loss: 2.306639\n",
      "(Iteration 24301 / 61200) loss: 2.306662\n",
      "(Iteration 24401 / 61200) loss: 2.306668\n",
      "(Epoch 32 / 80) train acc: 0.094000; val_acc: 0.099000\n",
      "(Iteration 24501 / 61200) loss: 2.306664\n",
      "(Iteration 24601 / 61200) loss: 2.306659\n",
      "(Iteration 24701 / 61200) loss: 2.306678\n",
      "(Iteration 24801 / 61200) loss: 2.306652\n",
      "(Iteration 24901 / 61200) loss: 2.306692\n",
      "(Iteration 25001 / 61200) loss: 2.306677\n",
      "(Iteration 25101 / 61200) loss: 2.306667\n",
      "(Iteration 25201 / 61200) loss: 2.306668\n",
      "(Epoch 33 / 80) train acc: 0.088000; val_acc: 0.099000\n",
      "(Iteration 25301 / 61200) loss: 2.306653\n",
      "(Iteration 25401 / 61200) loss: 2.306666\n",
      "(Iteration 25501 / 61200) loss: 2.306655\n",
      "(Iteration 25601 / 61200) loss: 2.306662\n",
      "(Iteration 25701 / 61200) loss: 2.306664\n",
      "(Iteration 25801 / 61200) loss: 2.306670\n",
      "(Iteration 25901 / 61200) loss: 2.306659\n",
      "(Iteration 26001 / 61200) loss: 2.306683\n",
      "(Epoch 34 / 80) train acc: 0.099000; val_acc: 0.099000\n",
      "(Iteration 26101 / 61200) loss: 2.306648\n",
      "(Iteration 26201 / 61200) loss: 2.306645\n",
      "(Iteration 26301 / 61200) loss: 2.306661\n",
      "(Iteration 26401 / 61200) loss: 2.306657\n",
      "(Iteration 26501 / 61200) loss: 2.306659\n",
      "(Iteration 26601 / 61200) loss: 2.306660\n",
      "(Iteration 26701 / 61200) loss: 2.306666\n",
      "(Epoch 35 / 80) train acc: 0.082000; val_acc: 0.099000\n",
      "(Iteration 26801 / 61200) loss: 2.306666\n",
      "(Iteration 26901 / 61200) loss: 2.306667\n",
      "(Iteration 27001 / 61200) loss: 2.306654\n",
      "(Iteration 27101 / 61200) loss: 2.306658\n",
      "(Iteration 27201 / 61200) loss: 2.306662\n",
      "(Iteration 27301 / 61200) loss: 2.306662\n",
      "(Iteration 27401 / 61200) loss: 2.306666\n",
      "(Iteration 27501 / 61200) loss: 2.306670\n",
      "(Epoch 36 / 80) train acc: 0.107000; val_acc: 0.099000\n",
      "(Iteration 27601 / 61200) loss: 2.306656\n",
      "(Iteration 27701 / 61200) loss: 2.306663\n",
      "(Iteration 27801 / 61200) loss: 2.306660\n",
      "(Iteration 27901 / 61200) loss: 2.306671\n",
      "(Iteration 28001 / 61200) loss: 2.306658\n",
      "(Iteration 28101 / 61200) loss: 2.306682\n",
      "(Iteration 28201 / 61200) loss: 2.306693\n",
      "(Iteration 28301 / 61200) loss: 2.306670\n",
      "(Epoch 37 / 80) train acc: 0.095000; val_acc: 0.099000\n",
      "(Iteration 28401 / 61200) loss: 2.306680\n",
      "(Iteration 28501 / 61200) loss: 2.306663\n",
      "(Iteration 28601 / 61200) loss: 2.306652\n",
      "(Iteration 28701 / 61200) loss: 2.306662\n",
      "(Iteration 28801 / 61200) loss: 2.306658\n",
      "(Iteration 28901 / 61200) loss: 2.306666\n",
      "(Iteration 29001 / 61200) loss: 2.306671\n",
      "(Epoch 38 / 80) train acc: 0.095000; val_acc: 0.099000\n",
      "(Iteration 29101 / 61200) loss: 2.306670\n",
      "(Iteration 29201 / 61200) loss: 2.306670\n",
      "(Iteration 29301 / 61200) loss: 2.306670\n",
      "(Iteration 29401 / 61200) loss: 2.306669\n",
      "(Iteration 29501 / 61200) loss: 2.306663\n",
      "(Iteration 29601 / 61200) loss: 2.306656\n",
      "(Iteration 29701 / 61200) loss: 2.306683\n",
      "(Iteration 29801 / 61200) loss: 2.306660\n",
      "(Epoch 39 / 80) train acc: 0.092000; val_acc: 0.099000\n",
      "(Iteration 29901 / 61200) loss: 2.306670\n",
      "(Iteration 30001 / 61200) loss: 2.306658\n",
      "(Iteration 30101 / 61200) loss: 2.306674\n",
      "(Iteration 30201 / 61200) loss: 2.306665\n",
      "(Iteration 30301 / 61200) loss: 2.306676\n",
      "(Iteration 30401 / 61200) loss: 2.306655\n",
      "(Iteration 30501 / 61200) loss: 2.306661\n",
      "(Epoch 40 / 80) train acc: 0.091000; val_acc: 0.099000\n",
      "(Iteration 30601 / 61200) loss: 2.306658\n",
      "(Iteration 30701 / 61200) loss: 2.306676\n",
      "(Iteration 30801 / 61200) loss: 2.306650\n",
      "(Iteration 30901 / 61200) loss: 2.306664\n",
      "(Iteration 31001 / 61200) loss: 2.306677\n",
      "(Iteration 31101 / 61200) loss: 2.306670\n",
      "(Iteration 31201 / 61200) loss: 2.306679\n",
      "(Iteration 31301 / 61200) loss: 2.306665\n",
      "(Epoch 41 / 80) train acc: 0.089000; val_acc: 0.099000\n",
      "(Iteration 31401 / 61200) loss: 2.306679\n",
      "(Iteration 31501 / 61200) loss: 2.306665\n",
      "(Iteration 31601 / 61200) loss: 2.306672\n",
      "(Iteration 31701 / 61200) loss: 2.306669\n",
      "(Iteration 31801 / 61200) loss: 2.306662\n",
      "(Iteration 31901 / 61200) loss: 2.306680\n",
      "(Iteration 32001 / 61200) loss: 2.306665\n",
      "(Iteration 32101 / 61200) loss: 2.306663\n",
      "(Epoch 42 / 80) train acc: 0.076000; val_acc: 0.099000\n",
      "(Iteration 32201 / 61200) loss: 2.306674\n",
      "(Iteration 32301 / 61200) loss: 2.306667\n",
      "(Iteration 32401 / 61200) loss: 2.306673\n",
      "(Iteration 32501 / 61200) loss: 2.306658\n",
      "(Iteration 32601 / 61200) loss: 2.306642\n",
      "(Iteration 32701 / 61200) loss: 2.306646\n",
      "(Iteration 32801 / 61200) loss: 2.306674\n",
      "(Epoch 43 / 80) train acc: 0.100000; val_acc: 0.099000\n",
      "(Iteration 32901 / 61200) loss: 2.306671\n",
      "(Iteration 33001 / 61200) loss: 2.306661\n",
      "(Iteration 33101 / 61200) loss: 2.306641\n",
      "(Iteration 33201 / 61200) loss: 2.306651\n",
      "(Iteration 33301 / 61200) loss: 2.306663\n",
      "(Iteration 33401 / 61200) loss: 2.306673\n",
      "(Iteration 33501 / 61200) loss: 2.306674\n",
      "(Iteration 33601 / 61200) loss: 2.306678\n",
      "(Epoch 44 / 80) train acc: 0.072000; val_acc: 0.099000\n",
      "(Iteration 33701 / 61200) loss: 2.306657\n",
      "(Iteration 33801 / 61200) loss: 2.306656\n",
      "(Iteration 33901 / 61200) loss: 2.306679\n",
      "(Iteration 34001 / 61200) loss: 2.306679\n",
      "(Iteration 34101 / 61200) loss: 2.306637\n",
      "(Iteration 34201 / 61200) loss: 2.306661\n",
      "(Iteration 34301 / 61200) loss: 2.306652\n",
      "(Iteration 34401 / 61200) loss: 2.306665\n",
      "(Epoch 45 / 80) train acc: 0.098000; val_acc: 0.099000\n",
      "(Iteration 34501 / 61200) loss: 2.306642\n",
      "(Iteration 34601 / 61200) loss: 2.306672\n",
      "(Iteration 34701 / 61200) loss: 2.306675\n",
      "(Iteration 34801 / 61200) loss: 2.306656\n",
      "(Iteration 34901 / 61200) loss: 2.306679\n",
      "(Iteration 35001 / 61200) loss: 2.306696\n",
      "(Iteration 35101 / 61200) loss: 2.306662\n",
      "(Epoch 46 / 80) train acc: 0.097000; val_acc: 0.099000\n",
      "(Iteration 35201 / 61200) loss: 2.306670\n",
      "(Iteration 35301 / 61200) loss: 2.306663\n",
      "(Iteration 35401 / 61200) loss: 2.306652\n",
      "(Iteration 35501 / 61200) loss: 2.306672\n",
      "(Iteration 35601 / 61200) loss: 2.306670\n",
      "(Iteration 35701 / 61200) loss: 2.306664\n",
      "(Iteration 35801 / 61200) loss: 2.306667\n",
      "(Iteration 35901 / 61200) loss: 2.306676\n",
      "(Epoch 47 / 80) train acc: 0.100000; val_acc: 0.099000\n",
      "(Iteration 36001 / 61200) loss: 2.306675\n",
      "(Iteration 36101 / 61200) loss: 2.306663\n",
      "(Iteration 36201 / 61200) loss: 2.306678\n",
      "(Iteration 36301 / 61200) loss: 2.306665\n",
      "(Iteration 36401 / 61200) loss: 2.306648\n",
      "(Iteration 36501 / 61200) loss: 2.306649\n",
      "(Iteration 36601 / 61200) loss: 2.306663\n",
      "(Iteration 36701 / 61200) loss: 2.306671\n",
      "(Epoch 48 / 80) train acc: 0.089000; val_acc: 0.099000\n",
      "(Iteration 36801 / 61200) loss: 2.306672\n",
      "(Iteration 36901 / 61200) loss: 2.306670\n",
      "(Iteration 37001 / 61200) loss: 2.306676\n",
      "(Iteration 37101 / 61200) loss: 2.306678\n",
      "(Iteration 37201 / 61200) loss: 2.306661\n",
      "(Iteration 37301 / 61200) loss: 2.306670\n",
      "(Iteration 37401 / 61200) loss: 2.306650\n",
      "(Epoch 49 / 80) train acc: 0.094000; val_acc: 0.099000\n",
      "(Iteration 37501 / 61200) loss: 2.306681\n",
      "(Iteration 37601 / 61200) loss: 2.306661\n",
      "(Iteration 37701 / 61200) loss: 2.306662\n",
      "(Iteration 37801 / 61200) loss: 2.306670\n",
      "(Iteration 37901 / 61200) loss: 2.306673\n",
      "(Iteration 38001 / 61200) loss: 2.306677\n",
      "(Iteration 38101 / 61200) loss: 2.306666\n",
      "(Iteration 38201 / 61200) loss: 2.306664\n",
      "(Epoch 50 / 80) train acc: 0.087000; val_acc: 0.099000\n",
      "(Iteration 38301 / 61200) loss: 2.306656\n",
      "(Iteration 38401 / 61200) loss: 2.306671\n",
      "(Iteration 38501 / 61200) loss: 2.306643\n",
      "(Iteration 38601 / 61200) loss: 2.306665\n",
      "(Iteration 38701 / 61200) loss: 2.306663\n",
      "(Iteration 38801 / 61200) loss: 2.306664\n",
      "(Iteration 38901 / 61200) loss: 2.306662\n",
      "(Iteration 39001 / 61200) loss: 2.306698\n",
      "(Epoch 51 / 80) train acc: 0.093000; val_acc: 0.099000\n",
      "(Iteration 39101 / 61200) loss: 2.306660\n",
      "(Iteration 39201 / 61200) loss: 2.306663\n",
      "(Iteration 39301 / 61200) loss: 2.306681\n",
      "(Iteration 39401 / 61200) loss: 2.306669\n",
      "(Iteration 39501 / 61200) loss: 2.306667\n",
      "(Iteration 39601 / 61200) loss: 2.306662\n",
      "(Iteration 39701 / 61200) loss: 2.306659\n",
      "(Epoch 52 / 80) train acc: 0.105000; val_acc: 0.099000\n",
      "(Iteration 39801 / 61200) loss: 2.306683\n",
      "(Iteration 39901 / 61200) loss: 2.306654\n",
      "(Iteration 40001 / 61200) loss: 2.306660\n",
      "(Iteration 40101 / 61200) loss: 2.306677\n",
      "(Iteration 40201 / 61200) loss: 2.306661\n",
      "(Iteration 40301 / 61200) loss: 2.306654\n",
      "(Iteration 40401 / 61200) loss: 2.306670\n",
      "(Iteration 40501 / 61200) loss: 2.306669\n",
      "(Epoch 53 / 80) train acc: 0.106000; val_acc: 0.099000\n",
      "(Iteration 40601 / 61200) loss: 2.306670\n",
      "(Iteration 40701 / 61200) loss: 2.306667\n",
      "(Iteration 40801 / 61200) loss: 2.306675\n",
      "(Iteration 40901 / 61200) loss: 2.306668\n",
      "(Iteration 41001 / 61200) loss: 2.306679\n",
      "(Iteration 41101 / 61200) loss: 2.306655\n",
      "(Iteration 41201 / 61200) loss: 2.306673\n",
      "(Iteration 41301 / 61200) loss: 2.306655\n",
      "(Epoch 54 / 80) train acc: 0.093000; val_acc: 0.099000\n",
      "(Iteration 41401 / 61200) loss: 2.306643\n",
      "(Iteration 41501 / 61200) loss: 2.306660\n",
      "(Iteration 41601 / 61200) loss: 2.306677\n",
      "(Iteration 41701 / 61200) loss: 2.306663\n",
      "(Iteration 41801 / 61200) loss: 2.306663\n",
      "(Iteration 41901 / 61200) loss: 2.306666\n",
      "(Iteration 42001 / 61200) loss: 2.306662\n",
      "(Epoch 55 / 80) train acc: 0.088000; val_acc: 0.099000\n",
      "(Iteration 42101 / 61200) loss: 2.306660\n",
      "(Iteration 42201 / 61200) loss: 2.306653\n",
      "(Iteration 42301 / 61200) loss: 2.306667\n",
      "(Iteration 42401 / 61200) loss: 2.306674\n",
      "(Iteration 42501 / 61200) loss: 2.306662\n",
      "(Iteration 42601 / 61200) loss: 2.306676\n",
      "(Iteration 42701 / 61200) loss: 2.306680\n",
      "(Iteration 42801 / 61200) loss: 2.306670\n",
      "(Epoch 56 / 80) train acc: 0.130000; val_acc: 0.099000\n",
      "(Iteration 42901 / 61200) loss: 2.306653\n",
      "(Iteration 43001 / 61200) loss: 2.306679\n",
      "(Iteration 43101 / 61200) loss: 2.306661\n",
      "(Iteration 43201 / 61200) loss: 2.306687\n",
      "(Iteration 43301 / 61200) loss: 2.306683\n",
      "(Iteration 43401 / 61200) loss: 2.306664\n",
      "(Iteration 43501 / 61200) loss: 2.306657\n",
      "(Iteration 43601 / 61200) loss: 2.306670\n",
      "(Epoch 57 / 80) train acc: 0.110000; val_acc: 0.099000\n",
      "(Iteration 43701 / 61200) loss: 2.306658\n",
      "(Iteration 43801 / 61200) loss: 2.306673\n",
      "(Iteration 43901 / 61200) loss: 2.306661\n",
      "(Iteration 44001 / 61200) loss: 2.306675\n",
      "(Iteration 44101 / 61200) loss: 2.306657\n",
      "(Iteration 44201 / 61200) loss: 2.306665\n",
      "(Iteration 44301 / 61200) loss: 2.306683\n",
      "(Epoch 58 / 80) train acc: 0.098000; val_acc: 0.099000\n",
      "(Iteration 44401 / 61200) loss: 2.306658\n",
      "(Iteration 44501 / 61200) loss: 2.306680\n",
      "(Iteration 44601 / 61200) loss: 2.306673\n",
      "(Iteration 44701 / 61200) loss: 2.306659\n",
      "(Iteration 44801 / 61200) loss: 2.306661\n",
      "(Iteration 44901 / 61200) loss: 2.306682\n",
      "(Iteration 45001 / 61200) loss: 2.306668\n",
      "(Iteration 45101 / 61200) loss: 2.306668\n",
      "(Epoch 59 / 80) train acc: 0.095000; val_acc: 0.099000\n",
      "(Iteration 45201 / 61200) loss: 2.306654\n",
      "(Iteration 45301 / 61200) loss: 2.306678\n",
      "(Iteration 45401 / 61200) loss: 2.306668\n",
      "(Iteration 45501 / 61200) loss: 2.306680\n",
      "(Iteration 45601 / 61200) loss: 2.306661\n",
      "(Iteration 45701 / 61200) loss: 2.306665\n",
      "(Iteration 45801 / 61200) loss: 2.306670\n",
      "(Epoch 60 / 80) train acc: 0.093000; val_acc: 0.099000\n",
      "(Iteration 45901 / 61200) loss: 2.306679\n",
      "(Iteration 46001 / 61200) loss: 2.306668\n",
      "(Iteration 46101 / 61200) loss: 2.306654\n",
      "(Iteration 46201 / 61200) loss: 2.306668\n",
      "(Iteration 46301 / 61200) loss: 2.306669\n",
      "(Iteration 46401 / 61200) loss: 2.306668\n",
      "(Iteration 46501 / 61200) loss: 2.306690\n",
      "(Iteration 46601 / 61200) loss: 2.306669\n",
      "(Epoch 61 / 80) train acc: 0.110000; val_acc: 0.099000\n",
      "(Iteration 46701 / 61200) loss: 2.306655\n",
      "(Iteration 46801 / 61200) loss: 2.306669\n",
      "(Iteration 46901 / 61200) loss: 2.306661\n",
      "(Iteration 47001 / 61200) loss: 2.306664\n",
      "(Iteration 47101 / 61200) loss: 2.306674\n",
      "(Iteration 47201 / 61200) loss: 2.306666\n",
      "(Iteration 47301 / 61200) loss: 2.306666\n",
      "(Iteration 47401 / 61200) loss: 2.306664\n",
      "(Epoch 62 / 80) train acc: 0.097000; val_acc: 0.099000\n",
      "(Iteration 47501 / 61200) loss: 2.306648\n",
      "(Iteration 47601 / 61200) loss: 2.306662\n",
      "(Iteration 47701 / 61200) loss: 2.306671\n",
      "(Iteration 47801 / 61200) loss: 2.306678\n",
      "(Iteration 47901 / 61200) loss: 2.306682\n",
      "(Iteration 48001 / 61200) loss: 2.306646\n",
      "(Iteration 48101 / 61200) loss: 2.306685\n",
      "(Epoch 63 / 80) train acc: 0.105000; val_acc: 0.099000\n",
      "(Iteration 48201 / 61200) loss: 2.306659\n",
      "(Iteration 48301 / 61200) loss: 2.306679\n",
      "(Iteration 48401 / 61200) loss: 2.306665\n",
      "(Iteration 48501 / 61200) loss: 2.306662\n",
      "(Iteration 48601 / 61200) loss: 2.306682\n",
      "(Iteration 48701 / 61200) loss: 2.306661\n",
      "(Iteration 48801 / 61200) loss: 2.306667\n",
      "(Iteration 48901 / 61200) loss: 2.306665\n",
      "(Epoch 64 / 80) train acc: 0.106000; val_acc: 0.099000\n",
      "(Iteration 49001 / 61200) loss: 2.306659\n",
      "(Iteration 49101 / 61200) loss: 2.306664\n",
      "(Iteration 49201 / 61200) loss: 2.306670\n",
      "(Iteration 49301 / 61200) loss: 2.306666\n",
      "(Iteration 49401 / 61200) loss: 2.306664\n",
      "(Iteration 49501 / 61200) loss: 2.306669\n",
      "(Iteration 49601 / 61200) loss: 2.306677\n",
      "(Iteration 49701 / 61200) loss: 2.306660\n",
      "(Epoch 65 / 80) train acc: 0.100000; val_acc: 0.099000\n",
      "(Iteration 49801 / 61200) loss: 2.306671\n",
      "(Iteration 49901 / 61200) loss: 2.306679\n",
      "(Iteration 50001 / 61200) loss: 2.306664\n",
      "(Iteration 50101 / 61200) loss: 2.306668\n",
      "(Iteration 50201 / 61200) loss: 2.306663\n",
      "(Iteration 50301 / 61200) loss: 2.306662\n",
      "(Iteration 50401 / 61200) loss: 2.306665\n",
      "(Epoch 66 / 80) train acc: 0.106000; val_acc: 0.099000\n",
      "(Iteration 50501 / 61200) loss: 2.306666\n",
      "(Iteration 50601 / 61200) loss: 2.306662\n",
      "(Iteration 50701 / 61200) loss: 2.306676\n",
      "(Iteration 50801 / 61200) loss: 2.306660\n",
      "(Iteration 50901 / 61200) loss: 2.306659\n",
      "(Iteration 51001 / 61200) loss: 2.306661\n",
      "(Iteration 51101 / 61200) loss: 2.306665\n",
      "(Iteration 51201 / 61200) loss: 2.306659\n",
      "(Epoch 67 / 80) train acc: 0.094000; val_acc: 0.099000\n",
      "(Iteration 51301 / 61200) loss: 2.306683\n",
      "(Iteration 51401 / 61200) loss: 2.306662\n",
      "(Iteration 51501 / 61200) loss: 2.306646\n",
      "(Iteration 51601 / 61200) loss: 2.306668\n",
      "(Iteration 51701 / 61200) loss: 2.306654\n",
      "(Iteration 51801 / 61200) loss: 2.306670\n",
      "(Iteration 51901 / 61200) loss: 2.306673\n",
      "(Iteration 52001 / 61200) loss: 2.306664\n",
      "(Epoch 68 / 80) train acc: 0.082000; val_acc: 0.099000\n",
      "(Iteration 52101 / 61200) loss: 2.306655\n",
      "(Iteration 52201 / 61200) loss: 2.306654\n",
      "(Iteration 52301 / 61200) loss: 2.306677\n",
      "(Iteration 52401 / 61200) loss: 2.306655\n",
      "(Iteration 52501 / 61200) loss: 2.306675\n",
      "(Iteration 52601 / 61200) loss: 2.306706\n",
      "(Iteration 52701 / 61200) loss: 2.306651\n",
      "(Epoch 69 / 80) train acc: 0.099000; val_acc: 0.099000\n",
      "(Iteration 52801 / 61200) loss: 2.306644\n",
      "(Iteration 52901 / 61200) loss: 2.306664\n",
      "(Iteration 53001 / 61200) loss: 2.306649\n",
      "(Iteration 53101 / 61200) loss: 2.306668\n",
      "(Iteration 53201 / 61200) loss: 2.306679\n",
      "(Iteration 53301 / 61200) loss: 2.306671\n",
      "(Iteration 53401 / 61200) loss: 2.306677\n",
      "(Iteration 53501 / 61200) loss: 2.306687\n",
      "(Epoch 70 / 80) train acc: 0.082000; val_acc: 0.099000\n",
      "(Iteration 53601 / 61200) loss: 2.306667\n",
      "(Iteration 53701 / 61200) loss: 2.306665\n",
      "(Iteration 53801 / 61200) loss: 2.306665\n",
      "(Iteration 53901 / 61200) loss: 2.306658\n",
      "(Iteration 54001 / 61200) loss: 2.306655\n",
      "(Iteration 54101 / 61200) loss: 2.306674\n",
      "(Iteration 54201 / 61200) loss: 2.306677\n",
      "(Iteration 54301 / 61200) loss: 2.306676\n",
      "(Epoch 71 / 80) train acc: 0.099000; val_acc: 0.099000\n",
      "(Iteration 54401 / 61200) loss: 2.306669\n",
      "(Iteration 54501 / 61200) loss: 2.306662\n",
      "(Iteration 54601 / 61200) loss: 2.306669\n",
      "(Iteration 54701 / 61200) loss: 2.306679\n",
      "(Iteration 54801 / 61200) loss: 2.306676\n",
      "(Iteration 54901 / 61200) loss: 2.306669\n",
      "(Iteration 55001 / 61200) loss: 2.306697\n",
      "(Epoch 72 / 80) train acc: 0.096000; val_acc: 0.099000\n",
      "(Iteration 55101 / 61200) loss: 2.306657\n",
      "(Iteration 55201 / 61200) loss: 2.306675\n",
      "(Iteration 55301 / 61200) loss: 2.306683\n",
      "(Iteration 55401 / 61200) loss: 2.306663\n",
      "(Iteration 55501 / 61200) loss: 2.306678\n",
      "(Iteration 55601 / 61200) loss: 2.306658\n",
      "(Iteration 55701 / 61200) loss: 2.306654\n",
      "(Iteration 55801 / 61200) loss: 2.306676\n",
      "(Epoch 73 / 80) train acc: 0.087000; val_acc: 0.099000\n",
      "(Iteration 55901 / 61200) loss: 2.306673\n",
      "(Iteration 56001 / 61200) loss: 2.306649\n",
      "(Iteration 56101 / 61200) loss: 2.306664\n",
      "(Iteration 56201 / 61200) loss: 2.306683\n",
      "(Iteration 56301 / 61200) loss: 2.306661\n",
      "(Iteration 56401 / 61200) loss: 2.306658\n",
      "(Iteration 56501 / 61200) loss: 2.306667\n",
      "(Iteration 56601 / 61200) loss: 2.306685\n",
      "(Epoch 74 / 80) train acc: 0.103000; val_acc: 0.099000\n",
      "(Iteration 56701 / 61200) loss: 2.306662\n",
      "(Iteration 56801 / 61200) loss: 2.306674\n",
      "(Iteration 56901 / 61200) loss: 2.306656\n",
      "(Iteration 57001 / 61200) loss: 2.306670\n",
      "(Iteration 57101 / 61200) loss: 2.306663\n",
      "(Iteration 57201 / 61200) loss: 2.306669\n",
      "(Iteration 57301 / 61200) loss: 2.306651\n",
      "(Epoch 75 / 80) train acc: 0.085000; val_acc: 0.099000\n",
      "(Iteration 57401 / 61200) loss: 2.306679\n",
      "(Iteration 57501 / 61200) loss: 2.306660\n",
      "(Iteration 57601 / 61200) loss: 2.306659\n",
      "(Iteration 57701 / 61200) loss: 2.306677\n",
      "(Iteration 57801 / 61200) loss: 2.306653\n",
      "(Iteration 57901 / 61200) loss: 2.306658\n",
      "(Iteration 58001 / 61200) loss: 2.306669\n",
      "(Iteration 58101 / 61200) loss: 2.306676\n",
      "(Epoch 76 / 80) train acc: 0.087000; val_acc: 0.099000\n",
      "(Iteration 58201 / 61200) loss: 2.306665\n",
      "(Iteration 58301 / 61200) loss: 2.306678\n",
      "(Iteration 58401 / 61200) loss: 2.306654\n",
      "(Iteration 58501 / 61200) loss: 2.306676\n",
      "(Iteration 58601 / 61200) loss: 2.306649\n",
      "(Iteration 58701 / 61200) loss: 2.306673\n",
      "(Iteration 58801 / 61200) loss: 2.306674\n",
      "(Iteration 58901 / 61200) loss: 2.306665\n",
      "(Epoch 77 / 80) train acc: 0.081000; val_acc: 0.099000\n",
      "(Iteration 59001 / 61200) loss: 2.306681\n",
      "(Iteration 59101 / 61200) loss: 2.306662\n",
      "(Iteration 59201 / 61200) loss: 2.306683\n",
      "(Iteration 59301 / 61200) loss: 2.306684\n",
      "(Iteration 59401 / 61200) loss: 2.306665\n",
      "(Iteration 59501 / 61200) loss: 2.306670\n",
      "(Iteration 59601 / 61200) loss: 2.306659\n",
      "(Epoch 78 / 80) train acc: 0.096000; val_acc: 0.099000\n",
      "(Iteration 59701 / 61200) loss: 2.306683\n",
      "(Iteration 59801 / 61200) loss: 2.306670\n",
      "(Iteration 59901 / 61200) loss: 2.306674\n",
      "(Iteration 60001 / 61200) loss: 2.306653\n",
      "(Iteration 60101 / 61200) loss: 2.306659\n",
      "(Iteration 60201 / 61200) loss: 2.306641\n",
      "(Iteration 60301 / 61200) loss: 2.306641\n",
      "(Iteration 60401 / 61200) loss: 2.306664\n",
      "(Epoch 79 / 80) train acc: 0.091000; val_acc: 0.099000\n",
      "(Iteration 60501 / 61200) loss: 2.306648\n",
      "(Iteration 60601 / 61200) loss: 2.306662\n",
      "(Iteration 60701 / 61200) loss: 2.306685\n",
      "(Iteration 60801 / 61200) loss: 2.306655\n",
      "(Iteration 60901 / 61200) loss: 2.306655\n",
      "(Iteration 61001 / 61200) loss: 2.306674\n",
      "(Iteration 61101 / 61200) loss: 2.306651\n",
      "(Epoch 80 / 80) train acc: 0.094000; val_acc: 0.099000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.306653\n",
      "(Epoch 0 / 80) train acc: 0.071000; val_acc: 0.088000\n",
      "(Iteration 101 / 30560) loss: 2.306648\n",
      "(Iteration 201 / 30560) loss: 2.306647\n",
      "(Iteration 301 / 30560) loss: 2.306641\n",
      "(Epoch 1 / 80) train acc: 0.088000; val_acc: 0.088000\n",
      "(Iteration 401 / 30560) loss: 2.306649\n",
      "(Iteration 501 / 30560) loss: 2.306646\n",
      "(Iteration 601 / 30560) loss: 2.306652\n",
      "(Iteration 701 / 30560) loss: 2.306650\n",
      "(Epoch 2 / 80) train acc: 0.078000; val_acc: 0.088000\n",
      "(Iteration 801 / 30560) loss: 2.306638\n",
      "(Iteration 901 / 30560) loss: 2.306650\n",
      "(Iteration 1001 / 30560) loss: 2.306654\n",
      "(Iteration 1101 / 30560) loss: 2.306647\n",
      "(Epoch 3 / 80) train acc: 0.073000; val_acc: 0.088000\n",
      "(Iteration 1201 / 30560) loss: 2.306648\n",
      "(Iteration 1301 / 30560) loss: 2.306649\n",
      "(Iteration 1401 / 30560) loss: 2.306643\n",
      "(Iteration 1501 / 30560) loss: 2.306664\n",
      "(Epoch 4 / 80) train acc: 0.088000; val_acc: 0.088000\n",
      "(Iteration 1601 / 30560) loss: 2.306648\n",
      "(Iteration 1701 / 30560) loss: 2.306646\n",
      "(Iteration 1801 / 30560) loss: 2.306660\n",
      "(Iteration 1901 / 30560) loss: 2.306646\n",
      "(Epoch 5 / 80) train acc: 0.077000; val_acc: 0.088000\n",
      "(Iteration 2001 / 30560) loss: 2.306640\n",
      "(Iteration 2101 / 30560) loss: 2.306649\n",
      "(Iteration 2201 / 30560) loss: 2.306661\n",
      "(Epoch 6 / 80) train acc: 0.074000; val_acc: 0.088000\n",
      "(Iteration 2301 / 30560) loss: 2.306646\n",
      "(Iteration 2401 / 30560) loss: 2.306648\n",
      "(Iteration 2501 / 30560) loss: 2.306668\n",
      "(Iteration 2601 / 30560) loss: 2.306660\n",
      "(Epoch 7 / 80) train acc: 0.087000; val_acc: 0.088000\n",
      "(Iteration 2701 / 30560) loss: 2.306643\n",
      "(Iteration 2801 / 30560) loss: 2.306656\n",
      "(Iteration 2901 / 30560) loss: 2.306646\n",
      "(Iteration 3001 / 30560) loss: 2.306637\n",
      "(Epoch 8 / 80) train acc: 0.093000; val_acc: 0.088000\n",
      "(Iteration 3101 / 30560) loss: 2.306654\n",
      "(Iteration 3201 / 30560) loss: 2.306645\n",
      "(Iteration 3301 / 30560) loss: 2.306637\n",
      "(Iteration 3401 / 30560) loss: 2.306641\n",
      "(Epoch 9 / 80) train acc: 0.083000; val_acc: 0.088000\n",
      "(Iteration 3501 / 30560) loss: 2.306652\n",
      "(Iteration 3601 / 30560) loss: 2.306660\n",
      "(Iteration 3701 / 30560) loss: 2.306638\n",
      "(Iteration 3801 / 30560) loss: 2.306644\n",
      "(Epoch 10 / 80) train acc: 0.089000; val_acc: 0.088000\n",
      "(Iteration 3901 / 30560) loss: 2.306640\n",
      "(Iteration 4001 / 30560) loss: 2.306648\n",
      "(Iteration 4101 / 30560) loss: 2.306657\n",
      "(Iteration 4201 / 30560) loss: 2.306640\n",
      "(Epoch 11 / 80) train acc: 0.082000; val_acc: 0.088000\n",
      "(Iteration 4301 / 30560) loss: 2.306639\n",
      "(Iteration 4401 / 30560) loss: 2.306653\n",
      "(Iteration 4501 / 30560) loss: 2.306655\n",
      "(Epoch 12 / 80) train acc: 0.083000; val_acc: 0.088000\n",
      "(Iteration 4601 / 30560) loss: 2.306641\n",
      "(Iteration 4701 / 30560) loss: 2.306655\n",
      "(Iteration 4801 / 30560) loss: 2.306641\n",
      "(Iteration 4901 / 30560) loss: 2.306641\n",
      "(Epoch 13 / 80) train acc: 0.075000; val_acc: 0.088000\n",
      "(Iteration 5001 / 30560) loss: 2.306637\n",
      "(Iteration 5101 / 30560) loss: 2.306655\n",
      "(Iteration 5201 / 30560) loss: 2.306660\n",
      "(Iteration 5301 / 30560) loss: 2.306659\n",
      "(Epoch 14 / 80) train acc: 0.081000; val_acc: 0.088000\n",
      "(Iteration 5401 / 30560) loss: 2.306652\n",
      "(Iteration 5501 / 30560) loss: 2.306654\n",
      "(Iteration 5601 / 30560) loss: 2.306654\n",
      "(Iteration 5701 / 30560) loss: 2.306658\n",
      "(Epoch 15 / 80) train acc: 0.090000; val_acc: 0.088000\n",
      "(Iteration 5801 / 30560) loss: 2.306667\n",
      "(Iteration 5901 / 30560) loss: 2.306667\n",
      "(Iteration 6001 / 30560) loss: 2.306640\n",
      "(Iteration 6101 / 30560) loss: 2.306637\n",
      "(Epoch 16 / 80) train acc: 0.092000; val_acc: 0.088000\n",
      "(Iteration 6201 / 30560) loss: 2.306642\n",
      "(Iteration 6301 / 30560) loss: 2.306653\n",
      "(Iteration 6401 / 30560) loss: 2.306639\n",
      "(Epoch 17 / 80) train acc: 0.081000; val_acc: 0.088000\n",
      "(Iteration 6501 / 30560) loss: 2.306637\n",
      "(Iteration 6601 / 30560) loss: 2.306658\n",
      "(Iteration 6701 / 30560) loss: 2.306646\n",
      "(Iteration 6801 / 30560) loss: 2.306641\n",
      "(Epoch 18 / 80) train acc: 0.082000; val_acc: 0.088000\n",
      "(Iteration 6901 / 30560) loss: 2.306648\n",
      "(Iteration 7001 / 30560) loss: 2.306646\n",
      "(Iteration 7101 / 30560) loss: 2.306643\n",
      "(Iteration 7201 / 30560) loss: 2.306649\n",
      "(Epoch 19 / 80) train acc: 0.079000; val_acc: 0.088000\n",
      "(Iteration 7301 / 30560) loss: 2.306655\n",
      "(Iteration 7401 / 30560) loss: 2.306645\n",
      "(Iteration 7501 / 30560) loss: 2.306647\n",
      "(Iteration 7601 / 30560) loss: 2.306649\n",
      "(Epoch 20 / 80) train acc: 0.074000; val_acc: 0.088000\n",
      "(Iteration 7701 / 30560) loss: 2.306639\n",
      "(Iteration 7801 / 30560) loss: 2.306641\n",
      "(Iteration 7901 / 30560) loss: 2.306645\n",
      "(Iteration 8001 / 30560) loss: 2.306659\n",
      "(Epoch 21 / 80) train acc: 0.082000; val_acc: 0.088000\n",
      "(Iteration 8101 / 30560) loss: 2.306659\n",
      "(Iteration 8201 / 30560) loss: 2.306649\n",
      "(Iteration 8301 / 30560) loss: 2.306646\n",
      "(Iteration 8401 / 30560) loss: 2.306653\n",
      "(Epoch 22 / 80) train acc: 0.076000; val_acc: 0.088000\n",
      "(Iteration 8501 / 30560) loss: 2.306659\n",
      "(Iteration 8601 / 30560) loss: 2.306649\n",
      "(Iteration 8701 / 30560) loss: 2.306655\n",
      "(Epoch 23 / 80) train acc: 0.079000; val_acc: 0.088000\n",
      "(Iteration 8801 / 30560) loss: 2.306657\n",
      "(Iteration 8901 / 30560) loss: 2.306653\n",
      "(Iteration 9001 / 30560) loss: 2.306652\n",
      "(Iteration 9101 / 30560) loss: 2.306648\n",
      "(Epoch 24 / 80) train acc: 0.067000; val_acc: 0.088000\n",
      "(Iteration 9201 / 30560) loss: 2.306651\n",
      "(Iteration 9301 / 30560) loss: 2.306657\n",
      "(Iteration 9401 / 30560) loss: 2.306656\n",
      "(Iteration 9501 / 30560) loss: 2.306654\n",
      "(Epoch 25 / 80) train acc: 0.075000; val_acc: 0.088000\n",
      "(Iteration 9601 / 30560) loss: 2.306644\n",
      "(Iteration 9701 / 30560) loss: 2.306669\n",
      "(Iteration 9801 / 30560) loss: 2.306648\n",
      "(Iteration 9901 / 30560) loss: 2.306633\n",
      "(Epoch 26 / 80) train acc: 0.088000; val_acc: 0.088000\n",
      "(Iteration 10001 / 30560) loss: 2.306649\n",
      "(Iteration 10101 / 30560) loss: 2.306638\n",
      "(Iteration 10201 / 30560) loss: 2.306648\n",
      "(Iteration 10301 / 30560) loss: 2.306652\n",
      "(Epoch 27 / 80) train acc: 0.090000; val_acc: 0.088000\n",
      "(Iteration 10401 / 30560) loss: 2.306657\n",
      "(Iteration 10501 / 30560) loss: 2.306651\n",
      "(Iteration 10601 / 30560) loss: 2.306661\n",
      "(Epoch 28 / 80) train acc: 0.075000; val_acc: 0.088000\n",
      "(Iteration 10701 / 30560) loss: 2.306652\n",
      "(Iteration 10801 / 30560) loss: 2.306645\n",
      "(Iteration 10901 / 30560) loss: 2.306641\n",
      "(Iteration 11001 / 30560) loss: 2.306662\n",
      "(Epoch 29 / 80) train acc: 0.070000; val_acc: 0.088000\n",
      "(Iteration 11101 / 30560) loss: 2.306656\n",
      "(Iteration 11201 / 30560) loss: 2.306647\n",
      "(Iteration 11301 / 30560) loss: 2.306652\n",
      "(Iteration 11401 / 30560) loss: 2.306658\n",
      "(Epoch 30 / 80) train acc: 0.074000; val_acc: 0.088000\n",
      "(Iteration 11501 / 30560) loss: 2.306642\n",
      "(Iteration 11601 / 30560) loss: 2.306656\n",
      "(Iteration 11701 / 30560) loss: 2.306637\n",
      "(Iteration 11801 / 30560) loss: 2.306640\n",
      "(Epoch 31 / 80) train acc: 0.087000; val_acc: 0.088000\n",
      "(Iteration 11901 / 30560) loss: 2.306660\n",
      "(Iteration 12001 / 30560) loss: 2.306652\n",
      "(Iteration 12101 / 30560) loss: 2.306648\n",
      "(Iteration 12201 / 30560) loss: 2.306642\n",
      "(Epoch 32 / 80) train acc: 0.089000; val_acc: 0.088000\n",
      "(Iteration 12301 / 30560) loss: 2.306649\n",
      "(Iteration 12401 / 30560) loss: 2.306652\n",
      "(Iteration 12501 / 30560) loss: 2.306639\n",
      "(Iteration 12601 / 30560) loss: 2.306653\n",
      "(Epoch 33 / 80) train acc: 0.081000; val_acc: 0.088000\n",
      "(Iteration 12701 / 30560) loss: 2.306655\n",
      "(Iteration 12801 / 30560) loss: 2.306656\n",
      "(Iteration 12901 / 30560) loss: 2.306647\n",
      "(Epoch 34 / 80) train acc: 0.073000; val_acc: 0.088000\n",
      "(Iteration 13001 / 30560) loss: 2.306647\n",
      "(Iteration 13101 / 30560) loss: 2.306659\n",
      "(Iteration 13201 / 30560) loss: 2.306656\n",
      "(Iteration 13301 / 30560) loss: 2.306654\n",
      "(Epoch 35 / 80) train acc: 0.080000; val_acc: 0.088000\n",
      "(Iteration 13401 / 30560) loss: 2.306650\n",
      "(Iteration 13501 / 30560) loss: 2.306656\n",
      "(Iteration 13601 / 30560) loss: 2.306658\n",
      "(Iteration 13701 / 30560) loss: 2.306655\n",
      "(Epoch 36 / 80) train acc: 0.077000; val_acc: 0.088000\n",
      "(Iteration 13801 / 30560) loss: 2.306642\n",
      "(Iteration 13901 / 30560) loss: 2.306647\n",
      "(Iteration 14001 / 30560) loss: 2.306658\n",
      "(Iteration 14101 / 30560) loss: 2.306656\n",
      "(Epoch 37 / 80) train acc: 0.094000; val_acc: 0.088000\n",
      "(Iteration 14201 / 30560) loss: 2.306650\n",
      "(Iteration 14301 / 30560) loss: 2.306657\n",
      "(Iteration 14401 / 30560) loss: 2.306648\n",
      "(Iteration 14501 / 30560) loss: 2.306654\n",
      "(Epoch 38 / 80) train acc: 0.089000; val_acc: 0.088000\n",
      "(Iteration 14601 / 30560) loss: 2.306642\n",
      "(Iteration 14701 / 30560) loss: 2.306649\n",
      "(Iteration 14801 / 30560) loss: 2.306652\n",
      "(Epoch 39 / 80) train acc: 0.087000; val_acc: 0.088000\n",
      "(Iteration 14901 / 30560) loss: 2.306658\n",
      "(Iteration 15001 / 30560) loss: 2.306652\n",
      "(Iteration 15101 / 30560) loss: 2.306654\n",
      "(Iteration 15201 / 30560) loss: 2.306658\n",
      "(Epoch 40 / 80) train acc: 0.075000; val_acc: 0.088000\n",
      "(Iteration 15301 / 30560) loss: 2.306634\n",
      "(Iteration 15401 / 30560) loss: 2.306640\n",
      "(Iteration 15501 / 30560) loss: 2.306644\n",
      "(Iteration 15601 / 30560) loss: 2.306637\n",
      "(Epoch 41 / 80) train acc: 0.085000; val_acc: 0.088000\n",
      "(Iteration 15701 / 30560) loss: 2.306645\n",
      "(Iteration 15801 / 30560) loss: 2.306636\n",
      "(Iteration 15901 / 30560) loss: 2.306648\n",
      "(Iteration 16001 / 30560) loss: 2.306644\n",
      "(Epoch 42 / 80) train acc: 0.088000; val_acc: 0.088000\n",
      "(Iteration 16101 / 30560) loss: 2.306642\n",
      "(Iteration 16201 / 30560) loss: 2.306652\n",
      "(Iteration 16301 / 30560) loss: 2.306657\n",
      "(Iteration 16401 / 30560) loss: 2.306654\n",
      "(Epoch 43 / 80) train acc: 0.080000; val_acc: 0.088000\n",
      "(Iteration 16501 / 30560) loss: 2.306659\n",
      "(Iteration 16601 / 30560) loss: 2.306639\n",
      "(Iteration 16701 / 30560) loss: 2.306659\n",
      "(Iteration 16801 / 30560) loss: 2.306654\n",
      "(Epoch 44 / 80) train acc: 0.093000; val_acc: 0.088000\n",
      "(Iteration 16901 / 30560) loss: 2.306663\n",
      "(Iteration 17001 / 30560) loss: 2.306652\n",
      "(Iteration 17101 / 30560) loss: 2.306652\n",
      "(Epoch 45 / 80) train acc: 0.077000; val_acc: 0.088000\n",
      "(Iteration 17201 / 30560) loss: 2.306651\n",
      "(Iteration 17301 / 30560) loss: 2.306661\n",
      "(Iteration 17401 / 30560) loss: 2.306648\n",
      "(Iteration 17501 / 30560) loss: 2.306656\n",
      "(Epoch 46 / 80) train acc: 0.094000; val_acc: 0.088000\n",
      "(Iteration 17601 / 30560) loss: 2.306648\n",
      "(Iteration 17701 / 30560) loss: 2.306644\n",
      "(Iteration 17801 / 30560) loss: 2.306651\n",
      "(Iteration 17901 / 30560) loss: 2.306671\n",
      "(Epoch 47 / 80) train acc: 0.076000; val_acc: 0.088000\n",
      "(Iteration 18001 / 30560) loss: 2.306650\n",
      "(Iteration 18101 / 30560) loss: 2.306656\n",
      "(Iteration 18201 / 30560) loss: 2.306636\n",
      "(Iteration 18301 / 30560) loss: 2.306654\n",
      "(Epoch 48 / 80) train acc: 0.070000; val_acc: 0.088000\n",
      "(Iteration 18401 / 30560) loss: 2.306645\n",
      "(Iteration 18501 / 30560) loss: 2.306645\n",
      "(Iteration 18601 / 30560) loss: 2.306659\n",
      "(Iteration 18701 / 30560) loss: 2.306660\n",
      "(Epoch 49 / 80) train acc: 0.081000; val_acc: 0.088000\n",
      "(Iteration 18801 / 30560) loss: 2.306660\n",
      "(Iteration 18901 / 30560) loss: 2.306643\n",
      "(Iteration 19001 / 30560) loss: 2.306636\n",
      "(Epoch 50 / 80) train acc: 0.089000; val_acc: 0.088000\n",
      "(Iteration 19101 / 30560) loss: 2.306645\n",
      "(Iteration 19201 / 30560) loss: 2.306642\n",
      "(Iteration 19301 / 30560) loss: 2.306660\n",
      "(Iteration 19401 / 30560) loss: 2.306654\n",
      "(Epoch 51 / 80) train acc: 0.073000; val_acc: 0.088000\n",
      "(Iteration 19501 / 30560) loss: 2.306658\n",
      "(Iteration 19601 / 30560) loss: 2.306646\n",
      "(Iteration 19701 / 30560) loss: 2.306651\n",
      "(Iteration 19801 / 30560) loss: 2.306655\n",
      "(Epoch 52 / 80) train acc: 0.069000; val_acc: 0.088000\n",
      "(Iteration 19901 / 30560) loss: 2.306656\n",
      "(Iteration 20001 / 30560) loss: 2.306649\n",
      "(Iteration 20101 / 30560) loss: 2.306652\n",
      "(Iteration 20201 / 30560) loss: 2.306646\n",
      "(Epoch 53 / 80) train acc: 0.075000; val_acc: 0.088000\n",
      "(Iteration 20301 / 30560) loss: 2.306648\n",
      "(Iteration 20401 / 30560) loss: 2.306647\n",
      "(Iteration 20501 / 30560) loss: 2.306651\n",
      "(Iteration 20601 / 30560) loss: 2.306661\n",
      "(Epoch 54 / 80) train acc: 0.067000; val_acc: 0.088000\n",
      "(Iteration 20701 / 30560) loss: 2.306638\n",
      "(Iteration 20801 / 30560) loss: 2.306650\n",
      "(Iteration 20901 / 30560) loss: 2.306642\n",
      "(Iteration 21001 / 30560) loss: 2.306645\n",
      "(Epoch 55 / 80) train acc: 0.096000; val_acc: 0.088000\n",
      "(Iteration 21101 / 30560) loss: 2.306644\n",
      "(Iteration 21201 / 30560) loss: 2.306662\n",
      "(Iteration 21301 / 30560) loss: 2.306646\n",
      "(Epoch 56 / 80) train acc: 0.089000; val_acc: 0.088000\n",
      "(Iteration 21401 / 30560) loss: 2.306659\n",
      "(Iteration 21501 / 30560) loss: 2.306653\n",
      "(Iteration 21601 / 30560) loss: 2.306651\n",
      "(Iteration 21701 / 30560) loss: 2.306648\n",
      "(Epoch 57 / 80) train acc: 0.077000; val_acc: 0.088000\n",
      "(Iteration 21801 / 30560) loss: 2.306642\n",
      "(Iteration 21901 / 30560) loss: 2.306627\n",
      "(Iteration 22001 / 30560) loss: 2.306643\n",
      "(Iteration 22101 / 30560) loss: 2.306656\n",
      "(Epoch 58 / 80) train acc: 0.083000; val_acc: 0.088000\n",
      "(Iteration 22201 / 30560) loss: 2.306640\n",
      "(Iteration 22301 / 30560) loss: 2.306648\n",
      "(Iteration 22401 / 30560) loss: 2.306665\n",
      "(Iteration 22501 / 30560) loss: 2.306659\n",
      "(Epoch 59 / 80) train acc: 0.088000; val_acc: 0.088000\n",
      "(Iteration 22601 / 30560) loss: 2.306648\n",
      "(Iteration 22701 / 30560) loss: 2.306651\n",
      "(Iteration 22801 / 30560) loss: 2.306648\n",
      "(Iteration 22901 / 30560) loss: 2.306654\n",
      "(Epoch 60 / 80) train acc: 0.077000; val_acc: 0.088000\n",
      "(Iteration 23001 / 30560) loss: 2.306645\n",
      "(Iteration 23101 / 30560) loss: 2.306644\n",
      "(Iteration 23201 / 30560) loss: 2.306658\n",
      "(Iteration 23301 / 30560) loss: 2.306649\n",
      "(Epoch 61 / 80) train acc: 0.076000; val_acc: 0.088000\n",
      "(Iteration 23401 / 30560) loss: 2.306669\n",
      "(Iteration 23501 / 30560) loss: 2.306644\n",
      "(Iteration 23601 / 30560) loss: 2.306653\n",
      "(Epoch 62 / 80) train acc: 0.102000; val_acc: 0.088000\n",
      "(Iteration 23701 / 30560) loss: 2.306638\n",
      "(Iteration 23801 / 30560) loss: 2.306651\n",
      "(Iteration 23901 / 30560) loss: 2.306649\n",
      "(Iteration 24001 / 30560) loss: 2.306658\n",
      "(Epoch 63 / 80) train acc: 0.085000; val_acc: 0.088000\n",
      "(Iteration 24101 / 30560) loss: 2.306646\n",
      "(Iteration 24201 / 30560) loss: 2.306667\n",
      "(Iteration 24301 / 30560) loss: 2.306641\n",
      "(Iteration 24401 / 30560) loss: 2.306652\n",
      "(Epoch 64 / 80) train acc: 0.085000; val_acc: 0.088000\n",
      "(Iteration 24501 / 30560) loss: 2.306668\n",
      "(Iteration 24601 / 30560) loss: 2.306648\n",
      "(Iteration 24701 / 30560) loss: 2.306660\n",
      "(Iteration 24801 / 30560) loss: 2.306642\n",
      "(Epoch 65 / 80) train acc: 0.072000; val_acc: 0.088000\n",
      "(Iteration 24901 / 30560) loss: 2.306648\n",
      "(Iteration 25001 / 30560) loss: 2.306656\n",
      "(Iteration 25101 / 30560) loss: 2.306649\n",
      "(Iteration 25201 / 30560) loss: 2.306641\n",
      "(Epoch 66 / 80) train acc: 0.067000; val_acc: 0.088000\n",
      "(Iteration 25301 / 30560) loss: 2.306641\n",
      "(Iteration 25401 / 30560) loss: 2.306649\n",
      "(Iteration 25501 / 30560) loss: 2.306653\n",
      "(Epoch 67 / 80) train acc: 0.092000; val_acc: 0.088000\n",
      "(Iteration 25601 / 30560) loss: 2.306646\n",
      "(Iteration 25701 / 30560) loss: 2.306649\n",
      "(Iteration 25801 / 30560) loss: 2.306654\n",
      "(Iteration 25901 / 30560) loss: 2.306649\n",
      "(Epoch 68 / 80) train acc: 0.074000; val_acc: 0.088000\n",
      "(Iteration 26001 / 30560) loss: 2.306645\n",
      "(Iteration 26101 / 30560) loss: 2.306651\n",
      "(Iteration 26201 / 30560) loss: 2.306665\n",
      "(Iteration 26301 / 30560) loss: 2.306652\n",
      "(Epoch 69 / 80) train acc: 0.087000; val_acc: 0.088000\n",
      "(Iteration 26401 / 30560) loss: 2.306634\n",
      "(Iteration 26501 / 30560) loss: 2.306655\n",
      "(Iteration 26601 / 30560) loss: 2.306645\n",
      "(Iteration 26701 / 30560) loss: 2.306642\n",
      "(Epoch 70 / 80) train acc: 0.071000; val_acc: 0.088000\n",
      "(Iteration 26801 / 30560) loss: 2.306648\n",
      "(Iteration 26901 / 30560) loss: 2.306656\n",
      "(Iteration 27001 / 30560) loss: 2.306638\n",
      "(Iteration 27101 / 30560) loss: 2.306646\n",
      "(Epoch 71 / 80) train acc: 0.078000; val_acc: 0.088000\n",
      "(Iteration 27201 / 30560) loss: 2.306647\n",
      "(Iteration 27301 / 30560) loss: 2.306663\n",
      "(Iteration 27401 / 30560) loss: 2.306654\n",
      "(Iteration 27501 / 30560) loss: 2.306646\n",
      "(Epoch 72 / 80) train acc: 0.088000; val_acc: 0.088000\n",
      "(Iteration 27601 / 30560) loss: 2.306630\n",
      "(Iteration 27701 / 30560) loss: 2.306653\n",
      "(Iteration 27801 / 30560) loss: 2.306650\n",
      "(Epoch 73 / 80) train acc: 0.086000; val_acc: 0.088000\n",
      "(Iteration 27901 / 30560) loss: 2.306657\n",
      "(Iteration 28001 / 30560) loss: 2.306650\n",
      "(Iteration 28101 / 30560) loss: 2.306662\n",
      "(Iteration 28201 / 30560) loss: 2.306668\n",
      "(Epoch 74 / 80) train acc: 0.069000; val_acc: 0.088000\n",
      "(Iteration 28301 / 30560) loss: 2.306639\n",
      "(Iteration 28401 / 30560) loss: 2.306631\n",
      "(Iteration 28501 / 30560) loss: 2.306644\n",
      "(Iteration 28601 / 30560) loss: 2.306646\n",
      "(Epoch 75 / 80) train acc: 0.103000; val_acc: 0.088000\n",
      "(Iteration 28701 / 30560) loss: 2.306646\n",
      "(Iteration 28801 / 30560) loss: 2.306647\n",
      "(Iteration 28901 / 30560) loss: 2.306654\n",
      "(Iteration 29001 / 30560) loss: 2.306645\n",
      "(Epoch 76 / 80) train acc: 0.076000; val_acc: 0.088000\n",
      "(Iteration 29101 / 30560) loss: 2.306669\n",
      "(Iteration 29201 / 30560) loss: 2.306662\n",
      "(Iteration 29301 / 30560) loss: 2.306655\n",
      "(Iteration 29401 / 30560) loss: 2.306658\n",
      "(Epoch 77 / 80) train acc: 0.075000; val_acc: 0.088000\n",
      "(Iteration 29501 / 30560) loss: 2.306648\n",
      "(Iteration 29601 / 30560) loss: 2.306650\n",
      "(Iteration 29701 / 30560) loss: 2.306655\n",
      "(Epoch 78 / 80) train acc: 0.081000; val_acc: 0.088000\n",
      "(Iteration 29801 / 30560) loss: 2.306653\n",
      "(Iteration 29901 / 30560) loss: 2.306659\n",
      "(Iteration 30001 / 30560) loss: 2.306643\n",
      "(Iteration 30101 / 30560) loss: 2.306648\n",
      "(Epoch 79 / 80) train acc: 0.101000; val_acc: 0.088000\n",
      "(Iteration 30201 / 30560) loss: 2.306645\n",
      "(Iteration 30301 / 30560) loss: 2.306643\n",
      "(Iteration 30401 / 30560) loss: 2.306635\n",
      "(Iteration 30501 / 30560) loss: 2.306647\n",
      "(Epoch 80 / 80) train acc: 0.090000; val_acc: 0.088000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.306725\n",
      "(Epoch 0 / 80) train acc: 0.079000; val_acc: 0.077000\n",
      "(Iteration 101 / 61200) loss: 2.306749\n",
      "(Iteration 201 / 61200) loss: 2.306761\n",
      "(Iteration 301 / 61200) loss: 2.306736\n",
      "(Iteration 401 / 61200) loss: 2.306755\n",
      "(Iteration 501 / 61200) loss: 2.306751\n",
      "(Iteration 601 / 61200) loss: 2.306763\n",
      "(Iteration 701 / 61200) loss: 2.306735\n",
      "(Epoch 1 / 80) train acc: 0.079000; val_acc: 0.077000\n",
      "(Iteration 801 / 61200) loss: 2.306735\n",
      "(Iteration 901 / 61200) loss: 2.306756\n",
      "(Iteration 1001 / 61200) loss: 2.306755\n",
      "(Iteration 1101 / 61200) loss: 2.306757\n",
      "(Iteration 1201 / 61200) loss: 2.306764\n",
      "(Iteration 1301 / 61200) loss: 2.306749\n",
      "(Iteration 1401 / 61200) loss: 2.306766\n",
      "(Iteration 1501 / 61200) loss: 2.306738\n",
      "(Epoch 2 / 80) train acc: 0.079000; val_acc: 0.077000\n",
      "(Iteration 1601 / 61200) loss: 2.306725\n",
      "(Iteration 1701 / 61200) loss: 2.306733\n",
      "(Iteration 1801 / 61200) loss: 2.306732\n",
      "(Iteration 1901 / 61200) loss: 2.306742\n",
      "(Iteration 2001 / 61200) loss: 2.306757\n",
      "(Iteration 2101 / 61200) loss: 2.306743\n",
      "(Iteration 2201 / 61200) loss: 2.306734\n",
      "(Epoch 3 / 80) train acc: 0.092000; val_acc: 0.077000\n",
      "(Iteration 2301 / 61200) loss: 2.306746\n",
      "(Iteration 2401 / 61200) loss: 2.306745\n",
      "(Iteration 2501 / 61200) loss: 2.306761\n",
      "(Iteration 2601 / 61200) loss: 2.306751\n",
      "(Iteration 2701 / 61200) loss: 2.306759\n",
      "(Iteration 2801 / 61200) loss: 2.306745\n",
      "(Iteration 2901 / 61200) loss: 2.306757\n",
      "(Iteration 3001 / 61200) loss: 2.306748\n",
      "(Epoch 4 / 80) train acc: 0.082000; val_acc: 0.077000\n",
      "(Iteration 3101 / 61200) loss: 2.306750\n",
      "(Iteration 3201 / 61200) loss: 2.306747\n",
      "(Iteration 3301 / 61200) loss: 2.306755\n",
      "(Iteration 3401 / 61200) loss: 2.306744\n",
      "(Iteration 3501 / 61200) loss: 2.306737\n",
      "(Iteration 3601 / 61200) loss: 2.306726\n",
      "(Iteration 3701 / 61200) loss: 2.306729\n",
      "(Iteration 3801 / 61200) loss: 2.306740\n",
      "(Epoch 5 / 80) train acc: 0.099000; val_acc: 0.077000\n",
      "(Iteration 3901 / 61200) loss: 2.306720\n",
      "(Iteration 4001 / 61200) loss: 2.306748\n",
      "(Iteration 4101 / 61200) loss: 2.306734\n",
      "(Iteration 4201 / 61200) loss: 2.306759\n",
      "(Iteration 4301 / 61200) loss: 2.306759\n",
      "(Iteration 4401 / 61200) loss: 2.306727\n",
      "(Iteration 4501 / 61200) loss: 2.306768\n",
      "(Epoch 6 / 80) train acc: 0.092000; val_acc: 0.077000\n",
      "(Iteration 4601 / 61200) loss: 2.306744\n",
      "(Iteration 4701 / 61200) loss: 2.306742\n",
      "(Iteration 4801 / 61200) loss: 2.306743\n",
      "(Iteration 4901 / 61200) loss: 2.306737\n",
      "(Iteration 5001 / 61200) loss: 2.306753\n",
      "(Iteration 5101 / 61200) loss: 2.306752\n",
      "(Iteration 5201 / 61200) loss: 2.306727\n",
      "(Iteration 5301 / 61200) loss: 2.306735\n",
      "(Epoch 7 / 80) train acc: 0.103000; val_acc: 0.077000\n",
      "(Iteration 5401 / 61200) loss: 2.306751\n",
      "(Iteration 5501 / 61200) loss: 2.306740\n",
      "(Iteration 5601 / 61200) loss: 2.306757\n",
      "(Iteration 5701 / 61200) loss: 2.306746\n",
      "(Iteration 5801 / 61200) loss: 2.306752\n",
      "(Iteration 5901 / 61200) loss: 2.306735\n",
      "(Iteration 6001 / 61200) loss: 2.306736\n",
      "(Iteration 6101 / 61200) loss: 2.306731\n",
      "(Epoch 8 / 80) train acc: 0.084000; val_acc: 0.077000\n",
      "(Iteration 6201 / 61200) loss: 2.306764\n",
      "(Iteration 6301 / 61200) loss: 2.306746\n",
      "(Iteration 6401 / 61200) loss: 2.306740\n",
      "(Iteration 6501 / 61200) loss: 2.306741\n",
      "(Iteration 6601 / 61200) loss: 2.306739\n",
      "(Iteration 6701 / 61200) loss: 2.306765\n",
      "(Iteration 6801 / 61200) loss: 2.306737\n",
      "(Epoch 9 / 80) train acc: 0.083000; val_acc: 0.077000\n",
      "(Iteration 6901 / 61200) loss: 2.306750\n",
      "(Iteration 7001 / 61200) loss: 2.306752\n",
      "(Iteration 7101 / 61200) loss: 2.306734\n",
      "(Iteration 7201 / 61200) loss: 2.306757\n",
      "(Iteration 7301 / 61200) loss: 2.306752\n",
      "(Iteration 7401 / 61200) loss: 2.306721\n",
      "(Iteration 7501 / 61200) loss: 2.306761\n",
      "(Iteration 7601 / 61200) loss: 2.306716\n",
      "(Epoch 10 / 80) train acc: 0.074000; val_acc: 0.077000\n",
      "(Iteration 7701 / 61200) loss: 2.306744\n",
      "(Iteration 7801 / 61200) loss: 2.306744\n",
      "(Iteration 7901 / 61200) loss: 2.306748\n",
      "(Iteration 8001 / 61200) loss: 2.306732\n",
      "(Iteration 8101 / 61200) loss: 2.306755\n",
      "(Iteration 8201 / 61200) loss: 2.306741\n",
      "(Iteration 8301 / 61200) loss: 2.306736\n",
      "(Iteration 8401 / 61200) loss: 2.306744\n",
      "(Epoch 11 / 80) train acc: 0.112000; val_acc: 0.077000\n",
      "(Iteration 8501 / 61200) loss: 2.306750\n",
      "(Iteration 8601 / 61200) loss: 2.306762\n",
      "(Iteration 8701 / 61200) loss: 2.306729\n",
      "(Iteration 8801 / 61200) loss: 2.306750\n",
      "(Iteration 8901 / 61200) loss: 2.306729\n",
      "(Iteration 9001 / 61200) loss: 2.306764\n",
      "(Iteration 9101 / 61200) loss: 2.306738\n",
      "(Epoch 12 / 80) train acc: 0.093000; val_acc: 0.077000\n",
      "(Iteration 9201 / 61200) loss: 2.306736\n",
      "(Iteration 9301 / 61200) loss: 2.306736\n",
      "(Iteration 9401 / 61200) loss: 2.306731\n",
      "(Iteration 9501 / 61200) loss: 2.306720\n",
      "(Iteration 9601 / 61200) loss: 2.306739\n",
      "(Iteration 9701 / 61200) loss: 2.306738\n",
      "(Iteration 9801 / 61200) loss: 2.306740\n",
      "(Iteration 9901 / 61200) loss: 2.306741\n",
      "(Epoch 13 / 80) train acc: 0.083000; val_acc: 0.077000\n",
      "(Iteration 10001 / 61200) loss: 2.306729\n",
      "(Iteration 10101 / 61200) loss: 2.306738\n",
      "(Iteration 10201 / 61200) loss: 2.306736\n",
      "(Iteration 10301 / 61200) loss: 2.306745\n",
      "(Iteration 10401 / 61200) loss: 2.306754\n",
      "(Iteration 10501 / 61200) loss: 2.306746\n",
      "(Iteration 10601 / 61200) loss: 2.306722\n",
      "(Iteration 10701 / 61200) loss: 2.306744\n",
      "(Epoch 14 / 80) train acc: 0.069000; val_acc: 0.077000\n",
      "(Iteration 10801 / 61200) loss: 2.306759\n",
      "(Iteration 10901 / 61200) loss: 2.306737\n",
      "(Iteration 11001 / 61200) loss: 2.306744\n",
      "(Iteration 11101 / 61200) loss: 2.306745\n",
      "(Iteration 11201 / 61200) loss: 2.306755\n",
      "(Iteration 11301 / 61200) loss: 2.306753\n",
      "(Iteration 11401 / 61200) loss: 2.306746\n",
      "(Epoch 15 / 80) train acc: 0.095000; val_acc: 0.077000\n",
      "(Iteration 11501 / 61200) loss: 2.306741\n",
      "(Iteration 11601 / 61200) loss: 2.306745\n",
      "(Iteration 11701 / 61200) loss: 2.306738\n",
      "(Iteration 11801 / 61200) loss: 2.306735\n",
      "(Iteration 11901 / 61200) loss: 2.306763\n",
      "(Iteration 12001 / 61200) loss: 2.306736\n",
      "(Iteration 12101 / 61200) loss: 2.306761\n",
      "(Iteration 12201 / 61200) loss: 2.306752\n",
      "(Epoch 16 / 80) train acc: 0.086000; val_acc: 0.077000\n",
      "(Iteration 12301 / 61200) loss: 2.306738\n",
      "(Iteration 12401 / 61200) loss: 2.306737\n",
      "(Iteration 12501 / 61200) loss: 2.306743\n",
      "(Iteration 12601 / 61200) loss: 2.306745\n",
      "(Iteration 12701 / 61200) loss: 2.306738\n",
      "(Iteration 12801 / 61200) loss: 2.306755\n",
      "(Iteration 12901 / 61200) loss: 2.306719\n",
      "(Iteration 13001 / 61200) loss: 2.306737\n",
      "(Epoch 17 / 80) train acc: 0.082000; val_acc: 0.077000\n",
      "(Iteration 13101 / 61200) loss: 2.306740\n",
      "(Iteration 13201 / 61200) loss: 2.306749\n",
      "(Iteration 13301 / 61200) loss: 2.306724\n",
      "(Iteration 13401 / 61200) loss: 2.306760\n",
      "(Iteration 13501 / 61200) loss: 2.306744\n",
      "(Iteration 13601 / 61200) loss: 2.306766\n",
      "(Iteration 13701 / 61200) loss: 2.306744\n",
      "(Epoch 18 / 80) train acc: 0.091000; val_acc: 0.077000\n",
      "(Iteration 13801 / 61200) loss: 2.306736\n",
      "(Iteration 13901 / 61200) loss: 2.306747\n",
      "(Iteration 14001 / 61200) loss: 2.306718\n",
      "(Iteration 14101 / 61200) loss: 2.306735\n",
      "(Iteration 14201 / 61200) loss: 2.306699\n",
      "(Iteration 14301 / 61200) loss: 2.306747\n",
      "(Iteration 14401 / 61200) loss: 2.306758\n",
      "(Iteration 14501 / 61200) loss: 2.306758\n",
      "(Epoch 19 / 80) train acc: 0.096000; val_acc: 0.077000\n",
      "(Iteration 14601 / 61200) loss: 2.306745\n",
      "(Iteration 14701 / 61200) loss: 2.306747\n",
      "(Iteration 14801 / 61200) loss: 2.306741\n",
      "(Iteration 14901 / 61200) loss: 2.306742\n",
      "(Iteration 15001 / 61200) loss: 2.306733\n",
      "(Iteration 15101 / 61200) loss: 2.306757\n",
      "(Iteration 15201 / 61200) loss: 2.306739\n",
      "(Epoch 20 / 80) train acc: 0.072000; val_acc: 0.077000\n",
      "(Iteration 15301 / 61200) loss: 2.306752\n",
      "(Iteration 15401 / 61200) loss: 2.306740\n",
      "(Iteration 15501 / 61200) loss: 2.306735\n",
      "(Iteration 15601 / 61200) loss: 2.306717\n",
      "(Iteration 15701 / 61200) loss: 2.306748\n",
      "(Iteration 15801 / 61200) loss: 2.306743\n",
      "(Iteration 15901 / 61200) loss: 2.306736\n",
      "(Iteration 16001 / 61200) loss: 2.306739\n",
      "(Epoch 21 / 80) train acc: 0.078000; val_acc: 0.077000\n",
      "(Iteration 16101 / 61200) loss: 2.306745\n",
      "(Iteration 16201 / 61200) loss: 2.306739\n",
      "(Iteration 16301 / 61200) loss: 2.306756\n",
      "(Iteration 16401 / 61200) loss: 2.306753\n",
      "(Iteration 16501 / 61200) loss: 2.306743\n",
      "(Iteration 16601 / 61200) loss: 2.306743\n",
      "(Iteration 16701 / 61200) loss: 2.306743\n",
      "(Iteration 16801 / 61200) loss: 2.306712\n",
      "(Epoch 22 / 80) train acc: 0.110000; val_acc: 0.077000\n",
      "(Iteration 16901 / 61200) loss: 2.306741\n",
      "(Iteration 17001 / 61200) loss: 2.306740\n",
      "(Iteration 17101 / 61200) loss: 2.306738\n",
      "(Iteration 17201 / 61200) loss: 2.306735\n",
      "(Iteration 17301 / 61200) loss: 2.306745\n",
      "(Iteration 17401 / 61200) loss: 2.306724\n",
      "(Iteration 17501 / 61200) loss: 2.306741\n",
      "(Epoch 23 / 80) train acc: 0.090000; val_acc: 0.077000\n",
      "(Iteration 17601 / 61200) loss: 2.306726\n",
      "(Iteration 17701 / 61200) loss: 2.306746\n",
      "(Iteration 17801 / 61200) loss: 2.306747\n",
      "(Iteration 17901 / 61200) loss: 2.306749\n",
      "(Iteration 18001 / 61200) loss: 2.306734\n",
      "(Iteration 18101 / 61200) loss: 2.306727\n",
      "(Iteration 18201 / 61200) loss: 2.306754\n",
      "(Iteration 18301 / 61200) loss: 2.306749\n",
      "(Epoch 24 / 80) train acc: 0.086000; val_acc: 0.077000\n",
      "(Iteration 18401 / 61200) loss: 2.306729\n",
      "(Iteration 18501 / 61200) loss: 2.306739\n",
      "(Iteration 18601 / 61200) loss: 2.306733\n",
      "(Iteration 18701 / 61200) loss: 2.306727\n",
      "(Iteration 18801 / 61200) loss: 2.306746\n",
      "(Iteration 18901 / 61200) loss: 2.306750\n",
      "(Iteration 19001 / 61200) loss: 2.306750\n",
      "(Iteration 19101 / 61200) loss: 2.306773\n",
      "(Epoch 25 / 80) train acc: 0.078000; val_acc: 0.077000\n",
      "(Iteration 19201 / 61200) loss: 2.306745\n",
      "(Iteration 19301 / 61200) loss: 2.306747\n",
      "(Iteration 19401 / 61200) loss: 2.306711\n",
      "(Iteration 19501 / 61200) loss: 2.306725\n",
      "(Iteration 19601 / 61200) loss: 2.306756\n",
      "(Iteration 19701 / 61200) loss: 2.306736\n",
      "(Iteration 19801 / 61200) loss: 2.306749\n",
      "(Epoch 26 / 80) train acc: 0.082000; val_acc: 0.077000\n",
      "(Iteration 19901 / 61200) loss: 2.306745\n",
      "(Iteration 20001 / 61200) loss: 2.306756\n",
      "(Iteration 20101 / 61200) loss: 2.306738\n",
      "(Iteration 20201 / 61200) loss: 2.306717\n",
      "(Iteration 20301 / 61200) loss: 2.306732\n",
      "(Iteration 20401 / 61200) loss: 2.306730\n",
      "(Iteration 20501 / 61200) loss: 2.306759\n",
      "(Iteration 20601 / 61200) loss: 2.306757\n",
      "(Epoch 27 / 80) train acc: 0.092000; val_acc: 0.077000\n",
      "(Iteration 20701 / 61200) loss: 2.306747\n",
      "(Iteration 20801 / 61200) loss: 2.306723\n",
      "(Iteration 20901 / 61200) loss: 2.306746\n",
      "(Iteration 21001 / 61200) loss: 2.306746\n",
      "(Iteration 21101 / 61200) loss: 2.306749\n",
      "(Iteration 21201 / 61200) loss: 2.306739\n",
      "(Iteration 21301 / 61200) loss: 2.306777\n",
      "(Iteration 21401 / 61200) loss: 2.306726\n",
      "(Epoch 28 / 80) train acc: 0.091000; val_acc: 0.077000\n",
      "(Iteration 21501 / 61200) loss: 2.306748\n",
      "(Iteration 21601 / 61200) loss: 2.306728\n",
      "(Iteration 21701 / 61200) loss: 2.306741\n",
      "(Iteration 21801 / 61200) loss: 2.306743\n",
      "(Iteration 21901 / 61200) loss: 2.306757\n",
      "(Iteration 22001 / 61200) loss: 2.306745\n",
      "(Iteration 22101 / 61200) loss: 2.306744\n",
      "(Epoch 29 / 80) train acc: 0.091000; val_acc: 0.077000\n",
      "(Iteration 22201 / 61200) loss: 2.306735\n",
      "(Iteration 22301 / 61200) loss: 2.306741\n",
      "(Iteration 22401 / 61200) loss: 2.306752\n",
      "(Iteration 22501 / 61200) loss: 2.306750\n",
      "(Iteration 22601 / 61200) loss: 2.306759\n",
      "(Iteration 22701 / 61200) loss: 2.306744\n",
      "(Iteration 22801 / 61200) loss: 2.306721\n",
      "(Iteration 22901 / 61200) loss: 2.306745\n",
      "(Epoch 30 / 80) train acc: 0.091000; val_acc: 0.077000\n",
      "(Iteration 23001 / 61200) loss: 2.306729\n",
      "(Iteration 23101 / 61200) loss: 2.306728\n",
      "(Iteration 23201 / 61200) loss: 2.306743\n",
      "(Iteration 23301 / 61200) loss: 2.306758\n",
      "(Iteration 23401 / 61200) loss: 2.306740\n",
      "(Iteration 23501 / 61200) loss: 2.306739\n",
      "(Iteration 23601 / 61200) loss: 2.306739\n",
      "(Iteration 23701 / 61200) loss: 2.306744\n",
      "(Epoch 31 / 80) train acc: 0.080000; val_acc: 0.077000\n",
      "(Iteration 23801 / 61200) loss: 2.306753\n",
      "(Iteration 23901 / 61200) loss: 2.306738\n",
      "(Iteration 24001 / 61200) loss: 2.306745\n",
      "(Iteration 24101 / 61200) loss: 2.306746\n",
      "(Iteration 24201 / 61200) loss: 2.306751\n",
      "(Iteration 24301 / 61200) loss: 2.306762\n",
      "(Iteration 24401 / 61200) loss: 2.306731\n",
      "(Epoch 32 / 80) train acc: 0.087000; val_acc: 0.077000\n",
      "(Iteration 24501 / 61200) loss: 2.306742\n",
      "(Iteration 24601 / 61200) loss: 2.306742\n",
      "(Iteration 24701 / 61200) loss: 2.306749\n",
      "(Iteration 24801 / 61200) loss: 2.306740\n",
      "(Iteration 24901 / 61200) loss: 2.306740\n",
      "(Iteration 25001 / 61200) loss: 2.306746\n",
      "(Iteration 25101 / 61200) loss: 2.306738\n",
      "(Iteration 25201 / 61200) loss: 2.306737\n",
      "(Epoch 33 / 80) train acc: 0.077000; val_acc: 0.077000\n",
      "(Iteration 25301 / 61200) loss: 2.306704\n",
      "(Iteration 25401 / 61200) loss: 2.306734\n",
      "(Iteration 25501 / 61200) loss: 2.306745\n",
      "(Iteration 25601 / 61200) loss: 2.306746\n",
      "(Iteration 25701 / 61200) loss: 2.306749\n",
      "(Iteration 25801 / 61200) loss: 2.306725\n",
      "(Iteration 25901 / 61200) loss: 2.306753\n",
      "(Iteration 26001 / 61200) loss: 2.306740\n",
      "(Epoch 34 / 80) train acc: 0.079000; val_acc: 0.077000\n",
      "(Iteration 26101 / 61200) loss: 2.306755\n",
      "(Iteration 26201 / 61200) loss: 2.306742\n",
      "(Iteration 26301 / 61200) loss: 2.306748\n",
      "(Iteration 26401 / 61200) loss: 2.306747\n",
      "(Iteration 26501 / 61200) loss: 2.306759\n",
      "(Iteration 26601 / 61200) loss: 2.306735\n",
      "(Iteration 26701 / 61200) loss: 2.306744\n",
      "(Epoch 35 / 80) train acc: 0.089000; val_acc: 0.077000\n",
      "(Iteration 26801 / 61200) loss: 2.306759\n",
      "(Iteration 26901 / 61200) loss: 2.306744\n",
      "(Iteration 27001 / 61200) loss: 2.306726\n",
      "(Iteration 27101 / 61200) loss: 2.306738\n",
      "(Iteration 27201 / 61200) loss: 2.306717\n",
      "(Iteration 27301 / 61200) loss: 2.306737\n",
      "(Iteration 27401 / 61200) loss: 2.306746\n",
      "(Iteration 27501 / 61200) loss: 2.306744\n",
      "(Epoch 36 / 80) train acc: 0.080000; val_acc: 0.077000\n",
      "(Iteration 27601 / 61200) loss: 2.306733\n",
      "(Iteration 27701 / 61200) loss: 2.306733\n",
      "(Iteration 27801 / 61200) loss: 2.306757\n",
      "(Iteration 27901 / 61200) loss: 2.306743\n",
      "(Iteration 28001 / 61200) loss: 2.306745\n",
      "(Iteration 28101 / 61200) loss: 2.306763\n",
      "(Iteration 28201 / 61200) loss: 2.306752\n",
      "(Iteration 28301 / 61200) loss: 2.306746\n",
      "(Epoch 37 / 80) train acc: 0.085000; val_acc: 0.077000\n",
      "(Iteration 28401 / 61200) loss: 2.306714\n",
      "(Iteration 28501 / 61200) loss: 2.306753\n",
      "(Iteration 28601 / 61200) loss: 2.306726\n",
      "(Iteration 28701 / 61200) loss: 2.306760\n",
      "(Iteration 28801 / 61200) loss: 2.306742\n",
      "(Iteration 28901 / 61200) loss: 2.306729\n",
      "(Iteration 29001 / 61200) loss: 2.306744\n",
      "(Epoch 38 / 80) train acc: 0.085000; val_acc: 0.077000\n",
      "(Iteration 29101 / 61200) loss: 2.306748\n",
      "(Iteration 29201 / 61200) loss: 2.306743\n",
      "(Iteration 29301 / 61200) loss: 2.306740\n",
      "(Iteration 29401 / 61200) loss: 2.306738\n",
      "(Iteration 29501 / 61200) loss: 2.306759\n",
      "(Iteration 29601 / 61200) loss: 2.306732\n",
      "(Iteration 29701 / 61200) loss: 2.306742\n",
      "(Iteration 29801 / 61200) loss: 2.306749\n",
      "(Epoch 39 / 80) train acc: 0.097000; val_acc: 0.077000\n",
      "(Iteration 29901 / 61200) loss: 2.306743\n",
      "(Iteration 30001 / 61200) loss: 2.306737\n",
      "(Iteration 30101 / 61200) loss: 2.306728\n",
      "(Iteration 30201 / 61200) loss: 2.306729\n",
      "(Iteration 30301 / 61200) loss: 2.306737\n",
      "(Iteration 30401 / 61200) loss: 2.306745\n",
      "(Iteration 30501 / 61200) loss: 2.306753\n",
      "(Epoch 40 / 80) train acc: 0.104000; val_acc: 0.077000\n",
      "(Iteration 30601 / 61200) loss: 2.306727\n",
      "(Iteration 30701 / 61200) loss: 2.306726\n",
      "(Iteration 30801 / 61200) loss: 2.306727\n",
      "(Iteration 30901 / 61200) loss: 2.306730\n",
      "(Iteration 31001 / 61200) loss: 2.306746\n",
      "(Iteration 31101 / 61200) loss: 2.306729\n",
      "(Iteration 31201 / 61200) loss: 2.306726\n",
      "(Iteration 31301 / 61200) loss: 2.306753\n",
      "(Epoch 41 / 80) train acc: 0.080000; val_acc: 0.077000\n",
      "(Iteration 31401 / 61200) loss: 2.306740\n",
      "(Iteration 31501 / 61200) loss: 2.306724\n",
      "(Iteration 31601 / 61200) loss: 2.306734\n",
      "(Iteration 31701 / 61200) loss: 2.306743\n",
      "(Iteration 31801 / 61200) loss: 2.306750\n",
      "(Iteration 31901 / 61200) loss: 2.306732\n",
      "(Iteration 32001 / 61200) loss: 2.306732\n",
      "(Iteration 32101 / 61200) loss: 2.306734\n",
      "(Epoch 42 / 80) train acc: 0.085000; val_acc: 0.077000\n",
      "(Iteration 32201 / 61200) loss: 2.306757\n",
      "(Iteration 32301 / 61200) loss: 2.306746\n",
      "(Iteration 32401 / 61200) loss: 2.306757\n",
      "(Iteration 32501 / 61200) loss: 2.306755\n",
      "(Iteration 32601 / 61200) loss: 2.306734\n",
      "(Iteration 32701 / 61200) loss: 2.306733\n",
      "(Iteration 32801 / 61200) loss: 2.306751\n",
      "(Epoch 43 / 80) train acc: 0.093000; val_acc: 0.077000\n",
      "(Iteration 32901 / 61200) loss: 2.306754\n",
      "(Iteration 33001 / 61200) loss: 2.306742\n",
      "(Iteration 33101 / 61200) loss: 2.306732\n",
      "(Iteration 33201 / 61200) loss: 2.306730\n",
      "(Iteration 33301 / 61200) loss: 2.306753\n",
      "(Iteration 33401 / 61200) loss: 2.306761\n",
      "(Iteration 33501 / 61200) loss: 2.306749\n",
      "(Iteration 33601 / 61200) loss: 2.306730\n",
      "(Epoch 44 / 80) train acc: 0.098000; val_acc: 0.077000\n",
      "(Iteration 33701 / 61200) loss: 2.306748\n",
      "(Iteration 33801 / 61200) loss: 2.306743\n",
      "(Iteration 33901 / 61200) loss: 2.306758\n",
      "(Iteration 34001 / 61200) loss: 2.306741\n",
      "(Iteration 34101 / 61200) loss: 2.306739\n",
      "(Iteration 34201 / 61200) loss: 2.306742\n",
      "(Iteration 34301 / 61200) loss: 2.306751\n",
      "(Iteration 34401 / 61200) loss: 2.306741\n",
      "(Epoch 45 / 80) train acc: 0.107000; val_acc: 0.077000\n",
      "(Iteration 34501 / 61200) loss: 2.306737\n",
      "(Iteration 34601 / 61200) loss: 2.306733\n",
      "(Iteration 34701 / 61200) loss: 2.306748\n",
      "(Iteration 34801 / 61200) loss: 2.306728\n",
      "(Iteration 34901 / 61200) loss: 2.306747\n",
      "(Iteration 35001 / 61200) loss: 2.306735\n",
      "(Iteration 35101 / 61200) loss: 2.306732\n",
      "(Epoch 46 / 80) train acc: 0.097000; val_acc: 0.077000\n",
      "(Iteration 35201 / 61200) loss: 2.306749\n",
      "(Iteration 35301 / 61200) loss: 2.306728\n",
      "(Iteration 35401 / 61200) loss: 2.306744\n",
      "(Iteration 35501 / 61200) loss: 2.306742\n",
      "(Iteration 35601 / 61200) loss: 2.306734\n",
      "(Iteration 35701 / 61200) loss: 2.306746\n",
      "(Iteration 35801 / 61200) loss: 2.306741\n",
      "(Iteration 35901 / 61200) loss: 2.306717\n",
      "(Epoch 47 / 80) train acc: 0.088000; val_acc: 0.077000\n",
      "(Iteration 36001 / 61200) loss: 2.306754\n",
      "(Iteration 36101 / 61200) loss: 2.306745\n",
      "(Iteration 36201 / 61200) loss: 2.306735\n",
      "(Iteration 36301 / 61200) loss: 2.306723\n",
      "(Iteration 36401 / 61200) loss: 2.306736\n",
      "(Iteration 36501 / 61200) loss: 2.306738\n",
      "(Iteration 36601 / 61200) loss: 2.306751\n",
      "(Iteration 36701 / 61200) loss: 2.306729\n",
      "(Epoch 48 / 80) train acc: 0.087000; val_acc: 0.077000\n",
      "(Iteration 36801 / 61200) loss: 2.306724\n",
      "(Iteration 36901 / 61200) loss: 2.306730\n",
      "(Iteration 37001 / 61200) loss: 2.306743\n",
      "(Iteration 37101 / 61200) loss: 2.306738\n",
      "(Iteration 37201 / 61200) loss: 2.306749\n",
      "(Iteration 37301 / 61200) loss: 2.306741\n",
      "(Iteration 37401 / 61200) loss: 2.306739\n",
      "(Epoch 49 / 80) train acc: 0.094000; val_acc: 0.077000\n",
      "(Iteration 37501 / 61200) loss: 2.306737\n",
      "(Iteration 37601 / 61200) loss: 2.306747\n",
      "(Iteration 37701 / 61200) loss: 2.306736\n",
      "(Iteration 37801 / 61200) loss: 2.306747\n",
      "(Iteration 37901 / 61200) loss: 2.306732\n",
      "(Iteration 38001 / 61200) loss: 2.306738\n",
      "(Iteration 38101 / 61200) loss: 2.306752\n",
      "(Iteration 38201 / 61200) loss: 2.306741\n",
      "(Epoch 50 / 80) train acc: 0.095000; val_acc: 0.077000\n",
      "(Iteration 38301 / 61200) loss: 2.306728\n",
      "(Iteration 38401 / 61200) loss: 2.306724\n",
      "(Iteration 38501 / 61200) loss: 2.306750\n",
      "(Iteration 38601 / 61200) loss: 2.306758\n",
      "(Iteration 38701 / 61200) loss: 2.306743\n",
      "(Iteration 38801 / 61200) loss: 2.306758\n",
      "(Iteration 38901 / 61200) loss: 2.306733\n",
      "(Iteration 39001 / 61200) loss: 2.306727\n",
      "(Epoch 51 / 80) train acc: 0.093000; val_acc: 0.077000\n",
      "(Iteration 39101 / 61200) loss: 2.306719\n",
      "(Iteration 39201 / 61200) loss: 2.306751\n",
      "(Iteration 39301 / 61200) loss: 2.306737\n",
      "(Iteration 39401 / 61200) loss: 2.306721\n",
      "(Iteration 39501 / 61200) loss: 2.306722\n",
      "(Iteration 39601 / 61200) loss: 2.306727\n",
      "(Iteration 39701 / 61200) loss: 2.306747\n",
      "(Epoch 52 / 80) train acc: 0.076000; val_acc: 0.077000\n",
      "(Iteration 39801 / 61200) loss: 2.306750\n",
      "(Iteration 39901 / 61200) loss: 2.306753\n",
      "(Iteration 40001 / 61200) loss: 2.306729\n",
      "(Iteration 40101 / 61200) loss: 2.306745\n",
      "(Iteration 40201 / 61200) loss: 2.306730\n",
      "(Iteration 40301 / 61200) loss: 2.306743\n",
      "(Iteration 40401 / 61200) loss: 2.306728\n",
      "(Iteration 40501 / 61200) loss: 2.306715\n",
      "(Epoch 53 / 80) train acc: 0.083000; val_acc: 0.077000\n",
      "(Iteration 40601 / 61200) loss: 2.306729\n",
      "(Iteration 40701 / 61200) loss: 2.306749\n",
      "(Iteration 40801 / 61200) loss: 2.306721\n",
      "(Iteration 40901 / 61200) loss: 2.306768\n",
      "(Iteration 41001 / 61200) loss: 2.306754\n",
      "(Iteration 41101 / 61200) loss: 2.306713\n",
      "(Iteration 41201 / 61200) loss: 2.306734\n",
      "(Iteration 41301 / 61200) loss: 2.306725\n",
      "(Epoch 54 / 80) train acc: 0.079000; val_acc: 0.077000\n",
      "(Iteration 41401 / 61200) loss: 2.306738\n",
      "(Iteration 41501 / 61200) loss: 2.306726\n",
      "(Iteration 41601 / 61200) loss: 2.306747\n",
      "(Iteration 41701 / 61200) loss: 2.306719\n",
      "(Iteration 41801 / 61200) loss: 2.306727\n",
      "(Iteration 41901 / 61200) loss: 2.306722\n",
      "(Iteration 42001 / 61200) loss: 2.306738\n",
      "(Epoch 55 / 80) train acc: 0.081000; val_acc: 0.077000\n",
      "(Iteration 42101 / 61200) loss: 2.306761\n",
      "(Iteration 42201 / 61200) loss: 2.306747\n",
      "(Iteration 42301 / 61200) loss: 2.306743\n",
      "(Iteration 42401 / 61200) loss: 2.306726\n",
      "(Iteration 42501 / 61200) loss: 2.306751\n",
      "(Iteration 42601 / 61200) loss: 2.306741\n",
      "(Iteration 42701 / 61200) loss: 2.306768\n",
      "(Iteration 42801 / 61200) loss: 2.306724\n",
      "(Epoch 56 / 80) train acc: 0.093000; val_acc: 0.077000\n",
      "(Iteration 42901 / 61200) loss: 2.306734\n",
      "(Iteration 43001 / 61200) loss: 2.306741\n",
      "(Iteration 43101 / 61200) loss: 2.306722\n",
      "(Iteration 43201 / 61200) loss: 2.306746\n",
      "(Iteration 43301 / 61200) loss: 2.306746\n",
      "(Iteration 43401 / 61200) loss: 2.306734\n",
      "(Iteration 43501 / 61200) loss: 2.306720\n",
      "(Iteration 43601 / 61200) loss: 2.306723\n",
      "(Epoch 57 / 80) train acc: 0.091000; val_acc: 0.077000\n",
      "(Iteration 43701 / 61200) loss: 2.306729\n",
      "(Iteration 43801 / 61200) loss: 2.306738\n",
      "(Iteration 43901 / 61200) loss: 2.306739\n",
      "(Iteration 44001 / 61200) loss: 2.306737\n",
      "(Iteration 44101 / 61200) loss: 2.306747\n",
      "(Iteration 44201 / 61200) loss: 2.306740\n",
      "(Iteration 44301 / 61200) loss: 2.306731\n",
      "(Epoch 58 / 80) train acc: 0.107000; val_acc: 0.077000\n",
      "(Iteration 44401 / 61200) loss: 2.306734\n",
      "(Iteration 44501 / 61200) loss: 2.306726\n",
      "(Iteration 44601 / 61200) loss: 2.306738\n",
      "(Iteration 44701 / 61200) loss: 2.306750\n",
      "(Iteration 44801 / 61200) loss: 2.306752\n",
      "(Iteration 44901 / 61200) loss: 2.306725\n",
      "(Iteration 45001 / 61200) loss: 2.306743\n",
      "(Iteration 45101 / 61200) loss: 2.306745\n",
      "(Epoch 59 / 80) train acc: 0.077000; val_acc: 0.077000\n",
      "(Iteration 45201 / 61200) loss: 2.306721\n",
      "(Iteration 45301 / 61200) loss: 2.306746\n",
      "(Iteration 45401 / 61200) loss: 2.306732\n",
      "(Iteration 45501 / 61200) loss: 2.306723\n",
      "(Iteration 45601 / 61200) loss: 2.306731\n",
      "(Iteration 45701 / 61200) loss: 2.306734\n",
      "(Iteration 45801 / 61200) loss: 2.306748\n",
      "(Epoch 60 / 80) train acc: 0.093000; val_acc: 0.077000\n",
      "(Iteration 45901 / 61200) loss: 2.306745\n",
      "(Iteration 46001 / 61200) loss: 2.306728\n",
      "(Iteration 46101 / 61200) loss: 2.306743\n",
      "(Iteration 46201 / 61200) loss: 2.306727\n",
      "(Iteration 46301 / 61200) loss: 2.306757\n",
      "(Iteration 46401 / 61200) loss: 2.306755\n",
      "(Iteration 46501 / 61200) loss: 2.306741\n",
      "(Iteration 46601 / 61200) loss: 2.306728\n",
      "(Epoch 61 / 80) train acc: 0.090000; val_acc: 0.077000\n",
      "(Iteration 46701 / 61200) loss: 2.306746\n",
      "(Iteration 46801 / 61200) loss: 2.306751\n",
      "(Iteration 46901 / 61200) loss: 2.306761\n",
      "(Iteration 47001 / 61200) loss: 2.306728\n",
      "(Iteration 47101 / 61200) loss: 2.306743\n",
      "(Iteration 47201 / 61200) loss: 2.306755\n",
      "(Iteration 47301 / 61200) loss: 2.306755\n",
      "(Iteration 47401 / 61200) loss: 2.306735\n",
      "(Epoch 62 / 80) train acc: 0.078000; val_acc: 0.077000\n",
      "(Iteration 47501 / 61200) loss: 2.306748\n",
      "(Iteration 47601 / 61200) loss: 2.306746\n",
      "(Iteration 47701 / 61200) loss: 2.306743\n",
      "(Iteration 47801 / 61200) loss: 2.306740\n",
      "(Iteration 47901 / 61200) loss: 2.306738\n",
      "(Iteration 48001 / 61200) loss: 2.306737\n",
      "(Iteration 48101 / 61200) loss: 2.306743\n",
      "(Epoch 63 / 80) train acc: 0.073000; val_acc: 0.077000\n",
      "(Iteration 48201 / 61200) loss: 2.306729\n",
      "(Iteration 48301 / 61200) loss: 2.306739\n",
      "(Iteration 48401 / 61200) loss: 2.306739\n",
      "(Iteration 48501 / 61200) loss: 2.306739\n",
      "(Iteration 48601 / 61200) loss: 2.306741\n",
      "(Iteration 48701 / 61200) loss: 2.306748\n",
      "(Iteration 48801 / 61200) loss: 2.306756\n",
      "(Iteration 48901 / 61200) loss: 2.306725\n",
      "(Epoch 64 / 80) train acc: 0.086000; val_acc: 0.077000\n",
      "(Iteration 49001 / 61200) loss: 2.306745\n",
      "(Iteration 49101 / 61200) loss: 2.306721\n",
      "(Iteration 49201 / 61200) loss: 2.306749\n",
      "(Iteration 49301 / 61200) loss: 2.306739\n",
      "(Iteration 49401 / 61200) loss: 2.306742\n",
      "(Iteration 49501 / 61200) loss: 2.306735\n",
      "(Iteration 49601 / 61200) loss: 2.306733\n",
      "(Iteration 49701 / 61200) loss: 2.306730\n",
      "(Epoch 65 / 80) train acc: 0.097000; val_acc: 0.077000\n",
      "(Iteration 49801 / 61200) loss: 2.306716\n",
      "(Iteration 49901 / 61200) loss: 2.306716\n",
      "(Iteration 50001 / 61200) loss: 2.306750\n",
      "(Iteration 50101 / 61200) loss: 2.306748\n",
      "(Iteration 50201 / 61200) loss: 2.306749\n",
      "(Iteration 50301 / 61200) loss: 2.306736\n",
      "(Iteration 50401 / 61200) loss: 2.306731\n",
      "(Epoch 66 / 80) train acc: 0.072000; val_acc: 0.077000\n",
      "(Iteration 50501 / 61200) loss: 2.306716\n",
      "(Iteration 50601 / 61200) loss: 2.306735\n",
      "(Iteration 50701 / 61200) loss: 2.306736\n",
      "(Iteration 50801 / 61200) loss: 2.306732\n",
      "(Iteration 50901 / 61200) loss: 2.306727\n",
      "(Iteration 51001 / 61200) loss: 2.306741\n",
      "(Iteration 51101 / 61200) loss: 2.306733\n",
      "(Iteration 51201 / 61200) loss: 2.306726\n",
      "(Epoch 67 / 80) train acc: 0.066000; val_acc: 0.077000\n",
      "(Iteration 51301 / 61200) loss: 2.306743\n",
      "(Iteration 51401 / 61200) loss: 2.306752\n",
      "(Iteration 51501 / 61200) loss: 2.306754\n",
      "(Iteration 51601 / 61200) loss: 2.306749\n",
      "(Iteration 51701 / 61200) loss: 2.306738\n",
      "(Iteration 51801 / 61200) loss: 2.306737\n",
      "(Iteration 51901 / 61200) loss: 2.306726\n",
      "(Iteration 52001 / 61200) loss: 2.306747\n",
      "(Epoch 68 / 80) train acc: 0.103000; val_acc: 0.077000\n",
      "(Iteration 52101 / 61200) loss: 2.306735\n",
      "(Iteration 52201 / 61200) loss: 2.306742\n",
      "(Iteration 52301 / 61200) loss: 2.306735\n",
      "(Iteration 52401 / 61200) loss: 2.306746\n",
      "(Iteration 52501 / 61200) loss: 2.306722\n",
      "(Iteration 52601 / 61200) loss: 2.306745\n",
      "(Iteration 52701 / 61200) loss: 2.306746\n",
      "(Epoch 69 / 80) train acc: 0.095000; val_acc: 0.077000\n",
      "(Iteration 52801 / 61200) loss: 2.306754\n",
      "(Iteration 52901 / 61200) loss: 2.306748\n",
      "(Iteration 53001 / 61200) loss: 2.306740\n",
      "(Iteration 53101 / 61200) loss: 2.306719\n",
      "(Iteration 53201 / 61200) loss: 2.306733\n",
      "(Iteration 53301 / 61200) loss: 2.306734\n",
      "(Iteration 53401 / 61200) loss: 2.306741\n",
      "(Iteration 53501 / 61200) loss: 2.306751\n",
      "(Epoch 70 / 80) train acc: 0.090000; val_acc: 0.077000\n",
      "(Iteration 53601 / 61200) loss: 2.306756\n",
      "(Iteration 53701 / 61200) loss: 2.306742\n",
      "(Iteration 53801 / 61200) loss: 2.306732\n",
      "(Iteration 53901 / 61200) loss: 2.306758\n",
      "(Iteration 54001 / 61200) loss: 2.306735\n",
      "(Iteration 54101 / 61200) loss: 2.306717\n",
      "(Iteration 54201 / 61200) loss: 2.306748\n",
      "(Iteration 54301 / 61200) loss: 2.306733\n",
      "(Epoch 71 / 80) train acc: 0.091000; val_acc: 0.077000\n",
      "(Iteration 54401 / 61200) loss: 2.306728\n",
      "(Iteration 54501 / 61200) loss: 2.306711\n",
      "(Iteration 54601 / 61200) loss: 2.306742\n",
      "(Iteration 54701 / 61200) loss: 2.306748\n",
      "(Iteration 54801 / 61200) loss: 2.306730\n",
      "(Iteration 54901 / 61200) loss: 2.306733\n",
      "(Iteration 55001 / 61200) loss: 2.306734\n",
      "(Epoch 72 / 80) train acc: 0.106000; val_acc: 0.077000\n",
      "(Iteration 55101 / 61200) loss: 2.306749\n",
      "(Iteration 55201 / 61200) loss: 2.306757\n",
      "(Iteration 55301 / 61200) loss: 2.306738\n",
      "(Iteration 55401 / 61200) loss: 2.306740\n",
      "(Iteration 55501 / 61200) loss: 2.306730\n",
      "(Iteration 55601 / 61200) loss: 2.306724\n",
      "(Iteration 55701 / 61200) loss: 2.306754\n",
      "(Iteration 55801 / 61200) loss: 2.306732\n",
      "(Epoch 73 / 80) train acc: 0.088000; val_acc: 0.077000\n",
      "(Iteration 55901 / 61200) loss: 2.306748\n",
      "(Iteration 56001 / 61200) loss: 2.306735\n",
      "(Iteration 56101 / 61200) loss: 2.306744\n",
      "(Iteration 56201 / 61200) loss: 2.306749\n",
      "(Iteration 56301 / 61200) loss: 2.306721\n",
      "(Iteration 56401 / 61200) loss: 2.306743\n",
      "(Iteration 56501 / 61200) loss: 2.306726\n",
      "(Iteration 56601 / 61200) loss: 2.306745\n",
      "(Epoch 74 / 80) train acc: 0.079000; val_acc: 0.077000\n",
      "(Iteration 56701 / 61200) loss: 2.306759\n",
      "(Iteration 56801 / 61200) loss: 2.306737\n",
      "(Iteration 56901 / 61200) loss: 2.306748\n",
      "(Iteration 57001 / 61200) loss: 2.306735\n",
      "(Iteration 57101 / 61200) loss: 2.306734\n",
      "(Iteration 57201 / 61200) loss: 2.306721\n",
      "(Iteration 57301 / 61200) loss: 2.306730\n",
      "(Epoch 75 / 80) train acc: 0.103000; val_acc: 0.077000\n",
      "(Iteration 57401 / 61200) loss: 2.306733\n",
      "(Iteration 57501 / 61200) loss: 2.306738\n",
      "(Iteration 57601 / 61200) loss: 2.306741\n",
      "(Iteration 57701 / 61200) loss: 2.306742\n",
      "(Iteration 57801 / 61200) loss: 2.306720\n",
      "(Iteration 57901 / 61200) loss: 2.306728\n",
      "(Iteration 58001 / 61200) loss: 2.306743\n",
      "(Iteration 58101 / 61200) loss: 2.306723\n",
      "(Epoch 76 / 80) train acc: 0.078000; val_acc: 0.077000\n",
      "(Iteration 58201 / 61200) loss: 2.306754\n",
      "(Iteration 58301 / 61200) loss: 2.306746\n",
      "(Iteration 58401 / 61200) loss: 2.306731\n",
      "(Iteration 58501 / 61200) loss: 2.306728\n",
      "(Iteration 58601 / 61200) loss: 2.306758\n",
      "(Iteration 58701 / 61200) loss: 2.306722\n",
      "(Iteration 58801 / 61200) loss: 2.306733\n",
      "(Iteration 58901 / 61200) loss: 2.306760\n",
      "(Epoch 77 / 80) train acc: 0.084000; val_acc: 0.077000\n",
      "(Iteration 59001 / 61200) loss: 2.306743\n",
      "(Iteration 59101 / 61200) loss: 2.306759\n",
      "(Iteration 59201 / 61200) loss: 2.306721\n",
      "(Iteration 59301 / 61200) loss: 2.306747\n",
      "(Iteration 59401 / 61200) loss: 2.306747\n",
      "(Iteration 59501 / 61200) loss: 2.306742\n",
      "(Iteration 59601 / 61200) loss: 2.306740\n",
      "(Epoch 78 / 80) train acc: 0.085000; val_acc: 0.077000\n",
      "(Iteration 59701 / 61200) loss: 2.306746\n",
      "(Iteration 59801 / 61200) loss: 2.306729\n",
      "(Iteration 59901 / 61200) loss: 2.306731\n",
      "(Iteration 60001 / 61200) loss: 2.306728\n",
      "(Iteration 60101 / 61200) loss: 2.306748\n",
      "(Iteration 60201 / 61200) loss: 2.306741\n",
      "(Iteration 60301 / 61200) loss: 2.306775\n",
      "(Iteration 60401 / 61200) loss: 2.306719\n",
      "(Epoch 79 / 80) train acc: 0.092000; val_acc: 0.077000\n",
      "(Iteration 60501 / 61200) loss: 2.306746\n",
      "(Iteration 60601 / 61200) loss: 2.306722\n",
      "(Iteration 60701 / 61200) loss: 2.306747\n",
      "(Iteration 60801 / 61200) loss: 2.306730\n",
      "(Iteration 60901 / 61200) loss: 2.306746\n",
      "(Iteration 61001 / 61200) loss: 2.306734\n",
      "(Iteration 61101 / 61200) loss: 2.306745\n",
      "(Epoch 80 / 80) train acc: 0.092000; val_acc: 0.077000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.306666\n",
      "(Epoch 0 / 80) train acc: 0.103000; val_acc: 0.094000\n",
      "(Iteration 101 / 30560) loss: 2.306669\n",
      "(Iteration 201 / 30560) loss: 2.306687\n",
      "(Iteration 301 / 30560) loss: 2.306672\n",
      "(Epoch 1 / 80) train acc: 0.108000; val_acc: 0.094000\n",
      "(Iteration 401 / 30560) loss: 2.306678\n",
      "(Iteration 501 / 30560) loss: 2.306678\n",
      "(Iteration 601 / 30560) loss: 2.306669\n",
      "(Iteration 701 / 30560) loss: 2.306671\n",
      "(Epoch 2 / 80) train acc: 0.104000; val_acc: 0.094000\n",
      "(Iteration 801 / 30560) loss: 2.306668\n",
      "(Iteration 901 / 30560) loss: 2.306678\n",
      "(Iteration 1001 / 30560) loss: 2.306663\n",
      "(Iteration 1101 / 30560) loss: 2.306671\n",
      "(Epoch 3 / 80) train acc: 0.113000; val_acc: 0.094000\n",
      "(Iteration 1201 / 30560) loss: 2.306659\n",
      "(Iteration 1301 / 30560) loss: 2.306662\n",
      "(Iteration 1401 / 30560) loss: 2.306672\n",
      "(Iteration 1501 / 30560) loss: 2.306664\n",
      "(Epoch 4 / 80) train acc: 0.100000; val_acc: 0.094000\n",
      "(Iteration 1601 / 30560) loss: 2.306668\n",
      "(Iteration 1701 / 30560) loss: 2.306674\n",
      "(Iteration 1801 / 30560) loss: 2.306671\n",
      "(Iteration 1901 / 30560) loss: 2.306670\n",
      "(Epoch 5 / 80) train acc: 0.103000; val_acc: 0.094000\n",
      "(Iteration 2001 / 30560) loss: 2.306673\n",
      "(Iteration 2101 / 30560) loss: 2.306667\n",
      "(Iteration 2201 / 30560) loss: 2.306677\n",
      "(Epoch 6 / 80) train acc: 0.110000; val_acc: 0.094000\n",
      "(Iteration 2301 / 30560) loss: 2.306654\n",
      "(Iteration 2401 / 30560) loss: 2.306665\n",
      "(Iteration 2501 / 30560) loss: 2.306667\n",
      "(Iteration 2601 / 30560) loss: 2.306667\n",
      "(Epoch 7 / 80) train acc: 0.116000; val_acc: 0.094000\n",
      "(Iteration 2701 / 30560) loss: 2.306674\n",
      "(Iteration 2801 / 30560) loss: 2.306665\n",
      "(Iteration 2901 / 30560) loss: 2.306667\n",
      "(Iteration 3001 / 30560) loss: 2.306667\n",
      "(Epoch 8 / 80) train acc: 0.096000; val_acc: 0.094000\n",
      "(Iteration 3101 / 30560) loss: 2.306671\n",
      "(Iteration 3201 / 30560) loss: 2.306671\n",
      "(Iteration 3301 / 30560) loss: 2.306668\n",
      "(Iteration 3401 / 30560) loss: 2.306664\n",
      "(Epoch 9 / 80) train acc: 0.117000; val_acc: 0.094000\n",
      "(Iteration 3501 / 30560) loss: 2.306667\n",
      "(Iteration 3601 / 30560) loss: 2.306673\n",
      "(Iteration 3701 / 30560) loss: 2.306670\n",
      "(Iteration 3801 / 30560) loss: 2.306674\n",
      "(Epoch 10 / 80) train acc: 0.107000; val_acc: 0.094000\n",
      "(Iteration 3901 / 30560) loss: 2.306665\n",
      "(Iteration 4001 / 30560) loss: 2.306668\n",
      "(Iteration 4101 / 30560) loss: 2.306663\n",
      "(Iteration 4201 / 30560) loss: 2.306659\n",
      "(Epoch 11 / 80) train acc: 0.108000; val_acc: 0.094000\n",
      "(Iteration 4301 / 30560) loss: 2.306671\n",
      "(Iteration 4401 / 30560) loss: 2.306677\n",
      "(Iteration 4501 / 30560) loss: 2.306666\n",
      "(Epoch 12 / 80) train acc: 0.103000; val_acc: 0.094000\n",
      "(Iteration 4601 / 30560) loss: 2.306674\n",
      "(Iteration 4701 / 30560) loss: 2.306667\n",
      "(Iteration 4801 / 30560) loss: 2.306671\n",
      "(Iteration 4901 / 30560) loss: 2.306670\n",
      "(Epoch 13 / 80) train acc: 0.105000; val_acc: 0.094000\n",
      "(Iteration 5001 / 30560) loss: 2.306670\n",
      "(Iteration 5101 / 30560) loss: 2.306673\n",
      "(Iteration 5201 / 30560) loss: 2.306664\n",
      "(Iteration 5301 / 30560) loss: 2.306664\n",
      "(Epoch 14 / 80) train acc: 0.101000; val_acc: 0.094000\n",
      "(Iteration 5401 / 30560) loss: 2.306656\n",
      "(Iteration 5501 / 30560) loss: 2.306674\n",
      "(Iteration 5601 / 30560) loss: 2.306664\n",
      "(Iteration 5701 / 30560) loss: 2.306676\n",
      "(Epoch 15 / 80) train acc: 0.118000; val_acc: 0.094000\n",
      "(Iteration 5801 / 30560) loss: 2.306678\n",
      "(Iteration 5901 / 30560) loss: 2.306659\n",
      "(Iteration 6001 / 30560) loss: 2.306675\n",
      "(Iteration 6101 / 30560) loss: 2.306666\n",
      "(Epoch 16 / 80) train acc: 0.103000; val_acc: 0.094000\n",
      "(Iteration 6201 / 30560) loss: 2.306668\n",
      "(Iteration 6301 / 30560) loss: 2.306663\n",
      "(Iteration 6401 / 30560) loss: 2.306673\n",
      "(Epoch 17 / 80) train acc: 0.099000; val_acc: 0.094000\n",
      "(Iteration 6501 / 30560) loss: 2.306666\n",
      "(Iteration 6601 / 30560) loss: 2.306673\n",
      "(Iteration 6701 / 30560) loss: 2.306666\n",
      "(Iteration 6801 / 30560) loss: 2.306669\n",
      "(Epoch 18 / 80) train acc: 0.102000; val_acc: 0.094000\n",
      "(Iteration 6901 / 30560) loss: 2.306676\n",
      "(Iteration 7001 / 30560) loss: 2.306655\n",
      "(Iteration 7101 / 30560) loss: 2.306662\n",
      "(Iteration 7201 / 30560) loss: 2.306658\n",
      "(Epoch 19 / 80) train acc: 0.099000; val_acc: 0.094000\n",
      "(Iteration 7301 / 30560) loss: 2.306659\n",
      "(Iteration 7401 / 30560) loss: 2.306670\n",
      "(Iteration 7501 / 30560) loss: 2.306661\n",
      "(Iteration 7601 / 30560) loss: 2.306659\n",
      "(Epoch 20 / 80) train acc: 0.101000; val_acc: 0.094000\n",
      "(Iteration 7701 / 30560) loss: 2.306675\n",
      "(Iteration 7801 / 30560) loss: 2.306666\n",
      "(Iteration 7901 / 30560) loss: 2.306662\n",
      "(Iteration 8001 / 30560) loss: 2.306671\n",
      "(Epoch 21 / 80) train acc: 0.084000; val_acc: 0.094000\n",
      "(Iteration 8101 / 30560) loss: 2.306662\n",
      "(Iteration 8201 / 30560) loss: 2.306663\n",
      "(Iteration 8301 / 30560) loss: 2.306660\n",
      "(Iteration 8401 / 30560) loss: 2.306668\n",
      "(Epoch 22 / 80) train acc: 0.127000; val_acc: 0.094000\n",
      "(Iteration 8501 / 30560) loss: 2.306669\n",
      "(Iteration 8601 / 30560) loss: 2.306662\n",
      "(Iteration 8701 / 30560) loss: 2.306671\n",
      "(Epoch 23 / 80) train acc: 0.112000; val_acc: 0.094000\n",
      "(Iteration 8801 / 30560) loss: 2.306665\n",
      "(Iteration 8901 / 30560) loss: 2.306659\n",
      "(Iteration 9001 / 30560) loss: 2.306663\n",
      "(Iteration 9101 / 30560) loss: 2.306675\n",
      "(Epoch 24 / 80) train acc: 0.100000; val_acc: 0.094000\n",
      "(Iteration 9201 / 30560) loss: 2.306673\n",
      "(Iteration 9301 / 30560) loss: 2.306665\n",
      "(Iteration 9401 / 30560) loss: 2.306668\n",
      "(Iteration 9501 / 30560) loss: 2.306665\n",
      "(Epoch 25 / 80) train acc: 0.104000; val_acc: 0.095000\n",
      "(Iteration 9601 / 30560) loss: 2.306678\n",
      "(Iteration 9701 / 30560) loss: 2.306666\n",
      "(Iteration 9801 / 30560) loss: 2.306671\n",
      "(Iteration 9901 / 30560) loss: 2.306667\n",
      "(Epoch 26 / 80) train acc: 0.109000; val_acc: 0.094000\n",
      "(Iteration 10001 / 30560) loss: 2.306667\n",
      "(Iteration 10101 / 30560) loss: 2.306668\n",
      "(Iteration 10201 / 30560) loss: 2.306664\n",
      "(Iteration 10301 / 30560) loss: 2.306675\n",
      "(Epoch 27 / 80) train acc: 0.115000; val_acc: 0.094000\n",
      "(Iteration 10401 / 30560) loss: 2.306664\n",
      "(Iteration 10501 / 30560) loss: 2.306656\n",
      "(Iteration 10601 / 30560) loss: 2.306657\n",
      "(Epoch 28 / 80) train acc: 0.102000; val_acc: 0.094000\n",
      "(Iteration 10701 / 30560) loss: 2.306662\n",
      "(Iteration 10801 / 30560) loss: 2.306673\n",
      "(Iteration 10901 / 30560) loss: 2.306671\n",
      "(Iteration 11001 / 30560) loss: 2.306661\n",
      "(Epoch 29 / 80) train acc: 0.116000; val_acc: 0.094000\n",
      "(Iteration 11101 / 30560) loss: 2.306670\n",
      "(Iteration 11201 / 30560) loss: 2.306676\n",
      "(Iteration 11301 / 30560) loss: 2.306666\n",
      "(Iteration 11401 / 30560) loss: 2.306658\n",
      "(Epoch 30 / 80) train acc: 0.114000; val_acc: 0.094000\n",
      "(Iteration 11501 / 30560) loss: 2.306655\n",
      "(Iteration 11601 / 30560) loss: 2.306674\n",
      "(Iteration 11701 / 30560) loss: 2.306674\n",
      "(Iteration 11801 / 30560) loss: 2.306675\n",
      "(Epoch 31 / 80) train acc: 0.103000; val_acc: 0.094000\n",
      "(Iteration 11901 / 30560) loss: 2.306665\n",
      "(Iteration 12001 / 30560) loss: 2.306659\n",
      "(Iteration 12101 / 30560) loss: 2.306669\n",
      "(Iteration 12201 / 30560) loss: 2.306659\n",
      "(Epoch 32 / 80) train acc: 0.108000; val_acc: 0.094000\n",
      "(Iteration 12301 / 30560) loss: 2.306673\n",
      "(Iteration 12401 / 30560) loss: 2.306664\n",
      "(Iteration 12501 / 30560) loss: 2.306673\n",
      "(Iteration 12601 / 30560) loss: 2.306679\n",
      "(Epoch 33 / 80) train acc: 0.087000; val_acc: 0.094000\n",
      "(Iteration 12701 / 30560) loss: 2.306671\n",
      "(Iteration 12801 / 30560) loss: 2.306658\n",
      "(Iteration 12901 / 30560) loss: 2.306670\n",
      "(Epoch 34 / 80) train acc: 0.120000; val_acc: 0.095000\n",
      "(Iteration 13001 / 30560) loss: 2.306663\n",
      "(Iteration 13101 / 30560) loss: 2.306668\n",
      "(Iteration 13201 / 30560) loss: 2.306670\n",
      "(Iteration 13301 / 30560) loss: 2.306666\n",
      "(Epoch 35 / 80) train acc: 0.103000; val_acc: 0.095000\n",
      "(Iteration 13401 / 30560) loss: 2.306666\n",
      "(Iteration 13501 / 30560) loss: 2.306668\n",
      "(Iteration 13601 / 30560) loss: 2.306657\n",
      "(Iteration 13701 / 30560) loss: 2.306669\n",
      "(Epoch 36 / 80) train acc: 0.097000; val_acc: 0.095000\n",
      "(Iteration 13801 / 30560) loss: 2.306663\n",
      "(Iteration 13901 / 30560) loss: 2.306669\n",
      "(Iteration 14001 / 30560) loss: 2.306674\n",
      "(Iteration 14101 / 30560) loss: 2.306671\n",
      "(Epoch 37 / 80) train acc: 0.093000; val_acc: 0.095000\n",
      "(Iteration 14201 / 30560) loss: 2.306675\n",
      "(Iteration 14301 / 30560) loss: 2.306660\n",
      "(Iteration 14401 / 30560) loss: 2.306660\n",
      "(Iteration 14501 / 30560) loss: 2.306664\n",
      "(Epoch 38 / 80) train acc: 0.099000; val_acc: 0.094000\n",
      "(Iteration 14601 / 30560) loss: 2.306662\n",
      "(Iteration 14701 / 30560) loss: 2.306656\n",
      "(Iteration 14801 / 30560) loss: 2.306661\n",
      "(Epoch 39 / 80) train acc: 0.093000; val_acc: 0.095000\n",
      "(Iteration 14901 / 30560) loss: 2.306669\n",
      "(Iteration 15001 / 30560) loss: 2.306674\n",
      "(Iteration 15101 / 30560) loss: 2.306668\n",
      "(Iteration 15201 / 30560) loss: 2.306669\n",
      "(Epoch 40 / 80) train acc: 0.104000; val_acc: 0.095000\n",
      "(Iteration 15301 / 30560) loss: 2.306674\n",
      "(Iteration 15401 / 30560) loss: 2.306658\n",
      "(Iteration 15501 / 30560) loss: 2.306669\n",
      "(Iteration 15601 / 30560) loss: 2.306665\n",
      "(Epoch 41 / 80) train acc: 0.110000; val_acc: 0.095000\n",
      "(Iteration 15701 / 30560) loss: 2.306657\n",
      "(Iteration 15801 / 30560) loss: 2.306650\n",
      "(Iteration 15901 / 30560) loss: 2.306659\n",
      "(Iteration 16001 / 30560) loss: 2.306660\n",
      "(Epoch 42 / 80) train acc: 0.103000; val_acc: 0.095000\n",
      "(Iteration 16101 / 30560) loss: 2.306657\n",
      "(Iteration 16201 / 30560) loss: 2.306660\n",
      "(Iteration 16301 / 30560) loss: 2.306653\n",
      "(Iteration 16401 / 30560) loss: 2.306661\n",
      "(Epoch 43 / 80) train acc: 0.118000; val_acc: 0.095000\n",
      "(Iteration 16501 / 30560) loss: 2.306669\n",
      "(Iteration 16601 / 30560) loss: 2.306663\n",
      "(Iteration 16701 / 30560) loss: 2.306655\n",
      "(Iteration 16801 / 30560) loss: 2.306665\n",
      "(Epoch 44 / 80) train acc: 0.107000; val_acc: 0.095000\n",
      "(Iteration 16901 / 30560) loss: 2.306668\n",
      "(Iteration 17001 / 30560) loss: 2.306661\n",
      "(Iteration 17101 / 30560) loss: 2.306665\n",
      "(Epoch 45 / 80) train acc: 0.098000; val_acc: 0.095000\n",
      "(Iteration 17201 / 30560) loss: 2.306668\n",
      "(Iteration 17301 / 30560) loss: 2.306660\n",
      "(Iteration 17401 / 30560) loss: 2.306661\n",
      "(Iteration 17501 / 30560) loss: 2.306665\n",
      "(Epoch 46 / 80) train acc: 0.087000; val_acc: 0.095000\n",
      "(Iteration 17601 / 30560) loss: 2.306673\n",
      "(Iteration 17701 / 30560) loss: 2.306673\n",
      "(Iteration 17801 / 30560) loss: 2.306663\n",
      "(Iteration 17901 / 30560) loss: 2.306663\n",
      "(Epoch 47 / 80) train acc: 0.114000; val_acc: 0.095000\n",
      "(Iteration 18001 / 30560) loss: 2.306662\n",
      "(Iteration 18101 / 30560) loss: 2.306666\n",
      "(Iteration 18201 / 30560) loss: 2.306662\n",
      "(Iteration 18301 / 30560) loss: 2.306670\n",
      "(Epoch 48 / 80) train acc: 0.092000; val_acc: 0.095000\n",
      "(Iteration 18401 / 30560) loss: 2.306670\n",
      "(Iteration 18501 / 30560) loss: 2.306658\n",
      "(Iteration 18601 / 30560) loss: 2.306652\n",
      "(Iteration 18701 / 30560) loss: 2.306666\n",
      "(Epoch 49 / 80) train acc: 0.092000; val_acc: 0.095000\n",
      "(Iteration 18801 / 30560) loss: 2.306668\n",
      "(Iteration 18901 / 30560) loss: 2.306664\n",
      "(Iteration 19001 / 30560) loss: 2.306663\n",
      "(Epoch 50 / 80) train acc: 0.083000; val_acc: 0.095000\n",
      "(Iteration 19101 / 30560) loss: 2.306679\n",
      "(Iteration 19201 / 30560) loss: 2.306679\n",
      "(Iteration 19301 / 30560) loss: 2.306672\n",
      "(Iteration 19401 / 30560) loss: 2.306662\n",
      "(Epoch 51 / 80) train acc: 0.092000; val_acc: 0.095000\n",
      "(Iteration 19501 / 30560) loss: 2.306653\n",
      "(Iteration 19601 / 30560) loss: 2.306664\n",
      "(Iteration 19701 / 30560) loss: 2.306675\n",
      "(Iteration 19801 / 30560) loss: 2.306667\n",
      "(Epoch 52 / 80) train acc: 0.097000; val_acc: 0.095000\n",
      "(Iteration 19901 / 30560) loss: 2.306669\n",
      "(Iteration 20001 / 30560) loss: 2.306667\n",
      "(Iteration 20101 / 30560) loss: 2.306664\n",
      "(Iteration 20201 / 30560) loss: 2.306668\n",
      "(Epoch 53 / 80) train acc: 0.097000; val_acc: 0.095000\n",
      "(Iteration 20301 / 30560) loss: 2.306663\n",
      "(Iteration 20401 / 30560) loss: 2.306661\n",
      "(Iteration 20501 / 30560) loss: 2.306652\n",
      "(Iteration 20601 / 30560) loss: 2.306666\n",
      "(Epoch 54 / 80) train acc: 0.109000; val_acc: 0.095000\n",
      "(Iteration 20701 / 30560) loss: 2.306671\n",
      "(Iteration 20801 / 30560) loss: 2.306663\n",
      "(Iteration 20901 / 30560) loss: 2.306661\n",
      "(Iteration 21001 / 30560) loss: 2.306668\n",
      "(Epoch 55 / 80) train acc: 0.111000; val_acc: 0.095000\n",
      "(Iteration 21101 / 30560) loss: 2.306655\n",
      "(Iteration 21201 / 30560) loss: 2.306674\n",
      "(Iteration 21301 / 30560) loss: 2.306667\n",
      "(Epoch 56 / 80) train acc: 0.100000; val_acc: 0.095000\n",
      "(Iteration 21401 / 30560) loss: 2.306673\n",
      "(Iteration 21501 / 30560) loss: 2.306662\n",
      "(Iteration 21601 / 30560) loss: 2.306672\n",
      "(Iteration 21701 / 30560) loss: 2.306669\n",
      "(Epoch 57 / 80) train acc: 0.109000; val_acc: 0.095000\n",
      "(Iteration 21801 / 30560) loss: 2.306668\n",
      "(Iteration 21901 / 30560) loss: 2.306664\n",
      "(Iteration 22001 / 30560) loss: 2.306680\n",
      "(Iteration 22101 / 30560) loss: 2.306665\n",
      "(Epoch 58 / 80) train acc: 0.098000; val_acc: 0.095000\n",
      "(Iteration 22201 / 30560) loss: 2.306665\n",
      "(Iteration 22301 / 30560) loss: 2.306674\n",
      "(Iteration 22401 / 30560) loss: 2.306665\n",
      "(Iteration 22501 / 30560) loss: 2.306659\n",
      "(Epoch 59 / 80) train acc: 0.106000; val_acc: 0.095000\n",
      "(Iteration 22601 / 30560) loss: 2.306672\n",
      "(Iteration 22701 / 30560) loss: 2.306670\n",
      "(Iteration 22801 / 30560) loss: 2.306664\n",
      "(Iteration 22901 / 30560) loss: 2.306656\n",
      "(Epoch 60 / 80) train acc: 0.102000; val_acc: 0.095000\n",
      "(Iteration 23001 / 30560) loss: 2.306657\n",
      "(Iteration 23101 / 30560) loss: 2.306668\n",
      "(Iteration 23201 / 30560) loss: 2.306655\n",
      "(Iteration 23301 / 30560) loss: 2.306671\n",
      "(Epoch 61 / 80) train acc: 0.104000; val_acc: 0.095000\n",
      "(Iteration 23401 / 30560) loss: 2.306675\n",
      "(Iteration 23501 / 30560) loss: 2.306663\n",
      "(Iteration 23601 / 30560) loss: 2.306670\n",
      "(Epoch 62 / 80) train acc: 0.102000; val_acc: 0.095000\n",
      "(Iteration 23701 / 30560) loss: 2.306661\n",
      "(Iteration 23801 / 30560) loss: 2.306665\n",
      "(Iteration 23901 / 30560) loss: 2.306665\n",
      "(Iteration 24001 / 30560) loss: 2.306660\n",
      "(Epoch 63 / 80) train acc: 0.101000; val_acc: 0.095000\n",
      "(Iteration 24101 / 30560) loss: 2.306666\n",
      "(Iteration 24201 / 30560) loss: 2.306679\n",
      "(Iteration 24301 / 30560) loss: 2.306664\n",
      "(Iteration 24401 / 30560) loss: 2.306665\n",
      "(Epoch 64 / 80) train acc: 0.106000; val_acc: 0.095000\n",
      "(Iteration 24501 / 30560) loss: 2.306667\n",
      "(Iteration 24601 / 30560) loss: 2.306666\n",
      "(Iteration 24701 / 30560) loss: 2.306674\n",
      "(Iteration 24801 / 30560) loss: 2.306671\n",
      "(Epoch 65 / 80) train acc: 0.103000; val_acc: 0.095000\n",
      "(Iteration 24901 / 30560) loss: 2.306657\n",
      "(Iteration 25001 / 30560) loss: 2.306665\n",
      "(Iteration 25101 / 30560) loss: 2.306674\n",
      "(Iteration 25201 / 30560) loss: 2.306664\n",
      "(Epoch 66 / 80) train acc: 0.108000; val_acc: 0.095000\n",
      "(Iteration 25301 / 30560) loss: 2.306662\n",
      "(Iteration 25401 / 30560) loss: 2.306656\n",
      "(Iteration 25501 / 30560) loss: 2.306666\n",
      "(Epoch 67 / 80) train acc: 0.095000; val_acc: 0.095000\n",
      "(Iteration 25601 / 30560) loss: 2.306674\n",
      "(Iteration 25701 / 30560) loss: 2.306670\n",
      "(Iteration 25801 / 30560) loss: 2.306668\n",
      "(Iteration 25901 / 30560) loss: 2.306666\n",
      "(Epoch 68 / 80) train acc: 0.111000; val_acc: 0.095000\n",
      "(Iteration 26001 / 30560) loss: 2.306680\n",
      "(Iteration 26101 / 30560) loss: 2.306654\n",
      "(Iteration 26201 / 30560) loss: 2.306671\n",
      "(Iteration 26301 / 30560) loss: 2.306668\n",
      "(Epoch 69 / 80) train acc: 0.106000; val_acc: 0.095000\n",
      "(Iteration 26401 / 30560) loss: 2.306661\n",
      "(Iteration 26501 / 30560) loss: 2.306663\n",
      "(Iteration 26601 / 30560) loss: 2.306667\n",
      "(Iteration 26701 / 30560) loss: 2.306666\n",
      "(Epoch 70 / 80) train acc: 0.115000; val_acc: 0.095000\n",
      "(Iteration 26801 / 30560) loss: 2.306662\n",
      "(Iteration 26901 / 30560) loss: 2.306660\n",
      "(Iteration 27001 / 30560) loss: 2.306661\n",
      "(Iteration 27101 / 30560) loss: 2.306661\n",
      "(Epoch 71 / 80) train acc: 0.106000; val_acc: 0.095000\n",
      "(Iteration 27201 / 30560) loss: 2.306674\n",
      "(Iteration 27301 / 30560) loss: 2.306667\n",
      "(Iteration 27401 / 30560) loss: 2.306664\n",
      "(Iteration 27501 / 30560) loss: 2.306666\n",
      "(Epoch 72 / 80) train acc: 0.115000; val_acc: 0.095000\n",
      "(Iteration 27601 / 30560) loss: 2.306659\n",
      "(Iteration 27701 / 30560) loss: 2.306670\n",
      "(Iteration 27801 / 30560) loss: 2.306661\n",
      "(Epoch 73 / 80) train acc: 0.113000; val_acc: 0.095000\n",
      "(Iteration 27901 / 30560) loss: 2.306675\n",
      "(Iteration 28001 / 30560) loss: 2.306660\n",
      "(Iteration 28101 / 30560) loss: 2.306663\n",
      "(Iteration 28201 / 30560) loss: 2.306667\n",
      "(Epoch 74 / 80) train acc: 0.112000; val_acc: 0.095000\n",
      "(Iteration 28301 / 30560) loss: 2.306664\n",
      "(Iteration 28401 / 30560) loss: 2.306672\n",
      "(Iteration 28501 / 30560) loss: 2.306660\n",
      "(Iteration 28601 / 30560) loss: 2.306662\n",
      "(Epoch 75 / 80) train acc: 0.101000; val_acc: 0.095000\n",
      "(Iteration 28701 / 30560) loss: 2.306662\n",
      "(Iteration 28801 / 30560) loss: 2.306663\n",
      "(Iteration 28901 / 30560) loss: 2.306665\n",
      "(Iteration 29001 / 30560) loss: 2.306670\n",
      "(Epoch 76 / 80) train acc: 0.099000; val_acc: 0.095000\n",
      "(Iteration 29101 / 30560) loss: 2.306677\n",
      "(Iteration 29201 / 30560) loss: 2.306660\n",
      "(Iteration 29301 / 30560) loss: 2.306662\n",
      "(Iteration 29401 / 30560) loss: 2.306678\n",
      "(Epoch 77 / 80) train acc: 0.110000; val_acc: 0.095000\n",
      "(Iteration 29501 / 30560) loss: 2.306657\n",
      "(Iteration 29601 / 30560) loss: 2.306661\n",
      "(Iteration 29701 / 30560) loss: 2.306681\n",
      "(Epoch 78 / 80) train acc: 0.112000; val_acc: 0.095000\n",
      "(Iteration 29801 / 30560) loss: 2.306662\n",
      "(Iteration 29901 / 30560) loss: 2.306677\n",
      "(Iteration 30001 / 30560) loss: 2.306676\n",
      "(Iteration 30101 / 30560) loss: 2.306673\n",
      "(Epoch 79 / 80) train acc: 0.103000; val_acc: 0.095000\n",
      "(Iteration 30201 / 30560) loss: 2.306666\n",
      "(Iteration 30301 / 30560) loss: 2.306663\n",
      "(Iteration 30401 / 30560) loss: 2.306663\n",
      "(Iteration 30501 / 30560) loss: 2.306658\n",
      "(Epoch 80 / 80) train acc: 0.091000; val_acc: 0.095000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.308279\n",
      "(Epoch 0 / 80) train acc: 0.091000; val_acc: 0.104000\n",
      "(Iteration 101 / 61200) loss: 2.308276\n",
      "(Iteration 201 / 61200) loss: 2.308291\n",
      "(Iteration 301 / 61200) loss: 2.308288\n",
      "(Iteration 401 / 61200) loss: 2.308302\n",
      "(Iteration 501 / 61200) loss: 2.308293\n",
      "(Iteration 601 / 61200) loss: 2.308275\n",
      "(Iteration 701 / 61200) loss: 2.308298\n",
      "(Epoch 1 / 80) train acc: 0.089000; val_acc: 0.104000\n",
      "(Iteration 801 / 61200) loss: 2.308299\n",
      "(Iteration 901 / 61200) loss: 2.308280\n",
      "(Iteration 1001 / 61200) loss: 2.308294\n",
      "(Iteration 1101 / 61200) loss: 2.308303\n",
      "(Iteration 1201 / 61200) loss: 2.308296\n",
      "(Iteration 1301 / 61200) loss: 2.308297\n",
      "(Iteration 1401 / 61200) loss: 2.308277\n",
      "(Iteration 1501 / 61200) loss: 2.308289\n",
      "(Epoch 2 / 80) train acc: 0.090000; val_acc: 0.104000\n",
      "(Iteration 1601 / 61200) loss: 2.308284\n",
      "(Iteration 1701 / 61200) loss: 2.308307\n",
      "(Iteration 1801 / 61200) loss: 2.308309\n",
      "(Iteration 1901 / 61200) loss: 2.308293\n",
      "(Iteration 2001 / 61200) loss: 2.308283\n",
      "(Iteration 2101 / 61200) loss: 2.308304\n",
      "(Iteration 2201 / 61200) loss: 2.308310\n",
      "(Epoch 3 / 80) train acc: 0.075000; val_acc: 0.104000\n",
      "(Iteration 2301 / 61200) loss: 2.308305\n",
      "(Iteration 2401 / 61200) loss: 2.308307\n",
      "(Iteration 2501 / 61200) loss: 2.308281\n",
      "(Iteration 2601 / 61200) loss: 2.308296\n",
      "(Iteration 2701 / 61200) loss: 2.308270\n",
      "(Iteration 2801 / 61200) loss: 2.308285\n",
      "(Iteration 2901 / 61200) loss: 2.308297\n",
      "(Iteration 3001 / 61200) loss: 2.308288\n",
      "(Epoch 4 / 80) train acc: 0.090000; val_acc: 0.104000\n",
      "(Iteration 3101 / 61200) loss: 2.308308\n",
      "(Iteration 3201 / 61200) loss: 2.308289\n",
      "(Iteration 3301 / 61200) loss: 2.308284\n",
      "(Iteration 3401 / 61200) loss: 2.308285\n",
      "(Iteration 3501 / 61200) loss: 2.308282\n",
      "(Iteration 3601 / 61200) loss: 2.308299\n",
      "(Iteration 3701 / 61200) loss: 2.308294\n",
      "(Iteration 3801 / 61200) loss: 2.308285\n",
      "(Epoch 5 / 80) train acc: 0.102000; val_acc: 0.104000\n",
      "(Iteration 3901 / 61200) loss: 2.308285\n",
      "(Iteration 4001 / 61200) loss: 2.308289\n",
      "(Iteration 4101 / 61200) loss: 2.308280\n",
      "(Iteration 4201 / 61200) loss: 2.308279\n",
      "(Iteration 4301 / 61200) loss: 2.308297\n",
      "(Iteration 4401 / 61200) loss: 2.308293\n",
      "(Iteration 4501 / 61200) loss: 2.308299\n",
      "(Epoch 6 / 80) train acc: 0.090000; val_acc: 0.105000\n",
      "(Iteration 4601 / 61200) loss: 2.308298\n",
      "(Iteration 4701 / 61200) loss: 2.308288\n",
      "(Iteration 4801 / 61200) loss: 2.308288\n",
      "(Iteration 4901 / 61200) loss: 2.308300\n",
      "(Iteration 5001 / 61200) loss: 2.308307\n",
      "(Iteration 5101 / 61200) loss: 2.308302\n",
      "(Iteration 5201 / 61200) loss: 2.308281\n",
      "(Iteration 5301 / 61200) loss: 2.308286\n",
      "(Epoch 7 / 80) train acc: 0.081000; val_acc: 0.105000\n",
      "(Iteration 5401 / 61200) loss: 2.308285\n",
      "(Iteration 5501 / 61200) loss: 2.308300\n",
      "(Iteration 5601 / 61200) loss: 2.308290\n",
      "(Iteration 5701 / 61200) loss: 2.308307\n",
      "(Iteration 5801 / 61200) loss: 2.308283\n",
      "(Iteration 5901 / 61200) loss: 2.308288\n",
      "(Iteration 6001 / 61200) loss: 2.308285\n",
      "(Iteration 6101 / 61200) loss: 2.308295\n",
      "(Epoch 8 / 80) train acc: 0.086000; val_acc: 0.106000\n",
      "(Iteration 6201 / 61200) loss: 2.308289\n",
      "(Iteration 6301 / 61200) loss: 2.308280\n",
      "(Iteration 6401 / 61200) loss: 2.308305\n",
      "(Iteration 6501 / 61200) loss: 2.308289\n",
      "(Iteration 6601 / 61200) loss: 2.308290\n",
      "(Iteration 6701 / 61200) loss: 2.308288\n",
      "(Iteration 6801 / 61200) loss: 2.308276\n",
      "(Epoch 9 / 80) train acc: 0.098000; val_acc: 0.106000\n",
      "(Iteration 6901 / 61200) loss: 2.308294\n",
      "(Iteration 7001 / 61200) loss: 2.308293\n",
      "(Iteration 7101 / 61200) loss: 2.308285\n",
      "(Iteration 7201 / 61200) loss: 2.308303\n",
      "(Iteration 7301 / 61200) loss: 2.308297\n",
      "(Iteration 7401 / 61200) loss: 2.308293\n",
      "(Iteration 7501 / 61200) loss: 2.308293\n",
      "(Iteration 7601 / 61200) loss: 2.308282\n",
      "(Epoch 10 / 80) train acc: 0.086000; val_acc: 0.106000\n",
      "(Iteration 7701 / 61200) loss: 2.308290\n",
      "(Iteration 7801 / 61200) loss: 2.308302\n",
      "(Iteration 7901 / 61200) loss: 2.308293\n",
      "(Iteration 8001 / 61200) loss: 2.308285\n",
      "(Iteration 8101 / 61200) loss: 2.308284\n",
      "(Iteration 8201 / 61200) loss: 2.308286\n",
      "(Iteration 8301 / 61200) loss: 2.308284\n",
      "(Iteration 8401 / 61200) loss: 2.308300\n",
      "(Epoch 11 / 80) train acc: 0.082000; val_acc: 0.106000\n",
      "(Iteration 8501 / 61200) loss: 2.308292\n",
      "(Iteration 8601 / 61200) loss: 2.308268\n",
      "(Iteration 8701 / 61200) loss: 2.308269\n",
      "(Iteration 8801 / 61200) loss: 2.308292\n",
      "(Iteration 8901 / 61200) loss: 2.308300\n",
      "(Iteration 9001 / 61200) loss: 2.308293\n",
      "(Iteration 9101 / 61200) loss: 2.308283\n",
      "(Epoch 12 / 80) train acc: 0.099000; val_acc: 0.106000\n",
      "(Iteration 9201 / 61200) loss: 2.308298\n",
      "(Iteration 9301 / 61200) loss: 2.308304\n",
      "(Iteration 9401 / 61200) loss: 2.308284\n",
      "(Iteration 9501 / 61200) loss: 2.308302\n",
      "(Iteration 9601 / 61200) loss: 2.308299\n",
      "(Iteration 9701 / 61200) loss: 2.308282\n",
      "(Iteration 9801 / 61200) loss: 2.308289\n",
      "(Iteration 9901 / 61200) loss: 2.308295\n",
      "(Epoch 13 / 80) train acc: 0.094000; val_acc: 0.106000\n",
      "(Iteration 10001 / 61200) loss: 2.308284\n",
      "(Iteration 10101 / 61200) loss: 2.308302\n",
      "(Iteration 10201 / 61200) loss: 2.308281\n",
      "(Iteration 10301 / 61200) loss: 2.308281\n",
      "(Iteration 10401 / 61200) loss: 2.308279\n",
      "(Iteration 10501 / 61200) loss: 2.308294\n",
      "(Iteration 10601 / 61200) loss: 2.308299\n",
      "(Iteration 10701 / 61200) loss: 2.308283\n",
      "(Epoch 14 / 80) train acc: 0.081000; val_acc: 0.106000\n",
      "(Iteration 10801 / 61200) loss: 2.308294\n",
      "(Iteration 10901 / 61200) loss: 2.308285\n",
      "(Iteration 11001 / 61200) loss: 2.308304\n",
      "(Iteration 11101 / 61200) loss: 2.308290\n",
      "(Iteration 11201 / 61200) loss: 2.308293\n",
      "(Iteration 11301 / 61200) loss: 2.308297\n",
      "(Iteration 11401 / 61200) loss: 2.308294\n",
      "(Epoch 15 / 80) train acc: 0.073000; val_acc: 0.106000\n",
      "(Iteration 11501 / 61200) loss: 2.308304\n",
      "(Iteration 11601 / 61200) loss: 2.308287\n",
      "(Iteration 11701 / 61200) loss: 2.308291\n",
      "(Iteration 11801 / 61200) loss: 2.308277\n",
      "(Iteration 11901 / 61200) loss: 2.308302\n",
      "(Iteration 12001 / 61200) loss: 2.308292\n",
      "(Iteration 12101 / 61200) loss: 2.308284\n",
      "(Iteration 12201 / 61200) loss: 2.308277\n",
      "(Epoch 16 / 80) train acc: 0.079000; val_acc: 0.106000\n",
      "(Iteration 12301 / 61200) loss: 2.308287\n",
      "(Iteration 12401 / 61200) loss: 2.308276\n",
      "(Iteration 12501 / 61200) loss: 2.308282\n",
      "(Iteration 12601 / 61200) loss: 2.308293\n",
      "(Iteration 12701 / 61200) loss: 2.308289\n",
      "(Iteration 12801 / 61200) loss: 2.308294\n",
      "(Iteration 12901 / 61200) loss: 2.308283\n",
      "(Iteration 13001 / 61200) loss: 2.308305\n",
      "(Epoch 17 / 80) train acc: 0.090000; val_acc: 0.106000\n",
      "(Iteration 13101 / 61200) loss: 2.308297\n",
      "(Iteration 13201 / 61200) loss: 2.308284\n",
      "(Iteration 13301 / 61200) loss: 2.308288\n",
      "(Iteration 13401 / 61200) loss: 2.308282\n",
      "(Iteration 13501 / 61200) loss: 2.308292\n",
      "(Iteration 13601 / 61200) loss: 2.308300\n",
      "(Iteration 13701 / 61200) loss: 2.308292\n",
      "(Epoch 18 / 80) train acc: 0.097000; val_acc: 0.106000\n",
      "(Iteration 13801 / 61200) loss: 2.308267\n",
      "(Iteration 13901 / 61200) loss: 2.308294\n",
      "(Iteration 14001 / 61200) loss: 2.308292\n",
      "(Iteration 14101 / 61200) loss: 2.308296\n",
      "(Iteration 14201 / 61200) loss: 2.308291\n",
      "(Iteration 14301 / 61200) loss: 2.308286\n",
      "(Iteration 14401 / 61200) loss: 2.308297\n",
      "(Iteration 14501 / 61200) loss: 2.308299\n",
      "(Epoch 19 / 80) train acc: 0.090000; val_acc: 0.106000\n",
      "(Iteration 14601 / 61200) loss: 2.308274\n",
      "(Iteration 14701 / 61200) loss: 2.308277\n",
      "(Iteration 14801 / 61200) loss: 2.308288\n",
      "(Iteration 14901 / 61200) loss: 2.308278\n",
      "(Iteration 15001 / 61200) loss: 2.308284\n",
      "(Iteration 15101 / 61200) loss: 2.308292\n",
      "(Iteration 15201 / 61200) loss: 2.308296\n",
      "(Epoch 20 / 80) train acc: 0.084000; val_acc: 0.106000\n",
      "(Iteration 15301 / 61200) loss: 2.308294\n",
      "(Iteration 15401 / 61200) loss: 2.308274\n",
      "(Iteration 15501 / 61200) loss: 2.308298\n",
      "(Iteration 15601 / 61200) loss: 2.308278\n",
      "(Iteration 15701 / 61200) loss: 2.308277\n",
      "(Iteration 15801 / 61200) loss: 2.308285\n",
      "(Iteration 15901 / 61200) loss: 2.308280\n",
      "(Iteration 16001 / 61200) loss: 2.308281\n",
      "(Epoch 21 / 80) train acc: 0.082000; val_acc: 0.106000\n",
      "(Iteration 16101 / 61200) loss: 2.308276\n",
      "(Iteration 16201 / 61200) loss: 2.308316\n",
      "(Iteration 16301 / 61200) loss: 2.308302\n",
      "(Iteration 16401 / 61200) loss: 2.308300\n",
      "(Iteration 16501 / 61200) loss: 2.308281\n",
      "(Iteration 16601 / 61200) loss: 2.308273\n",
      "(Iteration 16701 / 61200) loss: 2.308297\n",
      "(Iteration 16801 / 61200) loss: 2.308280\n",
      "(Epoch 22 / 80) train acc: 0.093000; val_acc: 0.106000\n",
      "(Iteration 16901 / 61200) loss: 2.308287\n",
      "(Iteration 17001 / 61200) loss: 2.308295\n",
      "(Iteration 17101 / 61200) loss: 2.308299\n",
      "(Iteration 17201 / 61200) loss: 2.308299\n",
      "(Iteration 17301 / 61200) loss: 2.308291\n",
      "(Iteration 17401 / 61200) loss: 2.308268\n",
      "(Iteration 17501 / 61200) loss: 2.308285\n",
      "(Epoch 23 / 80) train acc: 0.077000; val_acc: 0.106000\n",
      "(Iteration 17601 / 61200) loss: 2.308305\n",
      "(Iteration 17701 / 61200) loss: 2.308288\n",
      "(Iteration 17801 / 61200) loss: 2.308296\n",
      "(Iteration 17901 / 61200) loss: 2.308321\n",
      "(Iteration 18001 / 61200) loss: 2.308294\n",
      "(Iteration 18101 / 61200) loss: 2.308287\n",
      "(Iteration 18201 / 61200) loss: 2.308295\n",
      "(Iteration 18301 / 61200) loss: 2.308297\n",
      "(Epoch 24 / 80) train acc: 0.084000; val_acc: 0.106000\n",
      "(Iteration 18401 / 61200) loss: 2.308299\n",
      "(Iteration 18501 / 61200) loss: 2.308300\n",
      "(Iteration 18601 / 61200) loss: 2.308297\n",
      "(Iteration 18701 / 61200) loss: 2.308282\n",
      "(Iteration 18801 / 61200) loss: 2.308304\n",
      "(Iteration 18901 / 61200) loss: 2.308283\n",
      "(Iteration 19001 / 61200) loss: 2.308285\n",
      "(Iteration 19101 / 61200) loss: 2.308279\n",
      "(Epoch 25 / 80) train acc: 0.097000; val_acc: 0.106000\n",
      "(Iteration 19201 / 61200) loss: 2.308268\n",
      "(Iteration 19301 / 61200) loss: 2.308286\n",
      "(Iteration 19401 / 61200) loss: 2.308282\n",
      "(Iteration 19501 / 61200) loss: 2.308296\n",
      "(Iteration 19601 / 61200) loss: 2.308301\n",
      "(Iteration 19701 / 61200) loss: 2.308273\n",
      "(Iteration 19801 / 61200) loss: 2.308297\n",
      "(Epoch 26 / 80) train acc: 0.086000; val_acc: 0.106000\n",
      "(Iteration 19901 / 61200) loss: 2.308280\n",
      "(Iteration 20001 / 61200) loss: 2.308285\n",
      "(Iteration 20101 / 61200) loss: 2.308276\n",
      "(Iteration 20201 / 61200) loss: 2.308297\n",
      "(Iteration 20301 / 61200) loss: 2.308305\n",
      "(Iteration 20401 / 61200) loss: 2.308289\n",
      "(Iteration 20501 / 61200) loss: 2.308291\n",
      "(Iteration 20601 / 61200) loss: 2.308287\n",
      "(Epoch 27 / 80) train acc: 0.085000; val_acc: 0.106000\n",
      "(Iteration 20701 / 61200) loss: 2.308305\n",
      "(Iteration 20801 / 61200) loss: 2.308297\n",
      "(Iteration 20901 / 61200) loss: 2.308299\n",
      "(Iteration 21001 / 61200) loss: 2.308301\n",
      "(Iteration 21101 / 61200) loss: 2.308295\n",
      "(Iteration 21201 / 61200) loss: 2.308284\n",
      "(Iteration 21301 / 61200) loss: 2.308288\n",
      "(Iteration 21401 / 61200) loss: 2.308291\n",
      "(Epoch 28 / 80) train acc: 0.084000; val_acc: 0.106000\n",
      "(Iteration 21501 / 61200) loss: 2.308283\n",
      "(Iteration 21601 / 61200) loss: 2.308274\n",
      "(Iteration 21701 / 61200) loss: 2.308289\n",
      "(Iteration 21801 / 61200) loss: 2.308286\n",
      "(Iteration 21901 / 61200) loss: 2.308289\n",
      "(Iteration 22001 / 61200) loss: 2.308300\n",
      "(Iteration 22101 / 61200) loss: 2.308287\n",
      "(Epoch 29 / 80) train acc: 0.095000; val_acc: 0.106000\n",
      "(Iteration 22201 / 61200) loss: 2.308299\n",
      "(Iteration 22301 / 61200) loss: 2.308296\n",
      "(Iteration 22401 / 61200) loss: 2.308278\n",
      "(Iteration 22501 / 61200) loss: 2.308284\n",
      "(Iteration 22601 / 61200) loss: 2.308286\n",
      "(Iteration 22701 / 61200) loss: 2.308287\n",
      "(Iteration 22801 / 61200) loss: 2.308277\n",
      "(Iteration 22901 / 61200) loss: 2.308306\n",
      "(Epoch 30 / 80) train acc: 0.086000; val_acc: 0.106000\n",
      "(Iteration 23001 / 61200) loss: 2.308298\n",
      "(Iteration 23101 / 61200) loss: 2.308281\n",
      "(Iteration 23201 / 61200) loss: 2.308264\n",
      "(Iteration 23301 / 61200) loss: 2.308284\n",
      "(Iteration 23401 / 61200) loss: 2.308282\n",
      "(Iteration 23501 / 61200) loss: 2.308301\n",
      "(Iteration 23601 / 61200) loss: 2.308280\n",
      "(Iteration 23701 / 61200) loss: 2.308288\n",
      "(Epoch 31 / 80) train acc: 0.071000; val_acc: 0.106000\n",
      "(Iteration 23801 / 61200) loss: 2.308291\n",
      "(Iteration 23901 / 61200) loss: 2.308289\n",
      "(Iteration 24001 / 61200) loss: 2.308283\n",
      "(Iteration 24101 / 61200) loss: 2.308294\n",
      "(Iteration 24201 / 61200) loss: 2.308297\n",
      "(Iteration 24301 / 61200) loss: 2.308278\n",
      "(Iteration 24401 / 61200) loss: 2.308283\n",
      "(Epoch 32 / 80) train acc: 0.090000; val_acc: 0.106000\n",
      "(Iteration 24501 / 61200) loss: 2.308308\n",
      "(Iteration 24601 / 61200) loss: 2.308298\n",
      "(Iteration 24701 / 61200) loss: 2.308277\n",
      "(Iteration 24801 / 61200) loss: 2.308294\n",
      "(Iteration 24901 / 61200) loss: 2.308291\n",
      "(Iteration 25001 / 61200) loss: 2.308283\n",
      "(Iteration 25101 / 61200) loss: 2.308289\n",
      "(Iteration 25201 / 61200) loss: 2.308283\n",
      "(Epoch 33 / 80) train acc: 0.097000; val_acc: 0.106000\n",
      "(Iteration 25301 / 61200) loss: 2.308258\n",
      "(Iteration 25401 / 61200) loss: 2.308297\n",
      "(Iteration 25501 / 61200) loss: 2.308284\n",
      "(Iteration 25601 / 61200) loss: 2.308303\n",
      "(Iteration 25701 / 61200) loss: 2.308298\n",
      "(Iteration 25801 / 61200) loss: 2.308289\n",
      "(Iteration 25901 / 61200) loss: 2.308305\n",
      "(Iteration 26001 / 61200) loss: 2.308304\n",
      "(Epoch 34 / 80) train acc: 0.086000; val_acc: 0.106000\n",
      "(Iteration 26101 / 61200) loss: 2.308280\n",
      "(Iteration 26201 / 61200) loss: 2.308289\n",
      "(Iteration 26301 / 61200) loss: 2.308280\n",
      "(Iteration 26401 / 61200) loss: 2.308293\n",
      "(Iteration 26501 / 61200) loss: 2.308281\n",
      "(Iteration 26601 / 61200) loss: 2.308293\n",
      "(Iteration 26701 / 61200) loss: 2.308302\n",
      "(Epoch 35 / 80) train acc: 0.096000; val_acc: 0.106000\n",
      "(Iteration 26801 / 61200) loss: 2.308298\n",
      "(Iteration 26901 / 61200) loss: 2.308273\n",
      "(Iteration 27001 / 61200) loss: 2.308273\n",
      "(Iteration 27101 / 61200) loss: 2.308286\n",
      "(Iteration 27201 / 61200) loss: 2.308277\n",
      "(Iteration 27301 / 61200) loss: 2.308296\n",
      "(Iteration 27401 / 61200) loss: 2.308285\n",
      "(Iteration 27501 / 61200) loss: 2.308286\n",
      "(Epoch 36 / 80) train acc: 0.099000; val_acc: 0.106000\n",
      "(Iteration 27601 / 61200) loss: 2.308304\n",
      "(Iteration 27701 / 61200) loss: 2.308291\n",
      "(Iteration 27801 / 61200) loss: 2.308290\n",
      "(Iteration 27901 / 61200) loss: 2.308286\n",
      "(Iteration 28001 / 61200) loss: 2.308290\n",
      "(Iteration 28101 / 61200) loss: 2.308279\n",
      "(Iteration 28201 / 61200) loss: 2.308283\n",
      "(Iteration 28301 / 61200) loss: 2.308280\n",
      "(Epoch 37 / 80) train acc: 0.101000; val_acc: 0.106000\n",
      "(Iteration 28401 / 61200) loss: 2.308290\n",
      "(Iteration 28501 / 61200) loss: 2.308281\n",
      "(Iteration 28601 / 61200) loss: 2.308287\n",
      "(Iteration 28701 / 61200) loss: 2.308298\n",
      "(Iteration 28801 / 61200) loss: 2.308298\n",
      "(Iteration 28901 / 61200) loss: 2.308289\n",
      "(Iteration 29001 / 61200) loss: 2.308282\n",
      "(Epoch 38 / 80) train acc: 0.080000; val_acc: 0.106000\n",
      "(Iteration 29101 / 61200) loss: 2.308299\n",
      "(Iteration 29201 / 61200) loss: 2.308278\n",
      "(Iteration 29301 / 61200) loss: 2.308314\n",
      "(Iteration 29401 / 61200) loss: 2.308292\n",
      "(Iteration 29501 / 61200) loss: 2.308285\n",
      "(Iteration 29601 / 61200) loss: 2.308284\n",
      "(Iteration 29701 / 61200) loss: 2.308286\n",
      "(Iteration 29801 / 61200) loss: 2.308297\n",
      "(Epoch 39 / 80) train acc: 0.089000; val_acc: 0.106000\n",
      "(Iteration 29901 / 61200) loss: 2.308295\n",
      "(Iteration 30001 / 61200) loss: 2.308308\n",
      "(Iteration 30101 / 61200) loss: 2.308295\n",
      "(Iteration 30201 / 61200) loss: 2.308280\n",
      "(Iteration 30301 / 61200) loss: 2.308281\n",
      "(Iteration 30401 / 61200) loss: 2.308303\n",
      "(Iteration 30501 / 61200) loss: 2.308280\n",
      "(Epoch 40 / 80) train acc: 0.092000; val_acc: 0.106000\n",
      "(Iteration 30601 / 61200) loss: 2.308273\n",
      "(Iteration 30701 / 61200) loss: 2.308307\n",
      "(Iteration 30801 / 61200) loss: 2.308277\n",
      "(Iteration 30901 / 61200) loss: 2.308294\n",
      "(Iteration 31001 / 61200) loss: 2.308296\n",
      "(Iteration 31101 / 61200) loss: 2.308301\n",
      "(Iteration 31201 / 61200) loss: 2.308300\n",
      "(Iteration 31301 / 61200) loss: 2.308284\n",
      "(Epoch 41 / 80) train acc: 0.088000; val_acc: 0.106000\n",
      "(Iteration 31401 / 61200) loss: 2.308281\n",
      "(Iteration 31501 / 61200) loss: 2.308289\n",
      "(Iteration 31601 / 61200) loss: 2.308297\n",
      "(Iteration 31701 / 61200) loss: 2.308280\n",
      "(Iteration 31801 / 61200) loss: 2.308310\n",
      "(Iteration 31901 / 61200) loss: 2.308292\n",
      "(Iteration 32001 / 61200) loss: 2.308294\n",
      "(Iteration 32101 / 61200) loss: 2.308282\n",
      "(Epoch 42 / 80) train acc: 0.077000; val_acc: 0.106000\n",
      "(Iteration 32201 / 61200) loss: 2.308288\n",
      "(Iteration 32301 / 61200) loss: 2.308287\n",
      "(Iteration 32401 / 61200) loss: 2.308284\n",
      "(Iteration 32501 / 61200) loss: 2.308291\n",
      "(Iteration 32601 / 61200) loss: 2.308303\n",
      "(Iteration 32701 / 61200) loss: 2.308294\n",
      "(Iteration 32801 / 61200) loss: 2.308287\n",
      "(Epoch 43 / 80) train acc: 0.092000; val_acc: 0.106000\n",
      "(Iteration 32901 / 61200) loss: 2.308283\n",
      "(Iteration 33001 / 61200) loss: 2.308277\n",
      "(Iteration 33101 / 61200) loss: 2.308274\n",
      "(Iteration 33201 / 61200) loss: 2.308285\n",
      "(Iteration 33301 / 61200) loss: 2.308286\n",
      "(Iteration 33401 / 61200) loss: 2.308293\n",
      "(Iteration 33501 / 61200) loss: 2.308287\n",
      "(Iteration 33601 / 61200) loss: 2.308277\n",
      "(Epoch 44 / 80) train acc: 0.099000; val_acc: 0.106000\n",
      "(Iteration 33701 / 61200) loss: 2.308287\n",
      "(Iteration 33801 / 61200) loss: 2.308297\n",
      "(Iteration 33901 / 61200) loss: 2.308282\n",
      "(Iteration 34001 / 61200) loss: 2.308294\n",
      "(Iteration 34101 / 61200) loss: 2.308290\n",
      "(Iteration 34201 / 61200) loss: 2.308292\n",
      "(Iteration 34301 / 61200) loss: 2.308275\n",
      "(Iteration 34401 / 61200) loss: 2.308305\n",
      "(Epoch 45 / 80) train acc: 0.103000; val_acc: 0.106000\n",
      "(Iteration 34501 / 61200) loss: 2.308298\n",
      "(Iteration 34601 / 61200) loss: 2.308290\n",
      "(Iteration 34701 / 61200) loss: 2.308285\n",
      "(Iteration 34801 / 61200) loss: 2.308293\n",
      "(Iteration 34901 / 61200) loss: 2.308281\n",
      "(Iteration 35001 / 61200) loss: 2.308287\n",
      "(Iteration 35101 / 61200) loss: 2.308291\n",
      "(Epoch 46 / 80) train acc: 0.091000; val_acc: 0.106000\n",
      "(Iteration 35201 / 61200) loss: 2.308290\n",
      "(Iteration 35301 / 61200) loss: 2.308304\n",
      "(Iteration 35401 / 61200) loss: 2.308286\n",
      "(Iteration 35501 / 61200) loss: 2.308286\n",
      "(Iteration 35601 / 61200) loss: 2.308293\n",
      "(Iteration 35701 / 61200) loss: 2.308271\n",
      "(Iteration 35801 / 61200) loss: 2.308297\n",
      "(Iteration 35901 / 61200) loss: 2.308271\n",
      "(Epoch 47 / 80) train acc: 0.086000; val_acc: 0.106000\n",
      "(Iteration 36001 / 61200) loss: 2.308277\n",
      "(Iteration 36101 / 61200) loss: 2.308288\n",
      "(Iteration 36201 / 61200) loss: 2.308303\n",
      "(Iteration 36301 / 61200) loss: 2.308294\n",
      "(Iteration 36401 / 61200) loss: 2.308294\n",
      "(Iteration 36501 / 61200) loss: 2.308291\n",
      "(Iteration 36601 / 61200) loss: 2.308296\n",
      "(Iteration 36701 / 61200) loss: 2.308295\n",
      "(Epoch 48 / 80) train acc: 0.083000; val_acc: 0.106000\n",
      "(Iteration 36801 / 61200) loss: 2.308293\n",
      "(Iteration 36901 / 61200) loss: 2.308291\n",
      "(Iteration 37001 / 61200) loss: 2.308276\n",
      "(Iteration 37101 / 61200) loss: 2.308307\n",
      "(Iteration 37201 / 61200) loss: 2.308281\n",
      "(Iteration 37301 / 61200) loss: 2.308286\n",
      "(Iteration 37401 / 61200) loss: 2.308294\n",
      "(Epoch 49 / 80) train acc: 0.085000; val_acc: 0.106000\n",
      "(Iteration 37501 / 61200) loss: 2.308281\n",
      "(Iteration 37601 / 61200) loss: 2.308282\n",
      "(Iteration 37701 / 61200) loss: 2.308294\n",
      "(Iteration 37801 / 61200) loss: 2.308287\n",
      "(Iteration 37901 / 61200) loss: 2.308280\n",
      "(Iteration 38001 / 61200) loss: 2.308287\n",
      "(Iteration 38101 / 61200) loss: 2.308300\n",
      "(Iteration 38201 / 61200) loss: 2.308285\n",
      "(Epoch 50 / 80) train acc: 0.088000; val_acc: 0.106000\n",
      "(Iteration 38301 / 61200) loss: 2.308320\n",
      "(Iteration 38401 / 61200) loss: 2.308302\n",
      "(Iteration 38501 / 61200) loss: 2.308285\n",
      "(Iteration 38601 / 61200) loss: 2.308297\n",
      "(Iteration 38701 / 61200) loss: 2.308305\n",
      "(Iteration 38801 / 61200) loss: 2.308286\n",
      "(Iteration 38901 / 61200) loss: 2.308316\n",
      "(Iteration 39001 / 61200) loss: 2.308292\n",
      "(Epoch 51 / 80) train acc: 0.111000; val_acc: 0.106000\n",
      "(Iteration 39101 / 61200) loss: 2.308292\n",
      "(Iteration 39201 / 61200) loss: 2.308308\n",
      "(Iteration 39301 / 61200) loss: 2.308282\n",
      "(Iteration 39401 / 61200) loss: 2.308280\n",
      "(Iteration 39501 / 61200) loss: 2.308299\n",
      "(Iteration 39601 / 61200) loss: 2.308292\n",
      "(Iteration 39701 / 61200) loss: 2.308268\n",
      "(Epoch 52 / 80) train acc: 0.082000; val_acc: 0.106000\n",
      "(Iteration 39801 / 61200) loss: 2.308274\n",
      "(Iteration 39901 / 61200) loss: 2.308284\n",
      "(Iteration 40001 / 61200) loss: 2.308289\n",
      "(Iteration 40101 / 61200) loss: 2.308285\n",
      "(Iteration 40201 / 61200) loss: 2.308270\n",
      "(Iteration 40301 / 61200) loss: 2.308304\n",
      "(Iteration 40401 / 61200) loss: 2.308286\n",
      "(Iteration 40501 / 61200) loss: 2.308289\n",
      "(Epoch 53 / 80) train acc: 0.079000; val_acc: 0.106000\n",
      "(Iteration 40601 / 61200) loss: 2.308302\n",
      "(Iteration 40701 / 61200) loss: 2.308294\n",
      "(Iteration 40801 / 61200) loss: 2.308274\n",
      "(Iteration 40901 / 61200) loss: 2.308282\n",
      "(Iteration 41001 / 61200) loss: 2.308302\n",
      "(Iteration 41101 / 61200) loss: 2.308274\n",
      "(Iteration 41201 / 61200) loss: 2.308290\n",
      "(Iteration 41301 / 61200) loss: 2.308305\n",
      "(Epoch 54 / 80) train acc: 0.100000; val_acc: 0.106000\n",
      "(Iteration 41401 / 61200) loss: 2.308294\n",
      "(Iteration 41501 / 61200) loss: 2.308285\n",
      "(Iteration 41601 / 61200) loss: 2.308297\n",
      "(Iteration 41701 / 61200) loss: 2.308281\n",
      "(Iteration 41801 / 61200) loss: 2.308291\n",
      "(Iteration 41901 / 61200) loss: 2.308287\n",
      "(Iteration 42001 / 61200) loss: 2.308313\n",
      "(Epoch 55 / 80) train acc: 0.085000; val_acc: 0.106000\n",
      "(Iteration 42101 / 61200) loss: 2.308292\n",
      "(Iteration 42201 / 61200) loss: 2.308282\n",
      "(Iteration 42301 / 61200) loss: 2.308291\n",
      "(Iteration 42401 / 61200) loss: 2.308282\n",
      "(Iteration 42501 / 61200) loss: 2.308301\n",
      "(Iteration 42601 / 61200) loss: 2.308320\n",
      "(Iteration 42701 / 61200) loss: 2.308277\n",
      "(Iteration 42801 / 61200) loss: 2.308276\n",
      "(Epoch 56 / 80) train acc: 0.084000; val_acc: 0.106000\n",
      "(Iteration 42901 / 61200) loss: 2.308291\n",
      "(Iteration 43001 / 61200) loss: 2.308274\n",
      "(Iteration 43101 / 61200) loss: 2.308285\n",
      "(Iteration 43201 / 61200) loss: 2.308294\n",
      "(Iteration 43301 / 61200) loss: 2.308296\n",
      "(Iteration 43401 / 61200) loss: 2.308274\n",
      "(Iteration 43501 / 61200) loss: 2.308297\n",
      "(Iteration 43601 / 61200) loss: 2.308291\n",
      "(Epoch 57 / 80) train acc: 0.080000; val_acc: 0.106000\n",
      "(Iteration 43701 / 61200) loss: 2.308290\n",
      "(Iteration 43801 / 61200) loss: 2.308309\n",
      "(Iteration 43901 / 61200) loss: 2.308278\n",
      "(Iteration 44001 / 61200) loss: 2.308297\n",
      "(Iteration 44101 / 61200) loss: 2.308308\n",
      "(Iteration 44201 / 61200) loss: 2.308300\n",
      "(Iteration 44301 / 61200) loss: 2.308280\n",
      "(Epoch 58 / 80) train acc: 0.095000; val_acc: 0.106000\n",
      "(Iteration 44401 / 61200) loss: 2.308290\n",
      "(Iteration 44501 / 61200) loss: 2.308293\n",
      "(Iteration 44601 / 61200) loss: 2.308291\n",
      "(Iteration 44701 / 61200) loss: 2.308285\n",
      "(Iteration 44801 / 61200) loss: 2.308297\n",
      "(Iteration 44901 / 61200) loss: 2.308283\n",
      "(Iteration 45001 / 61200) loss: 2.308288\n",
      "(Iteration 45101 / 61200) loss: 2.308283\n",
      "(Epoch 59 / 80) train acc: 0.081000; val_acc: 0.106000\n",
      "(Iteration 45201 / 61200) loss: 2.308274\n",
      "(Iteration 45301 / 61200) loss: 2.308305\n",
      "(Iteration 45401 / 61200) loss: 2.308290\n",
      "(Iteration 45501 / 61200) loss: 2.308296\n",
      "(Iteration 45601 / 61200) loss: 2.308293\n",
      "(Iteration 45701 / 61200) loss: 2.308289\n",
      "(Iteration 45801 / 61200) loss: 2.308292\n",
      "(Epoch 60 / 80) train acc: 0.092000; val_acc: 0.106000\n",
      "(Iteration 45901 / 61200) loss: 2.308286\n",
      "(Iteration 46001 / 61200) loss: 2.308295\n",
      "(Iteration 46101 / 61200) loss: 2.308288\n",
      "(Iteration 46201 / 61200) loss: 2.308281\n",
      "(Iteration 46301 / 61200) loss: 2.308300\n",
      "(Iteration 46401 / 61200) loss: 2.308286\n",
      "(Iteration 46501 / 61200) loss: 2.308286\n",
      "(Iteration 46601 / 61200) loss: 2.308277\n",
      "(Epoch 61 / 80) train acc: 0.081000; val_acc: 0.106000\n",
      "(Iteration 46701 / 61200) loss: 2.308274\n",
      "(Iteration 46801 / 61200) loss: 2.308280\n",
      "(Iteration 46901 / 61200) loss: 2.308294\n",
      "(Iteration 47001 / 61200) loss: 2.308302\n",
      "(Iteration 47101 / 61200) loss: 2.308272\n",
      "(Iteration 47201 / 61200) loss: 2.308292\n",
      "(Iteration 47301 / 61200) loss: 2.308299\n",
      "(Iteration 47401 / 61200) loss: 2.308292\n",
      "(Epoch 62 / 80) train acc: 0.090000; val_acc: 0.106000\n",
      "(Iteration 47501 / 61200) loss: 2.308304\n",
      "(Iteration 47601 / 61200) loss: 2.308291\n",
      "(Iteration 47701 / 61200) loss: 2.308279\n",
      "(Iteration 47801 / 61200) loss: 2.308284\n",
      "(Iteration 47901 / 61200) loss: 2.308287\n",
      "(Iteration 48001 / 61200) loss: 2.308272\n",
      "(Iteration 48101 / 61200) loss: 2.308293\n",
      "(Epoch 63 / 80) train acc: 0.098000; val_acc: 0.106000\n",
      "(Iteration 48201 / 61200) loss: 2.308272\n",
      "(Iteration 48301 / 61200) loss: 2.308289\n",
      "(Iteration 48401 / 61200) loss: 2.308283\n",
      "(Iteration 48501 / 61200) loss: 2.308293\n",
      "(Iteration 48601 / 61200) loss: 2.308290\n",
      "(Iteration 48701 / 61200) loss: 2.308294\n",
      "(Iteration 48801 / 61200) loss: 2.308287\n",
      "(Iteration 48901 / 61200) loss: 2.308289\n",
      "(Epoch 64 / 80) train acc: 0.087000; val_acc: 0.106000\n",
      "(Iteration 49001 / 61200) loss: 2.308283\n",
      "(Iteration 49101 / 61200) loss: 2.308299\n",
      "(Iteration 49201 / 61200) loss: 2.308285\n",
      "(Iteration 49301 / 61200) loss: 2.308296\n",
      "(Iteration 49401 / 61200) loss: 2.308291\n",
      "(Iteration 49501 / 61200) loss: 2.308272\n",
      "(Iteration 49601 / 61200) loss: 2.308289\n",
      "(Iteration 49701 / 61200) loss: 2.308288\n",
      "(Epoch 65 / 80) train acc: 0.092000; val_acc: 0.106000\n",
      "(Iteration 49801 / 61200) loss: 2.308287\n",
      "(Iteration 49901 / 61200) loss: 2.308288\n",
      "(Iteration 50001 / 61200) loss: 2.308274\n",
      "(Iteration 50101 / 61200) loss: 2.308303\n",
      "(Iteration 50201 / 61200) loss: 2.308294\n",
      "(Iteration 50301 / 61200) loss: 2.308291\n",
      "(Iteration 50401 / 61200) loss: 2.308314\n",
      "(Epoch 66 / 80) train acc: 0.096000; val_acc: 0.106000\n",
      "(Iteration 50501 / 61200) loss: 2.308281\n",
      "(Iteration 50601 / 61200) loss: 2.308291\n",
      "(Iteration 50701 / 61200) loss: 2.308300\n",
      "(Iteration 50801 / 61200) loss: 2.308271\n",
      "(Iteration 50901 / 61200) loss: 2.308297\n",
      "(Iteration 51001 / 61200) loss: 2.308281\n",
      "(Iteration 51101 / 61200) loss: 2.308282\n",
      "(Iteration 51201 / 61200) loss: 2.308276\n",
      "(Epoch 67 / 80) train acc: 0.088000; val_acc: 0.106000\n",
      "(Iteration 51301 / 61200) loss: 2.308269\n",
      "(Iteration 51401 / 61200) loss: 2.308267\n",
      "(Iteration 51501 / 61200) loss: 2.308290\n",
      "(Iteration 51601 / 61200) loss: 2.308285\n",
      "(Iteration 51701 / 61200) loss: 2.308276\n",
      "(Iteration 51801 / 61200) loss: 2.308289\n",
      "(Iteration 51901 / 61200) loss: 2.308287\n",
      "(Iteration 52001 / 61200) loss: 2.308291\n",
      "(Epoch 68 / 80) train acc: 0.082000; val_acc: 0.106000\n",
      "(Iteration 52101 / 61200) loss: 2.308318\n",
      "(Iteration 52201 / 61200) loss: 2.308299\n",
      "(Iteration 52301 / 61200) loss: 2.308293\n",
      "(Iteration 52401 / 61200) loss: 2.308288\n",
      "(Iteration 52501 / 61200) loss: 2.308298\n",
      "(Iteration 52601 / 61200) loss: 2.308281\n",
      "(Iteration 52701 / 61200) loss: 2.308291\n",
      "(Epoch 69 / 80) train acc: 0.091000; val_acc: 0.106000\n",
      "(Iteration 52801 / 61200) loss: 2.308272\n",
      "(Iteration 52901 / 61200) loss: 2.308282\n",
      "(Iteration 53001 / 61200) loss: 2.308284\n",
      "(Iteration 53101 / 61200) loss: 2.308291\n",
      "(Iteration 53201 / 61200) loss: 2.308290\n",
      "(Iteration 53301 / 61200) loss: 2.308291\n",
      "(Iteration 53401 / 61200) loss: 2.308283\n",
      "(Iteration 53501 / 61200) loss: 2.308300\n",
      "(Epoch 70 / 80) train acc: 0.096000; val_acc: 0.106000\n",
      "(Iteration 53601 / 61200) loss: 2.308288\n",
      "(Iteration 53701 / 61200) loss: 2.308271\n",
      "(Iteration 53801 / 61200) loss: 2.308287\n",
      "(Iteration 53901 / 61200) loss: 2.308287\n",
      "(Iteration 54001 / 61200) loss: 2.308287\n",
      "(Iteration 54101 / 61200) loss: 2.308288\n",
      "(Iteration 54201 / 61200) loss: 2.308289\n",
      "(Iteration 54301 / 61200) loss: 2.308291\n",
      "(Epoch 71 / 80) train acc: 0.088000; val_acc: 0.106000\n",
      "(Iteration 54401 / 61200) loss: 2.308282\n",
      "(Iteration 54501 / 61200) loss: 2.308305\n",
      "(Iteration 54601 / 61200) loss: 2.308278\n",
      "(Iteration 54701 / 61200) loss: 2.308288\n",
      "(Iteration 54801 / 61200) loss: 2.308305\n",
      "(Iteration 54901 / 61200) loss: 2.308283\n",
      "(Iteration 55001 / 61200) loss: 2.308288\n",
      "(Epoch 72 / 80) train acc: 0.082000; val_acc: 0.106000\n",
      "(Iteration 55101 / 61200) loss: 2.308271\n",
      "(Iteration 55201 / 61200) loss: 2.308292\n",
      "(Iteration 55301 / 61200) loss: 2.308300\n",
      "(Iteration 55401 / 61200) loss: 2.308282\n",
      "(Iteration 55501 / 61200) loss: 2.308268\n",
      "(Iteration 55601 / 61200) loss: 2.308288\n",
      "(Iteration 55701 / 61200) loss: 2.308275\n",
      "(Iteration 55801 / 61200) loss: 2.308268\n",
      "(Epoch 73 / 80) train acc: 0.091000; val_acc: 0.106000\n",
      "(Iteration 55901 / 61200) loss: 2.308287\n",
      "(Iteration 56001 / 61200) loss: 2.308288\n",
      "(Iteration 56101 / 61200) loss: 2.308292\n",
      "(Iteration 56201 / 61200) loss: 2.308282\n",
      "(Iteration 56301 / 61200) loss: 2.308299\n",
      "(Iteration 56401 / 61200) loss: 2.308255\n",
      "(Iteration 56501 / 61200) loss: 2.308282\n",
      "(Iteration 56601 / 61200) loss: 2.308283\n",
      "(Epoch 74 / 80) train acc: 0.078000; val_acc: 0.106000\n",
      "(Iteration 56701 / 61200) loss: 2.308287\n",
      "(Iteration 56801 / 61200) loss: 2.308297\n",
      "(Iteration 56901 / 61200) loss: 2.308266\n",
      "(Iteration 57001 / 61200) loss: 2.308291\n",
      "(Iteration 57101 / 61200) loss: 2.308284\n",
      "(Iteration 57201 / 61200) loss: 2.308292\n",
      "(Iteration 57301 / 61200) loss: 2.308289\n",
      "(Epoch 75 / 80) train acc: 0.084000; val_acc: 0.106000\n",
      "(Iteration 57401 / 61200) loss: 2.308284\n",
      "(Iteration 57501 / 61200) loss: 2.308292\n",
      "(Iteration 57601 / 61200) loss: 2.308272\n",
      "(Iteration 57701 / 61200) loss: 2.308277\n",
      "(Iteration 57801 / 61200) loss: 2.308279\n",
      "(Iteration 57901 / 61200) loss: 2.308295\n",
      "(Iteration 58001 / 61200) loss: 2.308278\n",
      "(Iteration 58101 / 61200) loss: 2.308276\n",
      "(Epoch 76 / 80) train acc: 0.081000; val_acc: 0.106000\n",
      "(Iteration 58201 / 61200) loss: 2.308286\n",
      "(Iteration 58301 / 61200) loss: 2.308283\n",
      "(Iteration 58401 / 61200) loss: 2.308276\n",
      "(Iteration 58501 / 61200) loss: 2.308273\n",
      "(Iteration 58601 / 61200) loss: 2.308272\n",
      "(Iteration 58701 / 61200) loss: 2.308295\n",
      "(Iteration 58801 / 61200) loss: 2.308281\n",
      "(Iteration 58901 / 61200) loss: 2.308286\n",
      "(Epoch 77 / 80) train acc: 0.084000; val_acc: 0.106000\n",
      "(Iteration 59001 / 61200) loss: 2.308282\n",
      "(Iteration 59101 / 61200) loss: 2.308282\n",
      "(Iteration 59201 / 61200) loss: 2.308283\n",
      "(Iteration 59301 / 61200) loss: 2.308286\n",
      "(Iteration 59401 / 61200) loss: 2.308282\n",
      "(Iteration 59501 / 61200) loss: 2.308277\n",
      "(Iteration 59601 / 61200) loss: 2.308275\n",
      "(Epoch 78 / 80) train acc: 0.083000; val_acc: 0.106000\n",
      "(Iteration 59701 / 61200) loss: 2.308294\n",
      "(Iteration 59801 / 61200) loss: 2.308266\n",
      "(Iteration 59901 / 61200) loss: 2.308287\n",
      "(Iteration 60001 / 61200) loss: 2.308290\n",
      "(Iteration 60101 / 61200) loss: 2.308273\n",
      "(Iteration 60201 / 61200) loss: 2.308288\n",
      "(Iteration 60301 / 61200) loss: 2.308291\n",
      "(Iteration 60401 / 61200) loss: 2.308294\n",
      "(Epoch 79 / 80) train acc: 0.083000; val_acc: 0.106000\n",
      "(Iteration 60501 / 61200) loss: 2.308282\n",
      "(Iteration 60601 / 61200) loss: 2.308303\n",
      "(Iteration 60701 / 61200) loss: 2.308273\n",
      "(Iteration 60801 / 61200) loss: 2.308292\n",
      "(Iteration 60901 / 61200) loss: 2.308293\n",
      "(Iteration 61001 / 61200) loss: 2.308287\n",
      "(Iteration 61101 / 61200) loss: 2.308285\n",
      "(Epoch 80 / 80) train acc: 0.099000; val_acc: 0.106000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.308346\n",
      "(Epoch 0 / 80) train acc: 0.121000; val_acc: 0.116000\n",
      "(Iteration 101 / 30560) loss: 2.308342\n",
      "(Iteration 201 / 30560) loss: 2.308339\n",
      "(Iteration 301 / 30560) loss: 2.308327\n",
      "(Epoch 1 / 80) train acc: 0.120000; val_acc: 0.116000\n",
      "(Iteration 401 / 30560) loss: 2.308350\n",
      "(Iteration 501 / 30560) loss: 2.308331\n",
      "(Iteration 601 / 30560) loss: 2.308342\n",
      "(Iteration 701 / 30560) loss: 2.308338\n",
      "(Epoch 2 / 80) train acc: 0.103000; val_acc: 0.116000\n",
      "(Iteration 801 / 30560) loss: 2.308335\n",
      "(Iteration 901 / 30560) loss: 2.308329\n",
      "(Iteration 1001 / 30560) loss: 2.308341\n",
      "(Iteration 1101 / 30560) loss: 2.308331\n",
      "(Epoch 3 / 80) train acc: 0.091000; val_acc: 0.116000\n",
      "(Iteration 1201 / 30560) loss: 2.308334\n",
      "(Iteration 1301 / 30560) loss: 2.308342\n",
      "(Iteration 1401 / 30560) loss: 2.308340\n",
      "(Iteration 1501 / 30560) loss: 2.308343\n",
      "(Epoch 4 / 80) train acc: 0.092000; val_acc: 0.116000\n",
      "(Iteration 1601 / 30560) loss: 2.308333\n",
      "(Iteration 1701 / 30560) loss: 2.308329\n",
      "(Iteration 1801 / 30560) loss: 2.308348\n",
      "(Iteration 1901 / 30560) loss: 2.308346\n",
      "(Epoch 5 / 80) train acc: 0.091000; val_acc: 0.116000\n",
      "(Iteration 2001 / 30560) loss: 2.308338\n",
      "(Iteration 2101 / 30560) loss: 2.308349\n",
      "(Iteration 2201 / 30560) loss: 2.308335\n",
      "(Epoch 6 / 80) train acc: 0.115000; val_acc: 0.116000\n",
      "(Iteration 2301 / 30560) loss: 2.308339\n",
      "(Iteration 2401 / 30560) loss: 2.308334\n",
      "(Iteration 2501 / 30560) loss: 2.308334\n",
      "(Iteration 2601 / 30560) loss: 2.308349\n",
      "(Epoch 7 / 80) train acc: 0.120000; val_acc: 0.116000\n",
      "(Iteration 2701 / 30560) loss: 2.308339\n",
      "(Iteration 2801 / 30560) loss: 2.308353\n",
      "(Iteration 2901 / 30560) loss: 2.308336\n",
      "(Iteration 3001 / 30560) loss: 2.308345\n",
      "(Epoch 8 / 80) train acc: 0.096000; val_acc: 0.116000\n",
      "(Iteration 3101 / 30560) loss: 2.308334\n",
      "(Iteration 3201 / 30560) loss: 2.308337\n",
      "(Iteration 3301 / 30560) loss: 2.308332\n",
      "(Iteration 3401 / 30560) loss: 2.308328\n",
      "(Epoch 9 / 80) train acc: 0.105000; val_acc: 0.116000\n",
      "(Iteration 3501 / 30560) loss: 2.308359\n",
      "(Iteration 3601 / 30560) loss: 2.308335\n",
      "(Iteration 3701 / 30560) loss: 2.308332\n",
      "(Iteration 3801 / 30560) loss: 2.308329\n",
      "(Epoch 10 / 80) train acc: 0.100000; val_acc: 0.116000\n",
      "(Iteration 3901 / 30560) loss: 2.308351\n",
      "(Iteration 4001 / 30560) loss: 2.308330\n",
      "(Iteration 4101 / 30560) loss: 2.308334\n",
      "(Iteration 4201 / 30560) loss: 2.308329\n",
      "(Epoch 11 / 80) train acc: 0.093000; val_acc: 0.116000\n",
      "(Iteration 4301 / 30560) loss: 2.308329\n",
      "(Iteration 4401 / 30560) loss: 2.308340\n",
      "(Iteration 4501 / 30560) loss: 2.308335\n",
      "(Epoch 12 / 80) train acc: 0.075000; val_acc: 0.116000\n",
      "(Iteration 4601 / 30560) loss: 2.308334\n",
      "(Iteration 4701 / 30560) loss: 2.308345\n",
      "(Iteration 4801 / 30560) loss: 2.308342\n",
      "(Iteration 4901 / 30560) loss: 2.308345\n",
      "(Epoch 13 / 80) train acc: 0.098000; val_acc: 0.116000\n",
      "(Iteration 5001 / 30560) loss: 2.308342\n",
      "(Iteration 5101 / 30560) loss: 2.308350\n",
      "(Iteration 5201 / 30560) loss: 2.308328\n",
      "(Iteration 5301 / 30560) loss: 2.308342\n",
      "(Epoch 14 / 80) train acc: 0.094000; val_acc: 0.116000\n",
      "(Iteration 5401 / 30560) loss: 2.308328\n",
      "(Iteration 5501 / 30560) loss: 2.308344\n",
      "(Iteration 5601 / 30560) loss: 2.308341\n",
      "(Iteration 5701 / 30560) loss: 2.308334\n",
      "(Epoch 15 / 80) train acc: 0.106000; val_acc: 0.116000\n",
      "(Iteration 5801 / 30560) loss: 2.308334\n",
      "(Iteration 5901 / 30560) loss: 2.308328\n",
      "(Iteration 6001 / 30560) loss: 2.308332\n",
      "(Iteration 6101 / 30560) loss: 2.308334\n",
      "(Epoch 16 / 80) train acc: 0.097000; val_acc: 0.116000\n",
      "(Iteration 6201 / 30560) loss: 2.308332\n",
      "(Iteration 6301 / 30560) loss: 2.308327\n",
      "(Iteration 6401 / 30560) loss: 2.308339\n",
      "(Epoch 17 / 80) train acc: 0.103000; val_acc: 0.116000\n",
      "(Iteration 6501 / 30560) loss: 2.308342\n",
      "(Iteration 6601 / 30560) loss: 2.308325\n",
      "(Iteration 6701 / 30560) loss: 2.308341\n",
      "(Iteration 6801 / 30560) loss: 2.308320\n",
      "(Epoch 18 / 80) train acc: 0.096000; val_acc: 0.116000\n",
      "(Iteration 6901 / 30560) loss: 2.308329\n",
      "(Iteration 7001 / 30560) loss: 2.308345\n",
      "(Iteration 7101 / 30560) loss: 2.308333\n",
      "(Iteration 7201 / 30560) loss: 2.308336\n",
      "(Epoch 19 / 80) train acc: 0.118000; val_acc: 0.116000\n",
      "(Iteration 7301 / 30560) loss: 2.308326\n",
      "(Iteration 7401 / 30560) loss: 2.308338\n",
      "(Iteration 7501 / 30560) loss: 2.308339\n",
      "(Iteration 7601 / 30560) loss: 2.308338\n",
      "(Epoch 20 / 80) train acc: 0.094000; val_acc: 0.116000\n",
      "(Iteration 7701 / 30560) loss: 2.308336\n",
      "(Iteration 7801 / 30560) loss: 2.308329\n",
      "(Iteration 7901 / 30560) loss: 2.308318\n",
      "(Iteration 8001 / 30560) loss: 2.308346\n",
      "(Epoch 21 / 80) train acc: 0.100000; val_acc: 0.116000\n",
      "(Iteration 8101 / 30560) loss: 2.308335\n",
      "(Iteration 8201 / 30560) loss: 2.308341\n",
      "(Iteration 8301 / 30560) loss: 2.308340\n",
      "(Iteration 8401 / 30560) loss: 2.308339\n",
      "(Epoch 22 / 80) train acc: 0.100000; val_acc: 0.116000\n",
      "(Iteration 8501 / 30560) loss: 2.308336\n",
      "(Iteration 8601 / 30560) loss: 2.308334\n",
      "(Iteration 8701 / 30560) loss: 2.308337\n",
      "(Epoch 23 / 80) train acc: 0.109000; val_acc: 0.116000\n",
      "(Iteration 8801 / 30560) loss: 2.308339\n",
      "(Iteration 8901 / 30560) loss: 2.308340\n",
      "(Iteration 9001 / 30560) loss: 2.308335\n",
      "(Iteration 9101 / 30560) loss: 2.308327\n",
      "(Epoch 24 / 80) train acc: 0.111000; val_acc: 0.116000\n",
      "(Iteration 9201 / 30560) loss: 2.308333\n",
      "(Iteration 9301 / 30560) loss: 2.308334\n",
      "(Iteration 9401 / 30560) loss: 2.308346\n",
      "(Iteration 9501 / 30560) loss: 2.308336\n",
      "(Epoch 25 / 80) train acc: 0.096000; val_acc: 0.116000\n",
      "(Iteration 9601 / 30560) loss: 2.308338\n",
      "(Iteration 9701 / 30560) loss: 2.308341\n",
      "(Iteration 9801 / 30560) loss: 2.308345\n",
      "(Iteration 9901 / 30560) loss: 2.308327\n",
      "(Epoch 26 / 80) train acc: 0.083000; val_acc: 0.116000\n",
      "(Iteration 10001 / 30560) loss: 2.308350\n",
      "(Iteration 10101 / 30560) loss: 2.308339\n",
      "(Iteration 10201 / 30560) loss: 2.308339\n",
      "(Iteration 10301 / 30560) loss: 2.308341\n",
      "(Epoch 27 / 80) train acc: 0.105000; val_acc: 0.116000\n",
      "(Iteration 10401 / 30560) loss: 2.308335\n",
      "(Iteration 10501 / 30560) loss: 2.308340\n",
      "(Iteration 10601 / 30560) loss: 2.308337\n",
      "(Epoch 28 / 80) train acc: 0.098000; val_acc: 0.116000\n",
      "(Iteration 10701 / 30560) loss: 2.308350\n",
      "(Iteration 10801 / 30560) loss: 2.308338\n",
      "(Iteration 10901 / 30560) loss: 2.308348\n",
      "(Iteration 11001 / 30560) loss: 2.308331\n",
      "(Epoch 29 / 80) train acc: 0.099000; val_acc: 0.116000\n",
      "(Iteration 11101 / 30560) loss: 2.308350\n",
      "(Iteration 11201 / 30560) loss: 2.308337\n",
      "(Iteration 11301 / 30560) loss: 2.308338\n",
      "(Iteration 11401 / 30560) loss: 2.308353\n",
      "(Epoch 30 / 80) train acc: 0.099000; val_acc: 0.116000\n",
      "(Iteration 11501 / 30560) loss: 2.308337\n",
      "(Iteration 11601 / 30560) loss: 2.308339\n",
      "(Iteration 11701 / 30560) loss: 2.308334\n",
      "(Iteration 11801 / 30560) loss: 2.308329\n",
      "(Epoch 31 / 80) train acc: 0.097000; val_acc: 0.116000\n",
      "(Iteration 11901 / 30560) loss: 2.308331\n",
      "(Iteration 12001 / 30560) loss: 2.308334\n",
      "(Iteration 12101 / 30560) loss: 2.308354\n",
      "(Iteration 12201 / 30560) loss: 2.308338\n",
      "(Epoch 32 / 80) train acc: 0.105000; val_acc: 0.116000\n",
      "(Iteration 12301 / 30560) loss: 2.308336\n",
      "(Iteration 12401 / 30560) loss: 2.308351\n",
      "(Iteration 12501 / 30560) loss: 2.308331\n",
      "(Iteration 12601 / 30560) loss: 2.308336\n",
      "(Epoch 33 / 80) train acc: 0.093000; val_acc: 0.116000\n",
      "(Iteration 12701 / 30560) loss: 2.308326\n",
      "(Iteration 12801 / 30560) loss: 2.308343\n",
      "(Iteration 12901 / 30560) loss: 2.308342\n",
      "(Epoch 34 / 80) train acc: 0.109000; val_acc: 0.116000\n",
      "(Iteration 13001 / 30560) loss: 2.308321\n",
      "(Iteration 13101 / 30560) loss: 2.308340\n",
      "(Iteration 13201 / 30560) loss: 2.308333\n",
      "(Iteration 13301 / 30560) loss: 2.308334\n",
      "(Epoch 35 / 80) train acc: 0.093000; val_acc: 0.116000\n",
      "(Iteration 13401 / 30560) loss: 2.308320\n",
      "(Iteration 13501 / 30560) loss: 2.308357\n",
      "(Iteration 13601 / 30560) loss: 2.308338\n",
      "(Iteration 13701 / 30560) loss: 2.308325\n",
      "(Epoch 36 / 80) train acc: 0.099000; val_acc: 0.116000\n",
      "(Iteration 13801 / 30560) loss: 2.308346\n",
      "(Iteration 13901 / 30560) loss: 2.308329\n",
      "(Iteration 14001 / 30560) loss: 2.308332\n",
      "(Iteration 14101 / 30560) loss: 2.308336\n",
      "(Epoch 37 / 80) train acc: 0.108000; val_acc: 0.116000\n",
      "(Iteration 14201 / 30560) loss: 2.308337\n",
      "(Iteration 14301 / 30560) loss: 2.308341\n",
      "(Iteration 14401 / 30560) loss: 2.308338\n",
      "(Iteration 14501 / 30560) loss: 2.308338\n",
      "(Epoch 38 / 80) train acc: 0.105000; val_acc: 0.116000\n",
      "(Iteration 14601 / 30560) loss: 2.308347\n",
      "(Iteration 14701 / 30560) loss: 2.308334\n",
      "(Iteration 14801 / 30560) loss: 2.308329\n",
      "(Epoch 39 / 80) train acc: 0.108000; val_acc: 0.116000\n",
      "(Iteration 14901 / 30560) loss: 2.308324\n",
      "(Iteration 15001 / 30560) loss: 2.308315\n",
      "(Iteration 15101 / 30560) loss: 2.308340\n",
      "(Iteration 15201 / 30560) loss: 2.308329\n",
      "(Epoch 40 / 80) train acc: 0.097000; val_acc: 0.116000\n",
      "(Iteration 15301 / 30560) loss: 2.308334\n",
      "(Iteration 15401 / 30560) loss: 2.308333\n",
      "(Iteration 15501 / 30560) loss: 2.308331\n",
      "(Iteration 15601 / 30560) loss: 2.308342\n",
      "(Epoch 41 / 80) train acc: 0.110000; val_acc: 0.116000\n",
      "(Iteration 15701 / 30560) loss: 2.308336\n",
      "(Iteration 15801 / 30560) loss: 2.308345\n",
      "(Iteration 15901 / 30560) loss: 2.308337\n",
      "(Iteration 16001 / 30560) loss: 2.308334\n",
      "(Epoch 42 / 80) train acc: 0.106000; val_acc: 0.116000\n",
      "(Iteration 16101 / 30560) loss: 2.308332\n",
      "(Iteration 16201 / 30560) loss: 2.308334\n",
      "(Iteration 16301 / 30560) loss: 2.308330\n",
      "(Iteration 16401 / 30560) loss: 2.308330\n",
      "(Epoch 43 / 80) train acc: 0.097000; val_acc: 0.116000\n",
      "(Iteration 16501 / 30560) loss: 2.308344\n",
      "(Iteration 16601 / 30560) loss: 2.308345\n",
      "(Iteration 16701 / 30560) loss: 2.308323\n",
      "(Iteration 16801 / 30560) loss: 2.308331\n",
      "(Epoch 44 / 80) train acc: 0.092000; val_acc: 0.116000\n",
      "(Iteration 16901 / 30560) loss: 2.308337\n",
      "(Iteration 17001 / 30560) loss: 2.308332\n",
      "(Iteration 17101 / 30560) loss: 2.308331\n",
      "(Epoch 45 / 80) train acc: 0.104000; val_acc: 0.116000\n",
      "(Iteration 17201 / 30560) loss: 2.308337\n",
      "(Iteration 17301 / 30560) loss: 2.308338\n",
      "(Iteration 17401 / 30560) loss: 2.308336\n",
      "(Iteration 17501 / 30560) loss: 2.308322\n",
      "(Epoch 46 / 80) train acc: 0.088000; val_acc: 0.116000\n",
      "(Iteration 17601 / 30560) loss: 2.308337\n",
      "(Iteration 17701 / 30560) loss: 2.308339\n",
      "(Iteration 17801 / 30560) loss: 2.308331\n",
      "(Iteration 17901 / 30560) loss: 2.308324\n",
      "(Epoch 47 / 80) train acc: 0.100000; val_acc: 0.116000\n",
      "(Iteration 18001 / 30560) loss: 2.308346\n",
      "(Iteration 18101 / 30560) loss: 2.308336\n",
      "(Iteration 18201 / 30560) loss: 2.308344\n",
      "(Iteration 18301 / 30560) loss: 2.308336\n",
      "(Epoch 48 / 80) train acc: 0.115000; val_acc: 0.116000\n",
      "(Iteration 18401 / 30560) loss: 2.308333\n",
      "(Iteration 18501 / 30560) loss: 2.308331\n",
      "(Iteration 18601 / 30560) loss: 2.308330\n",
      "(Iteration 18701 / 30560) loss: 2.308337\n",
      "(Epoch 49 / 80) train acc: 0.099000; val_acc: 0.116000\n",
      "(Iteration 18801 / 30560) loss: 2.308352\n",
      "(Iteration 18901 / 30560) loss: 2.308330\n",
      "(Iteration 19001 / 30560) loss: 2.308322\n",
      "(Epoch 50 / 80) train acc: 0.095000; val_acc: 0.116000\n",
      "(Iteration 19101 / 30560) loss: 2.308342\n",
      "(Iteration 19201 / 30560) loss: 2.308343\n",
      "(Iteration 19301 / 30560) loss: 2.308328\n",
      "(Iteration 19401 / 30560) loss: 2.308322\n",
      "(Epoch 51 / 80) train acc: 0.097000; val_acc: 0.116000\n",
      "(Iteration 19501 / 30560) loss: 2.308333\n",
      "(Iteration 19601 / 30560) loss: 2.308349\n",
      "(Iteration 19701 / 30560) loss: 2.308333\n",
      "(Iteration 19801 / 30560) loss: 2.308335\n",
      "(Epoch 52 / 80) train acc: 0.088000; val_acc: 0.116000\n",
      "(Iteration 19901 / 30560) loss: 2.308331\n",
      "(Iteration 20001 / 30560) loss: 2.308324\n",
      "(Iteration 20101 / 30560) loss: 2.308335\n",
      "(Iteration 20201 / 30560) loss: 2.308341\n",
      "(Epoch 53 / 80) train acc: 0.102000; val_acc: 0.116000\n",
      "(Iteration 20301 / 30560) loss: 2.308343\n",
      "(Iteration 20401 / 30560) loss: 2.308337\n",
      "(Iteration 20501 / 30560) loss: 2.308322\n",
      "(Iteration 20601 / 30560) loss: 2.308332\n",
      "(Epoch 54 / 80) train acc: 0.102000; val_acc: 0.116000\n",
      "(Iteration 20701 / 30560) loss: 2.308330\n",
      "(Iteration 20801 / 30560) loss: 2.308332\n",
      "(Iteration 20901 / 30560) loss: 2.308332\n",
      "(Iteration 21001 / 30560) loss: 2.308327\n",
      "(Epoch 55 / 80) train acc: 0.108000; val_acc: 0.116000\n",
      "(Iteration 21101 / 30560) loss: 2.308343\n",
      "(Iteration 21201 / 30560) loss: 2.308327\n",
      "(Iteration 21301 / 30560) loss: 2.308342\n",
      "(Epoch 56 / 80) train acc: 0.115000; val_acc: 0.116000\n",
      "(Iteration 21401 / 30560) loss: 2.308341\n",
      "(Iteration 21501 / 30560) loss: 2.308345\n",
      "(Iteration 21601 / 30560) loss: 2.308337\n",
      "(Iteration 21701 / 30560) loss: 2.308339\n",
      "(Epoch 57 / 80) train acc: 0.090000; val_acc: 0.116000\n",
      "(Iteration 21801 / 30560) loss: 2.308327\n",
      "(Iteration 21901 / 30560) loss: 2.308334\n",
      "(Iteration 22001 / 30560) loss: 2.308330\n",
      "(Iteration 22101 / 30560) loss: 2.308330\n",
      "(Epoch 58 / 80) train acc: 0.098000; val_acc: 0.116000\n",
      "(Iteration 22201 / 30560) loss: 2.308334\n",
      "(Iteration 22301 / 30560) loss: 2.308341\n",
      "(Iteration 22401 / 30560) loss: 2.308332\n",
      "(Iteration 22501 / 30560) loss: 2.308331\n",
      "(Epoch 59 / 80) train acc: 0.095000; val_acc: 0.116000\n",
      "(Iteration 22601 / 30560) loss: 2.308334\n",
      "(Iteration 22701 / 30560) loss: 2.308339\n",
      "(Iteration 22801 / 30560) loss: 2.308337\n",
      "(Iteration 22901 / 30560) loss: 2.308336\n",
      "(Epoch 60 / 80) train acc: 0.102000; val_acc: 0.116000\n",
      "(Iteration 23001 / 30560) loss: 2.308327\n",
      "(Iteration 23101 / 30560) loss: 2.308339\n",
      "(Iteration 23201 / 30560) loss: 2.308333\n",
      "(Iteration 23301 / 30560) loss: 2.308342\n",
      "(Epoch 61 / 80) train acc: 0.095000; val_acc: 0.116000\n",
      "(Iteration 23401 / 30560) loss: 2.308344\n",
      "(Iteration 23501 / 30560) loss: 2.308343\n",
      "(Iteration 23601 / 30560) loss: 2.308341\n",
      "(Epoch 62 / 80) train acc: 0.113000; val_acc: 0.116000\n",
      "(Iteration 23701 / 30560) loss: 2.308327\n",
      "(Iteration 23801 / 30560) loss: 2.308334\n",
      "(Iteration 23901 / 30560) loss: 2.308341\n",
      "(Iteration 24001 / 30560) loss: 2.308351\n",
      "(Epoch 63 / 80) train acc: 0.101000; val_acc: 0.116000\n",
      "(Iteration 24101 / 30560) loss: 2.308335\n",
      "(Iteration 24201 / 30560) loss: 2.308340\n",
      "(Iteration 24301 / 30560) loss: 2.308333\n",
      "(Iteration 24401 / 30560) loss: 2.308329\n",
      "(Epoch 64 / 80) train acc: 0.120000; val_acc: 0.116000\n",
      "(Iteration 24501 / 30560) loss: 2.308339\n",
      "(Iteration 24601 / 30560) loss: 2.308324\n",
      "(Iteration 24701 / 30560) loss: 2.308341\n",
      "(Iteration 24801 / 30560) loss: 2.308345\n",
      "(Epoch 65 / 80) train acc: 0.101000; val_acc: 0.116000\n",
      "(Iteration 24901 / 30560) loss: 2.308330\n",
      "(Iteration 25001 / 30560) loss: 2.308329\n",
      "(Iteration 25101 / 30560) loss: 2.308347\n",
      "(Iteration 25201 / 30560) loss: 2.308348\n",
      "(Epoch 66 / 80) train acc: 0.099000; val_acc: 0.116000\n",
      "(Iteration 25301 / 30560) loss: 2.308338\n",
      "(Iteration 25401 / 30560) loss: 2.308334\n",
      "(Iteration 25501 / 30560) loss: 2.308343\n",
      "(Epoch 67 / 80) train acc: 0.120000; val_acc: 0.116000\n",
      "(Iteration 25601 / 30560) loss: 2.308330\n",
      "(Iteration 25701 / 30560) loss: 2.308322\n",
      "(Iteration 25801 / 30560) loss: 2.308340\n",
      "(Iteration 25901 / 30560) loss: 2.308336\n",
      "(Epoch 68 / 80) train acc: 0.101000; val_acc: 0.116000\n",
      "(Iteration 26001 / 30560) loss: 2.308330\n",
      "(Iteration 26101 / 30560) loss: 2.308337\n",
      "(Iteration 26201 / 30560) loss: 2.308322\n",
      "(Iteration 26301 / 30560) loss: 2.308336\n",
      "(Epoch 69 / 80) train acc: 0.093000; val_acc: 0.116000\n",
      "(Iteration 26401 / 30560) loss: 2.308345\n",
      "(Iteration 26501 / 30560) loss: 2.308334\n",
      "(Iteration 26601 / 30560) loss: 2.308340\n",
      "(Iteration 26701 / 30560) loss: 2.308336\n",
      "(Epoch 70 / 80) train acc: 0.119000; val_acc: 0.116000\n",
      "(Iteration 26801 / 30560) loss: 2.308350\n",
      "(Iteration 26901 / 30560) loss: 2.308331\n",
      "(Iteration 27001 / 30560) loss: 2.308337\n",
      "(Iteration 27101 / 30560) loss: 2.308340\n",
      "(Epoch 71 / 80) train acc: 0.108000; val_acc: 0.116000\n",
      "(Iteration 27201 / 30560) loss: 2.308316\n",
      "(Iteration 27301 / 30560) loss: 2.308330\n",
      "(Iteration 27401 / 30560) loss: 2.308341\n",
      "(Iteration 27501 / 30560) loss: 2.308334\n",
      "(Epoch 72 / 80) train acc: 0.098000; val_acc: 0.116000\n",
      "(Iteration 27601 / 30560) loss: 2.308345\n",
      "(Iteration 27701 / 30560) loss: 2.308342\n",
      "(Iteration 27801 / 30560) loss: 2.308324\n",
      "(Epoch 73 / 80) train acc: 0.086000; val_acc: 0.116000\n",
      "(Iteration 27901 / 30560) loss: 2.308340\n",
      "(Iteration 28001 / 30560) loss: 2.308339\n",
      "(Iteration 28101 / 30560) loss: 2.308342\n",
      "(Iteration 28201 / 30560) loss: 2.308338\n",
      "(Epoch 74 / 80) train acc: 0.108000; val_acc: 0.116000\n",
      "(Iteration 28301 / 30560) loss: 2.308340\n",
      "(Iteration 28401 / 30560) loss: 2.308341\n",
      "(Iteration 28501 / 30560) loss: 2.308332\n",
      "(Iteration 28601 / 30560) loss: 2.308341\n",
      "(Epoch 75 / 80) train acc: 0.098000; val_acc: 0.116000\n",
      "(Iteration 28701 / 30560) loss: 2.308335\n",
      "(Iteration 28801 / 30560) loss: 2.308345\n",
      "(Iteration 28901 / 30560) loss: 2.308343\n",
      "(Iteration 29001 / 30560) loss: 2.308338\n",
      "(Epoch 76 / 80) train acc: 0.102000; val_acc: 0.116000\n",
      "(Iteration 29101 / 30560) loss: 2.308343\n",
      "(Iteration 29201 / 30560) loss: 2.308345\n",
      "(Iteration 29301 / 30560) loss: 2.308341\n",
      "(Iteration 29401 / 30560) loss: 2.308338\n",
      "(Epoch 77 / 80) train acc: 0.100000; val_acc: 0.116000\n",
      "(Iteration 29501 / 30560) loss: 2.308335\n",
      "(Iteration 29601 / 30560) loss: 2.308324\n",
      "(Iteration 29701 / 30560) loss: 2.308336\n",
      "(Epoch 78 / 80) train acc: 0.109000; val_acc: 0.116000\n",
      "(Iteration 29801 / 30560) loss: 2.308353\n",
      "(Iteration 29901 / 30560) loss: 2.308323\n",
      "(Iteration 30001 / 30560) loss: 2.308335\n",
      "(Iteration 30101 / 30560) loss: 2.308335\n",
      "(Epoch 79 / 80) train acc: 0.098000; val_acc: 0.116000\n",
      "(Iteration 30201 / 30560) loss: 2.308327\n",
      "(Iteration 30301 / 30560) loss: 2.308340\n",
      "(Iteration 30401 / 30560) loss: 2.308336\n",
      "(Iteration 30501 / 30560) loss: 2.308323\n",
      "(Epoch 80 / 80) train acc: 0.100000; val_acc: 0.116000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.308258\n",
      "(Epoch 0 / 80) train acc: 0.081000; val_acc: 0.083000\n",
      "(Iteration 101 / 61200) loss: 2.308267\n",
      "(Iteration 201 / 61200) loss: 2.308259\n",
      "(Iteration 301 / 61200) loss: 2.308263\n",
      "(Iteration 401 / 61200) loss: 2.308255\n",
      "(Iteration 501 / 61200) loss: 2.308253\n",
      "(Iteration 601 / 61200) loss: 2.308264\n",
      "(Iteration 701 / 61200) loss: 2.308242\n",
      "(Epoch 1 / 80) train acc: 0.107000; val_acc: 0.083000\n",
      "(Iteration 801 / 61200) loss: 2.308251\n",
      "(Iteration 901 / 61200) loss: 2.308261\n",
      "(Iteration 1001 / 61200) loss: 2.308258\n",
      "(Iteration 1101 / 61200) loss: 2.308281\n",
      "(Iteration 1201 / 61200) loss: 2.308252\n",
      "(Iteration 1301 / 61200) loss: 2.308270\n",
      "(Iteration 1401 / 61200) loss: 2.308256\n",
      "(Iteration 1501 / 61200) loss: 2.308255\n",
      "(Epoch 2 / 80) train acc: 0.080000; val_acc: 0.083000\n",
      "(Iteration 1601 / 61200) loss: 2.308258\n",
      "(Iteration 1701 / 61200) loss: 2.308253\n",
      "(Iteration 1801 / 61200) loss: 2.308271\n",
      "(Iteration 1901 / 61200) loss: 2.308260\n",
      "(Iteration 2001 / 61200) loss: 2.308240\n",
      "(Iteration 2101 / 61200) loss: 2.308268\n",
      "(Iteration 2201 / 61200) loss: 2.308255\n",
      "(Epoch 3 / 80) train acc: 0.085000; val_acc: 0.083000\n",
      "(Iteration 2301 / 61200) loss: 2.308256\n",
      "(Iteration 2401 / 61200) loss: 2.308242\n",
      "(Iteration 2501 / 61200) loss: 2.308247\n",
      "(Iteration 2601 / 61200) loss: 2.308254\n",
      "(Iteration 2701 / 61200) loss: 2.308273\n",
      "(Iteration 2801 / 61200) loss: 2.308262\n",
      "(Iteration 2901 / 61200) loss: 2.308243\n",
      "(Iteration 3001 / 61200) loss: 2.308256\n",
      "(Epoch 4 / 80) train acc: 0.092000; val_acc: 0.083000\n",
      "(Iteration 3101 / 61200) loss: 2.308254\n",
      "(Iteration 3201 / 61200) loss: 2.308265\n",
      "(Iteration 3301 / 61200) loss: 2.308239\n",
      "(Iteration 3401 / 61200) loss: 2.308243\n",
      "(Iteration 3501 / 61200) loss: 2.308253\n",
      "(Iteration 3601 / 61200) loss: 2.308249\n",
      "(Iteration 3701 / 61200) loss: 2.308240\n",
      "(Iteration 3801 / 61200) loss: 2.308263\n",
      "(Epoch 5 / 80) train acc: 0.102000; val_acc: 0.083000\n",
      "(Iteration 3901 / 61200) loss: 2.308255\n",
      "(Iteration 4001 / 61200) loss: 2.308256\n",
      "(Iteration 4101 / 61200) loss: 2.308245\n",
      "(Iteration 4201 / 61200) loss: 2.308261\n",
      "(Iteration 4301 / 61200) loss: 2.308266\n",
      "(Iteration 4401 / 61200) loss: 2.308249\n",
      "(Iteration 4501 / 61200) loss: 2.308248\n",
      "(Epoch 6 / 80) train acc: 0.095000; val_acc: 0.083000\n",
      "(Iteration 4601 / 61200) loss: 2.308257\n",
      "(Iteration 4701 / 61200) loss: 2.308258\n",
      "(Iteration 4801 / 61200) loss: 2.308247\n",
      "(Iteration 4901 / 61200) loss: 2.308258\n",
      "(Iteration 5001 / 61200) loss: 2.308254\n",
      "(Iteration 5101 / 61200) loss: 2.308248\n",
      "(Iteration 5201 / 61200) loss: 2.308262\n",
      "(Iteration 5301 / 61200) loss: 2.308239\n",
      "(Epoch 7 / 80) train acc: 0.089000; val_acc: 0.083000\n",
      "(Iteration 5401 / 61200) loss: 2.308247\n",
      "(Iteration 5501 / 61200) loss: 2.308261\n",
      "(Iteration 5601 / 61200) loss: 2.308253\n",
      "(Iteration 5701 / 61200) loss: 2.308247\n",
      "(Iteration 5801 / 61200) loss: 2.308249\n",
      "(Iteration 5901 / 61200) loss: 2.308251\n",
      "(Iteration 6001 / 61200) loss: 2.308262\n",
      "(Iteration 6101 / 61200) loss: 2.308258\n",
      "(Epoch 8 / 80) train acc: 0.101000; val_acc: 0.083000\n",
      "(Iteration 6201 / 61200) loss: 2.308245\n",
      "(Iteration 6301 / 61200) loss: 2.308240\n",
      "(Iteration 6401 / 61200) loss: 2.308246\n",
      "(Iteration 6501 / 61200) loss: 2.308265\n",
      "(Iteration 6601 / 61200) loss: 2.308257\n",
      "(Iteration 6701 / 61200) loss: 2.308249\n",
      "(Iteration 6801 / 61200) loss: 2.308254\n",
      "(Epoch 9 / 80) train acc: 0.086000; val_acc: 0.083000\n",
      "(Iteration 6901 / 61200) loss: 2.308256\n",
      "(Iteration 7001 / 61200) loss: 2.308254\n",
      "(Iteration 7101 / 61200) loss: 2.308242\n",
      "(Iteration 7201 / 61200) loss: 2.308237\n",
      "(Iteration 7301 / 61200) loss: 2.308255\n",
      "(Iteration 7401 / 61200) loss: 2.308252\n",
      "(Iteration 7501 / 61200) loss: 2.308263\n",
      "(Iteration 7601 / 61200) loss: 2.308246\n",
      "(Epoch 10 / 80) train acc: 0.096000; val_acc: 0.083000\n",
      "(Iteration 7701 / 61200) loss: 2.308230\n",
      "(Iteration 7801 / 61200) loss: 2.308246\n",
      "(Iteration 7901 / 61200) loss: 2.308244\n",
      "(Iteration 8001 / 61200) loss: 2.308259\n",
      "(Iteration 8101 / 61200) loss: 2.308264\n",
      "(Iteration 8201 / 61200) loss: 2.308258\n",
      "(Iteration 8301 / 61200) loss: 2.308246\n",
      "(Iteration 8401 / 61200) loss: 2.308248\n",
      "(Epoch 11 / 80) train acc: 0.089000; val_acc: 0.083000\n",
      "(Iteration 8501 / 61200) loss: 2.308255\n",
      "(Iteration 8601 / 61200) loss: 2.308265\n",
      "(Iteration 8701 / 61200) loss: 2.308239\n",
      "(Iteration 8801 / 61200) loss: 2.308257\n",
      "(Iteration 8901 / 61200) loss: 2.308232\n",
      "(Iteration 9001 / 61200) loss: 2.308277\n",
      "(Iteration 9101 / 61200) loss: 2.308269\n",
      "(Epoch 12 / 80) train acc: 0.091000; val_acc: 0.083000\n",
      "(Iteration 9201 / 61200) loss: 2.308254\n",
      "(Iteration 9301 / 61200) loss: 2.308253\n",
      "(Iteration 9401 / 61200) loss: 2.308259\n",
      "(Iteration 9501 / 61200) loss: 2.308262\n",
      "(Iteration 9601 / 61200) loss: 2.308262\n",
      "(Iteration 9701 / 61200) loss: 2.308246\n",
      "(Iteration 9801 / 61200) loss: 2.308241\n",
      "(Iteration 9901 / 61200) loss: 2.308250\n",
      "(Epoch 13 / 80) train acc: 0.089000; val_acc: 0.083000\n",
      "(Iteration 10001 / 61200) loss: 2.308256\n",
      "(Iteration 10101 / 61200) loss: 2.308249\n",
      "(Iteration 10201 / 61200) loss: 2.308233\n",
      "(Iteration 10301 / 61200) loss: 2.308244\n",
      "(Iteration 10401 / 61200) loss: 2.308264\n",
      "(Iteration 10501 / 61200) loss: 2.308246\n",
      "(Iteration 10601 / 61200) loss: 2.308249\n",
      "(Iteration 10701 / 61200) loss: 2.308262\n",
      "(Epoch 14 / 80) train acc: 0.095000; val_acc: 0.083000\n",
      "(Iteration 10801 / 61200) loss: 2.308230\n",
      "(Iteration 10901 / 61200) loss: 2.308245\n",
      "(Iteration 11001 / 61200) loss: 2.308230\n",
      "(Iteration 11101 / 61200) loss: 2.308235\n",
      "(Iteration 11201 / 61200) loss: 2.308251\n",
      "(Iteration 11301 / 61200) loss: 2.308254\n",
      "(Iteration 11401 / 61200) loss: 2.308236\n",
      "(Epoch 15 / 80) train acc: 0.093000; val_acc: 0.083000\n",
      "(Iteration 11501 / 61200) loss: 2.308241\n",
      "(Iteration 11601 / 61200) loss: 2.308258\n",
      "(Iteration 11701 / 61200) loss: 2.308259\n",
      "(Iteration 11801 / 61200) loss: 2.308244\n",
      "(Iteration 11901 / 61200) loss: 2.308247\n",
      "(Iteration 12001 / 61200) loss: 2.308248\n",
      "(Iteration 12101 / 61200) loss: 2.308247\n",
      "(Iteration 12201 / 61200) loss: 2.308251\n",
      "(Epoch 16 / 80) train acc: 0.089000; val_acc: 0.083000\n",
      "(Iteration 12301 / 61200) loss: 2.308239\n",
      "(Iteration 12401 / 61200) loss: 2.308245\n",
      "(Iteration 12501 / 61200) loss: 2.308251\n",
      "(Iteration 12601 / 61200) loss: 2.308265\n",
      "(Iteration 12701 / 61200) loss: 2.308244\n",
      "(Iteration 12801 / 61200) loss: 2.308252\n",
      "(Iteration 12901 / 61200) loss: 2.308244\n",
      "(Iteration 13001 / 61200) loss: 2.308243\n",
      "(Epoch 17 / 80) train acc: 0.084000; val_acc: 0.083000\n",
      "(Iteration 13101 / 61200) loss: 2.308246\n",
      "(Iteration 13201 / 61200) loss: 2.308252\n",
      "(Iteration 13301 / 61200) loss: 2.308236\n",
      "(Iteration 13401 / 61200) loss: 2.308235\n",
      "(Iteration 13501 / 61200) loss: 2.308255\n",
      "(Iteration 13601 / 61200) loss: 2.308247\n",
      "(Iteration 13701 / 61200) loss: 2.308269\n",
      "(Epoch 18 / 80) train acc: 0.086000; val_acc: 0.083000\n",
      "(Iteration 13801 / 61200) loss: 2.308243\n",
      "(Iteration 13901 / 61200) loss: 2.308250\n",
      "(Iteration 14001 / 61200) loss: 2.308251\n",
      "(Iteration 14101 / 61200) loss: 2.308270\n",
      "(Iteration 14201 / 61200) loss: 2.308264\n",
      "(Iteration 14301 / 61200) loss: 2.308241\n",
      "(Iteration 14401 / 61200) loss: 2.308248\n",
      "(Iteration 14501 / 61200) loss: 2.308262\n",
      "(Epoch 19 / 80) train acc: 0.098000; val_acc: 0.083000\n",
      "(Iteration 14601 / 61200) loss: 2.308254\n",
      "(Iteration 14701 / 61200) loss: 2.308255\n",
      "(Iteration 14801 / 61200) loss: 2.308245\n",
      "(Iteration 14901 / 61200) loss: 2.308243\n",
      "(Iteration 15001 / 61200) loss: 2.308240\n",
      "(Iteration 15101 / 61200) loss: 2.308250\n",
      "(Iteration 15201 / 61200) loss: 2.308241\n",
      "(Epoch 20 / 80) train acc: 0.088000; val_acc: 0.083000\n",
      "(Iteration 15301 / 61200) loss: 2.308248\n",
      "(Iteration 15401 / 61200) loss: 2.308248\n",
      "(Iteration 15501 / 61200) loss: 2.308276\n",
      "(Iteration 15601 / 61200) loss: 2.308245\n",
      "(Iteration 15701 / 61200) loss: 2.308248\n",
      "(Iteration 15801 / 61200) loss: 2.308238\n",
      "(Iteration 15901 / 61200) loss: 2.308237\n",
      "(Iteration 16001 / 61200) loss: 2.308260\n",
      "(Epoch 21 / 80) train acc: 0.095000; val_acc: 0.083000\n",
      "(Iteration 16101 / 61200) loss: 2.308251\n",
      "(Iteration 16201 / 61200) loss: 2.308243\n",
      "(Iteration 16301 / 61200) loss: 2.308256\n",
      "(Iteration 16401 / 61200) loss: 2.308254\n",
      "(Iteration 16501 / 61200) loss: 2.308262\n",
      "(Iteration 16601 / 61200) loss: 2.308254\n",
      "(Iteration 16701 / 61200) loss: 2.308272\n",
      "(Iteration 16801 / 61200) loss: 2.308251\n",
      "(Epoch 22 / 80) train acc: 0.098000; val_acc: 0.083000\n",
      "(Iteration 16901 / 61200) loss: 2.308242\n",
      "(Iteration 17001 / 61200) loss: 2.308253\n",
      "(Iteration 17101 / 61200) loss: 2.308241\n",
      "(Iteration 17201 / 61200) loss: 2.308258\n",
      "(Iteration 17301 / 61200) loss: 2.308256\n",
      "(Iteration 17401 / 61200) loss: 2.308241\n",
      "(Iteration 17501 / 61200) loss: 2.308237\n",
      "(Epoch 23 / 80) train acc: 0.099000; val_acc: 0.083000\n",
      "(Iteration 17601 / 61200) loss: 2.308262\n",
      "(Iteration 17701 / 61200) loss: 2.308247\n",
      "(Iteration 17801 / 61200) loss: 2.308249\n",
      "(Iteration 17901 / 61200) loss: 2.308244\n",
      "(Iteration 18001 / 61200) loss: 2.308238\n",
      "(Iteration 18101 / 61200) loss: 2.308238\n",
      "(Iteration 18201 / 61200) loss: 2.308251\n",
      "(Iteration 18301 / 61200) loss: 2.308246\n",
      "(Epoch 24 / 80) train acc: 0.091000; val_acc: 0.083000\n",
      "(Iteration 18401 / 61200) loss: 2.308247\n",
      "(Iteration 18501 / 61200) loss: 2.308234\n",
      "(Iteration 18601 / 61200) loss: 2.308252\n",
      "(Iteration 18701 / 61200) loss: 2.308263\n",
      "(Iteration 18801 / 61200) loss: 2.308234\n",
      "(Iteration 18901 / 61200) loss: 2.308238\n",
      "(Iteration 19001 / 61200) loss: 2.308252\n",
      "(Iteration 19101 / 61200) loss: 2.308255\n",
      "(Epoch 25 / 80) train acc: 0.098000; val_acc: 0.083000\n",
      "(Iteration 19201 / 61200) loss: 2.308247\n",
      "(Iteration 19301 / 61200) loss: 2.308265\n",
      "(Iteration 19401 / 61200) loss: 2.308237\n",
      "(Iteration 19501 / 61200) loss: 2.308261\n",
      "(Iteration 19601 / 61200) loss: 2.308250\n",
      "(Iteration 19701 / 61200) loss: 2.308258\n",
      "(Iteration 19801 / 61200) loss: 2.308269\n",
      "(Epoch 26 / 80) train acc: 0.089000; val_acc: 0.083000\n",
      "(Iteration 19901 / 61200) loss: 2.308231\n",
      "(Iteration 20001 / 61200) loss: 2.308246\n",
      "(Iteration 20101 / 61200) loss: 2.308235\n",
      "(Iteration 20201 / 61200) loss: 2.308250\n",
      "(Iteration 20301 / 61200) loss: 2.308241\n",
      "(Iteration 20401 / 61200) loss: 2.308235\n",
      "(Iteration 20501 / 61200) loss: 2.308244\n",
      "(Iteration 20601 / 61200) loss: 2.308253\n",
      "(Epoch 27 / 80) train acc: 0.073000; val_acc: 0.083000\n",
      "(Iteration 20701 / 61200) loss: 2.308256\n",
      "(Iteration 20801 / 61200) loss: 2.308248\n",
      "(Iteration 20901 / 61200) loss: 2.308250\n",
      "(Iteration 21001 / 61200) loss: 2.308249\n",
      "(Iteration 21101 / 61200) loss: 2.308265\n",
      "(Iteration 21201 / 61200) loss: 2.308243\n",
      "(Iteration 21301 / 61200) loss: 2.308248\n",
      "(Iteration 21401 / 61200) loss: 2.308247\n",
      "(Epoch 28 / 80) train acc: 0.096000; val_acc: 0.083000\n",
      "(Iteration 21501 / 61200) loss: 2.308243\n",
      "(Iteration 21601 / 61200) loss: 2.308263\n",
      "(Iteration 21701 / 61200) loss: 2.308238\n",
      "(Iteration 21801 / 61200) loss: 2.308246\n",
      "(Iteration 21901 / 61200) loss: 2.308237\n",
      "(Iteration 22001 / 61200) loss: 2.308242\n",
      "(Iteration 22101 / 61200) loss: 2.308260\n",
      "(Epoch 29 / 80) train acc: 0.088000; val_acc: 0.083000\n",
      "(Iteration 22201 / 61200) loss: 2.308245\n",
      "(Iteration 22301 / 61200) loss: 2.308252\n",
      "(Iteration 22401 / 61200) loss: 2.308247\n",
      "(Iteration 22501 / 61200) loss: 2.308247\n",
      "(Iteration 22601 / 61200) loss: 2.308232\n",
      "(Iteration 22701 / 61200) loss: 2.308265\n",
      "(Iteration 22801 / 61200) loss: 2.308249\n",
      "(Iteration 22901 / 61200) loss: 2.308252\n",
      "(Epoch 30 / 80) train acc: 0.069000; val_acc: 0.083000\n",
      "(Iteration 23001 / 61200) loss: 2.308262\n",
      "(Iteration 23101 / 61200) loss: 2.308252\n",
      "(Iteration 23201 / 61200) loss: 2.308258\n",
      "(Iteration 23301 / 61200) loss: 2.308237\n",
      "(Iteration 23401 / 61200) loss: 2.308251\n",
      "(Iteration 23501 / 61200) loss: 2.308260\n",
      "(Iteration 23601 / 61200) loss: 2.308236\n",
      "(Iteration 23701 / 61200) loss: 2.308253\n",
      "(Epoch 31 / 80) train acc: 0.096000; val_acc: 0.083000\n",
      "(Iteration 23801 / 61200) loss: 2.308240\n",
      "(Iteration 23901 / 61200) loss: 2.308256\n",
      "(Iteration 24001 / 61200) loss: 2.308255\n",
      "(Iteration 24101 / 61200) loss: 2.308250\n",
      "(Iteration 24201 / 61200) loss: 2.308256\n",
      "(Iteration 24301 / 61200) loss: 2.308237\n",
      "(Iteration 24401 / 61200) loss: 2.308257\n",
      "(Epoch 32 / 80) train acc: 0.078000; val_acc: 0.083000\n",
      "(Iteration 24501 / 61200) loss: 2.308252\n",
      "(Iteration 24601 / 61200) loss: 2.308239\n",
      "(Iteration 24701 / 61200) loss: 2.308253\n",
      "(Iteration 24801 / 61200) loss: 2.308233\n",
      "(Iteration 24901 / 61200) loss: 2.308240\n",
      "(Iteration 25001 / 61200) loss: 2.308254\n",
      "(Iteration 25101 / 61200) loss: 2.308236\n",
      "(Iteration 25201 / 61200) loss: 2.308258\n",
      "(Epoch 33 / 80) train acc: 0.083000; val_acc: 0.083000\n",
      "(Iteration 25301 / 61200) loss: 2.308260\n",
      "(Iteration 25401 / 61200) loss: 2.308256\n",
      "(Iteration 25501 / 61200) loss: 2.308263\n",
      "(Iteration 25601 / 61200) loss: 2.308244\n",
      "(Iteration 25701 / 61200) loss: 2.308250\n",
      "(Iteration 25801 / 61200) loss: 2.308246\n",
      "(Iteration 25901 / 61200) loss: 2.308244\n",
      "(Iteration 26001 / 61200) loss: 2.308239\n",
      "(Epoch 34 / 80) train acc: 0.089000; val_acc: 0.083000\n",
      "(Iteration 26101 / 61200) loss: 2.308248\n",
      "(Iteration 26201 / 61200) loss: 2.308230\n",
      "(Iteration 26301 / 61200) loss: 2.308254\n",
      "(Iteration 26401 / 61200) loss: 2.308255\n",
      "(Iteration 26501 / 61200) loss: 2.308262\n",
      "(Iteration 26601 / 61200) loss: 2.308264\n",
      "(Iteration 26701 / 61200) loss: 2.308238\n",
      "(Epoch 35 / 80) train acc: 0.090000; val_acc: 0.083000\n",
      "(Iteration 26801 / 61200) loss: 2.308256\n",
      "(Iteration 26901 / 61200) loss: 2.308254\n",
      "(Iteration 27001 / 61200) loss: 2.308253\n",
      "(Iteration 27101 / 61200) loss: 2.308240\n",
      "(Iteration 27201 / 61200) loss: 2.308238\n",
      "(Iteration 27301 / 61200) loss: 2.308245\n",
      "(Iteration 27401 / 61200) loss: 2.308246\n",
      "(Iteration 27501 / 61200) loss: 2.308223\n",
      "(Epoch 36 / 80) train acc: 0.091000; val_acc: 0.083000\n",
      "(Iteration 27601 / 61200) loss: 2.308248\n",
      "(Iteration 27701 / 61200) loss: 2.308253\n",
      "(Iteration 27801 / 61200) loss: 2.308250\n",
      "(Iteration 27901 / 61200) loss: 2.308237\n",
      "(Iteration 28001 / 61200) loss: 2.308250\n",
      "(Iteration 28101 / 61200) loss: 2.308259\n",
      "(Iteration 28201 / 61200) loss: 2.308245\n",
      "(Iteration 28301 / 61200) loss: 2.308252\n",
      "(Epoch 37 / 80) train acc: 0.093000; val_acc: 0.083000\n",
      "(Iteration 28401 / 61200) loss: 2.308246\n",
      "(Iteration 28501 / 61200) loss: 2.308262\n",
      "(Iteration 28601 / 61200) loss: 2.308241\n",
      "(Iteration 28701 / 61200) loss: 2.308242\n",
      "(Iteration 28801 / 61200) loss: 2.308253\n",
      "(Iteration 28901 / 61200) loss: 2.308235\n",
      "(Iteration 29001 / 61200) loss: 2.308255\n",
      "(Epoch 38 / 80) train acc: 0.088000; val_acc: 0.083000\n",
      "(Iteration 29101 / 61200) loss: 2.308263\n",
      "(Iteration 29201 / 61200) loss: 2.308236\n",
      "(Iteration 29301 / 61200) loss: 2.308249\n",
      "(Iteration 29401 / 61200) loss: 2.308241\n",
      "(Iteration 29501 / 61200) loss: 2.308255\n",
      "(Iteration 29601 / 61200) loss: 2.308245\n",
      "(Iteration 29701 / 61200) loss: 2.308242\n",
      "(Iteration 29801 / 61200) loss: 2.308247\n",
      "(Epoch 39 / 80) train acc: 0.083000; val_acc: 0.083000\n",
      "(Iteration 29901 / 61200) loss: 2.308243\n",
      "(Iteration 30001 / 61200) loss: 2.308255\n",
      "(Iteration 30101 / 61200) loss: 2.308255\n",
      "(Iteration 30201 / 61200) loss: 2.308237\n",
      "(Iteration 30301 / 61200) loss: 2.308249\n",
      "(Iteration 30401 / 61200) loss: 2.308248\n",
      "(Iteration 30501 / 61200) loss: 2.308247\n",
      "(Epoch 40 / 80) train acc: 0.099000; val_acc: 0.083000\n",
      "(Iteration 30601 / 61200) loss: 2.308253\n",
      "(Iteration 30701 / 61200) loss: 2.308234\n",
      "(Iteration 30801 / 61200) loss: 2.308246\n",
      "(Iteration 30901 / 61200) loss: 2.308254\n",
      "(Iteration 31001 / 61200) loss: 2.308242\n",
      "(Iteration 31101 / 61200) loss: 2.308242\n",
      "(Iteration 31201 / 61200) loss: 2.308246\n",
      "(Iteration 31301 / 61200) loss: 2.308249\n",
      "(Epoch 41 / 80) train acc: 0.082000; val_acc: 0.083000\n",
      "(Iteration 31401 / 61200) loss: 2.308252\n",
      "(Iteration 31501 / 61200) loss: 2.308241\n",
      "(Iteration 31601 / 61200) loss: 2.308256\n",
      "(Iteration 31701 / 61200) loss: 2.308241\n",
      "(Iteration 31801 / 61200) loss: 2.308236\n",
      "(Iteration 31901 / 61200) loss: 2.308234\n",
      "(Iteration 32001 / 61200) loss: 2.308256\n",
      "(Iteration 32101 / 61200) loss: 2.308229\n",
      "(Epoch 42 / 80) train acc: 0.097000; val_acc: 0.083000\n",
      "(Iteration 32201 / 61200) loss: 2.308244\n",
      "(Iteration 32301 / 61200) loss: 2.308269\n",
      "(Iteration 32401 / 61200) loss: 2.308226\n",
      "(Iteration 32501 / 61200) loss: 2.308235\n",
      "(Iteration 32601 / 61200) loss: 2.308255\n",
      "(Iteration 32701 / 61200) loss: 2.308259\n",
      "(Iteration 32801 / 61200) loss: 2.308250\n",
      "(Epoch 43 / 80) train acc: 0.085000; val_acc: 0.083000\n",
      "(Iteration 32901 / 61200) loss: 2.308238\n",
      "(Iteration 33001 / 61200) loss: 2.308237\n",
      "(Iteration 33101 / 61200) loss: 2.308236\n",
      "(Iteration 33201 / 61200) loss: 2.308237\n",
      "(Iteration 33301 / 61200) loss: 2.308232\n",
      "(Iteration 33401 / 61200) loss: 2.308243\n",
      "(Iteration 33501 / 61200) loss: 2.308251\n",
      "(Iteration 33601 / 61200) loss: 2.308261\n",
      "(Epoch 44 / 80) train acc: 0.102000; val_acc: 0.083000\n",
      "(Iteration 33701 / 61200) loss: 2.308253\n",
      "(Iteration 33801 / 61200) loss: 2.308243\n",
      "(Iteration 33901 / 61200) loss: 2.308240\n",
      "(Iteration 34001 / 61200) loss: 2.308261\n",
      "(Iteration 34101 / 61200) loss: 2.308239\n",
      "(Iteration 34201 / 61200) loss: 2.308250\n",
      "(Iteration 34301 / 61200) loss: 2.308242\n",
      "(Iteration 34401 / 61200) loss: 2.308241\n",
      "(Epoch 45 / 80) train acc: 0.088000; val_acc: 0.083000\n",
      "(Iteration 34501 / 61200) loss: 2.308246\n",
      "(Iteration 34601 / 61200) loss: 2.308240\n",
      "(Iteration 34701 / 61200) loss: 2.308243\n",
      "(Iteration 34801 / 61200) loss: 2.308234\n",
      "(Iteration 34901 / 61200) loss: 2.308233\n",
      "(Iteration 35001 / 61200) loss: 2.308245\n",
      "(Iteration 35101 / 61200) loss: 2.308243\n",
      "(Epoch 46 / 80) train acc: 0.093000; val_acc: 0.083000\n",
      "(Iteration 35201 / 61200) loss: 2.308254\n",
      "(Iteration 35301 / 61200) loss: 2.308240\n",
      "(Iteration 35401 / 61200) loss: 2.308254\n",
      "(Iteration 35501 / 61200) loss: 2.308246\n",
      "(Iteration 35601 / 61200) loss: 2.308246\n",
      "(Iteration 35701 / 61200) loss: 2.308253\n",
      "(Iteration 35801 / 61200) loss: 2.308237\n",
      "(Iteration 35901 / 61200) loss: 2.308240\n",
      "(Epoch 47 / 80) train acc: 0.093000; val_acc: 0.083000\n",
      "(Iteration 36001 / 61200) loss: 2.308237\n",
      "(Iteration 36101 / 61200) loss: 2.308263\n",
      "(Iteration 36201 / 61200) loss: 2.308254\n",
      "(Iteration 36301 / 61200) loss: 2.308248\n",
      "(Iteration 36401 / 61200) loss: 2.308255\n",
      "(Iteration 36501 / 61200) loss: 2.308244\n",
      "(Iteration 36601 / 61200) loss: 2.308242\n",
      "(Iteration 36701 / 61200) loss: 2.308247\n",
      "(Epoch 48 / 80) train acc: 0.082000; val_acc: 0.083000\n",
      "(Iteration 36801 / 61200) loss: 2.308249\n",
      "(Iteration 36901 / 61200) loss: 2.308251\n",
      "(Iteration 37001 / 61200) loss: 2.308239\n",
      "(Iteration 37101 / 61200) loss: 2.308253\n",
      "(Iteration 37201 / 61200) loss: 2.308245\n",
      "(Iteration 37301 / 61200) loss: 2.308260\n",
      "(Iteration 37401 / 61200) loss: 2.308236\n",
      "(Epoch 49 / 80) train acc: 0.084000; val_acc: 0.083000\n",
      "(Iteration 37501 / 61200) loss: 2.308239\n",
      "(Iteration 37601 / 61200) loss: 2.308240\n",
      "(Iteration 37701 / 61200) loss: 2.308255\n",
      "(Iteration 37801 / 61200) loss: 2.308246\n",
      "(Iteration 37901 / 61200) loss: 2.308238\n",
      "(Iteration 38001 / 61200) loss: 2.308253\n",
      "(Iteration 38101 / 61200) loss: 2.308257\n",
      "(Iteration 38201 / 61200) loss: 2.308250\n",
      "(Epoch 50 / 80) train acc: 0.089000; val_acc: 0.083000\n",
      "(Iteration 38301 / 61200) loss: 2.308235\n",
      "(Iteration 38401 / 61200) loss: 2.308235\n",
      "(Iteration 38501 / 61200) loss: 2.308233\n",
      "(Iteration 38601 / 61200) loss: 2.308248\n",
      "(Iteration 38701 / 61200) loss: 2.308238\n",
      "(Iteration 38801 / 61200) loss: 2.308257\n",
      "(Iteration 38901 / 61200) loss: 2.308240\n",
      "(Iteration 39001 / 61200) loss: 2.308234\n",
      "(Epoch 51 / 80) train acc: 0.080000; val_acc: 0.083000\n",
      "(Iteration 39101 / 61200) loss: 2.308233\n",
      "(Iteration 39201 / 61200) loss: 2.308249\n",
      "(Iteration 39301 / 61200) loss: 2.308241\n",
      "(Iteration 39401 / 61200) loss: 2.308251\n",
      "(Iteration 39501 / 61200) loss: 2.308237\n",
      "(Iteration 39601 / 61200) loss: 2.308253\n",
      "(Iteration 39701 / 61200) loss: 2.308246\n",
      "(Epoch 52 / 80) train acc: 0.088000; val_acc: 0.083000\n",
      "(Iteration 39801 / 61200) loss: 2.308237\n",
      "(Iteration 39901 / 61200) loss: 2.308230\n",
      "(Iteration 40001 / 61200) loss: 2.308226\n",
      "(Iteration 40101 / 61200) loss: 2.308229\n",
      "(Iteration 40201 / 61200) loss: 2.308242\n",
      "(Iteration 40301 / 61200) loss: 2.308241\n",
      "(Iteration 40401 / 61200) loss: 2.308255\n",
      "(Iteration 40501 / 61200) loss: 2.308236\n",
      "(Epoch 53 / 80) train acc: 0.087000; val_acc: 0.083000\n",
      "(Iteration 40601 / 61200) loss: 2.308256\n",
      "(Iteration 40701 / 61200) loss: 2.308247\n",
      "(Iteration 40801 / 61200) loss: 2.308246\n",
      "(Iteration 40901 / 61200) loss: 2.308224\n",
      "(Iteration 41001 / 61200) loss: 2.308258\n",
      "(Iteration 41101 / 61200) loss: 2.308247\n",
      "(Iteration 41201 / 61200) loss: 2.308265\n",
      "(Iteration 41301 / 61200) loss: 2.308239\n",
      "(Epoch 54 / 80) train acc: 0.089000; val_acc: 0.083000\n",
      "(Iteration 41401 / 61200) loss: 2.308247\n",
      "(Iteration 41501 / 61200) loss: 2.308231\n",
      "(Iteration 41601 / 61200) loss: 2.308233\n",
      "(Iteration 41701 / 61200) loss: 2.308251\n",
      "(Iteration 41801 / 61200) loss: 2.308253\n",
      "(Iteration 41901 / 61200) loss: 2.308247\n",
      "(Iteration 42001 / 61200) loss: 2.308255\n",
      "(Epoch 55 / 80) train acc: 0.083000; val_acc: 0.083000\n",
      "(Iteration 42101 / 61200) loss: 2.308259\n",
      "(Iteration 42201 / 61200) loss: 2.308241\n",
      "(Iteration 42301 / 61200) loss: 2.308253\n",
      "(Iteration 42401 / 61200) loss: 2.308230\n",
      "(Iteration 42501 / 61200) loss: 2.308261\n",
      "(Iteration 42601 / 61200) loss: 2.308232\n",
      "(Iteration 42701 / 61200) loss: 2.308239\n",
      "(Iteration 42801 / 61200) loss: 2.308245\n",
      "(Epoch 56 / 80) train acc: 0.093000; val_acc: 0.083000\n",
      "(Iteration 42901 / 61200) loss: 2.308218\n",
      "(Iteration 43001 / 61200) loss: 2.308245\n",
      "(Iteration 43101 / 61200) loss: 2.308255\n",
      "(Iteration 43201 / 61200) loss: 2.308236\n",
      "(Iteration 43301 / 61200) loss: 2.308238\n",
      "(Iteration 43401 / 61200) loss: 2.308258\n",
      "(Iteration 43501 / 61200) loss: 2.308253\n",
      "(Iteration 43601 / 61200) loss: 2.308221\n",
      "(Epoch 57 / 80) train acc: 0.085000; val_acc: 0.083000\n",
      "(Iteration 43701 / 61200) loss: 2.308238\n",
      "(Iteration 43801 / 61200) loss: 2.308269\n",
      "(Iteration 43901 / 61200) loss: 2.308237\n",
      "(Iteration 44001 / 61200) loss: 2.308265\n",
      "(Iteration 44101 / 61200) loss: 2.308238\n",
      "(Iteration 44201 / 61200) loss: 2.308260\n",
      "(Iteration 44301 / 61200) loss: 2.308243\n",
      "(Epoch 58 / 80) train acc: 0.091000; val_acc: 0.083000\n",
      "(Iteration 44401 / 61200) loss: 2.308236\n",
      "(Iteration 44501 / 61200) loss: 2.308257\n",
      "(Iteration 44601 / 61200) loss: 2.308246\n",
      "(Iteration 44701 / 61200) loss: 2.308241\n",
      "(Iteration 44801 / 61200) loss: 2.308235\n",
      "(Iteration 44901 / 61200) loss: 2.308240\n",
      "(Iteration 45001 / 61200) loss: 2.308248\n",
      "(Iteration 45101 / 61200) loss: 2.308224\n",
      "(Epoch 59 / 80) train acc: 0.106000; val_acc: 0.083000\n",
      "(Iteration 45201 / 61200) loss: 2.308263\n",
      "(Iteration 45301 / 61200) loss: 2.308234\n",
      "(Iteration 45401 / 61200) loss: 2.308258\n",
      "(Iteration 45501 / 61200) loss: 2.308245\n",
      "(Iteration 45601 / 61200) loss: 2.308251\n",
      "(Iteration 45701 / 61200) loss: 2.308269\n",
      "(Iteration 45801 / 61200) loss: 2.308240\n",
      "(Epoch 60 / 80) train acc: 0.095000; val_acc: 0.083000\n",
      "(Iteration 45901 / 61200) loss: 2.308255\n",
      "(Iteration 46001 / 61200) loss: 2.308256\n",
      "(Iteration 46101 / 61200) loss: 2.308254\n",
      "(Iteration 46201 / 61200) loss: 2.308247\n",
      "(Iteration 46301 / 61200) loss: 2.308247\n",
      "(Iteration 46401 / 61200) loss: 2.308257\n",
      "(Iteration 46501 / 61200) loss: 2.308265\n",
      "(Iteration 46601 / 61200) loss: 2.308238\n",
      "(Epoch 61 / 80) train acc: 0.094000; val_acc: 0.083000\n",
      "(Iteration 46701 / 61200) loss: 2.308224\n",
      "(Iteration 46801 / 61200) loss: 2.308238\n",
      "(Iteration 46901 / 61200) loss: 2.308245\n",
      "(Iteration 47001 / 61200) loss: 2.308249\n",
      "(Iteration 47101 / 61200) loss: 2.308239\n",
      "(Iteration 47201 / 61200) loss: 2.308227\n",
      "(Iteration 47301 / 61200) loss: 2.308257\n",
      "(Iteration 47401 / 61200) loss: 2.308234\n",
      "(Epoch 62 / 80) train acc: 0.106000; val_acc: 0.083000\n",
      "(Iteration 47501 / 61200) loss: 2.308257\n",
      "(Iteration 47601 / 61200) loss: 2.308236\n",
      "(Iteration 47701 / 61200) loss: 2.308242\n",
      "(Iteration 47801 / 61200) loss: 2.308237\n",
      "(Iteration 47901 / 61200) loss: 2.308242\n",
      "(Iteration 48001 / 61200) loss: 2.308240\n",
      "(Iteration 48101 / 61200) loss: 2.308242\n",
      "(Epoch 63 / 80) train acc: 0.082000; val_acc: 0.083000\n",
      "(Iteration 48201 / 61200) loss: 2.308249\n",
      "(Iteration 48301 / 61200) loss: 2.308244\n",
      "(Iteration 48401 / 61200) loss: 2.308234\n",
      "(Iteration 48501 / 61200) loss: 2.308248\n",
      "(Iteration 48601 / 61200) loss: 2.308248\n",
      "(Iteration 48701 / 61200) loss: 2.308230\n",
      "(Iteration 48801 / 61200) loss: 2.308251\n",
      "(Iteration 48901 / 61200) loss: 2.308256\n",
      "(Epoch 64 / 80) train acc: 0.093000; val_acc: 0.083000\n",
      "(Iteration 49001 / 61200) loss: 2.308229\n",
      "(Iteration 49101 / 61200) loss: 2.308232\n",
      "(Iteration 49201 / 61200) loss: 2.308240\n",
      "(Iteration 49301 / 61200) loss: 2.308238\n",
      "(Iteration 49401 / 61200) loss: 2.308237\n",
      "(Iteration 49501 / 61200) loss: 2.308260\n",
      "(Iteration 49601 / 61200) loss: 2.308259\n",
      "(Iteration 49701 / 61200) loss: 2.308228\n",
      "(Epoch 65 / 80) train acc: 0.111000; val_acc: 0.083000\n",
      "(Iteration 49801 / 61200) loss: 2.308248\n",
      "(Iteration 49901 / 61200) loss: 2.308242\n",
      "(Iteration 50001 / 61200) loss: 2.308237\n",
      "(Iteration 50101 / 61200) loss: 2.308270\n",
      "(Iteration 50201 / 61200) loss: 2.308243\n",
      "(Iteration 50301 / 61200) loss: 2.308238\n",
      "(Iteration 50401 / 61200) loss: 2.308244\n",
      "(Epoch 66 / 80) train acc: 0.100000; val_acc: 0.083000\n",
      "(Iteration 50501 / 61200) loss: 2.308237\n",
      "(Iteration 50601 / 61200) loss: 2.308255\n",
      "(Iteration 50701 / 61200) loss: 2.308237\n",
      "(Iteration 50801 / 61200) loss: 2.308242\n",
      "(Iteration 50901 / 61200) loss: 2.308250\n",
      "(Iteration 51001 / 61200) loss: 2.308236\n",
      "(Iteration 51101 / 61200) loss: 2.308234\n",
      "(Iteration 51201 / 61200) loss: 2.308248\n",
      "(Epoch 67 / 80) train acc: 0.087000; val_acc: 0.083000\n",
      "(Iteration 51301 / 61200) loss: 2.308240\n",
      "(Iteration 51401 / 61200) loss: 2.308241\n",
      "(Iteration 51501 / 61200) loss: 2.308249\n",
      "(Iteration 51601 / 61200) loss: 2.308244\n",
      "(Iteration 51701 / 61200) loss: 2.308221\n",
      "(Iteration 51801 / 61200) loss: 2.308239\n",
      "(Iteration 51901 / 61200) loss: 2.308241\n",
      "(Iteration 52001 / 61200) loss: 2.308249\n",
      "(Epoch 68 / 80) train acc: 0.092000; val_acc: 0.083000\n",
      "(Iteration 52101 / 61200) loss: 2.308252\n",
      "(Iteration 52201 / 61200) loss: 2.308243\n",
      "(Iteration 52301 / 61200) loss: 2.308225\n",
      "(Iteration 52401 / 61200) loss: 2.308270\n",
      "(Iteration 52501 / 61200) loss: 2.308227\n",
      "(Iteration 52601 / 61200) loss: 2.308222\n",
      "(Iteration 52701 / 61200) loss: 2.308229\n",
      "(Epoch 69 / 80) train acc: 0.097000; val_acc: 0.083000\n",
      "(Iteration 52801 / 61200) loss: 2.308255\n",
      "(Iteration 52901 / 61200) loss: 2.308258\n",
      "(Iteration 53001 / 61200) loss: 2.308249\n",
      "(Iteration 53101 / 61200) loss: 2.308232\n",
      "(Iteration 53201 / 61200) loss: 2.308242\n",
      "(Iteration 53301 / 61200) loss: 2.308267\n",
      "(Iteration 53401 / 61200) loss: 2.308253\n",
      "(Iteration 53501 / 61200) loss: 2.308257\n",
      "(Epoch 70 / 80) train acc: 0.104000; val_acc: 0.083000\n",
      "(Iteration 53601 / 61200) loss: 2.308246\n",
      "(Iteration 53701 / 61200) loss: 2.308242\n",
      "(Iteration 53801 / 61200) loss: 2.308250\n",
      "(Iteration 53901 / 61200) loss: 2.308241\n",
      "(Iteration 54001 / 61200) loss: 2.308231\n",
      "(Iteration 54101 / 61200) loss: 2.308235\n",
      "(Iteration 54201 / 61200) loss: 2.308230\n",
      "(Iteration 54301 / 61200) loss: 2.308237\n",
      "(Epoch 71 / 80) train acc: 0.091000; val_acc: 0.083000\n",
      "(Iteration 54401 / 61200) loss: 2.308242\n",
      "(Iteration 54501 / 61200) loss: 2.308258\n",
      "(Iteration 54601 / 61200) loss: 2.308252\n",
      "(Iteration 54701 / 61200) loss: 2.308244\n",
      "(Iteration 54801 / 61200) loss: 2.308244\n",
      "(Iteration 54901 / 61200) loss: 2.308269\n",
      "(Iteration 55001 / 61200) loss: 2.308238\n",
      "(Epoch 72 / 80) train acc: 0.086000; val_acc: 0.083000\n",
      "(Iteration 55101 / 61200) loss: 2.308255\n",
      "(Iteration 55201 / 61200) loss: 2.308247\n",
      "(Iteration 55301 / 61200) loss: 2.308257\n",
      "(Iteration 55401 / 61200) loss: 2.308257\n",
      "(Iteration 55501 / 61200) loss: 2.308242\n",
      "(Iteration 55601 / 61200) loss: 2.308230\n",
      "(Iteration 55701 / 61200) loss: 2.308222\n",
      "(Iteration 55801 / 61200) loss: 2.308231\n",
      "(Epoch 73 / 80) train acc: 0.069000; val_acc: 0.083000\n",
      "(Iteration 55901 / 61200) loss: 2.308245\n",
      "(Iteration 56001 / 61200) loss: 2.308240\n",
      "(Iteration 56101 / 61200) loss: 2.308240\n",
      "(Iteration 56201 / 61200) loss: 2.308246\n",
      "(Iteration 56301 / 61200) loss: 2.308241\n",
      "(Iteration 56401 / 61200) loss: 2.308244\n",
      "(Iteration 56501 / 61200) loss: 2.308244\n",
      "(Iteration 56601 / 61200) loss: 2.308243\n",
      "(Epoch 74 / 80) train acc: 0.092000; val_acc: 0.083000\n",
      "(Iteration 56701 / 61200) loss: 2.308241\n",
      "(Iteration 56801 / 61200) loss: 2.308229\n",
      "(Iteration 56901 / 61200) loss: 2.308243\n",
      "(Iteration 57001 / 61200) loss: 2.308244\n",
      "(Iteration 57101 / 61200) loss: 2.308242\n",
      "(Iteration 57201 / 61200) loss: 2.308243\n",
      "(Iteration 57301 / 61200) loss: 2.308249\n",
      "(Epoch 75 / 80) train acc: 0.087000; val_acc: 0.083000\n",
      "(Iteration 57401 / 61200) loss: 2.308249\n",
      "(Iteration 57501 / 61200) loss: 2.308248\n",
      "(Iteration 57601 / 61200) loss: 2.308229\n",
      "(Iteration 57701 / 61200) loss: 2.308248\n",
      "(Iteration 57801 / 61200) loss: 2.308249\n",
      "(Iteration 57901 / 61200) loss: 2.308228\n",
      "(Iteration 58001 / 61200) loss: 2.308251\n",
      "(Iteration 58101 / 61200) loss: 2.308246\n",
      "(Epoch 76 / 80) train acc: 0.098000; val_acc: 0.083000\n",
      "(Iteration 58201 / 61200) loss: 2.308235\n",
      "(Iteration 58301 / 61200) loss: 2.308246\n",
      "(Iteration 58401 / 61200) loss: 2.308237\n",
      "(Iteration 58501 / 61200) loss: 2.308234\n",
      "(Iteration 58601 / 61200) loss: 2.308245\n",
      "(Iteration 58701 / 61200) loss: 2.308234\n",
      "(Iteration 58801 / 61200) loss: 2.308248\n",
      "(Iteration 58901 / 61200) loss: 2.308232\n",
      "(Epoch 77 / 80) train acc: 0.100000; val_acc: 0.083000\n",
      "(Iteration 59001 / 61200) loss: 2.308239\n",
      "(Iteration 59101 / 61200) loss: 2.308241\n",
      "(Iteration 59201 / 61200) loss: 2.308238\n",
      "(Iteration 59301 / 61200) loss: 2.308266\n",
      "(Iteration 59401 / 61200) loss: 2.308236\n",
      "(Iteration 59501 / 61200) loss: 2.308254\n",
      "(Iteration 59601 / 61200) loss: 2.308256\n",
      "(Epoch 78 / 80) train acc: 0.086000; val_acc: 0.083000\n",
      "(Iteration 59701 / 61200) loss: 2.308240\n",
      "(Iteration 59801 / 61200) loss: 2.308212\n",
      "(Iteration 59901 / 61200) loss: 2.308240\n",
      "(Iteration 60001 / 61200) loss: 2.308253\n",
      "(Iteration 60101 / 61200) loss: 2.308242\n",
      "(Iteration 60201 / 61200) loss: 2.308256\n",
      "(Iteration 60301 / 61200) loss: 2.308230\n",
      "(Iteration 60401 / 61200) loss: 2.308227\n",
      "(Epoch 79 / 80) train acc: 0.100000; val_acc: 0.083000\n",
      "(Iteration 60501 / 61200) loss: 2.308236\n",
      "(Iteration 60601 / 61200) loss: 2.308239\n",
      "(Iteration 60701 / 61200) loss: 2.308249\n",
      "(Iteration 60801 / 61200) loss: 2.308256\n",
      "(Iteration 60901 / 61200) loss: 2.308253\n",
      "(Iteration 61001 / 61200) loss: 2.308246\n",
      "(Iteration 61101 / 61200) loss: 2.308242\n",
      "(Epoch 80 / 80) train acc: 0.104000; val_acc: 0.083000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.308328\n",
      "(Epoch 0 / 80) train acc: 0.098000; val_acc: 0.082000\n",
      "(Iteration 101 / 30560) loss: 2.308340\n",
      "(Iteration 201 / 30560) loss: 2.308343\n",
      "(Iteration 301 / 30560) loss: 2.308331\n",
      "(Epoch 1 / 80) train acc: 0.077000; val_acc: 0.082000\n",
      "(Iteration 401 / 30560) loss: 2.308330\n",
      "(Iteration 501 / 30560) loss: 2.308337\n",
      "(Iteration 601 / 30560) loss: 2.308329\n",
      "(Iteration 701 / 30560) loss: 2.308328\n",
      "(Epoch 2 / 80) train acc: 0.089000; val_acc: 0.082000\n",
      "(Iteration 801 / 30560) loss: 2.308338\n",
      "(Iteration 901 / 30560) loss: 2.308341\n",
      "(Iteration 1001 / 30560) loss: 2.308344\n",
      "(Iteration 1101 / 30560) loss: 2.308339\n",
      "(Epoch 3 / 80) train acc: 0.098000; val_acc: 0.082000\n",
      "(Iteration 1201 / 30560) loss: 2.308345\n",
      "(Iteration 1301 / 30560) loss: 2.308330\n",
      "(Iteration 1401 / 30560) loss: 2.308346\n",
      "(Iteration 1501 / 30560) loss: 2.308337\n",
      "(Epoch 4 / 80) train acc: 0.102000; val_acc: 0.082000\n",
      "(Iteration 1601 / 30560) loss: 2.308337\n",
      "(Iteration 1701 / 30560) loss: 2.308341\n",
      "(Iteration 1801 / 30560) loss: 2.308341\n",
      "(Iteration 1901 / 30560) loss: 2.308329\n",
      "(Epoch 5 / 80) train acc: 0.101000; val_acc: 0.082000\n",
      "(Iteration 2001 / 30560) loss: 2.308336\n",
      "(Iteration 2101 / 30560) loss: 2.308340\n",
      "(Iteration 2201 / 30560) loss: 2.308328\n",
      "(Epoch 6 / 80) train acc: 0.105000; val_acc: 0.082000\n",
      "(Iteration 2301 / 30560) loss: 2.308343\n",
      "(Iteration 2401 / 30560) loss: 2.308343\n",
      "(Iteration 2501 / 30560) loss: 2.308332\n",
      "(Iteration 2601 / 30560) loss: 2.308337\n",
      "(Epoch 7 / 80) train acc: 0.107000; val_acc: 0.082000\n",
      "(Iteration 2701 / 30560) loss: 2.308330\n",
      "(Iteration 2801 / 30560) loss: 2.308343\n",
      "(Iteration 2901 / 30560) loss: 2.308326\n",
      "(Iteration 3001 / 30560) loss: 2.308335\n",
      "(Epoch 8 / 80) train acc: 0.103000; val_acc: 0.082000\n",
      "(Iteration 3101 / 30560) loss: 2.308345\n",
      "(Iteration 3201 / 30560) loss: 2.308323\n",
      "(Iteration 3301 / 30560) loss: 2.308333\n",
      "(Iteration 3401 / 30560) loss: 2.308332\n",
      "(Epoch 9 / 80) train acc: 0.082000; val_acc: 0.082000\n",
      "(Iteration 3501 / 30560) loss: 2.308342\n",
      "(Iteration 3601 / 30560) loss: 2.308338\n",
      "(Iteration 3701 / 30560) loss: 2.308322\n",
      "(Iteration 3801 / 30560) loss: 2.308321\n",
      "(Epoch 10 / 80) train acc: 0.097000; val_acc: 0.082000\n",
      "(Iteration 3901 / 30560) loss: 2.308333\n",
      "(Iteration 4001 / 30560) loss: 2.308333\n",
      "(Iteration 4101 / 30560) loss: 2.308338\n",
      "(Iteration 4201 / 30560) loss: 2.308335\n",
      "(Epoch 11 / 80) train acc: 0.079000; val_acc: 0.082000\n",
      "(Iteration 4301 / 30560) loss: 2.308341\n",
      "(Iteration 4401 / 30560) loss: 2.308322\n",
      "(Iteration 4501 / 30560) loss: 2.308326\n",
      "(Epoch 12 / 80) train acc: 0.093000; val_acc: 0.082000\n",
      "(Iteration 4601 / 30560) loss: 2.308341\n",
      "(Iteration 4701 / 30560) loss: 2.308315\n",
      "(Iteration 4801 / 30560) loss: 2.308326\n",
      "(Iteration 4901 / 30560) loss: 2.308341\n",
      "(Epoch 13 / 80) train acc: 0.091000; val_acc: 0.082000\n",
      "(Iteration 5001 / 30560) loss: 2.308326\n",
      "(Iteration 5101 / 30560) loss: 2.308331\n",
      "(Iteration 5201 / 30560) loss: 2.308336\n",
      "(Iteration 5301 / 30560) loss: 2.308337\n",
      "(Epoch 14 / 80) train acc: 0.095000; val_acc: 0.082000\n",
      "(Iteration 5401 / 30560) loss: 2.308343\n",
      "(Iteration 5501 / 30560) loss: 2.308335\n",
      "(Iteration 5601 / 30560) loss: 2.308339\n",
      "(Iteration 5701 / 30560) loss: 2.308332\n",
      "(Epoch 15 / 80) train acc: 0.095000; val_acc: 0.082000\n",
      "(Iteration 5801 / 30560) loss: 2.308331\n",
      "(Iteration 5901 / 30560) loss: 2.308333\n",
      "(Iteration 6001 / 30560) loss: 2.308325\n",
      "(Iteration 6101 / 30560) loss: 2.308344\n",
      "(Epoch 16 / 80) train acc: 0.108000; val_acc: 0.082000\n",
      "(Iteration 6201 / 30560) loss: 2.308325\n",
      "(Iteration 6301 / 30560) loss: 2.308337\n",
      "(Iteration 6401 / 30560) loss: 2.308338\n",
      "(Epoch 17 / 80) train acc: 0.097000; val_acc: 0.082000\n",
      "(Iteration 6501 / 30560) loss: 2.308348\n",
      "(Iteration 6601 / 30560) loss: 2.308330\n",
      "(Iteration 6701 / 30560) loss: 2.308325\n",
      "(Iteration 6801 / 30560) loss: 2.308332\n",
      "(Epoch 18 / 80) train acc: 0.102000; val_acc: 0.082000\n",
      "(Iteration 6901 / 30560) loss: 2.308329\n",
      "(Iteration 7001 / 30560) loss: 2.308331\n",
      "(Iteration 7101 / 30560) loss: 2.308346\n",
      "(Iteration 7201 / 30560) loss: 2.308332\n",
      "(Epoch 19 / 80) train acc: 0.112000; val_acc: 0.082000\n",
      "(Iteration 7301 / 30560) loss: 2.308349\n",
      "(Iteration 7401 / 30560) loss: 2.308328\n",
      "(Iteration 7501 / 30560) loss: 2.308335\n",
      "(Iteration 7601 / 30560) loss: 2.308330\n",
      "(Epoch 20 / 80) train acc: 0.070000; val_acc: 0.082000\n",
      "(Iteration 7701 / 30560) loss: 2.308338\n",
      "(Iteration 7801 / 30560) loss: 2.308323\n",
      "(Iteration 7901 / 30560) loss: 2.308331\n",
      "(Iteration 8001 / 30560) loss: 2.308329\n",
      "(Epoch 21 / 80) train acc: 0.094000; val_acc: 0.082000\n",
      "(Iteration 8101 / 30560) loss: 2.308346\n",
      "(Iteration 8201 / 30560) loss: 2.308339\n",
      "(Iteration 8301 / 30560) loss: 2.308320\n",
      "(Iteration 8401 / 30560) loss: 2.308341\n",
      "(Epoch 22 / 80) train acc: 0.091000; val_acc: 0.082000\n",
      "(Iteration 8501 / 30560) loss: 2.308327\n",
      "(Iteration 8601 / 30560) loss: 2.308324\n",
      "(Iteration 8701 / 30560) loss: 2.308324\n",
      "(Epoch 23 / 80) train acc: 0.092000; val_acc: 0.082000\n",
      "(Iteration 8801 / 30560) loss: 2.308335\n",
      "(Iteration 8901 / 30560) loss: 2.308328\n",
      "(Iteration 9001 / 30560) loss: 2.308336\n",
      "(Iteration 9101 / 30560) loss: 2.308327\n",
      "(Epoch 24 / 80) train acc: 0.117000; val_acc: 0.082000\n",
      "(Iteration 9201 / 30560) loss: 2.308331\n",
      "(Iteration 9301 / 30560) loss: 2.308335\n",
      "(Iteration 9401 / 30560) loss: 2.308333\n",
      "(Iteration 9501 / 30560) loss: 2.308333\n",
      "(Epoch 25 / 80) train acc: 0.095000; val_acc: 0.082000\n",
      "(Iteration 9601 / 30560) loss: 2.308327\n",
      "(Iteration 9701 / 30560) loss: 2.308329\n",
      "(Iteration 9801 / 30560) loss: 2.308330\n",
      "(Iteration 9901 / 30560) loss: 2.308335\n",
      "(Epoch 26 / 80) train acc: 0.103000; val_acc: 0.082000\n",
      "(Iteration 10001 / 30560) loss: 2.308344\n",
      "(Iteration 10101 / 30560) loss: 2.308327\n",
      "(Iteration 10201 / 30560) loss: 2.308334\n",
      "(Iteration 10301 / 30560) loss: 2.308337\n",
      "(Epoch 27 / 80) train acc: 0.099000; val_acc: 0.082000\n",
      "(Iteration 10401 / 30560) loss: 2.308330\n",
      "(Iteration 10501 / 30560) loss: 2.308336\n",
      "(Iteration 10601 / 30560) loss: 2.308332\n",
      "(Epoch 28 / 80) train acc: 0.094000; val_acc: 0.082000\n",
      "(Iteration 10701 / 30560) loss: 2.308328\n",
      "(Iteration 10801 / 30560) loss: 2.308326\n",
      "(Iteration 10901 / 30560) loss: 2.308339\n",
      "(Iteration 11001 / 30560) loss: 2.308340\n",
      "(Epoch 29 / 80) train acc: 0.093000; val_acc: 0.082000\n",
      "(Iteration 11101 / 30560) loss: 2.308327\n",
      "(Iteration 11201 / 30560) loss: 2.308346\n",
      "(Iteration 11301 / 30560) loss: 2.308322\n",
      "(Iteration 11401 / 30560) loss: 2.308332\n",
      "(Epoch 30 / 80) train acc: 0.090000; val_acc: 0.082000\n",
      "(Iteration 11501 / 30560) loss: 2.308314\n",
      "(Iteration 11601 / 30560) loss: 2.308333\n",
      "(Iteration 11701 / 30560) loss: 2.308329\n",
      "(Iteration 11801 / 30560) loss: 2.308337\n",
      "(Epoch 31 / 80) train acc: 0.084000; val_acc: 0.082000\n",
      "(Iteration 11901 / 30560) loss: 2.308339\n",
      "(Iteration 12001 / 30560) loss: 2.308323\n",
      "(Iteration 12101 / 30560) loss: 2.308331\n",
      "(Iteration 12201 / 30560) loss: 2.308333\n",
      "(Epoch 32 / 80) train acc: 0.094000; val_acc: 0.082000\n",
      "(Iteration 12301 / 30560) loss: 2.308330\n",
      "(Iteration 12401 / 30560) loss: 2.308325\n",
      "(Iteration 12501 / 30560) loss: 2.308346\n",
      "(Iteration 12601 / 30560) loss: 2.308332\n",
      "(Epoch 33 / 80) train acc: 0.086000; val_acc: 0.082000\n",
      "(Iteration 12701 / 30560) loss: 2.308336\n",
      "(Iteration 12801 / 30560) loss: 2.308316\n",
      "(Iteration 12901 / 30560) loss: 2.308327\n",
      "(Epoch 34 / 80) train acc: 0.083000; val_acc: 0.082000\n",
      "(Iteration 13001 / 30560) loss: 2.308327\n",
      "(Iteration 13101 / 30560) loss: 2.308322\n",
      "(Iteration 13201 / 30560) loss: 2.308331\n",
      "(Iteration 13301 / 30560) loss: 2.308333\n",
      "(Epoch 35 / 80) train acc: 0.085000; val_acc: 0.082000\n",
      "(Iteration 13401 / 30560) loss: 2.308331\n",
      "(Iteration 13501 / 30560) loss: 2.308343\n",
      "(Iteration 13601 / 30560) loss: 2.308340\n",
      "(Iteration 13701 / 30560) loss: 2.308345\n",
      "(Epoch 36 / 80) train acc: 0.109000; val_acc: 0.082000\n",
      "(Iteration 13801 / 30560) loss: 2.308330\n",
      "(Iteration 13901 / 30560) loss: 2.308331\n",
      "(Iteration 14001 / 30560) loss: 2.308330\n",
      "(Iteration 14101 / 30560) loss: 2.308340\n",
      "(Epoch 37 / 80) train acc: 0.112000; val_acc: 0.082000\n",
      "(Iteration 14201 / 30560) loss: 2.308334\n",
      "(Iteration 14301 / 30560) loss: 2.308338\n",
      "(Iteration 14401 / 30560) loss: 2.308324\n",
      "(Iteration 14501 / 30560) loss: 2.308331\n",
      "(Epoch 38 / 80) train acc: 0.097000; val_acc: 0.082000\n",
      "(Iteration 14601 / 30560) loss: 2.308339\n",
      "(Iteration 14701 / 30560) loss: 2.308332\n",
      "(Iteration 14801 / 30560) loss: 2.308332\n",
      "(Epoch 39 / 80) train acc: 0.105000; val_acc: 0.082000\n",
      "(Iteration 14901 / 30560) loss: 2.308326\n",
      "(Iteration 15001 / 30560) loss: 2.308331\n",
      "(Iteration 15101 / 30560) loss: 2.308346\n",
      "(Iteration 15201 / 30560) loss: 2.308343\n",
      "(Epoch 40 / 80) train acc: 0.102000; val_acc: 0.082000\n",
      "(Iteration 15301 / 30560) loss: 2.308327\n",
      "(Iteration 15401 / 30560) loss: 2.308319\n",
      "(Iteration 15501 / 30560) loss: 2.308329\n",
      "(Iteration 15601 / 30560) loss: 2.308330\n",
      "(Epoch 41 / 80) train acc: 0.091000; val_acc: 0.082000\n",
      "(Iteration 15701 / 30560) loss: 2.308310\n",
      "(Iteration 15801 / 30560) loss: 2.308313\n",
      "(Iteration 15901 / 30560) loss: 2.308326\n",
      "(Iteration 16001 / 30560) loss: 2.308333\n",
      "(Epoch 42 / 80) train acc: 0.094000; val_acc: 0.082000\n",
      "(Iteration 16101 / 30560) loss: 2.308327\n",
      "(Iteration 16201 / 30560) loss: 2.308338\n",
      "(Iteration 16301 / 30560) loss: 2.308328\n",
      "(Iteration 16401 / 30560) loss: 2.308334\n",
      "(Epoch 43 / 80) train acc: 0.094000; val_acc: 0.082000\n",
      "(Iteration 16501 / 30560) loss: 2.308335\n",
      "(Iteration 16601 / 30560) loss: 2.308335\n",
      "(Iteration 16701 / 30560) loss: 2.308335\n",
      "(Iteration 16801 / 30560) loss: 2.308329\n",
      "(Epoch 44 / 80) train acc: 0.098000; val_acc: 0.082000\n",
      "(Iteration 16901 / 30560) loss: 2.308340\n",
      "(Iteration 17001 / 30560) loss: 2.308328\n",
      "(Iteration 17101 / 30560) loss: 2.308341\n",
      "(Epoch 45 / 80) train acc: 0.072000; val_acc: 0.082000\n",
      "(Iteration 17201 / 30560) loss: 2.308336\n",
      "(Iteration 17301 / 30560) loss: 2.308328\n",
      "(Iteration 17401 / 30560) loss: 2.308326\n",
      "(Iteration 17501 / 30560) loss: 2.308330\n",
      "(Epoch 46 / 80) train acc: 0.104000; val_acc: 0.082000\n",
      "(Iteration 17601 / 30560) loss: 2.308337\n",
      "(Iteration 17701 / 30560) loss: 2.308321\n",
      "(Iteration 17801 / 30560) loss: 2.308330\n",
      "(Iteration 17901 / 30560) loss: 2.308326\n",
      "(Epoch 47 / 80) train acc: 0.092000; val_acc: 0.082000\n",
      "(Iteration 18001 / 30560) loss: 2.308345\n",
      "(Iteration 18101 / 30560) loss: 2.308334\n",
      "(Iteration 18201 / 30560) loss: 2.308326\n",
      "(Iteration 18301 / 30560) loss: 2.308342\n",
      "(Epoch 48 / 80) train acc: 0.095000; val_acc: 0.082000\n",
      "(Iteration 18401 / 30560) loss: 2.308325\n",
      "(Iteration 18501 / 30560) loss: 2.308343\n",
      "(Iteration 18601 / 30560) loss: 2.308329\n",
      "(Iteration 18701 / 30560) loss: 2.308326\n",
      "(Epoch 49 / 80) train acc: 0.095000; val_acc: 0.082000\n",
      "(Iteration 18801 / 30560) loss: 2.308344\n",
      "(Iteration 18901 / 30560) loss: 2.308332\n",
      "(Iteration 19001 / 30560) loss: 2.308327\n",
      "(Epoch 50 / 80) train acc: 0.091000; val_acc: 0.082000\n",
      "(Iteration 19101 / 30560) loss: 2.308334\n",
      "(Iteration 19201 / 30560) loss: 2.308332\n",
      "(Iteration 19301 / 30560) loss: 2.308326\n",
      "(Iteration 19401 / 30560) loss: 2.308317\n",
      "(Epoch 51 / 80) train acc: 0.083000; val_acc: 0.082000\n",
      "(Iteration 19501 / 30560) loss: 2.308322\n",
      "(Iteration 19601 / 30560) loss: 2.308328\n",
      "(Iteration 19701 / 30560) loss: 2.308333\n",
      "(Iteration 19801 / 30560) loss: 2.308327\n",
      "(Epoch 52 / 80) train acc: 0.103000; val_acc: 0.082000\n",
      "(Iteration 19901 / 30560) loss: 2.308339\n",
      "(Iteration 20001 / 30560) loss: 2.308322\n",
      "(Iteration 20101 / 30560) loss: 2.308336\n",
      "(Iteration 20201 / 30560) loss: 2.308314\n",
      "(Epoch 53 / 80) train acc: 0.084000; val_acc: 0.082000\n",
      "(Iteration 20301 / 30560) loss: 2.308337\n",
      "(Iteration 20401 / 30560) loss: 2.308330\n",
      "(Iteration 20501 / 30560) loss: 2.308336\n",
      "(Iteration 20601 / 30560) loss: 2.308334\n",
      "(Epoch 54 / 80) train acc: 0.093000; val_acc: 0.082000\n",
      "(Iteration 20701 / 30560) loss: 2.308327\n",
      "(Iteration 20801 / 30560) loss: 2.308331\n",
      "(Iteration 20901 / 30560) loss: 2.308337\n",
      "(Iteration 21001 / 30560) loss: 2.308331\n",
      "(Epoch 55 / 80) train acc: 0.084000; val_acc: 0.082000\n",
      "(Iteration 21101 / 30560) loss: 2.308332\n",
      "(Iteration 21201 / 30560) loss: 2.308326\n",
      "(Iteration 21301 / 30560) loss: 2.308325\n",
      "(Epoch 56 / 80) train acc: 0.083000; val_acc: 0.082000\n",
      "(Iteration 21401 / 30560) loss: 2.308323\n",
      "(Iteration 21501 / 30560) loss: 2.308329\n",
      "(Iteration 21601 / 30560) loss: 2.308333\n",
      "(Iteration 21701 / 30560) loss: 2.308320\n",
      "(Epoch 57 / 80) train acc: 0.085000; val_acc: 0.082000\n",
      "(Iteration 21801 / 30560) loss: 2.308334\n",
      "(Iteration 21901 / 30560) loss: 2.308325\n",
      "(Iteration 22001 / 30560) loss: 2.308335\n",
      "(Iteration 22101 / 30560) loss: 2.308325\n",
      "(Epoch 58 / 80) train acc: 0.112000; val_acc: 0.082000\n",
      "(Iteration 22201 / 30560) loss: 2.308326\n",
      "(Iteration 22301 / 30560) loss: 2.308324\n",
      "(Iteration 22401 / 30560) loss: 2.308339\n",
      "(Iteration 22501 / 30560) loss: 2.308329\n",
      "(Epoch 59 / 80) train acc: 0.096000; val_acc: 0.082000\n",
      "(Iteration 22601 / 30560) loss: 2.308325\n",
      "(Iteration 22701 / 30560) loss: 2.308323\n",
      "(Iteration 22801 / 30560) loss: 2.308330\n",
      "(Iteration 22901 / 30560) loss: 2.308339\n",
      "(Epoch 60 / 80) train acc: 0.107000; val_acc: 0.082000\n",
      "(Iteration 23001 / 30560) loss: 2.308318\n",
      "(Iteration 23101 / 30560) loss: 2.308325\n",
      "(Iteration 23201 / 30560) loss: 2.308334\n",
      "(Iteration 23301 / 30560) loss: 2.308331\n",
      "(Epoch 61 / 80) train acc: 0.104000; val_acc: 0.082000\n",
      "(Iteration 23401 / 30560) loss: 2.308325\n",
      "(Iteration 23501 / 30560) loss: 2.308329\n",
      "(Iteration 23601 / 30560) loss: 2.308340\n",
      "(Epoch 62 / 80) train acc: 0.077000; val_acc: 0.082000\n",
      "(Iteration 23701 / 30560) loss: 2.308318\n",
      "(Iteration 23801 / 30560) loss: 2.308339\n",
      "(Iteration 23901 / 30560) loss: 2.308321\n",
      "(Iteration 24001 / 30560) loss: 2.308335\n",
      "(Epoch 63 / 80) train acc: 0.086000; val_acc: 0.082000\n",
      "(Iteration 24101 / 30560) loss: 2.308330\n",
      "(Iteration 24201 / 30560) loss: 2.308333\n",
      "(Iteration 24301 / 30560) loss: 2.308311\n",
      "(Iteration 24401 / 30560) loss: 2.308335\n",
      "(Epoch 64 / 80) train acc: 0.086000; val_acc: 0.082000\n",
      "(Iteration 24501 / 30560) loss: 2.308332\n",
      "(Iteration 24601 / 30560) loss: 2.308325\n",
      "(Iteration 24701 / 30560) loss: 2.308326\n",
      "(Iteration 24801 / 30560) loss: 2.308336\n",
      "(Epoch 65 / 80) train acc: 0.090000; val_acc: 0.082000\n",
      "(Iteration 24901 / 30560) loss: 2.308331\n",
      "(Iteration 25001 / 30560) loss: 2.308336\n",
      "(Iteration 25101 / 30560) loss: 2.308325\n",
      "(Iteration 25201 / 30560) loss: 2.308333\n",
      "(Epoch 66 / 80) train acc: 0.101000; val_acc: 0.082000\n",
      "(Iteration 25301 / 30560) loss: 2.308324\n",
      "(Iteration 25401 / 30560) loss: 2.308339\n",
      "(Iteration 25501 / 30560) loss: 2.308331\n",
      "(Epoch 67 / 80) train acc: 0.093000; val_acc: 0.082000\n",
      "(Iteration 25601 / 30560) loss: 2.308326\n",
      "(Iteration 25701 / 30560) loss: 2.308322\n",
      "(Iteration 25801 / 30560) loss: 2.308324\n",
      "(Iteration 25901 / 30560) loss: 2.308336\n",
      "(Epoch 68 / 80) train acc: 0.085000; val_acc: 0.082000\n",
      "(Iteration 26001 / 30560) loss: 2.308334\n",
      "(Iteration 26101 / 30560) loss: 2.308333\n",
      "(Iteration 26201 / 30560) loss: 2.308339\n",
      "(Iteration 26301 / 30560) loss: 2.308320\n",
      "(Epoch 69 / 80) train acc: 0.097000; val_acc: 0.082000\n",
      "(Iteration 26401 / 30560) loss: 2.308331\n",
      "(Iteration 26501 / 30560) loss: 2.308333\n",
      "(Iteration 26601 / 30560) loss: 2.308334\n",
      "(Iteration 26701 / 30560) loss: 2.308346\n",
      "(Epoch 70 / 80) train acc: 0.092000; val_acc: 0.082000\n",
      "(Iteration 26801 / 30560) loss: 2.308340\n",
      "(Iteration 26901 / 30560) loss: 2.308333\n",
      "(Iteration 27001 / 30560) loss: 2.308326\n",
      "(Iteration 27101 / 30560) loss: 2.308329\n",
      "(Epoch 71 / 80) train acc: 0.092000; val_acc: 0.082000\n",
      "(Iteration 27201 / 30560) loss: 2.308321\n",
      "(Iteration 27301 / 30560) loss: 2.308332\n",
      "(Iteration 27401 / 30560) loss: 2.308335\n",
      "(Iteration 27501 / 30560) loss: 2.308324\n",
      "(Epoch 72 / 80) train acc: 0.108000; val_acc: 0.082000\n",
      "(Iteration 27601 / 30560) loss: 2.308325\n",
      "(Iteration 27701 / 30560) loss: 2.308339\n",
      "(Iteration 27801 / 30560) loss: 2.308335\n",
      "(Epoch 73 / 80) train acc: 0.096000; val_acc: 0.082000\n",
      "(Iteration 27901 / 30560) loss: 2.308333\n",
      "(Iteration 28001 / 30560) loss: 2.308336\n",
      "(Iteration 28101 / 30560) loss: 2.308321\n",
      "(Iteration 28201 / 30560) loss: 2.308338\n",
      "(Epoch 74 / 80) train acc: 0.086000; val_acc: 0.082000\n",
      "(Iteration 28301 / 30560) loss: 2.308331\n",
      "(Iteration 28401 / 30560) loss: 2.308331\n",
      "(Iteration 28501 / 30560) loss: 2.308338\n",
      "(Iteration 28601 / 30560) loss: 2.308334\n",
      "(Epoch 75 / 80) train acc: 0.094000; val_acc: 0.082000\n",
      "(Iteration 28701 / 30560) loss: 2.308336\n",
      "(Iteration 28801 / 30560) loss: 2.308343\n",
      "(Iteration 28901 / 30560) loss: 2.308322\n",
      "(Iteration 29001 / 30560) loss: 2.308328\n",
      "(Epoch 76 / 80) train acc: 0.091000; val_acc: 0.082000\n",
      "(Iteration 29101 / 30560) loss: 2.308337\n",
      "(Iteration 29201 / 30560) loss: 2.308331\n",
      "(Iteration 29301 / 30560) loss: 2.308343\n",
      "(Iteration 29401 / 30560) loss: 2.308319\n",
      "(Epoch 77 / 80) train acc: 0.095000; val_acc: 0.082000\n",
      "(Iteration 29501 / 30560) loss: 2.308340\n",
      "(Iteration 29601 / 30560) loss: 2.308324\n",
      "(Iteration 29701 / 30560) loss: 2.308336\n",
      "(Epoch 78 / 80) train acc: 0.099000; val_acc: 0.082000\n",
      "(Iteration 29801 / 30560) loss: 2.308333\n",
      "(Iteration 29901 / 30560) loss: 2.308329\n",
      "(Iteration 30001 / 30560) loss: 2.308327\n",
      "(Iteration 30101 / 30560) loss: 2.308330\n",
      "(Epoch 79 / 80) train acc: 0.093000; val_acc: 0.082000\n",
      "(Iteration 30201 / 30560) loss: 2.308330\n",
      "(Iteration 30301 / 30560) loss: 2.308324\n",
      "(Iteration 30401 / 30560) loss: 2.308324\n",
      "(Iteration 30501 / 30560) loss: 2.308328\n",
      "(Epoch 80 / 80) train acc: 0.107000; val_acc: 0.082000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.306659\n",
      "(Epoch 0 / 100) train acc: 0.097000; val_acc: 0.095000\n",
      "(Iteration 101 / 76500) loss: 2.306675\n",
      "(Iteration 201 / 76500) loss: 2.306678\n",
      "(Iteration 301 / 76500) loss: 2.306661\n",
      "(Iteration 401 / 76500) loss: 2.306657\n",
      "(Iteration 501 / 76500) loss: 2.306666\n",
      "(Iteration 601 / 76500) loss: 2.306658\n",
      "(Iteration 701 / 76500) loss: 2.306658\n",
      "(Epoch 1 / 100) train acc: 0.093000; val_acc: 0.095000\n",
      "(Iteration 801 / 76500) loss: 2.306659\n",
      "(Iteration 901 / 76500) loss: 2.306643\n",
      "(Iteration 1001 / 76500) loss: 2.306655\n",
      "(Iteration 1101 / 76500) loss: 2.306666\n",
      "(Iteration 1201 / 76500) loss: 2.306651\n",
      "(Iteration 1301 / 76500) loss: 2.306656\n",
      "(Iteration 1401 / 76500) loss: 2.306650\n",
      "(Iteration 1501 / 76500) loss: 2.306651\n",
      "(Epoch 2 / 100) train acc: 0.100000; val_acc: 0.096000\n",
      "(Iteration 1601 / 76500) loss: 2.306660\n",
      "(Iteration 1701 / 76500) loss: 2.306652\n",
      "(Iteration 1801 / 76500) loss: 2.306666\n",
      "(Iteration 1901 / 76500) loss: 2.306663\n",
      "(Iteration 2001 / 76500) loss: 2.306669\n",
      "(Iteration 2101 / 76500) loss: 2.306660\n",
      "(Iteration 2201 / 76500) loss: 2.306662\n",
      "(Epoch 3 / 100) train acc: 0.099000; val_acc: 0.095000\n",
      "(Iteration 2301 / 76500) loss: 2.306650\n",
      "(Iteration 2401 / 76500) loss: 2.306657\n",
      "(Iteration 2501 / 76500) loss: 2.306645\n",
      "(Iteration 2601 / 76500) loss: 2.306650\n",
      "(Iteration 2701 / 76500) loss: 2.306668\n",
      "(Iteration 2801 / 76500) loss: 2.306651\n",
      "(Iteration 2901 / 76500) loss: 2.306652\n",
      "(Iteration 3001 / 76500) loss: 2.306651\n",
      "(Epoch 4 / 100) train acc: 0.097000; val_acc: 0.096000\n",
      "(Iteration 3101 / 76500) loss: 2.306658\n",
      "(Iteration 3201 / 76500) loss: 2.306661\n",
      "(Iteration 3301 / 76500) loss: 2.306663\n",
      "(Iteration 3401 / 76500) loss: 2.306658\n",
      "(Iteration 3501 / 76500) loss: 2.306663\n",
      "(Iteration 3601 / 76500) loss: 2.306675\n",
      "(Iteration 3701 / 76500) loss: 2.306658\n",
      "(Iteration 3801 / 76500) loss: 2.306657\n",
      "(Epoch 5 / 100) train acc: 0.086000; val_acc: 0.095000\n",
      "(Iteration 3901 / 76500) loss: 2.306662\n",
      "(Iteration 4001 / 76500) loss: 2.306644\n",
      "(Iteration 4101 / 76500) loss: 2.306656\n",
      "(Iteration 4201 / 76500) loss: 2.306657\n",
      "(Iteration 4301 / 76500) loss: 2.306651\n",
      "(Iteration 4401 / 76500) loss: 2.306645\n",
      "(Iteration 4501 / 76500) loss: 2.306647\n",
      "(Epoch 6 / 100) train acc: 0.079000; val_acc: 0.096000\n",
      "(Iteration 4601 / 76500) loss: 2.306639\n",
      "(Iteration 4701 / 76500) loss: 2.306646\n",
      "(Iteration 4801 / 76500) loss: 2.306666\n",
      "(Iteration 4901 / 76500) loss: 2.306656\n",
      "(Iteration 5001 / 76500) loss: 2.306653\n",
      "(Iteration 5101 / 76500) loss: 2.306650\n",
      "(Iteration 5201 / 76500) loss: 2.306662\n",
      "(Iteration 5301 / 76500) loss: 2.306636\n",
      "(Epoch 7 / 100) train acc: 0.093000; val_acc: 0.095000\n",
      "(Iteration 5401 / 76500) loss: 2.306648\n",
      "(Iteration 5501 / 76500) loss: 2.306670\n",
      "(Iteration 5601 / 76500) loss: 2.306673\n",
      "(Iteration 5701 / 76500) loss: 2.306668\n",
      "(Iteration 5801 / 76500) loss: 2.306659\n",
      "(Iteration 5901 / 76500) loss: 2.306653\n",
      "(Iteration 6001 / 76500) loss: 2.306636\n",
      "(Iteration 6101 / 76500) loss: 2.306654\n",
      "(Epoch 8 / 100) train acc: 0.079000; val_acc: 0.095000\n",
      "(Iteration 6201 / 76500) loss: 2.306676\n",
      "(Iteration 6301 / 76500) loss: 2.306662\n",
      "(Iteration 6401 / 76500) loss: 2.306663\n",
      "(Iteration 6501 / 76500) loss: 2.306658\n",
      "(Iteration 6601 / 76500) loss: 2.306667\n",
      "(Iteration 6701 / 76500) loss: 2.306654\n",
      "(Iteration 6801 / 76500) loss: 2.306655\n",
      "(Epoch 9 / 100) train acc: 0.097000; val_acc: 0.095000\n",
      "(Iteration 6901 / 76500) loss: 2.306649\n",
      "(Iteration 7001 / 76500) loss: 2.306668\n",
      "(Iteration 7101 / 76500) loss: 2.306659\n",
      "(Iteration 7201 / 76500) loss: 2.306655\n",
      "(Iteration 7301 / 76500) loss: 2.306674\n",
      "(Iteration 7401 / 76500) loss: 2.306659\n",
      "(Iteration 7501 / 76500) loss: 2.306651\n",
      "(Iteration 7601 / 76500) loss: 2.306639\n",
      "(Epoch 10 / 100) train acc: 0.096000; val_acc: 0.095000\n",
      "(Iteration 7701 / 76500) loss: 2.306669\n",
      "(Iteration 7801 / 76500) loss: 2.306663\n",
      "(Iteration 7901 / 76500) loss: 2.306651\n",
      "(Iteration 8001 / 76500) loss: 2.306664\n",
      "(Iteration 8101 / 76500) loss: 2.306651\n",
      "(Iteration 8201 / 76500) loss: 2.306672\n",
      "(Iteration 8301 / 76500) loss: 2.306659\n",
      "(Iteration 8401 / 76500) loss: 2.306660\n",
      "(Epoch 11 / 100) train acc: 0.098000; val_acc: 0.095000\n",
      "(Iteration 8501 / 76500) loss: 2.306672\n",
      "(Iteration 8601 / 76500) loss: 2.306649\n",
      "(Iteration 8701 / 76500) loss: 2.306664\n",
      "(Iteration 8801 / 76500) loss: 2.306669\n",
      "(Iteration 8901 / 76500) loss: 2.306661\n",
      "(Iteration 9001 / 76500) loss: 2.306661\n",
      "(Iteration 9101 / 76500) loss: 2.306655\n",
      "(Epoch 12 / 100) train acc: 0.074000; val_acc: 0.095000\n",
      "(Iteration 9201 / 76500) loss: 2.306661\n",
      "(Iteration 9301 / 76500) loss: 2.306656\n",
      "(Iteration 9401 / 76500) loss: 2.306655\n",
      "(Iteration 9501 / 76500) loss: 2.306662\n",
      "(Iteration 9601 / 76500) loss: 2.306660\n",
      "(Iteration 9701 / 76500) loss: 2.306662\n",
      "(Iteration 9801 / 76500) loss: 2.306660\n",
      "(Iteration 9901 / 76500) loss: 2.306663\n",
      "(Epoch 13 / 100) train acc: 0.084000; val_acc: 0.095000\n",
      "(Iteration 10001 / 76500) loss: 2.306691\n",
      "(Iteration 10101 / 76500) loss: 2.306643\n",
      "(Iteration 10201 / 76500) loss: 2.306656\n",
      "(Iteration 10301 / 76500) loss: 2.306654\n",
      "(Iteration 10401 / 76500) loss: 2.306670\n",
      "(Iteration 10501 / 76500) loss: 2.306667\n",
      "(Iteration 10601 / 76500) loss: 2.306663\n",
      "(Iteration 10701 / 76500) loss: 2.306657\n",
      "(Epoch 14 / 100) train acc: 0.092000; val_acc: 0.095000\n",
      "(Iteration 10801 / 76500) loss: 2.306657\n",
      "(Iteration 10901 / 76500) loss: 2.306643\n",
      "(Iteration 11001 / 76500) loss: 2.306673\n",
      "(Iteration 11101 / 76500) loss: 2.306658\n",
      "(Iteration 11201 / 76500) loss: 2.306642\n",
      "(Iteration 11301 / 76500) loss: 2.306651\n",
      "(Iteration 11401 / 76500) loss: 2.306659\n",
      "(Epoch 15 / 100) train acc: 0.111000; val_acc: 0.095000\n",
      "(Iteration 11501 / 76500) loss: 2.306659\n",
      "(Iteration 11601 / 76500) loss: 2.306652\n",
      "(Iteration 11701 / 76500) loss: 2.306665\n",
      "(Iteration 11801 / 76500) loss: 2.306661\n",
      "(Iteration 11901 / 76500) loss: 2.306666\n",
      "(Iteration 12001 / 76500) loss: 2.306641\n",
      "(Iteration 12101 / 76500) loss: 2.306658\n",
      "(Iteration 12201 / 76500) loss: 2.306660\n",
      "(Epoch 16 / 100) train acc: 0.092000; val_acc: 0.095000\n",
      "(Iteration 12301 / 76500) loss: 2.306646\n",
      "(Iteration 12401 / 76500) loss: 2.306665\n",
      "(Iteration 12501 / 76500) loss: 2.306656\n",
      "(Iteration 12601 / 76500) loss: 2.306637\n",
      "(Iteration 12701 / 76500) loss: 2.306656\n",
      "(Iteration 12801 / 76500) loss: 2.306656\n",
      "(Iteration 12901 / 76500) loss: 2.306676\n",
      "(Iteration 13001 / 76500) loss: 2.306660\n",
      "(Epoch 17 / 100) train acc: 0.093000; val_acc: 0.095000\n",
      "(Iteration 13101 / 76500) loss: 2.306657\n",
      "(Iteration 13201 / 76500) loss: 2.306661\n",
      "(Iteration 13301 / 76500) loss: 2.306654\n",
      "(Iteration 13401 / 76500) loss: 2.306652\n",
      "(Iteration 13501 / 76500) loss: 2.306658\n",
      "(Iteration 13601 / 76500) loss: 2.306653\n",
      "(Iteration 13701 / 76500) loss: 2.306655\n",
      "(Epoch 18 / 100) train acc: 0.097000; val_acc: 0.095000\n",
      "(Iteration 13801 / 76500) loss: 2.306659\n",
      "(Iteration 13901 / 76500) loss: 2.306659\n",
      "(Iteration 14001 / 76500) loss: 2.306653\n",
      "(Iteration 14101 / 76500) loss: 2.306653\n",
      "(Iteration 14201 / 76500) loss: 2.306652\n",
      "(Iteration 14301 / 76500) loss: 2.306646\n",
      "(Iteration 14401 / 76500) loss: 2.306654\n",
      "(Iteration 14501 / 76500) loss: 2.306648\n",
      "(Epoch 19 / 100) train acc: 0.088000; val_acc: 0.095000\n",
      "(Iteration 14601 / 76500) loss: 2.306664\n",
      "(Iteration 14701 / 76500) loss: 2.306642\n",
      "(Iteration 14801 / 76500) loss: 2.306668\n",
      "(Iteration 14901 / 76500) loss: 2.306658\n",
      "(Iteration 15001 / 76500) loss: 2.306650\n",
      "(Iteration 15101 / 76500) loss: 2.306674\n",
      "(Iteration 15201 / 76500) loss: 2.306657\n",
      "(Epoch 20 / 100) train acc: 0.081000; val_acc: 0.095000\n",
      "(Iteration 15301 / 76500) loss: 2.306671\n",
      "(Iteration 15401 / 76500) loss: 2.306648\n",
      "(Iteration 15501 / 76500) loss: 2.306658\n",
      "(Iteration 15601 / 76500) loss: 2.306665\n",
      "(Iteration 15701 / 76500) loss: 2.306662\n",
      "(Iteration 15801 / 76500) loss: 2.306650\n",
      "(Iteration 15901 / 76500) loss: 2.306667\n",
      "(Iteration 16001 / 76500) loss: 2.306651\n",
      "(Epoch 21 / 100) train acc: 0.095000; val_acc: 0.095000\n",
      "(Iteration 16101 / 76500) loss: 2.306657\n",
      "(Iteration 16201 / 76500) loss: 2.306657\n",
      "(Iteration 16301 / 76500) loss: 2.306653\n",
      "(Iteration 16401 / 76500) loss: 2.306664\n",
      "(Iteration 16501 / 76500) loss: 2.306654\n",
      "(Iteration 16601 / 76500) loss: 2.306669\n",
      "(Iteration 16701 / 76500) loss: 2.306638\n",
      "(Iteration 16801 / 76500) loss: 2.306664\n",
      "(Epoch 22 / 100) train acc: 0.088000; val_acc: 0.095000\n",
      "(Iteration 16901 / 76500) loss: 2.306660\n",
      "(Iteration 17001 / 76500) loss: 2.306661\n",
      "(Iteration 17101 / 76500) loss: 2.306653\n",
      "(Iteration 17201 / 76500) loss: 2.306653\n",
      "(Iteration 17301 / 76500) loss: 2.306659\n",
      "(Iteration 17401 / 76500) loss: 2.306662\n",
      "(Iteration 17501 / 76500) loss: 2.306634\n",
      "(Epoch 23 / 100) train acc: 0.096000; val_acc: 0.095000\n",
      "(Iteration 17601 / 76500) loss: 2.306659\n",
      "(Iteration 17701 / 76500) loss: 2.306648\n",
      "(Iteration 17801 / 76500) loss: 2.306660\n",
      "(Iteration 17901 / 76500) loss: 2.306657\n",
      "(Iteration 18001 / 76500) loss: 2.306655\n",
      "(Iteration 18101 / 76500) loss: 2.306659\n",
      "(Iteration 18201 / 76500) loss: 2.306663\n",
      "(Iteration 18301 / 76500) loss: 2.306647\n",
      "(Epoch 24 / 100) train acc: 0.078000; val_acc: 0.095000\n",
      "(Iteration 18401 / 76500) loss: 2.306662\n",
      "(Iteration 18501 / 76500) loss: 2.306662\n",
      "(Iteration 18601 / 76500) loss: 2.306670\n",
      "(Iteration 18701 / 76500) loss: 2.306647\n",
      "(Iteration 18801 / 76500) loss: 2.306641\n",
      "(Iteration 18901 / 76500) loss: 2.306657\n",
      "(Iteration 19001 / 76500) loss: 2.306670\n",
      "(Iteration 19101 / 76500) loss: 2.306658\n",
      "(Epoch 25 / 100) train acc: 0.096000; val_acc: 0.095000\n",
      "(Iteration 19201 / 76500) loss: 2.306666\n",
      "(Iteration 19301 / 76500) loss: 2.306664\n",
      "(Iteration 19401 / 76500) loss: 2.306665\n",
      "(Iteration 19501 / 76500) loss: 2.306650\n",
      "(Iteration 19601 / 76500) loss: 2.306653\n",
      "(Iteration 19701 / 76500) loss: 2.306642\n",
      "(Iteration 19801 / 76500) loss: 2.306679\n",
      "(Epoch 26 / 100) train acc: 0.095000; val_acc: 0.095000\n",
      "(Iteration 19901 / 76500) loss: 2.306666\n",
      "(Iteration 20001 / 76500) loss: 2.306659\n",
      "(Iteration 20101 / 76500) loss: 2.306654\n",
      "(Iteration 20201 / 76500) loss: 2.306659\n",
      "(Iteration 20301 / 76500) loss: 2.306658\n",
      "(Iteration 20401 / 76500) loss: 2.306666\n",
      "(Iteration 20501 / 76500) loss: 2.306651\n",
      "(Iteration 20601 / 76500) loss: 2.306674\n",
      "(Epoch 27 / 100) train acc: 0.090000; val_acc: 0.095000\n",
      "(Iteration 20701 / 76500) loss: 2.306655\n",
      "(Iteration 20801 / 76500) loss: 2.306661\n",
      "(Iteration 20901 / 76500) loss: 2.306645\n",
      "(Iteration 21001 / 76500) loss: 2.306641\n",
      "(Iteration 21101 / 76500) loss: 2.306677\n",
      "(Iteration 21201 / 76500) loss: 2.306652\n",
      "(Iteration 21301 / 76500) loss: 2.306652\n",
      "(Iteration 21401 / 76500) loss: 2.306644\n",
      "(Epoch 28 / 100) train acc: 0.089000; val_acc: 0.095000\n",
      "(Iteration 21501 / 76500) loss: 2.306659\n",
      "(Iteration 21601 / 76500) loss: 2.306651\n",
      "(Iteration 21701 / 76500) loss: 2.306653\n",
      "(Iteration 21801 / 76500) loss: 2.306652\n",
      "(Iteration 21901 / 76500) loss: 2.306648\n",
      "(Iteration 22001 / 76500) loss: 2.306653\n",
      "(Iteration 22101 / 76500) loss: 2.306646\n",
      "(Epoch 29 / 100) train acc: 0.088000; val_acc: 0.095000\n",
      "(Iteration 22201 / 76500) loss: 2.306653\n",
      "(Iteration 22301 / 76500) loss: 2.306662\n",
      "(Iteration 22401 / 76500) loss: 2.306654\n",
      "(Iteration 22501 / 76500) loss: 2.306647\n",
      "(Iteration 22601 / 76500) loss: 2.306646\n",
      "(Iteration 22701 / 76500) loss: 2.306654\n",
      "(Iteration 22801 / 76500) loss: 2.306658\n",
      "(Iteration 22901 / 76500) loss: 2.306662\n",
      "(Epoch 30 / 100) train acc: 0.096000; val_acc: 0.095000\n",
      "(Iteration 23001 / 76500) loss: 2.306654\n",
      "(Iteration 23101 / 76500) loss: 2.306666\n",
      "(Iteration 23201 / 76500) loss: 2.306669\n",
      "(Iteration 23301 / 76500) loss: 2.306653\n",
      "(Iteration 23401 / 76500) loss: 2.306652\n",
      "(Iteration 23501 / 76500) loss: 2.306667\n",
      "(Iteration 23601 / 76500) loss: 2.306670\n",
      "(Iteration 23701 / 76500) loss: 2.306662\n",
      "(Epoch 31 / 100) train acc: 0.098000; val_acc: 0.095000\n",
      "(Iteration 23801 / 76500) loss: 2.306669\n",
      "(Iteration 23901 / 76500) loss: 2.306640\n",
      "(Iteration 24001 / 76500) loss: 2.306648\n",
      "(Iteration 24101 / 76500) loss: 2.306652\n",
      "(Iteration 24201 / 76500) loss: 2.306664\n",
      "(Iteration 24301 / 76500) loss: 2.306656\n",
      "(Iteration 24401 / 76500) loss: 2.306661\n",
      "(Epoch 32 / 100) train acc: 0.100000; val_acc: 0.095000\n",
      "(Iteration 24501 / 76500) loss: 2.306643\n",
      "(Iteration 24601 / 76500) loss: 2.306673\n",
      "(Iteration 24701 / 76500) loss: 2.306645\n",
      "(Iteration 24801 / 76500) loss: 2.306660\n",
      "(Iteration 24901 / 76500) loss: 2.306654\n",
      "(Iteration 25001 / 76500) loss: 2.306662\n",
      "(Iteration 25101 / 76500) loss: 2.306652\n",
      "(Iteration 25201 / 76500) loss: 2.306666\n",
      "(Epoch 33 / 100) train acc: 0.104000; val_acc: 0.095000\n",
      "(Iteration 25301 / 76500) loss: 2.306653\n",
      "(Iteration 25401 / 76500) loss: 2.306658\n",
      "(Iteration 25501 / 76500) loss: 2.306662\n",
      "(Iteration 25601 / 76500) loss: 2.306681\n",
      "(Iteration 25701 / 76500) loss: 2.306658\n",
      "(Iteration 25801 / 76500) loss: 2.306658\n",
      "(Iteration 25901 / 76500) loss: 2.306660\n",
      "(Iteration 26001 / 76500) loss: 2.306661\n",
      "(Epoch 34 / 100) train acc: 0.095000; val_acc: 0.095000\n",
      "(Iteration 26101 / 76500) loss: 2.306676\n",
      "(Iteration 26201 / 76500) loss: 2.306659\n",
      "(Iteration 26301 / 76500) loss: 2.306683\n",
      "(Iteration 26401 / 76500) loss: 2.306649\n",
      "(Iteration 26501 / 76500) loss: 2.306655\n",
      "(Iteration 26601 / 76500) loss: 2.306655\n",
      "(Iteration 26701 / 76500) loss: 2.306643\n",
      "(Epoch 35 / 100) train acc: 0.086000; val_acc: 0.095000\n",
      "(Iteration 26801 / 76500) loss: 2.306656\n",
      "(Iteration 26901 / 76500) loss: 2.306657\n",
      "(Iteration 27001 / 76500) loss: 2.306649\n",
      "(Iteration 27101 / 76500) loss: 2.306664\n",
      "(Iteration 27201 / 76500) loss: 2.306662\n",
      "(Iteration 27301 / 76500) loss: 2.306646\n",
      "(Iteration 27401 / 76500) loss: 2.306656\n",
      "(Iteration 27501 / 76500) loss: 2.306663\n",
      "(Epoch 36 / 100) train acc: 0.098000; val_acc: 0.095000\n",
      "(Iteration 27601 / 76500) loss: 2.306645\n",
      "(Iteration 27701 / 76500) loss: 2.306657\n",
      "(Iteration 27801 / 76500) loss: 2.306659\n",
      "(Iteration 27901 / 76500) loss: 2.306654\n",
      "(Iteration 28001 / 76500) loss: 2.306653\n",
      "(Iteration 28101 / 76500) loss: 2.306658\n",
      "(Iteration 28201 / 76500) loss: 2.306661\n",
      "(Iteration 28301 / 76500) loss: 2.306666\n",
      "(Epoch 37 / 100) train acc: 0.084000; val_acc: 0.095000\n",
      "(Iteration 28401 / 76500) loss: 2.306655\n",
      "(Iteration 28501 / 76500) loss: 2.306655\n",
      "(Iteration 28601 / 76500) loss: 2.306650\n",
      "(Iteration 28701 / 76500) loss: 2.306653\n",
      "(Iteration 28801 / 76500) loss: 2.306641\n",
      "(Iteration 28901 / 76500) loss: 2.306655\n",
      "(Iteration 29001 / 76500) loss: 2.306659\n",
      "(Epoch 38 / 100) train acc: 0.093000; val_acc: 0.095000\n",
      "(Iteration 29101 / 76500) loss: 2.306666\n",
      "(Iteration 29201 / 76500) loss: 2.306654\n",
      "(Iteration 29301 / 76500) loss: 2.306647\n",
      "(Iteration 29401 / 76500) loss: 2.306655\n",
      "(Iteration 29501 / 76500) loss: 2.306664\n",
      "(Iteration 29601 / 76500) loss: 2.306646\n",
      "(Iteration 29701 / 76500) loss: 2.306669\n",
      "(Iteration 29801 / 76500) loss: 2.306651\n",
      "(Epoch 39 / 100) train acc: 0.091000; val_acc: 0.095000\n",
      "(Iteration 29901 / 76500) loss: 2.306647\n",
      "(Iteration 30001 / 76500) loss: 2.306647\n",
      "(Iteration 30101 / 76500) loss: 2.306651\n",
      "(Iteration 30201 / 76500) loss: 2.306669\n",
      "(Iteration 30301 / 76500) loss: 2.306663\n",
      "(Iteration 30401 / 76500) loss: 2.306652\n",
      "(Iteration 30501 / 76500) loss: 2.306657\n",
      "(Epoch 40 / 100) train acc: 0.103000; val_acc: 0.095000\n",
      "(Iteration 30601 / 76500) loss: 2.306648\n",
      "(Iteration 30701 / 76500) loss: 2.306661\n",
      "(Iteration 30801 / 76500) loss: 2.306662\n",
      "(Iteration 30901 / 76500) loss: 2.306667\n",
      "(Iteration 31001 / 76500) loss: 2.306659\n",
      "(Iteration 31101 / 76500) loss: 2.306654\n",
      "(Iteration 31201 / 76500) loss: 2.306653\n",
      "(Iteration 31301 / 76500) loss: 2.306653\n",
      "(Epoch 41 / 100) train acc: 0.095000; val_acc: 0.095000\n",
      "(Iteration 31401 / 76500) loss: 2.306653\n",
      "(Iteration 31501 / 76500) loss: 2.306646\n",
      "(Iteration 31601 / 76500) loss: 2.306640\n",
      "(Iteration 31701 / 76500) loss: 2.306658\n",
      "(Iteration 31801 / 76500) loss: 2.306648\n",
      "(Iteration 31901 / 76500) loss: 2.306655\n",
      "(Iteration 32001 / 76500) loss: 2.306659\n",
      "(Iteration 32101 / 76500) loss: 2.306663\n",
      "(Epoch 42 / 100) train acc: 0.102000; val_acc: 0.095000\n",
      "(Iteration 32201 / 76500) loss: 2.306656\n",
      "(Iteration 32301 / 76500) loss: 2.306640\n",
      "(Iteration 32401 / 76500) loss: 2.306658\n",
      "(Iteration 32501 / 76500) loss: 2.306653\n",
      "(Iteration 32601 / 76500) loss: 2.306649\n",
      "(Iteration 32701 / 76500) loss: 2.306659\n",
      "(Iteration 32801 / 76500) loss: 2.306648\n",
      "(Epoch 43 / 100) train acc: 0.077000; val_acc: 0.095000\n",
      "(Iteration 32901 / 76500) loss: 2.306662\n",
      "(Iteration 33001 / 76500) loss: 2.306654\n",
      "(Iteration 33101 / 76500) loss: 2.306662\n",
      "(Iteration 33201 / 76500) loss: 2.306670\n",
      "(Iteration 33301 / 76500) loss: 2.306653\n",
      "(Iteration 33401 / 76500) loss: 2.306667\n",
      "(Iteration 33501 / 76500) loss: 2.306649\n",
      "(Iteration 33601 / 76500) loss: 2.306645\n",
      "(Epoch 44 / 100) train acc: 0.103000; val_acc: 0.095000\n",
      "(Iteration 33701 / 76500) loss: 2.306667\n",
      "(Iteration 33801 / 76500) loss: 2.306655\n",
      "(Iteration 33901 / 76500) loss: 2.306672\n",
      "(Iteration 34001 / 76500) loss: 2.306669\n",
      "(Iteration 34101 / 76500) loss: 2.306664\n",
      "(Iteration 34201 / 76500) loss: 2.306662\n",
      "(Iteration 34301 / 76500) loss: 2.306650\n",
      "(Iteration 34401 / 76500) loss: 2.306671\n",
      "(Epoch 45 / 100) train acc: 0.093000; val_acc: 0.095000\n",
      "(Iteration 34501 / 76500) loss: 2.306656\n",
      "(Iteration 34601 / 76500) loss: 2.306665\n",
      "(Iteration 34701 / 76500) loss: 2.306648\n",
      "(Iteration 34801 / 76500) loss: 2.306659\n",
      "(Iteration 34901 / 76500) loss: 2.306655\n",
      "(Iteration 35001 / 76500) loss: 2.306675\n",
      "(Iteration 35101 / 76500) loss: 2.306662\n",
      "(Epoch 46 / 100) train acc: 0.095000; val_acc: 0.095000\n",
      "(Iteration 35201 / 76500) loss: 2.306661\n",
      "(Iteration 35301 / 76500) loss: 2.306633\n",
      "(Iteration 35401 / 76500) loss: 2.306653\n",
      "(Iteration 35501 / 76500) loss: 2.306667\n",
      "(Iteration 35601 / 76500) loss: 2.306649\n",
      "(Iteration 35701 / 76500) loss: 2.306659\n",
      "(Iteration 35801 / 76500) loss: 2.306673\n",
      "(Iteration 35901 / 76500) loss: 2.306671\n",
      "(Epoch 47 / 100) train acc: 0.109000; val_acc: 0.095000\n",
      "(Iteration 36001 / 76500) loss: 2.306670\n",
      "(Iteration 36101 / 76500) loss: 2.306677\n",
      "(Iteration 36201 / 76500) loss: 2.306659\n",
      "(Iteration 36301 / 76500) loss: 2.306661\n",
      "(Iteration 36401 / 76500) loss: 2.306658\n",
      "(Iteration 36501 / 76500) loss: 2.306658\n",
      "(Iteration 36601 / 76500) loss: 2.306646\n",
      "(Iteration 36701 / 76500) loss: 2.306652\n",
      "(Epoch 48 / 100) train acc: 0.092000; val_acc: 0.095000\n",
      "(Iteration 36801 / 76500) loss: 2.306655\n",
      "(Iteration 36901 / 76500) loss: 2.306670\n",
      "(Iteration 37001 / 76500) loss: 2.306659\n",
      "(Iteration 37101 / 76500) loss: 2.306666\n",
      "(Iteration 37201 / 76500) loss: 2.306650\n",
      "(Iteration 37301 / 76500) loss: 2.306655\n",
      "(Iteration 37401 / 76500) loss: 2.306645\n",
      "(Epoch 49 / 100) train acc: 0.087000; val_acc: 0.095000\n",
      "(Iteration 37501 / 76500) loss: 2.306656\n",
      "(Iteration 37601 / 76500) loss: 2.306660\n",
      "(Iteration 37701 / 76500) loss: 2.306665\n",
      "(Iteration 37801 / 76500) loss: 2.306649\n",
      "(Iteration 37901 / 76500) loss: 2.306660\n",
      "(Iteration 38001 / 76500) loss: 2.306646\n",
      "(Iteration 38101 / 76500) loss: 2.306652\n",
      "(Iteration 38201 / 76500) loss: 2.306663\n",
      "(Epoch 50 / 100) train acc: 0.098000; val_acc: 0.095000\n",
      "(Iteration 38301 / 76500) loss: 2.306664\n",
      "(Iteration 38401 / 76500) loss: 2.306665\n",
      "(Iteration 38501 / 76500) loss: 2.306663\n",
      "(Iteration 38601 / 76500) loss: 2.306633\n",
      "(Iteration 38701 / 76500) loss: 2.306657\n",
      "(Iteration 38801 / 76500) loss: 2.306663\n",
      "(Iteration 38901 / 76500) loss: 2.306658\n",
      "(Iteration 39001 / 76500) loss: 2.306656\n",
      "(Epoch 51 / 100) train acc: 0.087000; val_acc: 0.095000\n",
      "(Iteration 39101 / 76500) loss: 2.306671\n",
      "(Iteration 39201 / 76500) loss: 2.306659\n",
      "(Iteration 39301 / 76500) loss: 2.306646\n",
      "(Iteration 39401 / 76500) loss: 2.306665\n",
      "(Iteration 39501 / 76500) loss: 2.306673\n",
      "(Iteration 39601 / 76500) loss: 2.306644\n",
      "(Iteration 39701 / 76500) loss: 2.306662\n",
      "(Epoch 52 / 100) train acc: 0.097000; val_acc: 0.095000\n",
      "(Iteration 39801 / 76500) loss: 2.306667\n",
      "(Iteration 39901 / 76500) loss: 2.306661\n",
      "(Iteration 40001 / 76500) loss: 2.306639\n",
      "(Iteration 40101 / 76500) loss: 2.306662\n",
      "(Iteration 40201 / 76500) loss: 2.306657\n",
      "(Iteration 40301 / 76500) loss: 2.306651\n",
      "(Iteration 40401 / 76500) loss: 2.306654\n",
      "(Iteration 40501 / 76500) loss: 2.306652\n",
      "(Epoch 53 / 100) train acc: 0.081000; val_acc: 0.095000\n",
      "(Iteration 40601 / 76500) loss: 2.306666\n",
      "(Iteration 40701 / 76500) loss: 2.306655\n",
      "(Iteration 40801 / 76500) loss: 2.306667\n",
      "(Iteration 40901 / 76500) loss: 2.306663\n",
      "(Iteration 41001 / 76500) loss: 2.306649\n",
      "(Iteration 41101 / 76500) loss: 2.306659\n",
      "(Iteration 41201 / 76500) loss: 2.306657\n",
      "(Iteration 41301 / 76500) loss: 2.306657\n",
      "(Epoch 54 / 100) train acc: 0.075000; val_acc: 0.095000\n",
      "(Iteration 41401 / 76500) loss: 2.306662\n",
      "(Iteration 41501 / 76500) loss: 2.306649\n",
      "(Iteration 41601 / 76500) loss: 2.306655\n",
      "(Iteration 41701 / 76500) loss: 2.306650\n",
      "(Iteration 41801 / 76500) loss: 2.306660\n",
      "(Iteration 41901 / 76500) loss: 2.306651\n",
      "(Iteration 42001 / 76500) loss: 2.306650\n",
      "(Epoch 55 / 100) train acc: 0.086000; val_acc: 0.095000\n",
      "(Iteration 42101 / 76500) loss: 2.306664\n",
      "(Iteration 42201 / 76500) loss: 2.306655\n",
      "(Iteration 42301 / 76500) loss: 2.306663\n",
      "(Iteration 42401 / 76500) loss: 2.306658\n",
      "(Iteration 42501 / 76500) loss: 2.306647\n",
      "(Iteration 42601 / 76500) loss: 2.306659\n",
      "(Iteration 42701 / 76500) loss: 2.306644\n",
      "(Iteration 42801 / 76500) loss: 2.306661\n",
      "(Epoch 56 / 100) train acc: 0.091000; val_acc: 0.095000\n",
      "(Iteration 42901 / 76500) loss: 2.306670\n",
      "(Iteration 43001 / 76500) loss: 2.306652\n",
      "(Iteration 43101 / 76500) loss: 2.306659\n",
      "(Iteration 43201 / 76500) loss: 2.306663\n",
      "(Iteration 43301 / 76500) loss: 2.306660\n",
      "(Iteration 43401 / 76500) loss: 2.306663\n",
      "(Iteration 43501 / 76500) loss: 2.306640\n",
      "(Iteration 43601 / 76500) loss: 2.306648\n",
      "(Epoch 57 / 100) train acc: 0.095000; val_acc: 0.095000\n",
      "(Iteration 43701 / 76500) loss: 2.306660\n",
      "(Iteration 43801 / 76500) loss: 2.306671\n",
      "(Iteration 43901 / 76500) loss: 2.306659\n",
      "(Iteration 44001 / 76500) loss: 2.306652\n",
      "(Iteration 44101 / 76500) loss: 2.306654\n",
      "(Iteration 44201 / 76500) loss: 2.306656\n",
      "(Iteration 44301 / 76500) loss: 2.306647\n",
      "(Epoch 58 / 100) train acc: 0.080000; val_acc: 0.095000\n",
      "(Iteration 44401 / 76500) loss: 2.306684\n",
      "(Iteration 44501 / 76500) loss: 2.306662\n",
      "(Iteration 44601 / 76500) loss: 2.306643\n",
      "(Iteration 44701 / 76500) loss: 2.306665\n",
      "(Iteration 44801 / 76500) loss: 2.306662\n",
      "(Iteration 44901 / 76500) loss: 2.306656\n",
      "(Iteration 45001 / 76500) loss: 2.306653\n",
      "(Iteration 45101 / 76500) loss: 2.306654\n",
      "(Epoch 59 / 100) train acc: 0.094000; val_acc: 0.095000\n",
      "(Iteration 45201 / 76500) loss: 2.306662\n",
      "(Iteration 45301 / 76500) loss: 2.306661\n",
      "(Iteration 45401 / 76500) loss: 2.306670\n",
      "(Iteration 45501 / 76500) loss: 2.306655\n",
      "(Iteration 45601 / 76500) loss: 2.306659\n",
      "(Iteration 45701 / 76500) loss: 2.306660\n",
      "(Iteration 45801 / 76500) loss: 2.306653\n",
      "(Epoch 60 / 100) train acc: 0.100000; val_acc: 0.095000\n",
      "(Iteration 45901 / 76500) loss: 2.306660\n",
      "(Iteration 46001 / 76500) loss: 2.306669\n",
      "(Iteration 46101 / 76500) loss: 2.306663\n",
      "(Iteration 46201 / 76500) loss: 2.306673\n",
      "(Iteration 46301 / 76500) loss: 2.306664\n",
      "(Iteration 46401 / 76500) loss: 2.306649\n",
      "(Iteration 46501 / 76500) loss: 2.306659\n",
      "(Iteration 46601 / 76500) loss: 2.306651\n",
      "(Epoch 61 / 100) train acc: 0.092000; val_acc: 0.095000\n",
      "(Iteration 46701 / 76500) loss: 2.306652\n",
      "(Iteration 46801 / 76500) loss: 2.306670\n",
      "(Iteration 46901 / 76500) loss: 2.306655\n",
      "(Iteration 47001 / 76500) loss: 2.306652\n",
      "(Iteration 47101 / 76500) loss: 2.306667\n",
      "(Iteration 47201 / 76500) loss: 2.306653\n",
      "(Iteration 47301 / 76500) loss: 2.306660\n",
      "(Iteration 47401 / 76500) loss: 2.306657\n",
      "(Epoch 62 / 100) train acc: 0.081000; val_acc: 0.095000\n",
      "(Iteration 47501 / 76500) loss: 2.306672\n",
      "(Iteration 47601 / 76500) loss: 2.306654\n",
      "(Iteration 47701 / 76500) loss: 2.306657\n",
      "(Iteration 47801 / 76500) loss: 2.306658\n",
      "(Iteration 47901 / 76500) loss: 2.306661\n",
      "(Iteration 48001 / 76500) loss: 2.306653\n",
      "(Iteration 48101 / 76500) loss: 2.306670\n",
      "(Epoch 63 / 100) train acc: 0.105000; val_acc: 0.095000\n",
      "(Iteration 48201 / 76500) loss: 2.306659\n",
      "(Iteration 48301 / 76500) loss: 2.306661\n",
      "(Iteration 48401 / 76500) loss: 2.306662\n",
      "(Iteration 48501 / 76500) loss: 2.306660\n",
      "(Iteration 48601 / 76500) loss: 2.306655\n",
      "(Iteration 48701 / 76500) loss: 2.306646\n",
      "(Iteration 48801 / 76500) loss: 2.306651\n",
      "(Iteration 48901 / 76500) loss: 2.306661\n",
      "(Epoch 64 / 100) train acc: 0.086000; val_acc: 0.095000\n",
      "(Iteration 49001 / 76500) loss: 2.306655\n",
      "(Iteration 49101 / 76500) loss: 2.306678\n",
      "(Iteration 49201 / 76500) loss: 2.306668\n",
      "(Iteration 49301 / 76500) loss: 2.306649\n",
      "(Iteration 49401 / 76500) loss: 2.306660\n",
      "(Iteration 49501 / 76500) loss: 2.306661\n",
      "(Iteration 49601 / 76500) loss: 2.306665\n",
      "(Iteration 49701 / 76500) loss: 2.306655\n",
      "(Epoch 65 / 100) train acc: 0.084000; val_acc: 0.095000\n",
      "(Iteration 49801 / 76500) loss: 2.306668\n",
      "(Iteration 49901 / 76500) loss: 2.306653\n",
      "(Iteration 50001 / 76500) loss: 2.306671\n",
      "(Iteration 50101 / 76500) loss: 2.306652\n",
      "(Iteration 50201 / 76500) loss: 2.306656\n",
      "(Iteration 50301 / 76500) loss: 2.306656\n",
      "(Iteration 50401 / 76500) loss: 2.306668\n",
      "(Epoch 66 / 100) train acc: 0.086000; val_acc: 0.095000\n",
      "(Iteration 50501 / 76500) loss: 2.306659\n",
      "(Iteration 50601 / 76500) loss: 2.306662\n",
      "(Iteration 50701 / 76500) loss: 2.306655\n",
      "(Iteration 50801 / 76500) loss: 2.306651\n",
      "(Iteration 50901 / 76500) loss: 2.306658\n",
      "(Iteration 51001 / 76500) loss: 2.306665\n",
      "(Iteration 51101 / 76500) loss: 2.306676\n",
      "(Iteration 51201 / 76500) loss: 2.306659\n",
      "(Epoch 67 / 100) train acc: 0.102000; val_acc: 0.095000\n",
      "(Iteration 51301 / 76500) loss: 2.306657\n",
      "(Iteration 51401 / 76500) loss: 2.306651\n",
      "(Iteration 51501 / 76500) loss: 2.306664\n",
      "(Iteration 51601 / 76500) loss: 2.306656\n",
      "(Iteration 51701 / 76500) loss: 2.306669\n",
      "(Iteration 51801 / 76500) loss: 2.306673\n",
      "(Iteration 51901 / 76500) loss: 2.306660\n",
      "(Iteration 52001 / 76500) loss: 2.306650\n",
      "(Epoch 68 / 100) train acc: 0.091000; val_acc: 0.095000\n",
      "(Iteration 52101 / 76500) loss: 2.306654\n",
      "(Iteration 52201 / 76500) loss: 2.306663\n",
      "(Iteration 52301 / 76500) loss: 2.306675\n",
      "(Iteration 52401 / 76500) loss: 2.306661\n",
      "(Iteration 52501 / 76500) loss: 2.306665\n",
      "(Iteration 52601 / 76500) loss: 2.306661\n",
      "(Iteration 52701 / 76500) loss: 2.306677\n",
      "(Epoch 69 / 100) train acc: 0.097000; val_acc: 0.095000\n",
      "(Iteration 52801 / 76500) loss: 2.306653\n",
      "(Iteration 52901 / 76500) loss: 2.306643\n",
      "(Iteration 53001 / 76500) loss: 2.306651\n",
      "(Iteration 53101 / 76500) loss: 2.306660\n",
      "(Iteration 53201 / 76500) loss: 2.306672\n",
      "(Iteration 53301 / 76500) loss: 2.306661\n",
      "(Iteration 53401 / 76500) loss: 2.306652\n",
      "(Iteration 53501 / 76500) loss: 2.306658\n",
      "(Epoch 70 / 100) train acc: 0.089000; val_acc: 0.095000\n",
      "(Iteration 53601 / 76500) loss: 2.306651\n",
      "(Iteration 53701 / 76500) loss: 2.306670\n",
      "(Iteration 53801 / 76500) loss: 2.306655\n",
      "(Iteration 53901 / 76500) loss: 2.306647\n",
      "(Iteration 54001 / 76500) loss: 2.306661\n",
      "(Iteration 54101 / 76500) loss: 2.306661\n",
      "(Iteration 54201 / 76500) loss: 2.306657\n",
      "(Iteration 54301 / 76500) loss: 2.306660\n",
      "(Epoch 71 / 100) train acc: 0.084000; val_acc: 0.095000\n",
      "(Iteration 54401 / 76500) loss: 2.306657\n",
      "(Iteration 54501 / 76500) loss: 2.306660\n",
      "(Iteration 54601 / 76500) loss: 2.306662\n",
      "(Iteration 54701 / 76500) loss: 2.306655\n",
      "(Iteration 54801 / 76500) loss: 2.306663\n",
      "(Iteration 54901 / 76500) loss: 2.306644\n",
      "(Iteration 55001 / 76500) loss: 2.306652\n",
      "(Epoch 72 / 100) train acc: 0.097000; val_acc: 0.095000\n",
      "(Iteration 55101 / 76500) loss: 2.306661\n",
      "(Iteration 55201 / 76500) loss: 2.306640\n",
      "(Iteration 55301 / 76500) loss: 2.306639\n",
      "(Iteration 55401 / 76500) loss: 2.306657\n",
      "(Iteration 55501 / 76500) loss: 2.306646\n",
      "(Iteration 55601 / 76500) loss: 2.306661\n",
      "(Iteration 55701 / 76500) loss: 2.306654\n",
      "(Iteration 55801 / 76500) loss: 2.306659\n",
      "(Epoch 73 / 100) train acc: 0.092000; val_acc: 0.095000\n",
      "(Iteration 55901 / 76500) loss: 2.306655\n",
      "(Iteration 56001 / 76500) loss: 2.306666\n",
      "(Iteration 56101 / 76500) loss: 2.306654\n",
      "(Iteration 56201 / 76500) loss: 2.306657\n",
      "(Iteration 56301 / 76500) loss: 2.306672\n",
      "(Iteration 56401 / 76500) loss: 2.306659\n",
      "(Iteration 56501 / 76500) loss: 2.306641\n",
      "(Iteration 56601 / 76500) loss: 2.306651\n",
      "(Epoch 74 / 100) train acc: 0.083000; val_acc: 0.095000\n",
      "(Iteration 56701 / 76500) loss: 2.306648\n",
      "(Iteration 56801 / 76500) loss: 2.306668\n",
      "(Iteration 56901 / 76500) loss: 2.306658\n",
      "(Iteration 57001 / 76500) loss: 2.306651\n",
      "(Iteration 57101 / 76500) loss: 2.306646\n",
      "(Iteration 57201 / 76500) loss: 2.306642\n",
      "(Iteration 57301 / 76500) loss: 2.306666\n",
      "(Epoch 75 / 100) train acc: 0.094000; val_acc: 0.095000\n",
      "(Iteration 57401 / 76500) loss: 2.306665\n",
      "(Iteration 57501 / 76500) loss: 2.306645\n",
      "(Iteration 57601 / 76500) loss: 2.306664\n",
      "(Iteration 57701 / 76500) loss: 2.306665\n",
      "(Iteration 57801 / 76500) loss: 2.306643\n",
      "(Iteration 57901 / 76500) loss: 2.306646\n",
      "(Iteration 58001 / 76500) loss: 2.306673\n",
      "(Iteration 58101 / 76500) loss: 2.306649\n",
      "(Epoch 76 / 100) train acc: 0.098000; val_acc: 0.095000\n",
      "(Iteration 58201 / 76500) loss: 2.306666\n",
      "(Iteration 58301 / 76500) loss: 2.306670\n",
      "(Iteration 58401 / 76500) loss: 2.306669\n",
      "(Iteration 58501 / 76500) loss: 2.306653\n",
      "(Iteration 58601 / 76500) loss: 2.306668\n",
      "(Iteration 58701 / 76500) loss: 2.306663\n",
      "(Iteration 58801 / 76500) loss: 2.306663\n",
      "(Iteration 58901 / 76500) loss: 2.306657\n",
      "(Epoch 77 / 100) train acc: 0.079000; val_acc: 0.095000\n",
      "(Iteration 59001 / 76500) loss: 2.306656\n",
      "(Iteration 59101 / 76500) loss: 2.306654\n",
      "(Iteration 59201 / 76500) loss: 2.306675\n",
      "(Iteration 59301 / 76500) loss: 2.306656\n",
      "(Iteration 59401 / 76500) loss: 2.306650\n",
      "(Iteration 59501 / 76500) loss: 2.306651\n",
      "(Iteration 59601 / 76500) loss: 2.306639\n",
      "(Epoch 78 / 100) train acc: 0.101000; val_acc: 0.095000\n",
      "(Iteration 59701 / 76500) loss: 2.306667\n",
      "(Iteration 59801 / 76500) loss: 2.306642\n",
      "(Iteration 59901 / 76500) loss: 2.306655\n",
      "(Iteration 60001 / 76500) loss: 2.306655\n",
      "(Iteration 60101 / 76500) loss: 2.306659\n",
      "(Iteration 60201 / 76500) loss: 2.306647\n",
      "(Iteration 60301 / 76500) loss: 2.306655\n",
      "(Iteration 60401 / 76500) loss: 2.306655\n",
      "(Epoch 79 / 100) train acc: 0.091000; val_acc: 0.095000\n",
      "(Iteration 60501 / 76500) loss: 2.306667\n",
      "(Iteration 60601 / 76500) loss: 2.306658\n",
      "(Iteration 60701 / 76500) loss: 2.306665\n",
      "(Iteration 60801 / 76500) loss: 2.306649\n",
      "(Iteration 60901 / 76500) loss: 2.306645\n",
      "(Iteration 61001 / 76500) loss: 2.306655\n",
      "(Iteration 61101 / 76500) loss: 2.306657\n",
      "(Epoch 80 / 100) train acc: 0.076000; val_acc: 0.095000\n",
      "(Iteration 61201 / 76500) loss: 2.306651\n",
      "(Iteration 61301 / 76500) loss: 2.306658\n",
      "(Iteration 61401 / 76500) loss: 2.306660\n",
      "(Iteration 61501 / 76500) loss: 2.306661\n",
      "(Iteration 61601 / 76500) loss: 2.306667\n",
      "(Iteration 61701 / 76500) loss: 2.306654\n",
      "(Iteration 61801 / 76500) loss: 2.306670\n",
      "(Iteration 61901 / 76500) loss: 2.306670\n",
      "(Epoch 81 / 100) train acc: 0.085000; val_acc: 0.095000\n",
      "(Iteration 62001 / 76500) loss: 2.306660\n",
      "(Iteration 62101 / 76500) loss: 2.306678\n",
      "(Iteration 62201 / 76500) loss: 2.306658\n",
      "(Iteration 62301 / 76500) loss: 2.306646\n",
      "(Iteration 62401 / 76500) loss: 2.306657\n",
      "(Iteration 62501 / 76500) loss: 2.306641\n",
      "(Iteration 62601 / 76500) loss: 2.306657\n",
      "(Iteration 62701 / 76500) loss: 2.306651\n",
      "(Epoch 82 / 100) train acc: 0.095000; val_acc: 0.095000\n",
      "(Iteration 62801 / 76500) loss: 2.306646\n",
      "(Iteration 62901 / 76500) loss: 2.306655\n",
      "(Iteration 63001 / 76500) loss: 2.306662\n",
      "(Iteration 63101 / 76500) loss: 2.306661\n",
      "(Iteration 63201 / 76500) loss: 2.306678\n",
      "(Iteration 63301 / 76500) loss: 2.306651\n",
      "(Iteration 63401 / 76500) loss: 2.306666\n",
      "(Epoch 83 / 100) train acc: 0.107000; val_acc: 0.095000\n",
      "(Iteration 63501 / 76500) loss: 2.306650\n",
      "(Iteration 63601 / 76500) loss: 2.306644\n",
      "(Iteration 63701 / 76500) loss: 2.306669\n",
      "(Iteration 63801 / 76500) loss: 2.306660\n",
      "(Iteration 63901 / 76500) loss: 2.306672\n",
      "(Iteration 64001 / 76500) loss: 2.306665\n",
      "(Iteration 64101 / 76500) loss: 2.306670\n",
      "(Iteration 64201 / 76500) loss: 2.306652\n",
      "(Epoch 84 / 100) train acc: 0.088000; val_acc: 0.095000\n",
      "(Iteration 64301 / 76500) loss: 2.306663\n",
      "(Iteration 64401 / 76500) loss: 2.306662\n",
      "(Iteration 64501 / 76500) loss: 2.306654\n",
      "(Iteration 64601 / 76500) loss: 2.306661\n",
      "(Iteration 64701 / 76500) loss: 2.306659\n",
      "(Iteration 64801 / 76500) loss: 2.306661\n",
      "(Iteration 64901 / 76500) loss: 2.306659\n",
      "(Iteration 65001 / 76500) loss: 2.306664\n",
      "(Epoch 85 / 100) train acc: 0.089000; val_acc: 0.095000\n",
      "(Iteration 65101 / 76500) loss: 2.306644\n",
      "(Iteration 65201 / 76500) loss: 2.306648\n",
      "(Iteration 65301 / 76500) loss: 2.306680\n",
      "(Iteration 65401 / 76500) loss: 2.306648\n",
      "(Iteration 65501 / 76500) loss: 2.306653\n",
      "(Iteration 65601 / 76500) loss: 2.306661\n",
      "(Iteration 65701 / 76500) loss: 2.306634\n",
      "(Epoch 86 / 100) train acc: 0.087000; val_acc: 0.095000\n",
      "(Iteration 65801 / 76500) loss: 2.306672\n",
      "(Iteration 65901 / 76500) loss: 2.306661\n",
      "(Iteration 66001 / 76500) loss: 2.306656\n",
      "(Iteration 66101 / 76500) loss: 2.306643\n",
      "(Iteration 66201 / 76500) loss: 2.306644\n",
      "(Iteration 66301 / 76500) loss: 2.306642\n",
      "(Iteration 66401 / 76500) loss: 2.306657\n",
      "(Iteration 66501 / 76500) loss: 2.306668\n",
      "(Epoch 87 / 100) train acc: 0.106000; val_acc: 0.095000\n",
      "(Iteration 66601 / 76500) loss: 2.306664\n",
      "(Iteration 66701 / 76500) loss: 2.306662\n",
      "(Iteration 66801 / 76500) loss: 2.306637\n",
      "(Iteration 66901 / 76500) loss: 2.306663\n",
      "(Iteration 67001 / 76500) loss: 2.306655\n",
      "(Iteration 67101 / 76500) loss: 2.306661\n",
      "(Iteration 67201 / 76500) loss: 2.306648\n",
      "(Iteration 67301 / 76500) loss: 2.306656\n",
      "(Epoch 88 / 100) train acc: 0.085000; val_acc: 0.095000\n",
      "(Iteration 67401 / 76500) loss: 2.306652\n",
      "(Iteration 67501 / 76500) loss: 2.306676\n",
      "(Iteration 67601 / 76500) loss: 2.306654\n",
      "(Iteration 67701 / 76500) loss: 2.306670\n",
      "(Iteration 67801 / 76500) loss: 2.306656\n",
      "(Iteration 67901 / 76500) loss: 2.306659\n",
      "(Iteration 68001 / 76500) loss: 2.306661\n",
      "(Epoch 89 / 100) train acc: 0.086000; val_acc: 0.095000\n",
      "(Iteration 68101 / 76500) loss: 2.306650\n",
      "(Iteration 68201 / 76500) loss: 2.306661\n",
      "(Iteration 68301 / 76500) loss: 2.306651\n",
      "(Iteration 68401 / 76500) loss: 2.306655\n",
      "(Iteration 68501 / 76500) loss: 2.306645\n",
      "(Iteration 68601 / 76500) loss: 2.306662\n",
      "(Iteration 68701 / 76500) loss: 2.306659\n",
      "(Iteration 68801 / 76500) loss: 2.306653\n",
      "(Epoch 90 / 100) train acc: 0.092000; val_acc: 0.095000\n",
      "(Iteration 68901 / 76500) loss: 2.306657\n",
      "(Iteration 69001 / 76500) loss: 2.306675\n",
      "(Iteration 69101 / 76500) loss: 2.306653\n",
      "(Iteration 69201 / 76500) loss: 2.306662\n",
      "(Iteration 69301 / 76500) loss: 2.306671\n",
      "(Iteration 69401 / 76500) loss: 2.306655\n",
      "(Iteration 69501 / 76500) loss: 2.306670\n",
      "(Iteration 69601 / 76500) loss: 2.306656\n",
      "(Epoch 91 / 100) train acc: 0.087000; val_acc: 0.095000\n",
      "(Iteration 69701 / 76500) loss: 2.306650\n",
      "(Iteration 69801 / 76500) loss: 2.306639\n",
      "(Iteration 69901 / 76500) loss: 2.306664\n",
      "(Iteration 70001 / 76500) loss: 2.306646\n",
      "(Iteration 70101 / 76500) loss: 2.306665\n",
      "(Iteration 70201 / 76500) loss: 2.306644\n",
      "(Iteration 70301 / 76500) loss: 2.306646\n",
      "(Epoch 92 / 100) train acc: 0.094000; val_acc: 0.095000\n",
      "(Iteration 70401 / 76500) loss: 2.306660\n",
      "(Iteration 70501 / 76500) loss: 2.306657\n",
      "(Iteration 70601 / 76500) loss: 2.306659\n",
      "(Iteration 70701 / 76500) loss: 2.306656\n",
      "(Iteration 70801 / 76500) loss: 2.306662\n",
      "(Iteration 70901 / 76500) loss: 2.306671\n",
      "(Iteration 71001 / 76500) loss: 2.306655\n",
      "(Iteration 71101 / 76500) loss: 2.306657\n",
      "(Epoch 93 / 100) train acc: 0.090000; val_acc: 0.095000\n",
      "(Iteration 71201 / 76500) loss: 2.306655\n",
      "(Iteration 71301 / 76500) loss: 2.306668\n",
      "(Iteration 71401 / 76500) loss: 2.306661\n",
      "(Iteration 71501 / 76500) loss: 2.306671\n",
      "(Iteration 71601 / 76500) loss: 2.306660\n",
      "(Iteration 71701 / 76500) loss: 2.306648\n",
      "(Iteration 71801 / 76500) loss: 2.306656\n",
      "(Iteration 71901 / 76500) loss: 2.306652\n",
      "(Epoch 94 / 100) train acc: 0.103000; val_acc: 0.095000\n",
      "(Iteration 72001 / 76500) loss: 2.306652\n",
      "(Iteration 72101 / 76500) loss: 2.306676\n",
      "(Iteration 72201 / 76500) loss: 2.306659\n",
      "(Iteration 72301 / 76500) loss: 2.306667\n",
      "(Iteration 72401 / 76500) loss: 2.306653\n",
      "(Iteration 72501 / 76500) loss: 2.306658\n",
      "(Iteration 72601 / 76500) loss: 2.306665\n",
      "(Epoch 95 / 100) train acc: 0.098000; val_acc: 0.095000\n",
      "(Iteration 72701 / 76500) loss: 2.306656\n",
      "(Iteration 72801 / 76500) loss: 2.306659\n",
      "(Iteration 72901 / 76500) loss: 2.306659\n",
      "(Iteration 73001 / 76500) loss: 2.306654\n",
      "(Iteration 73101 / 76500) loss: 2.306665\n",
      "(Iteration 73201 / 76500) loss: 2.306654\n",
      "(Iteration 73301 / 76500) loss: 2.306654\n",
      "(Iteration 73401 / 76500) loss: 2.306667\n",
      "(Epoch 96 / 100) train acc: 0.104000; val_acc: 0.095000\n",
      "(Iteration 73501 / 76500) loss: 2.306655\n",
      "(Iteration 73601 / 76500) loss: 2.306646\n",
      "(Iteration 73701 / 76500) loss: 2.306671\n",
      "(Iteration 73801 / 76500) loss: 2.306662\n",
      "(Iteration 73901 / 76500) loss: 2.306658\n",
      "(Iteration 74001 / 76500) loss: 2.306668\n",
      "(Iteration 74101 / 76500) loss: 2.306647\n",
      "(Iteration 74201 / 76500) loss: 2.306636\n",
      "(Epoch 97 / 100) train acc: 0.110000; val_acc: 0.095000\n",
      "(Iteration 74301 / 76500) loss: 2.306654\n",
      "(Iteration 74401 / 76500) loss: 2.306667\n",
      "(Iteration 74501 / 76500) loss: 2.306664\n",
      "(Iteration 74601 / 76500) loss: 2.306648\n",
      "(Iteration 74701 / 76500) loss: 2.306661\n",
      "(Iteration 74801 / 76500) loss: 2.306658\n",
      "(Iteration 74901 / 76500) loss: 2.306666\n",
      "(Epoch 98 / 100) train acc: 0.084000; val_acc: 0.095000\n",
      "(Iteration 75001 / 76500) loss: 2.306648\n",
      "(Iteration 75101 / 76500) loss: 2.306647\n",
      "(Iteration 75201 / 76500) loss: 2.306649\n",
      "(Iteration 75301 / 76500) loss: 2.306651\n",
      "(Iteration 75401 / 76500) loss: 2.306651\n",
      "(Iteration 75501 / 76500) loss: 2.306650\n",
      "(Iteration 75601 / 76500) loss: 2.306666\n",
      "(Iteration 75701 / 76500) loss: 2.306652\n",
      "(Epoch 99 / 100) train acc: 0.096000; val_acc: 0.095000\n",
      "(Iteration 75801 / 76500) loss: 2.306649\n",
      "(Iteration 75901 / 76500) loss: 2.306649\n",
      "(Iteration 76001 / 76500) loss: 2.306656\n",
      "(Iteration 76101 / 76500) loss: 2.306661\n",
      "(Iteration 76201 / 76500) loss: 2.306647\n",
      "(Iteration 76301 / 76500) loss: 2.306672\n",
      "(Iteration 76401 / 76500) loss: 2.306670\n",
      "(Epoch 100 / 100) train acc: 0.093000; val_acc: 0.095000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.306753\n",
      "(Epoch 0 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 101 / 38200) loss: 2.306753\n",
      "(Iteration 201 / 38200) loss: 2.306748\n",
      "(Iteration 301 / 38200) loss: 2.306744\n",
      "(Epoch 1 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 401 / 38200) loss: 2.306742\n",
      "(Iteration 501 / 38200) loss: 2.306749\n",
      "(Iteration 601 / 38200) loss: 2.306751\n",
      "(Iteration 701 / 38200) loss: 2.306755\n",
      "(Epoch 2 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 801 / 38200) loss: 2.306754\n",
      "(Iteration 901 / 38200) loss: 2.306739\n",
      "(Iteration 1001 / 38200) loss: 2.306739\n",
      "(Iteration 1101 / 38200) loss: 2.306742\n",
      "(Epoch 3 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 1201 / 38200) loss: 2.306739\n",
      "(Iteration 1301 / 38200) loss: 2.306735\n",
      "(Iteration 1401 / 38200) loss: 2.306751\n",
      "(Iteration 1501 / 38200) loss: 2.306757\n",
      "(Epoch 4 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 1601 / 38200) loss: 2.306758\n",
      "(Iteration 1701 / 38200) loss: 2.306747\n",
      "(Iteration 1801 / 38200) loss: 2.306748\n",
      "(Iteration 1901 / 38200) loss: 2.306745\n",
      "(Epoch 5 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 2001 / 38200) loss: 2.306747\n",
      "(Iteration 2101 / 38200) loss: 2.306744\n",
      "(Iteration 2201 / 38200) loss: 2.306742\n",
      "(Epoch 6 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 2301 / 38200) loss: 2.306749\n",
      "(Iteration 2401 / 38200) loss: 2.306743\n",
      "(Iteration 2501 / 38200) loss: 2.306732\n",
      "(Iteration 2601 / 38200) loss: 2.306739\n",
      "(Epoch 7 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 2701 / 38200) loss: 2.306753\n",
      "(Iteration 2801 / 38200) loss: 2.306749\n",
      "(Iteration 2901 / 38200) loss: 2.306752\n",
      "(Iteration 3001 / 38200) loss: 2.306749\n",
      "(Epoch 8 / 100) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 3101 / 38200) loss: 2.306745\n",
      "(Iteration 3201 / 38200) loss: 2.306742\n",
      "(Iteration 3301 / 38200) loss: 2.306750\n",
      "(Iteration 3401 / 38200) loss: 2.306739\n",
      "(Epoch 9 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 3501 / 38200) loss: 2.306739\n",
      "(Iteration 3601 / 38200) loss: 2.306753\n",
      "(Iteration 3701 / 38200) loss: 2.306744\n",
      "(Iteration 3801 / 38200) loss: 2.306733\n",
      "(Epoch 10 / 100) train acc: 0.085000; val_acc: 0.078000\n",
      "(Iteration 3901 / 38200) loss: 2.306744\n",
      "(Iteration 4001 / 38200) loss: 2.306738\n",
      "(Iteration 4101 / 38200) loss: 2.306746\n",
      "(Iteration 4201 / 38200) loss: 2.306725\n",
      "(Epoch 11 / 100) train acc: 0.133000; val_acc: 0.078000\n",
      "(Iteration 4301 / 38200) loss: 2.306745\n",
      "(Iteration 4401 / 38200) loss: 2.306744\n",
      "(Iteration 4501 / 38200) loss: 2.306733\n",
      "(Epoch 12 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 4601 / 38200) loss: 2.306747\n",
      "(Iteration 4701 / 38200) loss: 2.306740\n",
      "(Iteration 4801 / 38200) loss: 2.306755\n",
      "(Iteration 4901 / 38200) loss: 2.306742\n",
      "(Epoch 13 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 5001 / 38200) loss: 2.306763\n",
      "(Iteration 5101 / 38200) loss: 2.306742\n",
      "(Iteration 5201 / 38200) loss: 2.306740\n",
      "(Iteration 5301 / 38200) loss: 2.306753\n",
      "(Epoch 14 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 5401 / 38200) loss: 2.306733\n",
      "(Iteration 5501 / 38200) loss: 2.306749\n",
      "(Iteration 5601 / 38200) loss: 2.306760\n",
      "(Iteration 5701 / 38200) loss: 2.306753\n",
      "(Epoch 15 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 5801 / 38200) loss: 2.306741\n",
      "(Iteration 5901 / 38200) loss: 2.306740\n",
      "(Iteration 6001 / 38200) loss: 2.306744\n",
      "(Iteration 6101 / 38200) loss: 2.306751\n",
      "(Epoch 16 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 6201 / 38200) loss: 2.306733\n",
      "(Iteration 6301 / 38200) loss: 2.306738\n",
      "(Iteration 6401 / 38200) loss: 2.306748\n",
      "(Epoch 17 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 6501 / 38200) loss: 2.306742\n",
      "(Iteration 6601 / 38200) loss: 2.306744\n",
      "(Iteration 6701 / 38200) loss: 2.306736\n",
      "(Iteration 6801 / 38200) loss: 2.306748\n",
      "(Epoch 18 / 100) train acc: 0.111000; val_acc: 0.078000\n",
      "(Iteration 6901 / 38200) loss: 2.306738\n",
      "(Iteration 7001 / 38200) loss: 2.306732\n",
      "(Iteration 7101 / 38200) loss: 2.306744\n",
      "(Iteration 7201 / 38200) loss: 2.306743\n",
      "(Epoch 19 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 7301 / 38200) loss: 2.306761\n",
      "(Iteration 7401 / 38200) loss: 2.306739\n",
      "(Iteration 7501 / 38200) loss: 2.306758\n",
      "(Iteration 7601 / 38200) loss: 2.306752\n",
      "(Epoch 20 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 7701 / 38200) loss: 2.306742\n",
      "(Iteration 7801 / 38200) loss: 2.306739\n",
      "(Iteration 7901 / 38200) loss: 2.306749\n",
      "(Iteration 8001 / 38200) loss: 2.306747\n",
      "(Epoch 21 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 8101 / 38200) loss: 2.306748\n",
      "(Iteration 8201 / 38200) loss: 2.306746\n",
      "(Iteration 8301 / 38200) loss: 2.306740\n",
      "(Iteration 8401 / 38200) loss: 2.306749\n",
      "(Epoch 22 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 8501 / 38200) loss: 2.306744\n",
      "(Iteration 8601 / 38200) loss: 2.306741\n",
      "(Iteration 8701 / 38200) loss: 2.306742\n",
      "(Epoch 23 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 8801 / 38200) loss: 2.306740\n",
      "(Iteration 8901 / 38200) loss: 2.306742\n",
      "(Iteration 9001 / 38200) loss: 2.306743\n",
      "(Iteration 9101 / 38200) loss: 2.306752\n",
      "(Epoch 24 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 9201 / 38200) loss: 2.306745\n",
      "(Iteration 9301 / 38200) loss: 2.306739\n",
      "(Iteration 9401 / 38200) loss: 2.306745\n",
      "(Iteration 9501 / 38200) loss: 2.306740\n",
      "(Epoch 25 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 9601 / 38200) loss: 2.306754\n",
      "(Iteration 9701 / 38200) loss: 2.306738\n",
      "(Iteration 9801 / 38200) loss: 2.306739\n",
      "(Iteration 9901 / 38200) loss: 2.306758\n",
      "(Epoch 26 / 100) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 10001 / 38200) loss: 2.306730\n",
      "(Iteration 10101 / 38200) loss: 2.306743\n",
      "(Iteration 10201 / 38200) loss: 2.306742\n",
      "(Iteration 10301 / 38200) loss: 2.306744\n",
      "(Epoch 27 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 10401 / 38200) loss: 2.306744\n",
      "(Iteration 10501 / 38200) loss: 2.306752\n",
      "(Iteration 10601 / 38200) loss: 2.306752\n",
      "(Epoch 28 / 100) train acc: 0.126000; val_acc: 0.078000\n",
      "(Iteration 10701 / 38200) loss: 2.306762\n",
      "(Iteration 10801 / 38200) loss: 2.306740\n",
      "(Iteration 10901 / 38200) loss: 2.306730\n",
      "(Iteration 11001 / 38200) loss: 2.306736\n",
      "(Epoch 29 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 11101 / 38200) loss: 2.306761\n",
      "(Iteration 11201 / 38200) loss: 2.306751\n",
      "(Iteration 11301 / 38200) loss: 2.306759\n",
      "(Iteration 11401 / 38200) loss: 2.306752\n",
      "(Epoch 30 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 11501 / 38200) loss: 2.306742\n",
      "(Iteration 11601 / 38200) loss: 2.306742\n",
      "(Iteration 11701 / 38200) loss: 2.306749\n",
      "(Iteration 11801 / 38200) loss: 2.306749\n",
      "(Epoch 31 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 11901 / 38200) loss: 2.306741\n",
      "(Iteration 12001 / 38200) loss: 2.306734\n",
      "(Iteration 12101 / 38200) loss: 2.306760\n",
      "(Iteration 12201 / 38200) loss: 2.306734\n",
      "(Epoch 32 / 100) train acc: 0.117000; val_acc: 0.078000\n",
      "(Iteration 12301 / 38200) loss: 2.306754\n",
      "(Iteration 12401 / 38200) loss: 2.306745\n",
      "(Iteration 12501 / 38200) loss: 2.306757\n",
      "(Iteration 12601 / 38200) loss: 2.306747\n",
      "(Epoch 33 / 100) train acc: 0.078000; val_acc: 0.078000\n",
      "(Iteration 12701 / 38200) loss: 2.306748\n",
      "(Iteration 12801 / 38200) loss: 2.306738\n",
      "(Iteration 12901 / 38200) loss: 2.306746\n",
      "(Epoch 34 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 13001 / 38200) loss: 2.306751\n",
      "(Iteration 13101 / 38200) loss: 2.306747\n",
      "(Iteration 13201 / 38200) loss: 2.306742\n",
      "(Iteration 13301 / 38200) loss: 2.306743\n",
      "(Epoch 35 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 13401 / 38200) loss: 2.306749\n",
      "(Iteration 13501 / 38200) loss: 2.306757\n",
      "(Iteration 13601 / 38200) loss: 2.306754\n",
      "(Iteration 13701 / 38200) loss: 2.306743\n",
      "(Epoch 36 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 13801 / 38200) loss: 2.306746\n",
      "(Iteration 13901 / 38200) loss: 2.306750\n",
      "(Iteration 14001 / 38200) loss: 2.306753\n",
      "(Iteration 14101 / 38200) loss: 2.306752\n",
      "(Epoch 37 / 100) train acc: 0.083000; val_acc: 0.078000\n",
      "(Iteration 14201 / 38200) loss: 2.306742\n",
      "(Iteration 14301 / 38200) loss: 2.306728\n",
      "(Iteration 14401 / 38200) loss: 2.306750\n",
      "(Iteration 14501 / 38200) loss: 2.306749\n",
      "(Epoch 38 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 14601 / 38200) loss: 2.306747\n",
      "(Iteration 14701 / 38200) loss: 2.306750\n",
      "(Iteration 14801 / 38200) loss: 2.306734\n",
      "(Epoch 39 / 100) train acc: 0.114000; val_acc: 0.078000\n",
      "(Iteration 14901 / 38200) loss: 2.306740\n",
      "(Iteration 15001 / 38200) loss: 2.306741\n",
      "(Iteration 15101 / 38200) loss: 2.306751\n",
      "(Iteration 15201 / 38200) loss: 2.306738\n",
      "(Epoch 40 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 15301 / 38200) loss: 2.306746\n",
      "(Iteration 15401 / 38200) loss: 2.306746\n",
      "(Iteration 15501 / 38200) loss: 2.306733\n",
      "(Iteration 15601 / 38200) loss: 2.306750\n",
      "(Epoch 41 / 100) train acc: 0.119000; val_acc: 0.078000\n",
      "(Iteration 15701 / 38200) loss: 2.306746\n",
      "(Iteration 15801 / 38200) loss: 2.306741\n",
      "(Iteration 15901 / 38200) loss: 2.306741\n",
      "(Iteration 16001 / 38200) loss: 2.306743\n",
      "(Epoch 42 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 16101 / 38200) loss: 2.306755\n",
      "(Iteration 16201 / 38200) loss: 2.306742\n",
      "(Iteration 16301 / 38200) loss: 2.306759\n",
      "(Iteration 16401 / 38200) loss: 2.306736\n",
      "(Epoch 43 / 100) train acc: 0.116000; val_acc: 0.078000\n",
      "(Iteration 16501 / 38200) loss: 2.306738\n",
      "(Iteration 16601 / 38200) loss: 2.306754\n",
      "(Iteration 16701 / 38200) loss: 2.306741\n",
      "(Iteration 16801 / 38200) loss: 2.306748\n",
      "(Epoch 44 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 16901 / 38200) loss: 2.306747\n",
      "(Iteration 17001 / 38200) loss: 2.306748\n",
      "(Iteration 17101 / 38200) loss: 2.306745\n",
      "(Epoch 45 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 17201 / 38200) loss: 2.306744\n",
      "(Iteration 17301 / 38200) loss: 2.306742\n",
      "(Iteration 17401 / 38200) loss: 2.306746\n",
      "(Iteration 17501 / 38200) loss: 2.306743\n",
      "(Epoch 46 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 17601 / 38200) loss: 2.306753\n",
      "(Iteration 17701 / 38200) loss: 2.306742\n",
      "(Iteration 17801 / 38200) loss: 2.306756\n",
      "(Iteration 17901 / 38200) loss: 2.306749\n",
      "(Epoch 47 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 18001 / 38200) loss: 2.306756\n",
      "(Iteration 18101 / 38200) loss: 2.306741\n",
      "(Iteration 18201 / 38200) loss: 2.306745\n",
      "(Iteration 18301 / 38200) loss: 2.306742\n",
      "(Epoch 48 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 18401 / 38200) loss: 2.306744\n",
      "(Iteration 18501 / 38200) loss: 2.306744\n",
      "(Iteration 18601 / 38200) loss: 2.306745\n",
      "(Iteration 18701 / 38200) loss: 2.306743\n",
      "(Epoch 49 / 100) train acc: 0.112000; val_acc: 0.078000\n",
      "(Iteration 18801 / 38200) loss: 2.306750\n",
      "(Iteration 18901 / 38200) loss: 2.306749\n",
      "(Iteration 19001 / 38200) loss: 2.306736\n",
      "(Epoch 50 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 19101 / 38200) loss: 2.306738\n",
      "(Iteration 19201 / 38200) loss: 2.306748\n",
      "(Iteration 19301 / 38200) loss: 2.306751\n",
      "(Iteration 19401 / 38200) loss: 2.306734\n",
      "(Epoch 51 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 19501 / 38200) loss: 2.306742\n",
      "(Iteration 19601 / 38200) loss: 2.306736\n",
      "(Iteration 19701 / 38200) loss: 2.306749\n",
      "(Iteration 19801 / 38200) loss: 2.306765\n",
      "(Epoch 52 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 19901 / 38200) loss: 2.306762\n",
      "(Iteration 20001 / 38200) loss: 2.306751\n",
      "(Iteration 20101 / 38200) loss: 2.306744\n",
      "(Iteration 20201 / 38200) loss: 2.306757\n",
      "(Epoch 53 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 20301 / 38200) loss: 2.306740\n",
      "(Iteration 20401 / 38200) loss: 2.306753\n",
      "(Iteration 20501 / 38200) loss: 2.306740\n",
      "(Iteration 20601 / 38200) loss: 2.306754\n",
      "(Epoch 54 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 20701 / 38200) loss: 2.306743\n",
      "(Iteration 20801 / 38200) loss: 2.306757\n",
      "(Iteration 20901 / 38200) loss: 2.306744\n",
      "(Iteration 21001 / 38200) loss: 2.306734\n",
      "(Epoch 55 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 21101 / 38200) loss: 2.306745\n",
      "(Iteration 21201 / 38200) loss: 2.306737\n",
      "(Iteration 21301 / 38200) loss: 2.306757\n",
      "(Epoch 56 / 100) train acc: 0.079000; val_acc: 0.078000\n",
      "(Iteration 21401 / 38200) loss: 2.306749\n",
      "(Iteration 21501 / 38200) loss: 2.306746\n",
      "(Iteration 21601 / 38200) loss: 2.306755\n",
      "(Iteration 21701 / 38200) loss: 2.306754\n",
      "(Epoch 57 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 21801 / 38200) loss: 2.306736\n",
      "(Iteration 21901 / 38200) loss: 2.306742\n",
      "(Iteration 22001 / 38200) loss: 2.306753\n",
      "(Iteration 22101 / 38200) loss: 2.306750\n",
      "(Epoch 58 / 100) train acc: 0.085000; val_acc: 0.078000\n",
      "(Iteration 22201 / 38200) loss: 2.306752\n",
      "(Iteration 22301 / 38200) loss: 2.306756\n",
      "(Iteration 22401 / 38200) loss: 2.306735\n",
      "(Iteration 22501 / 38200) loss: 2.306744\n",
      "(Epoch 59 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 22601 / 38200) loss: 2.306747\n",
      "(Iteration 22701 / 38200) loss: 2.306752\n",
      "(Iteration 22801 / 38200) loss: 2.306754\n",
      "(Iteration 22901 / 38200) loss: 2.306745\n",
      "(Epoch 60 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 23001 / 38200) loss: 2.306749\n",
      "(Iteration 23101 / 38200) loss: 2.306742\n",
      "(Iteration 23201 / 38200) loss: 2.306746\n",
      "(Iteration 23301 / 38200) loss: 2.306748\n",
      "(Epoch 61 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 23401 / 38200) loss: 2.306746\n",
      "(Iteration 23501 / 38200) loss: 2.306739\n",
      "(Iteration 23601 / 38200) loss: 2.306739\n",
      "(Epoch 62 / 100) train acc: 0.120000; val_acc: 0.078000\n",
      "(Iteration 23701 / 38200) loss: 2.306742\n",
      "(Iteration 23801 / 38200) loss: 2.306739\n",
      "(Iteration 23901 / 38200) loss: 2.306757\n",
      "(Iteration 24001 / 38200) loss: 2.306742\n",
      "(Epoch 63 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 24101 / 38200) loss: 2.306755\n",
      "(Iteration 24201 / 38200) loss: 2.306764\n",
      "(Iteration 24301 / 38200) loss: 2.306742\n",
      "(Iteration 24401 / 38200) loss: 2.306745\n",
      "(Epoch 64 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 24501 / 38200) loss: 2.306741\n",
      "(Iteration 24601 / 38200) loss: 2.306737\n",
      "(Iteration 24701 / 38200) loss: 2.306746\n",
      "(Iteration 24801 / 38200) loss: 2.306740\n",
      "(Epoch 65 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 24901 / 38200) loss: 2.306738\n",
      "(Iteration 25001 / 38200) loss: 2.306758\n",
      "(Iteration 25101 / 38200) loss: 2.306752\n",
      "(Iteration 25201 / 38200) loss: 2.306735\n",
      "(Epoch 66 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 25301 / 38200) loss: 2.306733\n",
      "(Iteration 25401 / 38200) loss: 2.306750\n",
      "(Iteration 25501 / 38200) loss: 2.306755\n",
      "(Epoch 67 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 25601 / 38200) loss: 2.306756\n",
      "(Iteration 25701 / 38200) loss: 2.306742\n",
      "(Iteration 25801 / 38200) loss: 2.306743\n",
      "(Iteration 25901 / 38200) loss: 2.306750\n",
      "(Epoch 68 / 100) train acc: 0.123000; val_acc: 0.078000\n",
      "(Iteration 26001 / 38200) loss: 2.306748\n",
      "(Iteration 26101 / 38200) loss: 2.306746\n",
      "(Iteration 26201 / 38200) loss: 2.306739\n",
      "(Iteration 26301 / 38200) loss: 2.306753\n",
      "(Epoch 69 / 100) train acc: 0.079000; val_acc: 0.078000\n",
      "(Iteration 26401 / 38200) loss: 2.306747\n",
      "(Iteration 26501 / 38200) loss: 2.306745\n",
      "(Iteration 26601 / 38200) loss: 2.306745\n",
      "(Iteration 26701 / 38200) loss: 2.306750\n",
      "(Epoch 70 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 26801 / 38200) loss: 2.306744\n",
      "(Iteration 26901 / 38200) loss: 2.306733\n",
      "(Iteration 27001 / 38200) loss: 2.306748\n",
      "(Iteration 27101 / 38200) loss: 2.306744\n",
      "(Epoch 71 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 27201 / 38200) loss: 2.306753\n",
      "(Iteration 27301 / 38200) loss: 2.306746\n",
      "(Iteration 27401 / 38200) loss: 2.306748\n",
      "(Iteration 27501 / 38200) loss: 2.306749\n",
      "(Epoch 72 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 27601 / 38200) loss: 2.306741\n",
      "(Iteration 27701 / 38200) loss: 2.306755\n",
      "(Iteration 27801 / 38200) loss: 2.306753\n",
      "(Epoch 73 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 27901 / 38200) loss: 2.306747\n",
      "(Iteration 28001 / 38200) loss: 2.306753\n",
      "(Iteration 28101 / 38200) loss: 2.306745\n",
      "(Iteration 28201 / 38200) loss: 2.306749\n",
      "(Epoch 74 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 28301 / 38200) loss: 2.306753\n",
      "(Iteration 28401 / 38200) loss: 2.306735\n",
      "(Iteration 28501 / 38200) loss: 2.306747\n",
      "(Iteration 28601 / 38200) loss: 2.306750\n",
      "(Epoch 75 / 100) train acc: 0.116000; val_acc: 0.078000\n",
      "(Iteration 28701 / 38200) loss: 2.306750\n",
      "(Iteration 28801 / 38200) loss: 2.306746\n",
      "(Iteration 28901 / 38200) loss: 2.306744\n",
      "(Iteration 29001 / 38200) loss: 2.306742\n",
      "(Epoch 76 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 29101 / 38200) loss: 2.306741\n",
      "(Iteration 29201 / 38200) loss: 2.306741\n",
      "(Iteration 29301 / 38200) loss: 2.306735\n",
      "(Iteration 29401 / 38200) loss: 2.306740\n",
      "(Epoch 77 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 29501 / 38200) loss: 2.306746\n",
      "(Iteration 29601 / 38200) loss: 2.306741\n",
      "(Iteration 29701 / 38200) loss: 2.306748\n",
      "(Epoch 78 / 100) train acc: 0.118000; val_acc: 0.078000\n",
      "(Iteration 29801 / 38200) loss: 2.306755\n",
      "(Iteration 29901 / 38200) loss: 2.306735\n",
      "(Iteration 30001 / 38200) loss: 2.306752\n",
      "(Iteration 30101 / 38200) loss: 2.306747\n",
      "(Epoch 79 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 30201 / 38200) loss: 2.306749\n",
      "(Iteration 30301 / 38200) loss: 2.306754\n",
      "(Iteration 30401 / 38200) loss: 2.306759\n",
      "(Iteration 30501 / 38200) loss: 2.306759\n",
      "(Epoch 80 / 100) train acc: 0.120000; val_acc: 0.078000\n",
      "(Iteration 30601 / 38200) loss: 2.306741\n",
      "(Iteration 30701 / 38200) loss: 2.306759\n",
      "(Iteration 30801 / 38200) loss: 2.306743\n",
      "(Iteration 30901 / 38200) loss: 2.306744\n",
      "(Epoch 81 / 100) train acc: 0.112000; val_acc: 0.078000\n",
      "(Iteration 31001 / 38200) loss: 2.306741\n",
      "(Iteration 31101 / 38200) loss: 2.306740\n",
      "(Iteration 31201 / 38200) loss: 2.306743\n",
      "(Iteration 31301 / 38200) loss: 2.306745\n",
      "(Epoch 82 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 31401 / 38200) loss: 2.306743\n",
      "(Iteration 31501 / 38200) loss: 2.306753\n",
      "(Iteration 31601 / 38200) loss: 2.306745\n",
      "(Iteration 31701 / 38200) loss: 2.306743\n",
      "(Epoch 83 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 31801 / 38200) loss: 2.306749\n",
      "(Iteration 31901 / 38200) loss: 2.306743\n",
      "(Iteration 32001 / 38200) loss: 2.306753\n",
      "(Epoch 84 / 100) train acc: 0.111000; val_acc: 0.078000\n",
      "(Iteration 32101 / 38200) loss: 2.306735\n",
      "(Iteration 32201 / 38200) loss: 2.306749\n",
      "(Iteration 32301 / 38200) loss: 2.306757\n",
      "(Iteration 32401 / 38200) loss: 2.306748\n",
      "(Epoch 85 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 32501 / 38200) loss: 2.306744\n",
      "(Iteration 32601 / 38200) loss: 2.306749\n",
      "(Iteration 32701 / 38200) loss: 2.306750\n",
      "(Iteration 32801 / 38200) loss: 2.306750\n",
      "(Epoch 86 / 100) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 32901 / 38200) loss: 2.306754\n",
      "(Iteration 33001 / 38200) loss: 2.306749\n",
      "(Iteration 33101 / 38200) loss: 2.306744\n",
      "(Iteration 33201 / 38200) loss: 2.306740\n",
      "(Epoch 87 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 33301 / 38200) loss: 2.306751\n",
      "(Iteration 33401 / 38200) loss: 2.306762\n",
      "(Iteration 33501 / 38200) loss: 2.306738\n",
      "(Iteration 33601 / 38200) loss: 2.306762\n",
      "(Epoch 88 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 33701 / 38200) loss: 2.306730\n",
      "(Iteration 33801 / 38200) loss: 2.306754\n",
      "(Iteration 33901 / 38200) loss: 2.306756\n",
      "(Epoch 89 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 34001 / 38200) loss: 2.306747\n",
      "(Iteration 34101 / 38200) loss: 2.306730\n",
      "(Iteration 34201 / 38200) loss: 2.306758\n",
      "(Iteration 34301 / 38200) loss: 2.306759\n",
      "(Epoch 90 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 34401 / 38200) loss: 2.306742\n",
      "(Iteration 34501 / 38200) loss: 2.306747\n",
      "(Iteration 34601 / 38200) loss: 2.306752\n",
      "(Iteration 34701 / 38200) loss: 2.306747\n",
      "(Epoch 91 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 34801 / 38200) loss: 2.306746\n",
      "(Iteration 34901 / 38200) loss: 2.306743\n",
      "(Iteration 35001 / 38200) loss: 2.306747\n",
      "(Iteration 35101 / 38200) loss: 2.306747\n",
      "(Epoch 92 / 100) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 35201 / 38200) loss: 2.306736\n",
      "(Iteration 35301 / 38200) loss: 2.306751\n",
      "(Iteration 35401 / 38200) loss: 2.306742\n",
      "(Iteration 35501 / 38200) loss: 2.306740\n",
      "(Epoch 93 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 35601 / 38200) loss: 2.306739\n",
      "(Iteration 35701 / 38200) loss: 2.306747\n",
      "(Iteration 35801 / 38200) loss: 2.306735\n",
      "(Iteration 35901 / 38200) loss: 2.306747\n",
      "(Epoch 94 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 36001 / 38200) loss: 2.306750\n",
      "(Iteration 36101 / 38200) loss: 2.306749\n",
      "(Iteration 36201 / 38200) loss: 2.306748\n",
      "(Epoch 95 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 36301 / 38200) loss: 2.306750\n",
      "(Iteration 36401 / 38200) loss: 2.306754\n",
      "(Iteration 36501 / 38200) loss: 2.306736\n",
      "(Iteration 36601 / 38200) loss: 2.306755\n",
      "(Epoch 96 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 36701 / 38200) loss: 2.306737\n",
      "(Iteration 36801 / 38200) loss: 2.306740\n",
      "(Iteration 36901 / 38200) loss: 2.306746\n",
      "(Iteration 37001 / 38200) loss: 2.306737\n",
      "(Epoch 97 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 37101 / 38200) loss: 2.306745\n",
      "(Iteration 37201 / 38200) loss: 2.306745\n",
      "(Iteration 37301 / 38200) loss: 2.306738\n",
      "(Iteration 37401 / 38200) loss: 2.306740\n",
      "(Epoch 98 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 37501 / 38200) loss: 2.306744\n",
      "(Iteration 37601 / 38200) loss: 2.306731\n",
      "(Iteration 37701 / 38200) loss: 2.306748\n",
      "(Iteration 37801 / 38200) loss: 2.306758\n",
      "(Epoch 99 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 37901 / 38200) loss: 2.306748\n",
      "(Iteration 38001 / 38200) loss: 2.306749\n",
      "(Iteration 38101 / 38200) loss: 2.306740\n",
      "(Epoch 100 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.306721\n",
      "(Epoch 0 / 100) train acc: 0.101000; val_acc: 0.085000\n",
      "(Iteration 101 / 76500) loss: 2.306688\n",
      "(Iteration 201 / 76500) loss: 2.306730\n",
      "(Iteration 301 / 76500) loss: 2.306712\n",
      "(Iteration 401 / 76500) loss: 2.306670\n",
      "(Iteration 501 / 76500) loss: 2.306701\n",
      "(Iteration 601 / 76500) loss: 2.306696\n",
      "(Iteration 701 / 76500) loss: 2.306710\n",
      "(Epoch 1 / 100) train acc: 0.101000; val_acc: 0.085000\n",
      "(Iteration 801 / 76500) loss: 2.306693\n",
      "(Iteration 901 / 76500) loss: 2.306695\n",
      "(Iteration 1001 / 76500) loss: 2.306695\n",
      "(Iteration 1101 / 76500) loss: 2.306703\n",
      "(Iteration 1201 / 76500) loss: 2.306710\n",
      "(Iteration 1301 / 76500) loss: 2.306715\n",
      "(Iteration 1401 / 76500) loss: 2.306689\n",
      "(Iteration 1501 / 76500) loss: 2.306728\n",
      "(Epoch 2 / 100) train acc: 0.087000; val_acc: 0.085000\n",
      "(Iteration 1601 / 76500) loss: 2.306701\n",
      "(Iteration 1701 / 76500) loss: 2.306702\n",
      "(Iteration 1801 / 76500) loss: 2.306720\n",
      "(Iteration 1901 / 76500) loss: 2.306714\n",
      "(Iteration 2001 / 76500) loss: 2.306690\n",
      "(Iteration 2101 / 76500) loss: 2.306697\n",
      "(Iteration 2201 / 76500) loss: 2.306710\n",
      "(Epoch 3 / 100) train acc: 0.087000; val_acc: 0.085000\n",
      "(Iteration 2301 / 76500) loss: 2.306698\n",
      "(Iteration 2401 / 76500) loss: 2.306713\n",
      "(Iteration 2501 / 76500) loss: 2.306711\n",
      "(Iteration 2601 / 76500) loss: 2.306707\n",
      "(Iteration 2701 / 76500) loss: 2.306697\n",
      "(Iteration 2801 / 76500) loss: 2.306722\n",
      "(Iteration 2901 / 76500) loss: 2.306707\n",
      "(Iteration 3001 / 76500) loss: 2.306685\n",
      "(Epoch 4 / 100) train acc: 0.081000; val_acc: 0.085000\n",
      "(Iteration 3101 / 76500) loss: 2.306706\n",
      "(Iteration 3201 / 76500) loss: 2.306708\n",
      "(Iteration 3301 / 76500) loss: 2.306706\n",
      "(Iteration 3401 / 76500) loss: 2.306717\n",
      "(Iteration 3501 / 76500) loss: 2.306725\n",
      "(Iteration 3601 / 76500) loss: 2.306735\n",
      "(Iteration 3701 / 76500) loss: 2.306710\n",
      "(Iteration 3801 / 76500) loss: 2.306708\n",
      "(Epoch 5 / 100) train acc: 0.108000; val_acc: 0.085000\n",
      "(Iteration 3901 / 76500) loss: 2.306709\n",
      "(Iteration 4001 / 76500) loss: 2.306699\n",
      "(Iteration 4101 / 76500) loss: 2.306695\n",
      "(Iteration 4201 / 76500) loss: 2.306713\n",
      "(Iteration 4301 / 76500) loss: 2.306684\n",
      "(Iteration 4401 / 76500) loss: 2.306707\n",
      "(Iteration 4501 / 76500) loss: 2.306694\n",
      "(Epoch 6 / 100) train acc: 0.074000; val_acc: 0.085000\n",
      "(Iteration 4601 / 76500) loss: 2.306674\n",
      "(Iteration 4701 / 76500) loss: 2.306721\n",
      "(Iteration 4801 / 76500) loss: 2.306715\n",
      "(Iteration 4901 / 76500) loss: 2.306694\n",
      "(Iteration 5001 / 76500) loss: 2.306716\n",
      "(Iteration 5101 / 76500) loss: 2.306712\n",
      "(Iteration 5201 / 76500) loss: 2.306712\n",
      "(Iteration 5301 / 76500) loss: 2.306701\n",
      "(Epoch 7 / 100) train acc: 0.106000; val_acc: 0.085000\n",
      "(Iteration 5401 / 76500) loss: 2.306681\n",
      "(Iteration 5501 / 76500) loss: 2.306695\n",
      "(Iteration 5601 / 76500) loss: 2.306711\n",
      "(Iteration 5701 / 76500) loss: 2.306712\n",
      "(Iteration 5801 / 76500) loss: 2.306695\n",
      "(Iteration 5901 / 76500) loss: 2.306713\n",
      "(Iteration 6001 / 76500) loss: 2.306688\n",
      "(Iteration 6101 / 76500) loss: 2.306734\n",
      "(Epoch 8 / 100) train acc: 0.114000; val_acc: 0.085000\n",
      "(Iteration 6201 / 76500) loss: 2.306707\n",
      "(Iteration 6301 / 76500) loss: 2.306702\n",
      "(Iteration 6401 / 76500) loss: 2.306711\n",
      "(Iteration 6501 / 76500) loss: 2.306722\n",
      "(Iteration 6601 / 76500) loss: 2.306714\n",
      "(Iteration 6701 / 76500) loss: 2.306716\n",
      "(Iteration 6801 / 76500) loss: 2.306674\n",
      "(Epoch 9 / 100) train acc: 0.085000; val_acc: 0.085000\n",
      "(Iteration 6901 / 76500) loss: 2.306730\n",
      "(Iteration 7001 / 76500) loss: 2.306705\n",
      "(Iteration 7101 / 76500) loss: 2.306703\n",
      "(Iteration 7201 / 76500) loss: 2.306706\n",
      "(Iteration 7301 / 76500) loss: 2.306692\n",
      "(Iteration 7401 / 76500) loss: 2.306693\n",
      "(Iteration 7501 / 76500) loss: 2.306700\n",
      "(Iteration 7601 / 76500) loss: 2.306735\n",
      "(Epoch 10 / 100) train acc: 0.108000; val_acc: 0.085000\n",
      "(Iteration 7701 / 76500) loss: 2.306694\n",
      "(Iteration 7801 / 76500) loss: 2.306700\n",
      "(Iteration 7901 / 76500) loss: 2.306709\n",
      "(Iteration 8001 / 76500) loss: 2.306694\n",
      "(Iteration 8101 / 76500) loss: 2.306707\n",
      "(Iteration 8201 / 76500) loss: 2.306691\n",
      "(Iteration 8301 / 76500) loss: 2.306709\n",
      "(Iteration 8401 / 76500) loss: 2.306709\n",
      "(Epoch 11 / 100) train acc: 0.089000; val_acc: 0.085000\n",
      "(Iteration 8501 / 76500) loss: 2.306723\n",
      "(Iteration 8601 / 76500) loss: 2.306710\n",
      "(Iteration 8701 / 76500) loss: 2.306702\n",
      "(Iteration 8801 / 76500) loss: 2.306682\n",
      "(Iteration 8901 / 76500) loss: 2.306702\n",
      "(Iteration 9001 / 76500) loss: 2.306701\n",
      "(Iteration 9101 / 76500) loss: 2.306690\n",
      "(Epoch 12 / 100) train acc: 0.105000; val_acc: 0.085000\n",
      "(Iteration 9201 / 76500) loss: 2.306693\n",
      "(Iteration 9301 / 76500) loss: 2.306702\n",
      "(Iteration 9401 / 76500) loss: 2.306711\n",
      "(Iteration 9501 / 76500) loss: 2.306709\n",
      "(Iteration 9601 / 76500) loss: 2.306702\n",
      "(Iteration 9701 / 76500) loss: 2.306694\n",
      "(Iteration 9801 / 76500) loss: 2.306702\n",
      "(Iteration 9901 / 76500) loss: 2.306708\n",
      "(Epoch 13 / 100) train acc: 0.098000; val_acc: 0.085000\n",
      "(Iteration 10001 / 76500) loss: 2.306702\n",
      "(Iteration 10101 / 76500) loss: 2.306706\n",
      "(Iteration 10201 / 76500) loss: 2.306703\n",
      "(Iteration 10301 / 76500) loss: 2.306702\n",
      "(Iteration 10401 / 76500) loss: 2.306714\n",
      "(Iteration 10501 / 76500) loss: 2.306703\n",
      "(Iteration 10601 / 76500) loss: 2.306690\n",
      "(Iteration 10701 / 76500) loss: 2.306716\n",
      "(Epoch 14 / 100) train acc: 0.087000; val_acc: 0.085000\n",
      "(Iteration 10801 / 76500) loss: 2.306707\n",
      "(Iteration 10901 / 76500) loss: 2.306696\n",
      "(Iteration 11001 / 76500) loss: 2.306699\n",
      "(Iteration 11101 / 76500) loss: 2.306698\n",
      "(Iteration 11201 / 76500) loss: 2.306704\n",
      "(Iteration 11301 / 76500) loss: 2.306709\n",
      "(Iteration 11401 / 76500) loss: 2.306711\n",
      "(Epoch 15 / 100) train acc: 0.089000; val_acc: 0.085000\n",
      "(Iteration 11501 / 76500) loss: 2.306695\n",
      "(Iteration 11601 / 76500) loss: 2.306704\n",
      "(Iteration 11701 / 76500) loss: 2.306723\n",
      "(Iteration 11801 / 76500) loss: 2.306692\n",
      "(Iteration 11901 / 76500) loss: 2.306701\n",
      "(Iteration 12001 / 76500) loss: 2.306703\n",
      "(Iteration 12101 / 76500) loss: 2.306700\n",
      "(Iteration 12201 / 76500) loss: 2.306706\n",
      "(Epoch 16 / 100) train acc: 0.091000; val_acc: 0.086000\n",
      "(Iteration 12301 / 76500) loss: 2.306700\n",
      "(Iteration 12401 / 76500) loss: 2.306703\n",
      "(Iteration 12501 / 76500) loss: 2.306715\n",
      "(Iteration 12601 / 76500) loss: 2.306692\n",
      "(Iteration 12701 / 76500) loss: 2.306721\n",
      "(Iteration 12801 / 76500) loss: 2.306682\n",
      "(Iteration 12901 / 76500) loss: 2.306694\n",
      "(Iteration 13001 / 76500) loss: 2.306689\n",
      "(Epoch 17 / 100) train acc: 0.092000; val_acc: 0.087000\n",
      "(Iteration 13101 / 76500) loss: 2.306701\n",
      "(Iteration 13201 / 76500) loss: 2.306706\n",
      "(Iteration 13301 / 76500) loss: 2.306713\n",
      "(Iteration 13401 / 76500) loss: 2.306697\n",
      "(Iteration 13501 / 76500) loss: 2.306701\n",
      "(Iteration 13601 / 76500) loss: 2.306711\n",
      "(Iteration 13701 / 76500) loss: 2.306704\n",
      "(Epoch 18 / 100) train acc: 0.092000; val_acc: 0.085000\n",
      "(Iteration 13801 / 76500) loss: 2.306707\n",
      "(Iteration 13901 / 76500) loss: 2.306710\n",
      "(Iteration 14001 / 76500) loss: 2.306719\n",
      "(Iteration 14101 / 76500) loss: 2.306718\n",
      "(Iteration 14201 / 76500) loss: 2.306702\n",
      "(Iteration 14301 / 76500) loss: 2.306694\n",
      "(Iteration 14401 / 76500) loss: 2.306701\n",
      "(Iteration 14501 / 76500) loss: 2.306709\n",
      "(Epoch 19 / 100) train acc: 0.084000; val_acc: 0.085000\n",
      "(Iteration 14601 / 76500) loss: 2.306693\n",
      "(Iteration 14701 / 76500) loss: 2.306698\n",
      "(Iteration 14801 / 76500) loss: 2.306708\n",
      "(Iteration 14901 / 76500) loss: 2.306696\n",
      "(Iteration 15001 / 76500) loss: 2.306720\n",
      "(Iteration 15101 / 76500) loss: 2.306708\n",
      "(Iteration 15201 / 76500) loss: 2.306701\n",
      "(Epoch 20 / 100) train acc: 0.093000; val_acc: 0.085000\n",
      "(Iteration 15301 / 76500) loss: 2.306700\n",
      "(Iteration 15401 / 76500) loss: 2.306703\n",
      "(Iteration 15501 / 76500) loss: 2.306696\n",
      "(Iteration 15601 / 76500) loss: 2.306700\n",
      "(Iteration 15701 / 76500) loss: 2.306720\n",
      "(Iteration 15801 / 76500) loss: 2.306702\n",
      "(Iteration 15901 / 76500) loss: 2.306703\n",
      "(Iteration 16001 / 76500) loss: 2.306703\n",
      "(Epoch 21 / 100) train acc: 0.091000; val_acc: 0.085000\n",
      "(Iteration 16101 / 76500) loss: 2.306692\n",
      "(Iteration 16201 / 76500) loss: 2.306696\n",
      "(Iteration 16301 / 76500) loss: 2.306709\n",
      "(Iteration 16401 / 76500) loss: 2.306712\n",
      "(Iteration 16501 / 76500) loss: 2.306702\n",
      "(Iteration 16601 / 76500) loss: 2.306712\n",
      "(Iteration 16701 / 76500) loss: 2.306700\n",
      "(Iteration 16801 / 76500) loss: 2.306741\n",
      "(Epoch 22 / 100) train acc: 0.092000; val_acc: 0.085000\n",
      "(Iteration 16901 / 76500) loss: 2.306714\n",
      "(Iteration 17001 / 76500) loss: 2.306718\n",
      "(Iteration 17101 / 76500) loss: 2.306713\n",
      "(Iteration 17201 / 76500) loss: 2.306694\n",
      "(Iteration 17301 / 76500) loss: 2.306713\n",
      "(Iteration 17401 / 76500) loss: 2.306723\n",
      "(Iteration 17501 / 76500) loss: 2.306687\n",
      "(Epoch 23 / 100) train acc: 0.090000; val_acc: 0.085000\n",
      "(Iteration 17601 / 76500) loss: 2.306707\n",
      "(Iteration 17701 / 76500) loss: 2.306709\n",
      "(Iteration 17801 / 76500) loss: 2.306700\n",
      "(Iteration 17901 / 76500) loss: 2.306699\n",
      "(Iteration 18001 / 76500) loss: 2.306711\n",
      "(Iteration 18101 / 76500) loss: 2.306701\n",
      "(Iteration 18201 / 76500) loss: 2.306690\n",
      "(Iteration 18301 / 76500) loss: 2.306706\n",
      "(Epoch 24 / 100) train acc: 0.109000; val_acc: 0.085000\n",
      "(Iteration 18401 / 76500) loss: 2.306700\n",
      "(Iteration 18501 / 76500) loss: 2.306709\n",
      "(Iteration 18601 / 76500) loss: 2.306705\n",
      "(Iteration 18701 / 76500) loss: 2.306687\n",
      "(Iteration 18801 / 76500) loss: 2.306709\n",
      "(Iteration 18901 / 76500) loss: 2.306712\n",
      "(Iteration 19001 / 76500) loss: 2.306714\n",
      "(Iteration 19101 / 76500) loss: 2.306717\n",
      "(Epoch 25 / 100) train acc: 0.097000; val_acc: 0.085000\n",
      "(Iteration 19201 / 76500) loss: 2.306710\n",
      "(Iteration 19301 / 76500) loss: 2.306694\n",
      "(Iteration 19401 / 76500) loss: 2.306714\n",
      "(Iteration 19501 / 76500) loss: 2.306701\n",
      "(Iteration 19601 / 76500) loss: 2.306696\n",
      "(Iteration 19701 / 76500) loss: 2.306712\n",
      "(Iteration 19801 / 76500) loss: 2.306705\n",
      "(Epoch 26 / 100) train acc: 0.088000; val_acc: 0.085000\n",
      "(Iteration 19901 / 76500) loss: 2.306711\n",
      "(Iteration 20001 / 76500) loss: 2.306707\n",
      "(Iteration 20101 / 76500) loss: 2.306690\n",
      "(Iteration 20201 / 76500) loss: 2.306722\n",
      "(Iteration 20301 / 76500) loss: 2.306707\n",
      "(Iteration 20401 / 76500) loss: 2.306709\n",
      "(Iteration 20501 / 76500) loss: 2.306704\n",
      "(Iteration 20601 / 76500) loss: 2.306699\n",
      "(Epoch 27 / 100) train acc: 0.102000; val_acc: 0.085000\n",
      "(Iteration 20701 / 76500) loss: 2.306707\n",
      "(Iteration 20801 / 76500) loss: 2.306708\n",
      "(Iteration 20901 / 76500) loss: 2.306699\n",
      "(Iteration 21001 / 76500) loss: 2.306692\n",
      "(Iteration 21101 / 76500) loss: 2.306709\n",
      "(Iteration 21201 / 76500) loss: 2.306697\n",
      "(Iteration 21301 / 76500) loss: 2.306691\n",
      "(Iteration 21401 / 76500) loss: 2.306712\n",
      "(Epoch 28 / 100) train acc: 0.092000; val_acc: 0.085000\n",
      "(Iteration 21501 / 76500) loss: 2.306684\n",
      "(Iteration 21601 / 76500) loss: 2.306710\n",
      "(Iteration 21701 / 76500) loss: 2.306694\n",
      "(Iteration 21801 / 76500) loss: 2.306725\n",
      "(Iteration 21901 / 76500) loss: 2.306704\n",
      "(Iteration 22001 / 76500) loss: 2.306685\n",
      "(Iteration 22101 / 76500) loss: 2.306710\n",
      "(Epoch 29 / 100) train acc: 0.101000; val_acc: 0.085000\n",
      "(Iteration 22201 / 76500) loss: 2.306692\n",
      "(Iteration 22301 / 76500) loss: 2.306704\n",
      "(Iteration 22401 / 76500) loss: 2.306706\n",
      "(Iteration 22501 / 76500) loss: 2.306686\n",
      "(Iteration 22601 / 76500) loss: 2.306708\n",
      "(Iteration 22701 / 76500) loss: 2.306719\n",
      "(Iteration 22801 / 76500) loss: 2.306700\n",
      "(Iteration 22901 / 76500) loss: 2.306688\n",
      "(Epoch 30 / 100) train acc: 0.079000; val_acc: 0.085000\n",
      "(Iteration 23001 / 76500) loss: 2.306711\n",
      "(Iteration 23101 / 76500) loss: 2.306723\n",
      "(Iteration 23201 / 76500) loss: 2.306702\n",
      "(Iteration 23301 / 76500) loss: 2.306704\n",
      "(Iteration 23401 / 76500) loss: 2.306703\n",
      "(Iteration 23501 / 76500) loss: 2.306703\n",
      "(Iteration 23601 / 76500) loss: 2.306685\n",
      "(Iteration 23701 / 76500) loss: 2.306707\n",
      "(Epoch 31 / 100) train acc: 0.088000; val_acc: 0.085000\n",
      "(Iteration 23801 / 76500) loss: 2.306699\n",
      "(Iteration 23901 / 76500) loss: 2.306707\n",
      "(Iteration 24001 / 76500) loss: 2.306702\n",
      "(Iteration 24101 / 76500) loss: 2.306704\n",
      "(Iteration 24201 / 76500) loss: 2.306719\n",
      "(Iteration 24301 / 76500) loss: 2.306696\n",
      "(Iteration 24401 / 76500) loss: 2.306708\n",
      "(Epoch 32 / 100) train acc: 0.073000; val_acc: 0.085000\n",
      "(Iteration 24501 / 76500) loss: 2.306716\n",
      "(Iteration 24601 / 76500) loss: 2.306690\n",
      "(Iteration 24701 / 76500) loss: 2.306714\n",
      "(Iteration 24801 / 76500) loss: 2.306706\n",
      "(Iteration 24901 / 76500) loss: 2.306700\n",
      "(Iteration 25001 / 76500) loss: 2.306706\n",
      "(Iteration 25101 / 76500) loss: 2.306716\n",
      "(Iteration 25201 / 76500) loss: 2.306701\n",
      "(Epoch 33 / 100) train acc: 0.085000; val_acc: 0.085000\n",
      "(Iteration 25301 / 76500) loss: 2.306697\n",
      "(Iteration 25401 / 76500) loss: 2.306696\n",
      "(Iteration 25501 / 76500) loss: 2.306692\n",
      "(Iteration 25601 / 76500) loss: 2.306712\n",
      "(Iteration 25701 / 76500) loss: 2.306702\n",
      "(Iteration 25801 / 76500) loss: 2.306711\n",
      "(Iteration 25901 / 76500) loss: 2.306703\n",
      "(Iteration 26001 / 76500) loss: 2.306703\n",
      "(Epoch 34 / 100) train acc: 0.095000; val_acc: 0.085000\n",
      "(Iteration 26101 / 76500) loss: 2.306703\n",
      "(Iteration 26201 / 76500) loss: 2.306697\n",
      "(Iteration 26301 / 76500) loss: 2.306692\n",
      "(Iteration 26401 / 76500) loss: 2.306703\n",
      "(Iteration 26501 / 76500) loss: 2.306696\n",
      "(Iteration 26601 / 76500) loss: 2.306699\n",
      "(Iteration 26701 / 76500) loss: 2.306705\n",
      "(Epoch 35 / 100) train acc: 0.083000; val_acc: 0.085000\n",
      "(Iteration 26801 / 76500) loss: 2.306715\n",
      "(Iteration 26901 / 76500) loss: 2.306715\n",
      "(Iteration 27001 / 76500) loss: 2.306701\n",
      "(Iteration 27101 / 76500) loss: 2.306699\n",
      "(Iteration 27201 / 76500) loss: 2.306711\n",
      "(Iteration 27301 / 76500) loss: 2.306700\n",
      "(Iteration 27401 / 76500) loss: 2.306711\n",
      "(Iteration 27501 / 76500) loss: 2.306705\n",
      "(Epoch 36 / 100) train acc: 0.086000; val_acc: 0.085000\n",
      "(Iteration 27601 / 76500) loss: 2.306711\n",
      "(Iteration 27701 / 76500) loss: 2.306703\n",
      "(Iteration 27801 / 76500) loss: 2.306708\n",
      "(Iteration 27901 / 76500) loss: 2.306709\n",
      "(Iteration 28001 / 76500) loss: 2.306701\n",
      "(Iteration 28101 / 76500) loss: 2.306698\n",
      "(Iteration 28201 / 76500) loss: 2.306692\n",
      "(Iteration 28301 / 76500) loss: 2.306696\n",
      "(Epoch 37 / 100) train acc: 0.100000; val_acc: 0.085000\n",
      "(Iteration 28401 / 76500) loss: 2.306704\n",
      "(Iteration 28501 / 76500) loss: 2.306681\n",
      "(Iteration 28601 / 76500) loss: 2.306700\n",
      "(Iteration 28701 / 76500) loss: 2.306707\n",
      "(Iteration 28801 / 76500) loss: 2.306703\n",
      "(Iteration 28901 / 76500) loss: 2.306685\n",
      "(Iteration 29001 / 76500) loss: 2.306701\n",
      "(Epoch 38 / 100) train acc: 0.098000; val_acc: 0.085000\n",
      "(Iteration 29101 / 76500) loss: 2.306699\n",
      "(Iteration 29201 / 76500) loss: 2.306691\n",
      "(Iteration 29301 / 76500) loss: 2.306710\n",
      "(Iteration 29401 / 76500) loss: 2.306703\n",
      "(Iteration 29501 / 76500) loss: 2.306702\n",
      "(Iteration 29601 / 76500) loss: 2.306686\n",
      "(Iteration 29701 / 76500) loss: 2.306685\n",
      "(Iteration 29801 / 76500) loss: 2.306694\n",
      "(Epoch 39 / 100) train acc: 0.110000; val_acc: 0.085000\n",
      "(Iteration 29901 / 76500) loss: 2.306706\n",
      "(Iteration 30001 / 76500) loss: 2.306706\n",
      "(Iteration 30101 / 76500) loss: 2.306711\n",
      "(Iteration 30201 / 76500) loss: 2.306700\n",
      "(Iteration 30301 / 76500) loss: 2.306707\n",
      "(Iteration 30401 / 76500) loss: 2.306712\n",
      "(Iteration 30501 / 76500) loss: 2.306697\n",
      "(Epoch 40 / 100) train acc: 0.076000; val_acc: 0.085000\n",
      "(Iteration 30601 / 76500) loss: 2.306701\n",
      "(Iteration 30701 / 76500) loss: 2.306690\n",
      "(Iteration 30801 / 76500) loss: 2.306690\n",
      "(Iteration 30901 / 76500) loss: 2.306709\n",
      "(Iteration 31001 / 76500) loss: 2.306712\n",
      "(Iteration 31101 / 76500) loss: 2.306706\n",
      "(Iteration 31201 / 76500) loss: 2.306712\n",
      "(Iteration 31301 / 76500) loss: 2.306693\n",
      "(Epoch 41 / 100) train acc: 0.092000; val_acc: 0.085000\n",
      "(Iteration 31401 / 76500) loss: 2.306715\n",
      "(Iteration 31501 / 76500) loss: 2.306721\n",
      "(Iteration 31601 / 76500) loss: 2.306695\n",
      "(Iteration 31701 / 76500) loss: 2.306690\n",
      "(Iteration 31801 / 76500) loss: 2.306716\n",
      "(Iteration 31901 / 76500) loss: 2.306702\n",
      "(Iteration 32001 / 76500) loss: 2.306710\n",
      "(Iteration 32101 / 76500) loss: 2.306712\n",
      "(Epoch 42 / 100) train acc: 0.097000; val_acc: 0.085000\n",
      "(Iteration 32201 / 76500) loss: 2.306685\n",
      "(Iteration 32301 / 76500) loss: 2.306708\n",
      "(Iteration 32401 / 76500) loss: 2.306722\n",
      "(Iteration 32501 / 76500) loss: 2.306704\n",
      "(Iteration 32601 / 76500) loss: 2.306719\n",
      "(Iteration 32701 / 76500) loss: 2.306687\n",
      "(Iteration 32801 / 76500) loss: 2.306714\n",
      "(Epoch 43 / 100) train acc: 0.098000; val_acc: 0.085000\n",
      "(Iteration 32901 / 76500) loss: 2.306691\n",
      "(Iteration 33001 / 76500) loss: 2.306712\n",
      "(Iteration 33101 / 76500) loss: 2.306720\n",
      "(Iteration 33201 / 76500) loss: 2.306695\n",
      "(Iteration 33301 / 76500) loss: 2.306721\n",
      "(Iteration 33401 / 76500) loss: 2.306688\n",
      "(Iteration 33501 / 76500) loss: 2.306694\n",
      "(Iteration 33601 / 76500) loss: 2.306690\n",
      "(Epoch 44 / 100) train acc: 0.092000; val_acc: 0.085000\n",
      "(Iteration 33701 / 76500) loss: 2.306699\n",
      "(Iteration 33801 / 76500) loss: 2.306701\n",
      "(Iteration 33901 / 76500) loss: 2.306720\n",
      "(Iteration 34001 / 76500) loss: 2.306698\n",
      "(Iteration 34101 / 76500) loss: 2.306702\n",
      "(Iteration 34201 / 76500) loss: 2.306698\n",
      "(Iteration 34301 / 76500) loss: 2.306719\n",
      "(Iteration 34401 / 76500) loss: 2.306673\n",
      "(Epoch 45 / 100) train acc: 0.089000; val_acc: 0.085000\n",
      "(Iteration 34501 / 76500) loss: 2.306707\n",
      "(Iteration 34601 / 76500) loss: 2.306696\n",
      "(Iteration 34701 / 76500) loss: 2.306697\n",
      "(Iteration 34801 / 76500) loss: 2.306700\n",
      "(Iteration 34901 / 76500) loss: 2.306708\n",
      "(Iteration 35001 / 76500) loss: 2.306722\n",
      "(Iteration 35101 / 76500) loss: 2.306703\n",
      "(Epoch 46 / 100) train acc: 0.084000; val_acc: 0.085000\n",
      "(Iteration 35201 / 76500) loss: 2.306699\n",
      "(Iteration 35301 / 76500) loss: 2.306699\n",
      "(Iteration 35401 / 76500) loss: 2.306708\n",
      "(Iteration 35501 / 76500) loss: 2.306694\n",
      "(Iteration 35601 / 76500) loss: 2.306695\n",
      "(Iteration 35701 / 76500) loss: 2.306702\n",
      "(Iteration 35801 / 76500) loss: 2.306689\n",
      "(Iteration 35901 / 76500) loss: 2.306707\n",
      "(Epoch 47 / 100) train acc: 0.095000; val_acc: 0.085000\n",
      "(Iteration 36001 / 76500) loss: 2.306710\n",
      "(Iteration 36101 / 76500) loss: 2.306701\n",
      "(Iteration 36201 / 76500) loss: 2.306730\n",
      "(Iteration 36301 / 76500) loss: 2.306705\n",
      "(Iteration 36401 / 76500) loss: 2.306724\n",
      "(Iteration 36501 / 76500) loss: 2.306704\n",
      "(Iteration 36601 / 76500) loss: 2.306699\n",
      "(Iteration 36701 / 76500) loss: 2.306703\n",
      "(Epoch 48 / 100) train acc: 0.116000; val_acc: 0.085000\n",
      "(Iteration 36801 / 76500) loss: 2.306704\n",
      "(Iteration 36901 / 76500) loss: 2.306687\n",
      "(Iteration 37001 / 76500) loss: 2.306695\n",
      "(Iteration 37101 / 76500) loss: 2.306684\n",
      "(Iteration 37201 / 76500) loss: 2.306681\n",
      "(Iteration 37301 / 76500) loss: 2.306695\n",
      "(Iteration 37401 / 76500) loss: 2.306709\n",
      "(Epoch 49 / 100) train acc: 0.094000; val_acc: 0.085000\n",
      "(Iteration 37501 / 76500) loss: 2.306700\n",
      "(Iteration 37601 / 76500) loss: 2.306691\n",
      "(Iteration 37701 / 76500) loss: 2.306704\n",
      "(Iteration 37801 / 76500) loss: 2.306712\n",
      "(Iteration 37901 / 76500) loss: 2.306712\n",
      "(Iteration 38001 / 76500) loss: 2.306689\n",
      "(Iteration 38101 / 76500) loss: 2.306688\n",
      "(Iteration 38201 / 76500) loss: 2.306714\n",
      "(Epoch 50 / 100) train acc: 0.098000; val_acc: 0.085000\n",
      "(Iteration 38301 / 76500) loss: 2.306703\n",
      "(Iteration 38401 / 76500) loss: 2.306700\n",
      "(Iteration 38501 / 76500) loss: 2.306702\n",
      "(Iteration 38601 / 76500) loss: 2.306702\n",
      "(Iteration 38701 / 76500) loss: 2.306712\n",
      "(Iteration 38801 / 76500) loss: 2.306711\n",
      "(Iteration 38901 / 76500) loss: 2.306707\n",
      "(Iteration 39001 / 76500) loss: 2.306698\n",
      "(Epoch 51 / 100) train acc: 0.085000; val_acc: 0.085000\n",
      "(Iteration 39101 / 76500) loss: 2.306711\n",
      "(Iteration 39201 / 76500) loss: 2.306702\n",
      "(Iteration 39301 / 76500) loss: 2.306718\n",
      "(Iteration 39401 / 76500) loss: 2.306700\n",
      "(Iteration 39501 / 76500) loss: 2.306692\n",
      "(Iteration 39601 / 76500) loss: 2.306706\n",
      "(Iteration 39701 / 76500) loss: 2.306675\n",
      "(Epoch 52 / 100) train acc: 0.077000; val_acc: 0.085000\n",
      "(Iteration 39801 / 76500) loss: 2.306690\n",
      "(Iteration 39901 / 76500) loss: 2.306693\n",
      "(Iteration 40001 / 76500) loss: 2.306695\n",
      "(Iteration 40101 / 76500) loss: 2.306709\n",
      "(Iteration 40201 / 76500) loss: 2.306695\n",
      "(Iteration 40301 / 76500) loss: 2.306699\n",
      "(Iteration 40401 / 76500) loss: 2.306708\n",
      "(Iteration 40501 / 76500) loss: 2.306713\n",
      "(Epoch 53 / 100) train acc: 0.101000; val_acc: 0.085000\n",
      "(Iteration 40601 / 76500) loss: 2.306720\n",
      "(Iteration 40701 / 76500) loss: 2.306692\n",
      "(Iteration 40801 / 76500) loss: 2.306724\n",
      "(Iteration 40901 / 76500) loss: 2.306695\n",
      "(Iteration 41001 / 76500) loss: 2.306706\n",
      "(Iteration 41101 / 76500) loss: 2.306710\n",
      "(Iteration 41201 / 76500) loss: 2.306702\n",
      "(Iteration 41301 / 76500) loss: 2.306712\n",
      "(Epoch 54 / 100) train acc: 0.082000; val_acc: 0.085000\n",
      "(Iteration 41401 / 76500) loss: 2.306719\n",
      "(Iteration 41501 / 76500) loss: 2.306718\n",
      "(Iteration 41601 / 76500) loss: 2.306673\n",
      "(Iteration 41701 / 76500) loss: 2.306701\n",
      "(Iteration 41801 / 76500) loss: 2.306708\n",
      "(Iteration 41901 / 76500) loss: 2.306684\n",
      "(Iteration 42001 / 76500) loss: 2.306703\n",
      "(Epoch 55 / 100) train acc: 0.079000; val_acc: 0.085000\n",
      "(Iteration 42101 / 76500) loss: 2.306706\n",
      "(Iteration 42201 / 76500) loss: 2.306709\n",
      "(Iteration 42301 / 76500) loss: 2.306704\n",
      "(Iteration 42401 / 76500) loss: 2.306712\n",
      "(Iteration 42501 / 76500) loss: 2.306680\n",
      "(Iteration 42601 / 76500) loss: 2.306712\n",
      "(Iteration 42701 / 76500) loss: 2.306704\n",
      "(Iteration 42801 / 76500) loss: 2.306692\n",
      "(Epoch 56 / 100) train acc: 0.092000; val_acc: 0.085000\n",
      "(Iteration 42901 / 76500) loss: 2.306689\n",
      "(Iteration 43001 / 76500) loss: 2.306703\n",
      "(Iteration 43101 / 76500) loss: 2.306704\n",
      "(Iteration 43201 / 76500) loss: 2.306703\n",
      "(Iteration 43301 / 76500) loss: 2.306703\n",
      "(Iteration 43401 / 76500) loss: 2.306693\n",
      "(Iteration 43501 / 76500) loss: 2.306692\n",
      "(Iteration 43601 / 76500) loss: 2.306707\n",
      "(Epoch 57 / 100) train acc: 0.091000; val_acc: 0.085000\n",
      "(Iteration 43701 / 76500) loss: 2.306702\n",
      "(Iteration 43801 / 76500) loss: 2.306709\n",
      "(Iteration 43901 / 76500) loss: 2.306704\n",
      "(Iteration 44001 / 76500) loss: 2.306689\n",
      "(Iteration 44101 / 76500) loss: 2.306708\n",
      "(Iteration 44201 / 76500) loss: 2.306706\n",
      "(Iteration 44301 / 76500) loss: 2.306716\n",
      "(Epoch 58 / 100) train acc: 0.101000; val_acc: 0.085000\n",
      "(Iteration 44401 / 76500) loss: 2.306692\n",
      "(Iteration 44501 / 76500) loss: 2.306696\n",
      "(Iteration 44601 / 76500) loss: 2.306700\n",
      "(Iteration 44701 / 76500) loss: 2.306714\n",
      "(Iteration 44801 / 76500) loss: 2.306707\n",
      "(Iteration 44901 / 76500) loss: 2.306692\n",
      "(Iteration 45001 / 76500) loss: 2.306697\n",
      "(Iteration 45101 / 76500) loss: 2.306696\n",
      "(Epoch 59 / 100) train acc: 0.092000; val_acc: 0.085000\n",
      "(Iteration 45201 / 76500) loss: 2.306706\n",
      "(Iteration 45301 / 76500) loss: 2.306685\n",
      "(Iteration 45401 / 76500) loss: 2.306700\n",
      "(Iteration 45501 / 76500) loss: 2.306709\n",
      "(Iteration 45601 / 76500) loss: 2.306707\n",
      "(Iteration 45701 / 76500) loss: 2.306685\n",
      "(Iteration 45801 / 76500) loss: 2.306709\n",
      "(Epoch 60 / 100) train acc: 0.095000; val_acc: 0.085000\n",
      "(Iteration 45901 / 76500) loss: 2.306715\n",
      "(Iteration 46001 / 76500) loss: 2.306705\n",
      "(Iteration 46101 / 76500) loss: 2.306711\n",
      "(Iteration 46201 / 76500) loss: 2.306703\n",
      "(Iteration 46301 / 76500) loss: 2.306706\n",
      "(Iteration 46401 / 76500) loss: 2.306703\n",
      "(Iteration 46501 / 76500) loss: 2.306705\n",
      "(Iteration 46601 / 76500) loss: 2.306690\n",
      "(Epoch 61 / 100) train acc: 0.098000; val_acc: 0.085000\n",
      "(Iteration 46701 / 76500) loss: 2.306699\n",
      "(Iteration 46801 / 76500) loss: 2.306695\n",
      "(Iteration 46901 / 76500) loss: 2.306704\n",
      "(Iteration 47001 / 76500) loss: 2.306697\n",
      "(Iteration 47101 / 76500) loss: 2.306699\n",
      "(Iteration 47201 / 76500) loss: 2.306695\n",
      "(Iteration 47301 / 76500) loss: 2.306706\n",
      "(Iteration 47401 / 76500) loss: 2.306691\n",
      "(Epoch 62 / 100) train acc: 0.086000; val_acc: 0.085000\n",
      "(Iteration 47501 / 76500) loss: 2.306717\n",
      "(Iteration 47601 / 76500) loss: 2.306695\n",
      "(Iteration 47701 / 76500) loss: 2.306695\n",
      "(Iteration 47801 / 76500) loss: 2.306721\n",
      "(Iteration 47901 / 76500) loss: 2.306704\n",
      "(Iteration 48001 / 76500) loss: 2.306709\n",
      "(Iteration 48101 / 76500) loss: 2.306709\n",
      "(Epoch 63 / 100) train acc: 0.091000; val_acc: 0.085000\n",
      "(Iteration 48201 / 76500) loss: 2.306719\n",
      "(Iteration 48301 / 76500) loss: 2.306697\n",
      "(Iteration 48401 / 76500) loss: 2.306710\n",
      "(Iteration 48501 / 76500) loss: 2.306692\n",
      "(Iteration 48601 / 76500) loss: 2.306708\n",
      "(Iteration 48701 / 76500) loss: 2.306703\n",
      "(Iteration 48801 / 76500) loss: 2.306707\n",
      "(Iteration 48901 / 76500) loss: 2.306679\n",
      "(Epoch 64 / 100) train acc: 0.101000; val_acc: 0.085000\n",
      "(Iteration 49001 / 76500) loss: 2.306692\n",
      "(Iteration 49101 / 76500) loss: 2.306704\n",
      "(Iteration 49201 / 76500) loss: 2.306690\n",
      "(Iteration 49301 / 76500) loss: 2.306703\n",
      "(Iteration 49401 / 76500) loss: 2.306708\n",
      "(Iteration 49501 / 76500) loss: 2.306675\n",
      "(Iteration 49601 / 76500) loss: 2.306702\n",
      "(Iteration 49701 / 76500) loss: 2.306710\n",
      "(Epoch 65 / 100) train acc: 0.086000; val_acc: 0.085000\n",
      "(Iteration 49801 / 76500) loss: 2.306707\n",
      "(Iteration 49901 / 76500) loss: 2.306707\n",
      "(Iteration 50001 / 76500) loss: 2.306707\n",
      "(Iteration 50101 / 76500) loss: 2.306703\n",
      "(Iteration 50201 / 76500) loss: 2.306695\n",
      "(Iteration 50301 / 76500) loss: 2.306692\n",
      "(Iteration 50401 / 76500) loss: 2.306701\n",
      "(Epoch 66 / 100) train acc: 0.085000; val_acc: 0.085000\n",
      "(Iteration 50501 / 76500) loss: 2.306707\n",
      "(Iteration 50601 / 76500) loss: 2.306697\n",
      "(Iteration 50701 / 76500) loss: 2.306686\n",
      "(Iteration 50801 / 76500) loss: 2.306697\n",
      "(Iteration 50901 / 76500) loss: 2.306713\n",
      "(Iteration 51001 / 76500) loss: 2.306710\n",
      "(Iteration 51101 / 76500) loss: 2.306698\n",
      "(Iteration 51201 / 76500) loss: 2.306702\n",
      "(Epoch 67 / 100) train acc: 0.090000; val_acc: 0.085000\n",
      "(Iteration 51301 / 76500) loss: 2.306706\n",
      "(Iteration 51401 / 76500) loss: 2.306696\n",
      "(Iteration 51501 / 76500) loss: 2.306703\n",
      "(Iteration 51601 / 76500) loss: 2.306692\n",
      "(Iteration 51701 / 76500) loss: 2.306686\n",
      "(Iteration 51801 / 76500) loss: 2.306695\n",
      "(Iteration 51901 / 76500) loss: 2.306712\n",
      "(Iteration 52001 / 76500) loss: 2.306707\n",
      "(Epoch 68 / 100) train acc: 0.106000; val_acc: 0.085000\n",
      "(Iteration 52101 / 76500) loss: 2.306712\n",
      "(Iteration 52201 / 76500) loss: 2.306703\n",
      "(Iteration 52301 / 76500) loss: 2.306698\n",
      "(Iteration 52401 / 76500) loss: 2.306704\n",
      "(Iteration 52501 / 76500) loss: 2.306699\n",
      "(Iteration 52601 / 76500) loss: 2.306691\n",
      "(Iteration 52701 / 76500) loss: 2.306693\n",
      "(Epoch 69 / 100) train acc: 0.080000; val_acc: 0.085000\n",
      "(Iteration 52801 / 76500) loss: 2.306705\n",
      "(Iteration 52901 / 76500) loss: 2.306701\n",
      "(Iteration 53001 / 76500) loss: 2.306689\n",
      "(Iteration 53101 / 76500) loss: 2.306719\n",
      "(Iteration 53201 / 76500) loss: 2.306701\n",
      "(Iteration 53301 / 76500) loss: 2.306697\n",
      "(Iteration 53401 / 76500) loss: 2.306700\n",
      "(Iteration 53501 / 76500) loss: 2.306706\n",
      "(Epoch 70 / 100) train acc: 0.097000; val_acc: 0.085000\n",
      "(Iteration 53601 / 76500) loss: 2.306695\n",
      "(Iteration 53701 / 76500) loss: 2.306699\n",
      "(Iteration 53801 / 76500) loss: 2.306711\n",
      "(Iteration 53901 / 76500) loss: 2.306700\n",
      "(Iteration 54001 / 76500) loss: 2.306723\n",
      "(Iteration 54101 / 76500) loss: 2.306711\n",
      "(Iteration 54201 / 76500) loss: 2.306693\n",
      "(Iteration 54301 / 76500) loss: 2.306700\n",
      "(Epoch 71 / 100) train acc: 0.101000; val_acc: 0.085000\n",
      "(Iteration 54401 / 76500) loss: 2.306715\n",
      "(Iteration 54501 / 76500) loss: 2.306705\n",
      "(Iteration 54601 / 76500) loss: 2.306690\n",
      "(Iteration 54701 / 76500) loss: 2.306696\n",
      "(Iteration 54801 / 76500) loss: 2.306685\n",
      "(Iteration 54901 / 76500) loss: 2.306704\n",
      "(Iteration 55001 / 76500) loss: 2.306710\n",
      "(Epoch 72 / 100) train acc: 0.088000; val_acc: 0.085000\n",
      "(Iteration 55101 / 76500) loss: 2.306695\n",
      "(Iteration 55201 / 76500) loss: 2.306696\n",
      "(Iteration 55301 / 76500) loss: 2.306698\n",
      "(Iteration 55401 / 76500) loss: 2.306686\n",
      "(Iteration 55501 / 76500) loss: 2.306697\n",
      "(Iteration 55601 / 76500) loss: 2.306701\n",
      "(Iteration 55701 / 76500) loss: 2.306707\n",
      "(Iteration 55801 / 76500) loss: 2.306703\n",
      "(Epoch 73 / 100) train acc: 0.100000; val_acc: 0.085000\n",
      "(Iteration 55901 / 76500) loss: 2.306694\n",
      "(Iteration 56001 / 76500) loss: 2.306694\n",
      "(Iteration 56101 / 76500) loss: 2.306716\n",
      "(Iteration 56201 / 76500) loss: 2.306700\n",
      "(Iteration 56301 / 76500) loss: 2.306671\n",
      "(Iteration 56401 / 76500) loss: 2.306699\n",
      "(Iteration 56501 / 76500) loss: 2.306702\n",
      "(Iteration 56601 / 76500) loss: 2.306690\n",
      "(Epoch 74 / 100) train acc: 0.102000; val_acc: 0.085000\n",
      "(Iteration 56701 / 76500) loss: 2.306712\n",
      "(Iteration 56801 / 76500) loss: 2.306697\n",
      "(Iteration 56901 / 76500) loss: 2.306702\n",
      "(Iteration 57001 / 76500) loss: 2.306692\n",
      "(Iteration 57101 / 76500) loss: 2.306707\n",
      "(Iteration 57201 / 76500) loss: 2.306712\n",
      "(Iteration 57301 / 76500) loss: 2.306706\n",
      "(Epoch 75 / 100) train acc: 0.075000; val_acc: 0.085000\n",
      "(Iteration 57401 / 76500) loss: 2.306700\n",
      "(Iteration 57501 / 76500) loss: 2.306705\n",
      "(Iteration 57601 / 76500) loss: 2.306679\n",
      "(Iteration 57701 / 76500) loss: 2.306711\n",
      "(Iteration 57801 / 76500) loss: 2.306704\n",
      "(Iteration 57901 / 76500) loss: 2.306706\n",
      "(Iteration 58001 / 76500) loss: 2.306692\n",
      "(Iteration 58101 / 76500) loss: 2.306682\n",
      "(Epoch 76 / 100) train acc: 0.105000; val_acc: 0.085000\n",
      "(Iteration 58201 / 76500) loss: 2.306713\n",
      "(Iteration 58301 / 76500) loss: 2.306706\n",
      "(Iteration 58401 / 76500) loss: 2.306707\n",
      "(Iteration 58501 / 76500) loss: 2.306709\n",
      "(Iteration 58601 / 76500) loss: 2.306691\n",
      "(Iteration 58701 / 76500) loss: 2.306700\n",
      "(Iteration 58801 / 76500) loss: 2.306710\n",
      "(Iteration 58901 / 76500) loss: 2.306710\n",
      "(Epoch 77 / 100) train acc: 0.097000; val_acc: 0.085000\n",
      "(Iteration 59001 / 76500) loss: 2.306699\n",
      "(Iteration 59101 / 76500) loss: 2.306708\n",
      "(Iteration 59201 / 76500) loss: 2.306691\n",
      "(Iteration 59301 / 76500) loss: 2.306685\n",
      "(Iteration 59401 / 76500) loss: 2.306717\n",
      "(Iteration 59501 / 76500) loss: 2.306720\n",
      "(Iteration 59601 / 76500) loss: 2.306702\n",
      "(Epoch 78 / 100) train acc: 0.096000; val_acc: 0.085000\n",
      "(Iteration 59701 / 76500) loss: 2.306723\n",
      "(Iteration 59801 / 76500) loss: 2.306705\n",
      "(Iteration 59901 / 76500) loss: 2.306683\n",
      "(Iteration 60001 / 76500) loss: 2.306706\n",
      "(Iteration 60101 / 76500) loss: 2.306705\n",
      "(Iteration 60201 / 76500) loss: 2.306691\n",
      "(Iteration 60301 / 76500) loss: 2.306697\n",
      "(Iteration 60401 / 76500) loss: 2.306708\n",
      "(Epoch 79 / 100) train acc: 0.085000; val_acc: 0.085000\n",
      "(Iteration 60501 / 76500) loss: 2.306698\n",
      "(Iteration 60601 / 76500) loss: 2.306709\n",
      "(Iteration 60701 / 76500) loss: 2.306701\n",
      "(Iteration 60801 / 76500) loss: 2.306683\n",
      "(Iteration 60901 / 76500) loss: 2.306701\n",
      "(Iteration 61001 / 76500) loss: 2.306722\n",
      "(Iteration 61101 / 76500) loss: 2.306688\n",
      "(Epoch 80 / 100) train acc: 0.103000; val_acc: 0.085000\n",
      "(Iteration 61201 / 76500) loss: 2.306698\n",
      "(Iteration 61301 / 76500) loss: 2.306717\n",
      "(Iteration 61401 / 76500) loss: 2.306698\n",
      "(Iteration 61501 / 76500) loss: 2.306699\n",
      "(Iteration 61601 / 76500) loss: 2.306687\n",
      "(Iteration 61701 / 76500) loss: 2.306719\n",
      "(Iteration 61801 / 76500) loss: 2.306718\n",
      "(Iteration 61901 / 76500) loss: 2.306699\n",
      "(Epoch 81 / 100) train acc: 0.095000; val_acc: 0.085000\n",
      "(Iteration 62001 / 76500) loss: 2.306698\n",
      "(Iteration 62101 / 76500) loss: 2.306704\n",
      "(Iteration 62201 / 76500) loss: 2.306704\n",
      "(Iteration 62301 / 76500) loss: 2.306706\n",
      "(Iteration 62401 / 76500) loss: 2.306705\n",
      "(Iteration 62501 / 76500) loss: 2.306695\n",
      "(Iteration 62601 / 76500) loss: 2.306722\n",
      "(Iteration 62701 / 76500) loss: 2.306698\n",
      "(Epoch 82 / 100) train acc: 0.097000; val_acc: 0.085000\n",
      "(Iteration 62801 / 76500) loss: 2.306692\n",
      "(Iteration 62901 / 76500) loss: 2.306710\n",
      "(Iteration 63001 / 76500) loss: 2.306704\n",
      "(Iteration 63101 / 76500) loss: 2.306691\n",
      "(Iteration 63201 / 76500) loss: 2.306699\n",
      "(Iteration 63301 / 76500) loss: 2.306699\n",
      "(Iteration 63401 / 76500) loss: 2.306698\n",
      "(Epoch 83 / 100) train acc: 0.072000; val_acc: 0.085000\n",
      "(Iteration 63501 / 76500) loss: 2.306688\n",
      "(Iteration 63601 / 76500) loss: 2.306699\n",
      "(Iteration 63701 / 76500) loss: 2.306706\n",
      "(Iteration 63801 / 76500) loss: 2.306703\n",
      "(Iteration 63901 / 76500) loss: 2.306691\n",
      "(Iteration 64001 / 76500) loss: 2.306700\n",
      "(Iteration 64101 / 76500) loss: 2.306716\n",
      "(Iteration 64201 / 76500) loss: 2.306710\n",
      "(Epoch 84 / 100) train acc: 0.091000; val_acc: 0.085000\n",
      "(Iteration 64301 / 76500) loss: 2.306727\n",
      "(Iteration 64401 / 76500) loss: 2.306690\n",
      "(Iteration 64501 / 76500) loss: 2.306688\n",
      "(Iteration 64601 / 76500) loss: 2.306702\n",
      "(Iteration 64701 / 76500) loss: 2.306714\n",
      "(Iteration 64801 / 76500) loss: 2.306710\n",
      "(Iteration 64901 / 76500) loss: 2.306693\n",
      "(Iteration 65001 / 76500) loss: 2.306705\n",
      "(Epoch 85 / 100) train acc: 0.080000; val_acc: 0.085000\n",
      "(Iteration 65101 / 76500) loss: 2.306711\n",
      "(Iteration 65201 / 76500) loss: 2.306715\n",
      "(Iteration 65301 / 76500) loss: 2.306723\n",
      "(Iteration 65401 / 76500) loss: 2.306712\n",
      "(Iteration 65501 / 76500) loss: 2.306705\n",
      "(Iteration 65601 / 76500) loss: 2.306704\n",
      "(Iteration 65701 / 76500) loss: 2.306688\n",
      "(Epoch 86 / 100) train acc: 0.101000; val_acc: 0.085000\n",
      "(Iteration 65801 / 76500) loss: 2.306684\n",
      "(Iteration 65901 / 76500) loss: 2.306700\n",
      "(Iteration 66001 / 76500) loss: 2.306711\n",
      "(Iteration 66101 / 76500) loss: 2.306704\n",
      "(Iteration 66201 / 76500) loss: 2.306684\n",
      "(Iteration 66301 / 76500) loss: 2.306706\n",
      "(Iteration 66401 / 76500) loss: 2.306695\n",
      "(Iteration 66501 / 76500) loss: 2.306694\n",
      "(Epoch 87 / 100) train acc: 0.099000; val_acc: 0.085000\n",
      "(Iteration 66601 / 76500) loss: 2.306705\n",
      "(Iteration 66701 / 76500) loss: 2.306697\n",
      "(Iteration 66801 / 76500) loss: 2.306721\n",
      "(Iteration 66901 / 76500) loss: 2.306719\n",
      "(Iteration 67001 / 76500) loss: 2.306700\n",
      "(Iteration 67101 / 76500) loss: 2.306702\n",
      "(Iteration 67201 / 76500) loss: 2.306697\n",
      "(Iteration 67301 / 76500) loss: 2.306693\n",
      "(Epoch 88 / 100) train acc: 0.076000; val_acc: 0.085000\n",
      "(Iteration 67401 / 76500) loss: 2.306705\n",
      "(Iteration 67501 / 76500) loss: 2.306703\n",
      "(Iteration 67601 / 76500) loss: 2.306703\n",
      "(Iteration 67701 / 76500) loss: 2.306700\n",
      "(Iteration 67801 / 76500) loss: 2.306694\n",
      "(Iteration 67901 / 76500) loss: 2.306719\n",
      "(Iteration 68001 / 76500) loss: 2.306701\n",
      "(Epoch 89 / 100) train acc: 0.089000; val_acc: 0.085000\n",
      "(Iteration 68101 / 76500) loss: 2.306698\n",
      "(Iteration 68201 / 76500) loss: 2.306699\n",
      "(Iteration 68301 / 76500) loss: 2.306703\n",
      "(Iteration 68401 / 76500) loss: 2.306695\n",
      "(Iteration 68501 / 76500) loss: 2.306690\n",
      "(Iteration 68601 / 76500) loss: 2.306723\n",
      "(Iteration 68701 / 76500) loss: 2.306689\n",
      "(Iteration 68801 / 76500) loss: 2.306711\n",
      "(Epoch 90 / 100) train acc: 0.076000; val_acc: 0.085000\n",
      "(Iteration 68901 / 76500) loss: 2.306719\n",
      "(Iteration 69001 / 76500) loss: 2.306712\n",
      "(Iteration 69101 / 76500) loss: 2.306697\n",
      "(Iteration 69201 / 76500) loss: 2.306704\n",
      "(Iteration 69301 / 76500) loss: 2.306704\n",
      "(Iteration 69401 / 76500) loss: 2.306691\n",
      "(Iteration 69501 / 76500) loss: 2.306707\n",
      "(Iteration 69601 / 76500) loss: 2.306708\n",
      "(Epoch 91 / 100) train acc: 0.095000; val_acc: 0.085000\n",
      "(Iteration 69701 / 76500) loss: 2.306707\n",
      "(Iteration 69801 / 76500) loss: 2.306691\n",
      "(Iteration 69901 / 76500) loss: 2.306706\n",
      "(Iteration 70001 / 76500) loss: 2.306694\n",
      "(Iteration 70101 / 76500) loss: 2.306695\n",
      "(Iteration 70201 / 76500) loss: 2.306715\n",
      "(Iteration 70301 / 76500) loss: 2.306680\n",
      "(Epoch 92 / 100) train acc: 0.091000; val_acc: 0.085000\n",
      "(Iteration 70401 / 76500) loss: 2.306704\n",
      "(Iteration 70501 / 76500) loss: 2.306717\n",
      "(Iteration 70601 / 76500) loss: 2.306693\n",
      "(Iteration 70701 / 76500) loss: 2.306703\n",
      "(Iteration 70801 / 76500) loss: 2.306700\n",
      "(Iteration 70901 / 76500) loss: 2.306692\n",
      "(Iteration 71001 / 76500) loss: 2.306703\n",
      "(Iteration 71101 / 76500) loss: 2.306695\n",
      "(Epoch 93 / 100) train acc: 0.093000; val_acc: 0.085000\n",
      "(Iteration 71201 / 76500) loss: 2.306704\n",
      "(Iteration 71301 / 76500) loss: 2.306679\n",
      "(Iteration 71401 / 76500) loss: 2.306722\n",
      "(Iteration 71501 / 76500) loss: 2.306714\n",
      "(Iteration 71601 / 76500) loss: 2.306686\n",
      "(Iteration 71701 / 76500) loss: 2.306698\n",
      "(Iteration 71801 / 76500) loss: 2.306700\n",
      "(Iteration 71901 / 76500) loss: 2.306729\n",
      "(Epoch 94 / 100) train acc: 0.082000; val_acc: 0.085000\n",
      "(Iteration 72001 / 76500) loss: 2.306689\n",
      "(Iteration 72101 / 76500) loss: 2.306700\n",
      "(Iteration 72201 / 76500) loss: 2.306699\n",
      "(Iteration 72301 / 76500) loss: 2.306708\n",
      "(Iteration 72401 / 76500) loss: 2.306698\n",
      "(Iteration 72501 / 76500) loss: 2.306710\n",
      "(Iteration 72601 / 76500) loss: 2.306687\n",
      "(Epoch 95 / 100) train acc: 0.094000; val_acc: 0.085000\n",
      "(Iteration 72701 / 76500) loss: 2.306683\n",
      "(Iteration 72801 / 76500) loss: 2.306697\n",
      "(Iteration 72901 / 76500) loss: 2.306688\n",
      "(Iteration 73001 / 76500) loss: 2.306694\n",
      "(Iteration 73101 / 76500) loss: 2.306695\n",
      "(Iteration 73201 / 76500) loss: 2.306693\n",
      "(Iteration 73301 / 76500) loss: 2.306692\n",
      "(Iteration 73401 / 76500) loss: 2.306725\n",
      "(Epoch 96 / 100) train acc: 0.088000; val_acc: 0.085000\n",
      "(Iteration 73501 / 76500) loss: 2.306698\n",
      "(Iteration 73601 / 76500) loss: 2.306710\n",
      "(Iteration 73701 / 76500) loss: 2.306697\n",
      "(Iteration 73801 / 76500) loss: 2.306694\n",
      "(Iteration 73901 / 76500) loss: 2.306695\n",
      "(Iteration 74001 / 76500) loss: 2.306694\n",
      "(Iteration 74101 / 76500) loss: 2.306698\n",
      "(Iteration 74201 / 76500) loss: 2.306717\n",
      "(Epoch 97 / 100) train acc: 0.083000; val_acc: 0.085000\n",
      "(Iteration 74301 / 76500) loss: 2.306676\n",
      "(Iteration 74401 / 76500) loss: 2.306685\n",
      "(Iteration 74501 / 76500) loss: 2.306714\n",
      "(Iteration 74601 / 76500) loss: 2.306698\n",
      "(Iteration 74701 / 76500) loss: 2.306706\n",
      "(Iteration 74801 / 76500) loss: 2.306701\n",
      "(Iteration 74901 / 76500) loss: 2.306696\n",
      "(Epoch 98 / 100) train acc: 0.101000; val_acc: 0.085000\n",
      "(Iteration 75001 / 76500) loss: 2.306681\n",
      "(Iteration 75101 / 76500) loss: 2.306685\n",
      "(Iteration 75201 / 76500) loss: 2.306698\n",
      "(Iteration 75301 / 76500) loss: 2.306697\n",
      "(Iteration 75401 / 76500) loss: 2.306691\n",
      "(Iteration 75501 / 76500) loss: 2.306720\n",
      "(Iteration 75601 / 76500) loss: 2.306716\n",
      "(Iteration 75701 / 76500) loss: 2.306697\n",
      "(Epoch 99 / 100) train acc: 0.092000; val_acc: 0.085000\n",
      "(Iteration 75801 / 76500) loss: 2.306714\n",
      "(Iteration 75901 / 76500) loss: 2.306703\n",
      "(Iteration 76001 / 76500) loss: 2.306695\n",
      "(Iteration 76101 / 76500) loss: 2.306702\n",
      "(Iteration 76201 / 76500) loss: 2.306712\n",
      "(Iteration 76301 / 76500) loss: 2.306704\n",
      "(Iteration 76401 / 76500) loss: 2.306697\n",
      "(Epoch 100 / 100) train acc: 0.102000; val_acc: 0.085000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.306718\n",
      "(Epoch 0 / 100) train acc: 0.094000; val_acc: 0.089000\n",
      "(Iteration 101 / 38200) loss: 2.306711\n",
      "(Iteration 201 / 38200) loss: 2.306718\n",
      "(Iteration 301 / 38200) loss: 2.306713\n",
      "(Epoch 1 / 100) train acc: 0.096000; val_acc: 0.089000\n",
      "(Iteration 401 / 38200) loss: 2.306709\n",
      "(Iteration 501 / 38200) loss: 2.306723\n",
      "(Iteration 601 / 38200) loss: 2.306714\n",
      "(Iteration 701 / 38200) loss: 2.306710\n",
      "(Epoch 2 / 100) train acc: 0.106000; val_acc: 0.089000\n",
      "(Iteration 801 / 38200) loss: 2.306701\n",
      "(Iteration 901 / 38200) loss: 2.306713\n",
      "(Iteration 1001 / 38200) loss: 2.306704\n",
      "(Iteration 1101 / 38200) loss: 2.306724\n",
      "(Epoch 3 / 100) train acc: 0.095000; val_acc: 0.089000\n",
      "(Iteration 1201 / 38200) loss: 2.306718\n",
      "(Iteration 1301 / 38200) loss: 2.306701\n",
      "(Iteration 1401 / 38200) loss: 2.306718\n",
      "(Iteration 1501 / 38200) loss: 2.306717\n",
      "(Epoch 4 / 100) train acc: 0.088000; val_acc: 0.089000\n",
      "(Iteration 1601 / 38200) loss: 2.306706\n",
      "(Iteration 1701 / 38200) loss: 2.306716\n",
      "(Iteration 1801 / 38200) loss: 2.306704\n",
      "(Iteration 1901 / 38200) loss: 2.306712\n",
      "(Epoch 5 / 100) train acc: 0.091000; val_acc: 0.089000\n",
      "(Iteration 2001 / 38200) loss: 2.306721\n",
      "(Iteration 2101 / 38200) loss: 2.306715\n",
      "(Iteration 2201 / 38200) loss: 2.306731\n",
      "(Epoch 6 / 100) train acc: 0.094000; val_acc: 0.089000\n",
      "(Iteration 2301 / 38200) loss: 2.306703\n",
      "(Iteration 2401 / 38200) loss: 2.306707\n",
      "(Iteration 2501 / 38200) loss: 2.306719\n",
      "(Iteration 2601 / 38200) loss: 2.306712\n",
      "(Epoch 7 / 100) train acc: 0.102000; val_acc: 0.089000\n",
      "(Iteration 2701 / 38200) loss: 2.306713\n",
      "(Iteration 2801 / 38200) loss: 2.306721\n",
      "(Iteration 2901 / 38200) loss: 2.306713\n",
      "(Iteration 3001 / 38200) loss: 2.306718\n",
      "(Epoch 8 / 100) train acc: 0.103000; val_acc: 0.089000\n",
      "(Iteration 3101 / 38200) loss: 2.306728\n",
      "(Iteration 3201 / 38200) loss: 2.306705\n",
      "(Iteration 3301 / 38200) loss: 2.306718\n",
      "(Iteration 3401 / 38200) loss: 2.306713\n",
      "(Epoch 9 / 100) train acc: 0.079000; val_acc: 0.089000\n",
      "(Iteration 3501 / 38200) loss: 2.306715\n",
      "(Iteration 3601 / 38200) loss: 2.306713\n",
      "(Iteration 3701 / 38200) loss: 2.306713\n",
      "(Iteration 3801 / 38200) loss: 2.306718\n",
      "(Epoch 10 / 100) train acc: 0.085000; val_acc: 0.089000\n",
      "(Iteration 3901 / 38200) loss: 2.306720\n",
      "(Iteration 4001 / 38200) loss: 2.306721\n",
      "(Iteration 4101 / 38200) loss: 2.306712\n",
      "(Iteration 4201 / 38200) loss: 2.306719\n",
      "(Epoch 11 / 100) train acc: 0.097000; val_acc: 0.089000\n",
      "(Iteration 4301 / 38200) loss: 2.306715\n",
      "(Iteration 4401 / 38200) loss: 2.306711\n",
      "(Iteration 4501 / 38200) loss: 2.306704\n",
      "(Epoch 12 / 100) train acc: 0.080000; val_acc: 0.089000\n",
      "(Iteration 4601 / 38200) loss: 2.306700\n",
      "(Iteration 4701 / 38200) loss: 2.306718\n",
      "(Iteration 4801 / 38200) loss: 2.306721\n",
      "(Iteration 4901 / 38200) loss: 2.306717\n",
      "(Epoch 13 / 100) train acc: 0.085000; val_acc: 0.089000\n",
      "(Iteration 5001 / 38200) loss: 2.306713\n",
      "(Iteration 5101 / 38200) loss: 2.306718\n",
      "(Iteration 5201 / 38200) loss: 2.306713\n",
      "(Iteration 5301 / 38200) loss: 2.306715\n",
      "(Epoch 14 / 100) train acc: 0.087000; val_acc: 0.089000\n",
      "(Iteration 5401 / 38200) loss: 2.306720\n",
      "(Iteration 5501 / 38200) loss: 2.306708\n",
      "(Iteration 5601 / 38200) loss: 2.306695\n",
      "(Iteration 5701 / 38200) loss: 2.306715\n",
      "(Epoch 15 / 100) train acc: 0.077000; val_acc: 0.089000\n",
      "(Iteration 5801 / 38200) loss: 2.306706\n",
      "(Iteration 5901 / 38200) loss: 2.306717\n",
      "(Iteration 6001 / 38200) loss: 2.306708\n",
      "(Iteration 6101 / 38200) loss: 2.306722\n",
      "(Epoch 16 / 100) train acc: 0.100000; val_acc: 0.089000\n",
      "(Iteration 6201 / 38200) loss: 2.306705\n",
      "(Iteration 6301 / 38200) loss: 2.306722\n",
      "(Iteration 6401 / 38200) loss: 2.306719\n",
      "(Epoch 17 / 100) train acc: 0.088000; val_acc: 0.089000\n",
      "(Iteration 6501 / 38200) loss: 2.306704\n",
      "(Iteration 6601 / 38200) loss: 2.306724\n",
      "(Iteration 6701 / 38200) loss: 2.306707\n",
      "(Iteration 6801 / 38200) loss: 2.306718\n",
      "(Epoch 18 / 100) train acc: 0.094000; val_acc: 0.089000\n",
      "(Iteration 6901 / 38200) loss: 2.306719\n",
      "(Iteration 7001 / 38200) loss: 2.306717\n",
      "(Iteration 7101 / 38200) loss: 2.306712\n",
      "(Iteration 7201 / 38200) loss: 2.306705\n",
      "(Epoch 19 / 100) train acc: 0.089000; val_acc: 0.089000\n",
      "(Iteration 7301 / 38200) loss: 2.306709\n",
      "(Iteration 7401 / 38200) loss: 2.306707\n",
      "(Iteration 7501 / 38200) loss: 2.306707\n",
      "(Iteration 7601 / 38200) loss: 2.306709\n",
      "(Epoch 20 / 100) train acc: 0.104000; val_acc: 0.089000\n",
      "(Iteration 7701 / 38200) loss: 2.306719\n",
      "(Iteration 7801 / 38200) loss: 2.306718\n",
      "(Iteration 7901 / 38200) loss: 2.306712\n",
      "(Iteration 8001 / 38200) loss: 2.306714\n",
      "(Epoch 21 / 100) train acc: 0.079000; val_acc: 0.089000\n",
      "(Iteration 8101 / 38200) loss: 2.306702\n",
      "(Iteration 8201 / 38200) loss: 2.306721\n",
      "(Iteration 8301 / 38200) loss: 2.306716\n",
      "(Iteration 8401 / 38200) loss: 2.306707\n",
      "(Epoch 22 / 100) train acc: 0.095000; val_acc: 0.089000\n",
      "(Iteration 8501 / 38200) loss: 2.306701\n",
      "(Iteration 8601 / 38200) loss: 2.306719\n",
      "(Iteration 8701 / 38200) loss: 2.306708\n",
      "(Epoch 23 / 100) train acc: 0.080000; val_acc: 0.089000\n",
      "(Iteration 8801 / 38200) loss: 2.306702\n",
      "(Iteration 8901 / 38200) loss: 2.306718\n",
      "(Iteration 9001 / 38200) loss: 2.306709\n",
      "(Iteration 9101 / 38200) loss: 2.306709\n",
      "(Epoch 24 / 100) train acc: 0.104000; val_acc: 0.089000\n",
      "(Iteration 9201 / 38200) loss: 2.306718\n",
      "(Iteration 9301 / 38200) loss: 2.306710\n",
      "(Iteration 9401 / 38200) loss: 2.306710\n",
      "(Iteration 9501 / 38200) loss: 2.306718\n",
      "(Epoch 25 / 100) train acc: 0.091000; val_acc: 0.089000\n",
      "(Iteration 9601 / 38200) loss: 2.306701\n",
      "(Iteration 9701 / 38200) loss: 2.306702\n",
      "(Iteration 9801 / 38200) loss: 2.306705\n",
      "(Iteration 9901 / 38200) loss: 2.306707\n",
      "(Epoch 26 / 100) train acc: 0.088000; val_acc: 0.089000\n",
      "(Iteration 10001 / 38200) loss: 2.306724\n",
      "(Iteration 10101 / 38200) loss: 2.306710\n",
      "(Iteration 10201 / 38200) loss: 2.306705\n",
      "(Iteration 10301 / 38200) loss: 2.306707\n",
      "(Epoch 27 / 100) train acc: 0.080000; val_acc: 0.089000\n",
      "(Iteration 10401 / 38200) loss: 2.306722\n",
      "(Iteration 10501 / 38200) loss: 2.306699\n",
      "(Iteration 10601 / 38200) loss: 2.306702\n",
      "(Epoch 28 / 100) train acc: 0.084000; val_acc: 0.089000\n",
      "(Iteration 10701 / 38200) loss: 2.306713\n",
      "(Iteration 10801 / 38200) loss: 2.306711\n",
      "(Iteration 10901 / 38200) loss: 2.306729\n",
      "(Iteration 11001 / 38200) loss: 2.306702\n",
      "(Epoch 29 / 100) train acc: 0.090000; val_acc: 0.089000\n",
      "(Iteration 11101 / 38200) loss: 2.306718\n",
      "(Iteration 11201 / 38200) loss: 2.306707\n",
      "(Iteration 11301 / 38200) loss: 2.306719\n",
      "(Iteration 11401 / 38200) loss: 2.306720\n",
      "(Epoch 30 / 100) train acc: 0.085000; val_acc: 0.089000\n",
      "(Iteration 11501 / 38200) loss: 2.306715\n",
      "(Iteration 11601 / 38200) loss: 2.306701\n",
      "(Iteration 11701 / 38200) loss: 2.306717\n",
      "(Iteration 11801 / 38200) loss: 2.306725\n",
      "(Epoch 31 / 100) train acc: 0.098000; val_acc: 0.088000\n",
      "(Iteration 11901 / 38200) loss: 2.306706\n",
      "(Iteration 12001 / 38200) loss: 2.306712\n",
      "(Iteration 12101 / 38200) loss: 2.306714\n",
      "(Iteration 12201 / 38200) loss: 2.306712\n",
      "(Epoch 32 / 100) train acc: 0.086000; val_acc: 0.088000\n",
      "(Iteration 12301 / 38200) loss: 2.306715\n",
      "(Iteration 12401 / 38200) loss: 2.306708\n",
      "(Iteration 12501 / 38200) loss: 2.306705\n",
      "(Iteration 12601 / 38200) loss: 2.306718\n",
      "(Epoch 33 / 100) train acc: 0.100000; val_acc: 0.088000\n",
      "(Iteration 12701 / 38200) loss: 2.306715\n",
      "(Iteration 12801 / 38200) loss: 2.306719\n",
      "(Iteration 12901 / 38200) loss: 2.306707\n",
      "(Epoch 34 / 100) train acc: 0.093000; val_acc: 0.088000\n",
      "(Iteration 13001 / 38200) loss: 2.306713\n",
      "(Iteration 13101 / 38200) loss: 2.306708\n",
      "(Iteration 13201 / 38200) loss: 2.306716\n",
      "(Iteration 13301 / 38200) loss: 2.306714\n",
      "(Epoch 35 / 100) train acc: 0.088000; val_acc: 0.088000\n",
      "(Iteration 13401 / 38200) loss: 2.306703\n",
      "(Iteration 13501 / 38200) loss: 2.306712\n",
      "(Iteration 13601 / 38200) loss: 2.306719\n",
      "(Iteration 13701 / 38200) loss: 2.306715\n",
      "(Epoch 36 / 100) train acc: 0.090000; val_acc: 0.088000\n",
      "(Iteration 13801 / 38200) loss: 2.306718\n",
      "(Iteration 13901 / 38200) loss: 2.306713\n",
      "(Iteration 14001 / 38200) loss: 2.306727\n",
      "(Iteration 14101 / 38200) loss: 2.306713\n",
      "(Epoch 37 / 100) train acc: 0.089000; val_acc: 0.088000\n",
      "(Iteration 14201 / 38200) loss: 2.306715\n",
      "(Iteration 14301 / 38200) loss: 2.306709\n",
      "(Iteration 14401 / 38200) loss: 2.306713\n",
      "(Iteration 14501 / 38200) loss: 2.306712\n",
      "(Epoch 38 / 100) train acc: 0.092000; val_acc: 0.088000\n",
      "(Iteration 14601 / 38200) loss: 2.306702\n",
      "(Iteration 14701 / 38200) loss: 2.306715\n",
      "(Iteration 14801 / 38200) loss: 2.306698\n",
      "(Epoch 39 / 100) train acc: 0.098000; val_acc: 0.088000\n",
      "(Iteration 14901 / 38200) loss: 2.306711\n",
      "(Iteration 15001 / 38200) loss: 2.306715\n",
      "(Iteration 15101 / 38200) loss: 2.306705\n",
      "(Iteration 15201 / 38200) loss: 2.306703\n",
      "(Epoch 40 / 100) train acc: 0.099000; val_acc: 0.089000\n",
      "(Iteration 15301 / 38200) loss: 2.306704\n",
      "(Iteration 15401 / 38200) loss: 2.306709\n",
      "(Iteration 15501 / 38200) loss: 2.306707\n",
      "(Iteration 15601 / 38200) loss: 2.306701\n",
      "(Epoch 41 / 100) train acc: 0.095000; val_acc: 0.089000\n",
      "(Iteration 15701 / 38200) loss: 2.306722\n",
      "(Iteration 15801 / 38200) loss: 2.306722\n",
      "(Iteration 15901 / 38200) loss: 2.306706\n",
      "(Iteration 16001 / 38200) loss: 2.306703\n",
      "(Epoch 42 / 100) train acc: 0.087000; val_acc: 0.088000\n",
      "(Iteration 16101 / 38200) loss: 2.306715\n",
      "(Iteration 16201 / 38200) loss: 2.306710\n",
      "(Iteration 16301 / 38200) loss: 2.306716\n",
      "(Iteration 16401 / 38200) loss: 2.306718\n",
      "(Epoch 43 / 100) train acc: 0.094000; val_acc: 0.088000\n",
      "(Iteration 16501 / 38200) loss: 2.306715\n",
      "(Iteration 16601 / 38200) loss: 2.306704\n",
      "(Iteration 16701 / 38200) loss: 2.306708\n",
      "(Iteration 16801 / 38200) loss: 2.306717\n",
      "(Epoch 44 / 100) train acc: 0.102000; val_acc: 0.088000\n",
      "(Iteration 16901 / 38200) loss: 2.306705\n",
      "(Iteration 17001 / 38200) loss: 2.306711\n",
      "(Iteration 17101 / 38200) loss: 2.306719\n",
      "(Epoch 45 / 100) train acc: 0.100000; val_acc: 0.089000\n",
      "(Iteration 17201 / 38200) loss: 2.306711\n",
      "(Iteration 17301 / 38200) loss: 2.306714\n",
      "(Iteration 17401 / 38200) loss: 2.306705\n",
      "(Iteration 17501 / 38200) loss: 2.306712\n",
      "(Epoch 46 / 100) train acc: 0.099000; val_acc: 0.089000\n",
      "(Iteration 17601 / 38200) loss: 2.306716\n",
      "(Iteration 17701 / 38200) loss: 2.306708\n",
      "(Iteration 17801 / 38200) loss: 2.306711\n",
      "(Iteration 17901 / 38200) loss: 2.306711\n",
      "(Epoch 47 / 100) train acc: 0.085000; val_acc: 0.088000\n",
      "(Iteration 18001 / 38200) loss: 2.306719\n",
      "(Iteration 18101 / 38200) loss: 2.306704\n",
      "(Iteration 18201 / 38200) loss: 2.306720\n",
      "(Iteration 18301 / 38200) loss: 2.306712\n",
      "(Epoch 48 / 100) train acc: 0.073000; val_acc: 0.089000\n",
      "(Iteration 18401 / 38200) loss: 2.306720\n",
      "(Iteration 18501 / 38200) loss: 2.306704\n",
      "(Iteration 18601 / 38200) loss: 2.306711\n",
      "(Iteration 18701 / 38200) loss: 2.306721\n",
      "(Epoch 49 / 100) train acc: 0.088000; val_acc: 0.089000\n",
      "(Iteration 18801 / 38200) loss: 2.306705\n",
      "(Iteration 18901 / 38200) loss: 2.306724\n",
      "(Iteration 19001 / 38200) loss: 2.306716\n",
      "(Epoch 50 / 100) train acc: 0.085000; val_acc: 0.089000\n",
      "(Iteration 19101 / 38200) loss: 2.306716\n",
      "(Iteration 19201 / 38200) loss: 2.306708\n",
      "(Iteration 19301 / 38200) loss: 2.306721\n",
      "(Iteration 19401 / 38200) loss: 2.306718\n",
      "(Epoch 51 / 100) train acc: 0.091000; val_acc: 0.089000\n",
      "(Iteration 19501 / 38200) loss: 2.306715\n",
      "(Iteration 19601 / 38200) loss: 2.306714\n",
      "(Iteration 19701 / 38200) loss: 2.306711\n",
      "(Iteration 19801 / 38200) loss: 2.306710\n",
      "(Epoch 52 / 100) train acc: 0.081000; val_acc: 0.088000\n",
      "(Iteration 19901 / 38200) loss: 2.306709\n",
      "(Iteration 20001 / 38200) loss: 2.306715\n",
      "(Iteration 20101 / 38200) loss: 2.306716\n",
      "(Iteration 20201 / 38200) loss: 2.306708\n",
      "(Epoch 53 / 100) train acc: 0.099000; val_acc: 0.088000\n",
      "(Iteration 20301 / 38200) loss: 2.306710\n",
      "(Iteration 20401 / 38200) loss: 2.306712\n",
      "(Iteration 20501 / 38200) loss: 2.306717\n",
      "(Iteration 20601 / 38200) loss: 2.306721\n",
      "(Epoch 54 / 100) train acc: 0.094000; val_acc: 0.088000\n",
      "(Iteration 20701 / 38200) loss: 2.306704\n",
      "(Iteration 20801 / 38200) loss: 2.306715\n",
      "(Iteration 20901 / 38200) loss: 2.306723\n",
      "(Iteration 21001 / 38200) loss: 2.306719\n",
      "(Epoch 55 / 100) train acc: 0.092000; val_acc: 0.088000\n",
      "(Iteration 21101 / 38200) loss: 2.306719\n",
      "(Iteration 21201 / 38200) loss: 2.306715\n",
      "(Iteration 21301 / 38200) loss: 2.306711\n",
      "(Epoch 56 / 100) train acc: 0.099000; val_acc: 0.088000\n",
      "(Iteration 21401 / 38200) loss: 2.306697\n",
      "(Iteration 21501 / 38200) loss: 2.306707\n",
      "(Iteration 21601 / 38200) loss: 2.306701\n",
      "(Iteration 21701 / 38200) loss: 2.306710\n",
      "(Epoch 57 / 100) train acc: 0.078000; val_acc: 0.088000\n",
      "(Iteration 21801 / 38200) loss: 2.306722\n",
      "(Iteration 21901 / 38200) loss: 2.306716\n",
      "(Iteration 22001 / 38200) loss: 2.306707\n",
      "(Iteration 22101 / 38200) loss: 2.306705\n",
      "(Epoch 58 / 100) train acc: 0.096000; val_acc: 0.088000\n",
      "(Iteration 22201 / 38200) loss: 2.306723\n",
      "(Iteration 22301 / 38200) loss: 2.306706\n",
      "(Iteration 22401 / 38200) loss: 2.306723\n",
      "(Iteration 22501 / 38200) loss: 2.306702\n",
      "(Epoch 59 / 100) train acc: 0.102000; val_acc: 0.089000\n",
      "(Iteration 22601 / 38200) loss: 2.306707\n",
      "(Iteration 22701 / 38200) loss: 2.306727\n",
      "(Iteration 22801 / 38200) loss: 2.306722\n",
      "(Iteration 22901 / 38200) loss: 2.306709\n",
      "(Epoch 60 / 100) train acc: 0.100000; val_acc: 0.088000\n",
      "(Iteration 23001 / 38200) loss: 2.306706\n",
      "(Iteration 23101 / 38200) loss: 2.306714\n",
      "(Iteration 23201 / 38200) loss: 2.306718\n",
      "(Iteration 23301 / 38200) loss: 2.306712\n",
      "(Epoch 61 / 100) train acc: 0.079000; val_acc: 0.088000\n",
      "(Iteration 23401 / 38200) loss: 2.306721\n",
      "(Iteration 23501 / 38200) loss: 2.306719\n",
      "(Iteration 23601 / 38200) loss: 2.306702\n",
      "(Epoch 62 / 100) train acc: 0.073000; val_acc: 0.088000\n",
      "(Iteration 23701 / 38200) loss: 2.306710\n",
      "(Iteration 23801 / 38200) loss: 2.306713\n",
      "(Iteration 23901 / 38200) loss: 2.306707\n",
      "(Iteration 24001 / 38200) loss: 2.306714\n",
      "(Epoch 63 / 100) train acc: 0.101000; val_acc: 0.089000\n",
      "(Iteration 24101 / 38200) loss: 2.306711\n",
      "(Iteration 24201 / 38200) loss: 2.306700\n",
      "(Iteration 24301 / 38200) loss: 2.306717\n",
      "(Iteration 24401 / 38200) loss: 2.306714\n",
      "(Epoch 64 / 100) train acc: 0.110000; val_acc: 0.088000\n",
      "(Iteration 24501 / 38200) loss: 2.306719\n",
      "(Iteration 24601 / 38200) loss: 2.306706\n",
      "(Iteration 24701 / 38200) loss: 2.306718\n",
      "(Iteration 24801 / 38200) loss: 2.306709\n",
      "(Epoch 65 / 100) train acc: 0.076000; val_acc: 0.088000\n",
      "(Iteration 24901 / 38200) loss: 2.306709\n",
      "(Iteration 25001 / 38200) loss: 2.306706\n",
      "(Iteration 25101 / 38200) loss: 2.306713\n",
      "(Iteration 25201 / 38200) loss: 2.306715\n",
      "(Epoch 66 / 100) train acc: 0.099000; val_acc: 0.088000\n",
      "(Iteration 25301 / 38200) loss: 2.306713\n",
      "(Iteration 25401 / 38200) loss: 2.306702\n",
      "(Iteration 25501 / 38200) loss: 2.306713\n",
      "(Epoch 67 / 100) train acc: 0.076000; val_acc: 0.089000\n",
      "(Iteration 25601 / 38200) loss: 2.306709\n",
      "(Iteration 25701 / 38200) loss: 2.306711\n",
      "(Iteration 25801 / 38200) loss: 2.306706\n",
      "(Iteration 25901 / 38200) loss: 2.306704\n",
      "(Epoch 68 / 100) train acc: 0.088000; val_acc: 0.089000\n",
      "(Iteration 26001 / 38200) loss: 2.306696\n",
      "(Iteration 26101 / 38200) loss: 2.306710\n",
      "(Iteration 26201 / 38200) loss: 2.306704\n",
      "(Iteration 26301 / 38200) loss: 2.306710\n",
      "(Epoch 69 / 100) train acc: 0.096000; val_acc: 0.088000\n",
      "(Iteration 26401 / 38200) loss: 2.306706\n",
      "(Iteration 26501 / 38200) loss: 2.306699\n",
      "(Iteration 26601 / 38200) loss: 2.306710\n",
      "(Iteration 26701 / 38200) loss: 2.306717\n",
      "(Epoch 70 / 100) train acc: 0.099000; val_acc: 0.088000\n",
      "(Iteration 26801 / 38200) loss: 2.306703\n",
      "(Iteration 26901 / 38200) loss: 2.306708\n",
      "(Iteration 27001 / 38200) loss: 2.306717\n",
      "(Iteration 27101 / 38200) loss: 2.306711\n",
      "(Epoch 71 / 100) train acc: 0.100000; val_acc: 0.088000\n",
      "(Iteration 27201 / 38200) loss: 2.306709\n",
      "(Iteration 27301 / 38200) loss: 2.306692\n",
      "(Iteration 27401 / 38200) loss: 2.306715\n",
      "(Iteration 27501 / 38200) loss: 2.306696\n",
      "(Epoch 72 / 100) train acc: 0.101000; val_acc: 0.088000\n",
      "(Iteration 27601 / 38200) loss: 2.306708\n",
      "(Iteration 27701 / 38200) loss: 2.306712\n",
      "(Iteration 27801 / 38200) loss: 2.306704\n",
      "(Epoch 73 / 100) train acc: 0.103000; val_acc: 0.088000\n",
      "(Iteration 27901 / 38200) loss: 2.306708\n",
      "(Iteration 28001 / 38200) loss: 2.306709\n",
      "(Iteration 28101 / 38200) loss: 2.306702\n",
      "(Iteration 28201 / 38200) loss: 2.306719\n",
      "(Epoch 74 / 100) train acc: 0.073000; val_acc: 0.088000\n",
      "(Iteration 28301 / 38200) loss: 2.306719\n",
      "(Iteration 28401 / 38200) loss: 2.306729\n",
      "(Iteration 28501 / 38200) loss: 2.306708\n",
      "(Iteration 28601 / 38200) loss: 2.306710\n",
      "(Epoch 75 / 100) train acc: 0.089000; val_acc: 0.088000\n",
      "(Iteration 28701 / 38200) loss: 2.306712\n",
      "(Iteration 28801 / 38200) loss: 2.306702\n",
      "(Iteration 28901 / 38200) loss: 2.306720\n",
      "(Iteration 29001 / 38200) loss: 2.306710\n",
      "(Epoch 76 / 100) train acc: 0.087000; val_acc: 0.088000\n",
      "(Iteration 29101 / 38200) loss: 2.306713\n",
      "(Iteration 29201 / 38200) loss: 2.306702\n",
      "(Iteration 29301 / 38200) loss: 2.306708\n",
      "(Iteration 29401 / 38200) loss: 2.306715\n",
      "(Epoch 77 / 100) train acc: 0.086000; val_acc: 0.088000\n",
      "(Iteration 29501 / 38200) loss: 2.306717\n",
      "(Iteration 29601 / 38200) loss: 2.306720\n",
      "(Iteration 29701 / 38200) loss: 2.306696\n",
      "(Epoch 78 / 100) train acc: 0.090000; val_acc: 0.088000\n",
      "(Iteration 29801 / 38200) loss: 2.306708\n",
      "(Iteration 29901 / 38200) loss: 2.306708\n",
      "(Iteration 30001 / 38200) loss: 2.306716\n",
      "(Iteration 30101 / 38200) loss: 2.306717\n",
      "(Epoch 79 / 100) train acc: 0.102000; val_acc: 0.088000\n",
      "(Iteration 30201 / 38200) loss: 2.306711\n",
      "(Iteration 30301 / 38200) loss: 2.306720\n",
      "(Iteration 30401 / 38200) loss: 2.306715\n",
      "(Iteration 30501 / 38200) loss: 2.306707\n",
      "(Epoch 80 / 100) train acc: 0.082000; val_acc: 0.088000\n",
      "(Iteration 30601 / 38200) loss: 2.306702\n",
      "(Iteration 30701 / 38200) loss: 2.306706\n",
      "(Iteration 30801 / 38200) loss: 2.306714\n",
      "(Iteration 30901 / 38200) loss: 2.306706\n",
      "(Epoch 81 / 100) train acc: 0.110000; val_acc: 0.088000\n",
      "(Iteration 31001 / 38200) loss: 2.306701\n",
      "(Iteration 31101 / 38200) loss: 2.306719\n",
      "(Iteration 31201 / 38200) loss: 2.306703\n",
      "(Iteration 31301 / 38200) loss: 2.306715\n",
      "(Epoch 82 / 100) train acc: 0.095000; val_acc: 0.088000\n",
      "(Iteration 31401 / 38200) loss: 2.306726\n",
      "(Iteration 31501 / 38200) loss: 2.306722\n",
      "(Iteration 31601 / 38200) loss: 2.306710\n",
      "(Iteration 31701 / 38200) loss: 2.306715\n",
      "(Epoch 83 / 100) train acc: 0.094000; val_acc: 0.088000\n",
      "(Iteration 31801 / 38200) loss: 2.306720\n",
      "(Iteration 31901 / 38200) loss: 2.306719\n",
      "(Iteration 32001 / 38200) loss: 2.306704\n",
      "(Epoch 84 / 100) train acc: 0.086000; val_acc: 0.088000\n",
      "(Iteration 32101 / 38200) loss: 2.306719\n",
      "(Iteration 32201 / 38200) loss: 2.306719\n",
      "(Iteration 32301 / 38200) loss: 2.306705\n",
      "(Iteration 32401 / 38200) loss: 2.306715\n",
      "(Epoch 85 / 100) train acc: 0.090000; val_acc: 0.088000\n",
      "(Iteration 32501 / 38200) loss: 2.306711\n",
      "(Iteration 32601 / 38200) loss: 2.306709\n",
      "(Iteration 32701 / 38200) loss: 2.306706\n",
      "(Iteration 32801 / 38200) loss: 2.306709\n",
      "(Epoch 86 / 100) train acc: 0.102000; val_acc: 0.088000\n",
      "(Iteration 32901 / 38200) loss: 2.306712\n",
      "(Iteration 33001 / 38200) loss: 2.306712\n",
      "(Iteration 33101 / 38200) loss: 2.306711\n",
      "(Iteration 33201 / 38200) loss: 2.306700\n",
      "(Epoch 87 / 100) train acc: 0.079000; val_acc: 0.088000\n",
      "(Iteration 33301 / 38200) loss: 2.306720\n",
      "(Iteration 33401 / 38200) loss: 2.306719\n",
      "(Iteration 33501 / 38200) loss: 2.306710\n",
      "(Iteration 33601 / 38200) loss: 2.306694\n",
      "(Epoch 88 / 100) train acc: 0.099000; val_acc: 0.088000\n",
      "(Iteration 33701 / 38200) loss: 2.306706\n",
      "(Iteration 33801 / 38200) loss: 2.306711\n",
      "(Iteration 33901 / 38200) loss: 2.306712\n",
      "(Epoch 89 / 100) train acc: 0.094000; val_acc: 0.088000\n",
      "(Iteration 34001 / 38200) loss: 2.306703\n",
      "(Iteration 34101 / 38200) loss: 2.306712\n",
      "(Iteration 34201 / 38200) loss: 2.306706\n",
      "(Iteration 34301 / 38200) loss: 2.306722\n",
      "(Epoch 90 / 100) train acc: 0.083000; val_acc: 0.088000\n",
      "(Iteration 34401 / 38200) loss: 2.306708\n",
      "(Iteration 34501 / 38200) loss: 2.306711\n",
      "(Iteration 34601 / 38200) loss: 2.306707\n",
      "(Iteration 34701 / 38200) loss: 2.306720\n",
      "(Epoch 91 / 100) train acc: 0.093000; val_acc: 0.088000\n",
      "(Iteration 34801 / 38200) loss: 2.306706\n",
      "(Iteration 34901 / 38200) loss: 2.306705\n",
      "(Iteration 35001 / 38200) loss: 2.306701\n",
      "(Iteration 35101 / 38200) loss: 2.306709\n",
      "(Epoch 92 / 100) train acc: 0.085000; val_acc: 0.088000\n",
      "(Iteration 35201 / 38200) loss: 2.306716\n",
      "(Iteration 35301 / 38200) loss: 2.306713\n",
      "(Iteration 35401 / 38200) loss: 2.306704\n",
      "(Iteration 35501 / 38200) loss: 2.306711\n",
      "(Epoch 93 / 100) train acc: 0.092000; val_acc: 0.089000\n",
      "(Iteration 35601 / 38200) loss: 2.306714\n",
      "(Iteration 35701 / 38200) loss: 2.306715\n",
      "(Iteration 35801 / 38200) loss: 2.306705\n",
      "(Iteration 35901 / 38200) loss: 2.306707\n",
      "(Epoch 94 / 100) train acc: 0.097000; val_acc: 0.089000\n",
      "(Iteration 36001 / 38200) loss: 2.306698\n",
      "(Iteration 36101 / 38200) loss: 2.306719\n",
      "(Iteration 36201 / 38200) loss: 2.306708\n",
      "(Epoch 95 / 100) train acc: 0.094000; val_acc: 0.089000\n",
      "(Iteration 36301 / 38200) loss: 2.306714\n",
      "(Iteration 36401 / 38200) loss: 2.306703\n",
      "(Iteration 36501 / 38200) loss: 2.306707\n",
      "(Iteration 36601 / 38200) loss: 2.306710\n",
      "(Epoch 96 / 100) train acc: 0.101000; val_acc: 0.088000\n",
      "(Iteration 36701 / 38200) loss: 2.306716\n",
      "(Iteration 36801 / 38200) loss: 2.306712\n",
      "(Iteration 36901 / 38200) loss: 2.306724\n",
      "(Iteration 37001 / 38200) loss: 2.306705\n",
      "(Epoch 97 / 100) train acc: 0.089000; val_acc: 0.088000\n",
      "(Iteration 37101 / 38200) loss: 2.306718\n",
      "(Iteration 37201 / 38200) loss: 2.306718\n",
      "(Iteration 37301 / 38200) loss: 2.306707\n",
      "(Iteration 37401 / 38200) loss: 2.306720\n",
      "(Epoch 98 / 100) train acc: 0.092000; val_acc: 0.089000\n",
      "(Iteration 37501 / 38200) loss: 2.306724\n",
      "(Iteration 37601 / 38200) loss: 2.306712\n",
      "(Iteration 37701 / 38200) loss: 2.306715\n",
      "(Iteration 37801 / 38200) loss: 2.306701\n",
      "(Epoch 99 / 100) train acc: 0.085000; val_acc: 0.089000\n",
      "(Iteration 37901 / 38200) loss: 2.306711\n",
      "(Iteration 38001 / 38200) loss: 2.306721\n",
      "(Iteration 38101 / 38200) loss: 2.306708\n",
      "(Epoch 100 / 100) train acc: 0.108000; val_acc: 0.089000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.308349\n",
      "(Epoch 0 / 100) train acc: 0.114000; val_acc: 0.095000\n",
      "(Iteration 101 / 76500) loss: 2.308347\n",
      "(Iteration 201 / 76500) loss: 2.308366\n",
      "(Iteration 301 / 76500) loss: 2.308352\n",
      "(Iteration 401 / 76500) loss: 2.308345\n",
      "(Iteration 501 / 76500) loss: 2.308349\n",
      "(Iteration 601 / 76500) loss: 2.308335\n",
      "(Iteration 701 / 76500) loss: 2.308341\n",
      "(Epoch 1 / 100) train acc: 0.089000; val_acc: 0.095000\n",
      "(Iteration 801 / 76500) loss: 2.308367\n",
      "(Iteration 901 / 76500) loss: 2.308361\n",
      "(Iteration 1001 / 76500) loss: 2.308345\n",
      "(Iteration 1101 / 76500) loss: 2.308351\n",
      "(Iteration 1201 / 76500) loss: 2.308357\n",
      "(Iteration 1301 / 76500) loss: 2.308332\n",
      "(Iteration 1401 / 76500) loss: 2.308367\n",
      "(Iteration 1501 / 76500) loss: 2.308336\n",
      "(Epoch 2 / 100) train acc: 0.101000; val_acc: 0.095000\n",
      "(Iteration 1601 / 76500) loss: 2.308356\n",
      "(Iteration 1701 / 76500) loss: 2.308338\n",
      "(Iteration 1801 / 76500) loss: 2.308356\n",
      "(Iteration 1901 / 76500) loss: 2.308349\n",
      "(Iteration 2001 / 76500) loss: 2.308358\n",
      "(Iteration 2101 / 76500) loss: 2.308358\n",
      "(Iteration 2201 / 76500) loss: 2.308339\n",
      "(Epoch 3 / 100) train acc: 0.099000; val_acc: 0.095000\n",
      "(Iteration 2301 / 76500) loss: 2.308353\n",
      "(Iteration 2401 / 76500) loss: 2.308338\n",
      "(Iteration 2501 / 76500) loss: 2.308338\n",
      "(Iteration 2601 / 76500) loss: 2.308340\n",
      "(Iteration 2701 / 76500) loss: 2.308327\n",
      "(Iteration 2801 / 76500) loss: 2.308349\n",
      "(Iteration 2901 / 76500) loss: 2.308332\n",
      "(Iteration 3001 / 76500) loss: 2.308355\n",
      "(Epoch 4 / 100) train acc: 0.097000; val_acc: 0.095000\n",
      "(Iteration 3101 / 76500) loss: 2.308353\n",
      "(Iteration 3201 / 76500) loss: 2.308331\n",
      "(Iteration 3301 / 76500) loss: 2.308338\n",
      "(Iteration 3401 / 76500) loss: 2.308345\n",
      "(Iteration 3501 / 76500) loss: 2.308354\n",
      "(Iteration 3601 / 76500) loss: 2.308342\n",
      "(Iteration 3701 / 76500) loss: 2.308349\n",
      "(Iteration 3801 / 76500) loss: 2.308340\n",
      "(Epoch 5 / 100) train acc: 0.110000; val_acc: 0.095000\n",
      "(Iteration 3901 / 76500) loss: 2.308340\n",
      "(Iteration 4001 / 76500) loss: 2.308348\n",
      "(Iteration 4101 / 76500) loss: 2.308334\n",
      "(Iteration 4201 / 76500) loss: 2.308374\n",
      "(Iteration 4301 / 76500) loss: 2.308350\n",
      "(Iteration 4401 / 76500) loss: 2.308334\n",
      "(Iteration 4501 / 76500) loss: 2.308344\n",
      "(Epoch 6 / 100) train acc: 0.112000; val_acc: 0.095000\n",
      "(Iteration 4601 / 76500) loss: 2.308345\n",
      "(Iteration 4701 / 76500) loss: 2.308351\n",
      "(Iteration 4801 / 76500) loss: 2.308336\n",
      "(Iteration 4901 / 76500) loss: 2.308341\n",
      "(Iteration 5001 / 76500) loss: 2.308343\n",
      "(Iteration 5101 / 76500) loss: 2.308332\n",
      "(Iteration 5201 / 76500) loss: 2.308326\n",
      "(Iteration 5301 / 76500) loss: 2.308325\n",
      "(Epoch 7 / 100) train acc: 0.084000; val_acc: 0.095000\n",
      "(Iteration 5401 / 76500) loss: 2.308335\n",
      "(Iteration 5501 / 76500) loss: 2.308344\n",
      "(Iteration 5601 / 76500) loss: 2.308355\n",
      "(Iteration 5701 / 76500) loss: 2.308336\n",
      "(Iteration 5801 / 76500) loss: 2.308322\n",
      "(Iteration 5901 / 76500) loss: 2.308352\n",
      "(Iteration 6001 / 76500) loss: 2.308349\n",
      "(Iteration 6101 / 76500) loss: 2.308349\n",
      "(Epoch 8 / 100) train acc: 0.091000; val_acc: 0.094000\n",
      "(Iteration 6201 / 76500) loss: 2.308348\n",
      "(Iteration 6301 / 76500) loss: 2.308324\n",
      "(Iteration 6401 / 76500) loss: 2.308347\n",
      "(Iteration 6501 / 76500) loss: 2.308337\n",
      "(Iteration 6601 / 76500) loss: 2.308336\n",
      "(Iteration 6701 / 76500) loss: 2.308348\n",
      "(Iteration 6801 / 76500) loss: 2.308337\n",
      "(Epoch 9 / 100) train acc: 0.112000; val_acc: 0.095000\n",
      "(Iteration 6901 / 76500) loss: 2.308345\n",
      "(Iteration 7001 / 76500) loss: 2.308341\n",
      "(Iteration 7101 / 76500) loss: 2.308313\n",
      "(Iteration 7201 / 76500) loss: 2.308362\n",
      "(Iteration 7301 / 76500) loss: 2.308358\n",
      "(Iteration 7401 / 76500) loss: 2.308331\n",
      "(Iteration 7501 / 76500) loss: 2.308364\n",
      "(Iteration 7601 / 76500) loss: 2.308340\n",
      "(Epoch 10 / 100) train acc: 0.095000; val_acc: 0.094000\n",
      "(Iteration 7701 / 76500) loss: 2.308336\n",
      "(Iteration 7801 / 76500) loss: 2.308323\n",
      "(Iteration 7901 / 76500) loss: 2.308343\n",
      "(Iteration 8001 / 76500) loss: 2.308357\n",
      "(Iteration 8101 / 76500) loss: 2.308349\n",
      "(Iteration 8201 / 76500) loss: 2.308340\n",
      "(Iteration 8301 / 76500) loss: 2.308346\n",
      "(Iteration 8401 / 76500) loss: 2.308330\n",
      "(Epoch 11 / 100) train acc: 0.089000; val_acc: 0.094000\n",
      "(Iteration 8501 / 76500) loss: 2.308339\n",
      "(Iteration 8601 / 76500) loss: 2.308348\n",
      "(Iteration 8701 / 76500) loss: 2.308321\n",
      "(Iteration 8801 / 76500) loss: 2.308337\n",
      "(Iteration 8901 / 76500) loss: 2.308355\n",
      "(Iteration 9001 / 76500) loss: 2.308326\n",
      "(Iteration 9101 / 76500) loss: 2.308333\n",
      "(Epoch 12 / 100) train acc: 0.109000; val_acc: 0.094000\n",
      "(Iteration 9201 / 76500) loss: 2.308344\n",
      "(Iteration 9301 / 76500) loss: 2.308349\n",
      "(Iteration 9401 / 76500) loss: 2.308346\n",
      "(Iteration 9501 / 76500) loss: 2.308335\n",
      "(Iteration 9601 / 76500) loss: 2.308361\n",
      "(Iteration 9701 / 76500) loss: 2.308356\n",
      "(Iteration 9801 / 76500) loss: 2.308338\n",
      "(Iteration 9901 / 76500) loss: 2.308344\n",
      "(Epoch 13 / 100) train acc: 0.096000; val_acc: 0.094000\n",
      "(Iteration 10001 / 76500) loss: 2.308358\n",
      "(Iteration 10101 / 76500) loss: 2.308335\n",
      "(Iteration 10201 / 76500) loss: 2.308323\n",
      "(Iteration 10301 / 76500) loss: 2.308348\n",
      "(Iteration 10401 / 76500) loss: 2.308367\n",
      "(Iteration 10501 / 76500) loss: 2.308330\n",
      "(Iteration 10601 / 76500) loss: 2.308345\n",
      "(Iteration 10701 / 76500) loss: 2.308356\n",
      "(Epoch 14 / 100) train acc: 0.096000; val_acc: 0.094000\n",
      "(Iteration 10801 / 76500) loss: 2.308316\n",
      "(Iteration 10901 / 76500) loss: 2.308339\n",
      "(Iteration 11001 / 76500) loss: 2.308346\n",
      "(Iteration 11101 / 76500) loss: 2.308328\n",
      "(Iteration 11201 / 76500) loss: 2.308342\n",
      "(Iteration 11301 / 76500) loss: 2.308358\n",
      "(Iteration 11401 / 76500) loss: 2.308347\n",
      "(Epoch 15 / 100) train acc: 0.111000; val_acc: 0.094000\n",
      "(Iteration 11501 / 76500) loss: 2.308336\n",
      "(Iteration 11601 / 76500) loss: 2.308375\n",
      "(Iteration 11701 / 76500) loss: 2.308342\n",
      "(Iteration 11801 / 76500) loss: 2.308346\n",
      "(Iteration 11901 / 76500) loss: 2.308348\n",
      "(Iteration 12001 / 76500) loss: 2.308357\n",
      "(Iteration 12101 / 76500) loss: 2.308335\n",
      "(Iteration 12201 / 76500) loss: 2.308344\n",
      "(Epoch 16 / 100) train acc: 0.081000; val_acc: 0.094000\n",
      "(Iteration 12301 / 76500) loss: 2.308343\n",
      "(Iteration 12401 / 76500) loss: 2.308337\n",
      "(Iteration 12501 / 76500) loss: 2.308367\n",
      "(Iteration 12601 / 76500) loss: 2.308345\n",
      "(Iteration 12701 / 76500) loss: 2.308346\n",
      "(Iteration 12801 / 76500) loss: 2.308327\n",
      "(Iteration 12901 / 76500) loss: 2.308338\n",
      "(Iteration 13001 / 76500) loss: 2.308335\n",
      "(Epoch 17 / 100) train acc: 0.083000; val_acc: 0.094000\n",
      "(Iteration 13101 / 76500) loss: 2.308341\n",
      "(Iteration 13201 / 76500) loss: 2.308340\n",
      "(Iteration 13301 / 76500) loss: 2.308345\n",
      "(Iteration 13401 / 76500) loss: 2.308341\n",
      "(Iteration 13501 / 76500) loss: 2.308330\n",
      "(Iteration 13601 / 76500) loss: 2.308343\n",
      "(Iteration 13701 / 76500) loss: 2.308350\n",
      "(Epoch 18 / 100) train acc: 0.100000; val_acc: 0.094000\n",
      "(Iteration 13801 / 76500) loss: 2.308355\n",
      "(Iteration 13901 / 76500) loss: 2.308327\n",
      "(Iteration 14001 / 76500) loss: 2.308344\n",
      "(Iteration 14101 / 76500) loss: 2.308338\n",
      "(Iteration 14201 / 76500) loss: 2.308335\n",
      "(Iteration 14301 / 76500) loss: 2.308336\n",
      "(Iteration 14401 / 76500) loss: 2.308352\n",
      "(Iteration 14501 / 76500) loss: 2.308330\n",
      "(Epoch 19 / 100) train acc: 0.111000; val_acc: 0.094000\n",
      "(Iteration 14601 / 76500) loss: 2.308344\n",
      "(Iteration 14701 / 76500) loss: 2.308349\n",
      "(Iteration 14801 / 76500) loss: 2.308331\n",
      "(Iteration 14901 / 76500) loss: 2.308345\n",
      "(Iteration 15001 / 76500) loss: 2.308336\n",
      "(Iteration 15101 / 76500) loss: 2.308339\n",
      "(Iteration 15201 / 76500) loss: 2.308337\n",
      "(Epoch 20 / 100) train acc: 0.110000; val_acc: 0.094000\n",
      "(Iteration 15301 / 76500) loss: 2.308329\n",
      "(Iteration 15401 / 76500) loss: 2.308357\n",
      "(Iteration 15501 / 76500) loss: 2.308343\n",
      "(Iteration 15601 / 76500) loss: 2.308341\n",
      "(Iteration 15701 / 76500) loss: 2.308343\n",
      "(Iteration 15801 / 76500) loss: 2.308339\n",
      "(Iteration 15901 / 76500) loss: 2.308332\n",
      "(Iteration 16001 / 76500) loss: 2.308330\n",
      "(Epoch 21 / 100) train acc: 0.102000; val_acc: 0.094000\n",
      "(Iteration 16101 / 76500) loss: 2.308332\n",
      "(Iteration 16201 / 76500) loss: 2.308358\n",
      "(Iteration 16301 / 76500) loss: 2.308338\n",
      "(Iteration 16401 / 76500) loss: 2.308340\n",
      "(Iteration 16501 / 76500) loss: 2.308342\n",
      "(Iteration 16601 / 76500) loss: 2.308341\n",
      "(Iteration 16701 / 76500) loss: 2.308338\n",
      "(Iteration 16801 / 76500) loss: 2.308346\n",
      "(Epoch 22 / 100) train acc: 0.102000; val_acc: 0.094000\n",
      "(Iteration 16901 / 76500) loss: 2.308340\n",
      "(Iteration 17001 / 76500) loss: 2.308341\n",
      "(Iteration 17101 / 76500) loss: 2.308334\n",
      "(Iteration 17201 / 76500) loss: 2.308335\n",
      "(Iteration 17301 / 76500) loss: 2.308362\n",
      "(Iteration 17401 / 76500) loss: 2.308349\n",
      "(Iteration 17501 / 76500) loss: 2.308343\n",
      "(Epoch 23 / 100) train acc: 0.092000; val_acc: 0.094000\n",
      "(Iteration 17601 / 76500) loss: 2.308349\n",
      "(Iteration 17701 / 76500) loss: 2.308350\n",
      "(Iteration 17801 / 76500) loss: 2.308344\n",
      "(Iteration 17901 / 76500) loss: 2.308341\n",
      "(Iteration 18001 / 76500) loss: 2.308333\n",
      "(Iteration 18101 / 76500) loss: 2.308340\n",
      "(Iteration 18201 / 76500) loss: 2.308345\n",
      "(Iteration 18301 / 76500) loss: 2.308359\n",
      "(Epoch 24 / 100) train acc: 0.106000; val_acc: 0.094000\n",
      "(Iteration 18401 / 76500) loss: 2.308332\n",
      "(Iteration 18501 / 76500) loss: 2.308318\n",
      "(Iteration 18601 / 76500) loss: 2.308333\n",
      "(Iteration 18701 / 76500) loss: 2.308328\n",
      "(Iteration 18801 / 76500) loss: 2.308341\n",
      "(Iteration 18901 / 76500) loss: 2.308346\n",
      "(Iteration 19001 / 76500) loss: 2.308334\n",
      "(Iteration 19101 / 76500) loss: 2.308342\n",
      "(Epoch 25 / 100) train acc: 0.100000; val_acc: 0.094000\n",
      "(Iteration 19201 / 76500) loss: 2.308348\n",
      "(Iteration 19301 / 76500) loss: 2.308349\n",
      "(Iteration 19401 / 76500) loss: 2.308342\n",
      "(Iteration 19501 / 76500) loss: 2.308335\n",
      "(Iteration 19601 / 76500) loss: 2.308351\n",
      "(Iteration 19701 / 76500) loss: 2.308345\n",
      "(Iteration 19801 / 76500) loss: 2.308339\n",
      "(Epoch 26 / 100) train acc: 0.107000; val_acc: 0.094000\n",
      "(Iteration 19901 / 76500) loss: 2.308368\n",
      "(Iteration 20001 / 76500) loss: 2.308364\n",
      "(Iteration 20101 / 76500) loss: 2.308336\n",
      "(Iteration 20201 / 76500) loss: 2.308325\n",
      "(Iteration 20301 / 76500) loss: 2.308355\n",
      "(Iteration 20401 / 76500) loss: 2.308355\n",
      "(Iteration 20501 / 76500) loss: 2.308337\n",
      "(Iteration 20601 / 76500) loss: 2.308332\n",
      "(Epoch 27 / 100) train acc: 0.096000; val_acc: 0.094000\n",
      "(Iteration 20701 / 76500) loss: 2.308327\n",
      "(Iteration 20801 / 76500) loss: 2.308356\n",
      "(Iteration 20901 / 76500) loss: 2.308344\n",
      "(Iteration 21001 / 76500) loss: 2.308310\n",
      "(Iteration 21101 / 76500) loss: 2.308352\n",
      "(Iteration 21201 / 76500) loss: 2.308326\n",
      "(Iteration 21301 / 76500) loss: 2.308333\n",
      "(Iteration 21401 / 76500) loss: 2.308328\n",
      "(Epoch 28 / 100) train acc: 0.095000; val_acc: 0.094000\n",
      "(Iteration 21501 / 76500) loss: 2.308350\n",
      "(Iteration 21601 / 76500) loss: 2.308332\n",
      "(Iteration 21701 / 76500) loss: 2.308322\n",
      "(Iteration 21801 / 76500) loss: 2.308347\n",
      "(Iteration 21901 / 76500) loss: 2.308343\n",
      "(Iteration 22001 / 76500) loss: 2.308350\n",
      "(Iteration 22101 / 76500) loss: 2.308352\n",
      "(Epoch 29 / 100) train acc: 0.100000; val_acc: 0.094000\n",
      "(Iteration 22201 / 76500) loss: 2.308354\n",
      "(Iteration 22301 / 76500) loss: 2.308341\n",
      "(Iteration 22401 / 76500) loss: 2.308351\n",
      "(Iteration 22501 / 76500) loss: 2.308345\n",
      "(Iteration 22601 / 76500) loss: 2.308333\n",
      "(Iteration 22701 / 76500) loss: 2.308349\n",
      "(Iteration 22801 / 76500) loss: 2.308336\n",
      "(Iteration 22901 / 76500) loss: 2.308319\n",
      "(Epoch 30 / 100) train acc: 0.097000; val_acc: 0.094000\n",
      "(Iteration 23001 / 76500) loss: 2.308347\n",
      "(Iteration 23101 / 76500) loss: 2.308333\n",
      "(Iteration 23201 / 76500) loss: 2.308347\n",
      "(Iteration 23301 / 76500) loss: 2.308346\n",
      "(Iteration 23401 / 76500) loss: 2.308334\n",
      "(Iteration 23501 / 76500) loss: 2.308368\n",
      "(Iteration 23601 / 76500) loss: 2.308353\n",
      "(Iteration 23701 / 76500) loss: 2.308350\n",
      "(Epoch 31 / 100) train acc: 0.124000; val_acc: 0.094000\n",
      "(Iteration 23801 / 76500) loss: 2.308331\n",
      "(Iteration 23901 / 76500) loss: 2.308336\n",
      "(Iteration 24001 / 76500) loss: 2.308346\n",
      "(Iteration 24101 / 76500) loss: 2.308343\n",
      "(Iteration 24201 / 76500) loss: 2.308355\n",
      "(Iteration 24301 / 76500) loss: 2.308332\n",
      "(Iteration 24401 / 76500) loss: 2.308346\n",
      "(Epoch 32 / 100) train acc: 0.091000; val_acc: 0.094000\n",
      "(Iteration 24501 / 76500) loss: 2.308343\n",
      "(Iteration 24601 / 76500) loss: 2.308341\n",
      "(Iteration 24701 / 76500) loss: 2.308328\n",
      "(Iteration 24801 / 76500) loss: 2.308326\n",
      "(Iteration 24901 / 76500) loss: 2.308323\n",
      "(Iteration 25001 / 76500) loss: 2.308348\n",
      "(Iteration 25101 / 76500) loss: 2.308351\n",
      "(Iteration 25201 / 76500) loss: 2.308333\n",
      "(Epoch 33 / 100) train acc: 0.103000; val_acc: 0.094000\n",
      "(Iteration 25301 / 76500) loss: 2.308329\n",
      "(Iteration 25401 / 76500) loss: 2.308333\n",
      "(Iteration 25501 / 76500) loss: 2.308358\n",
      "(Iteration 25601 / 76500) loss: 2.308351\n",
      "(Iteration 25701 / 76500) loss: 2.308331\n",
      "(Iteration 25801 / 76500) loss: 2.308348\n",
      "(Iteration 25901 / 76500) loss: 2.308361\n",
      "(Iteration 26001 / 76500) loss: 2.308330\n",
      "(Epoch 34 / 100) train acc: 0.090000; val_acc: 0.094000\n",
      "(Iteration 26101 / 76500) loss: 2.308357\n",
      "(Iteration 26201 / 76500) loss: 2.308347\n",
      "(Iteration 26301 / 76500) loss: 2.308351\n",
      "(Iteration 26401 / 76500) loss: 2.308336\n",
      "(Iteration 26501 / 76500) loss: 2.308370\n",
      "(Iteration 26601 / 76500) loss: 2.308335\n",
      "(Iteration 26701 / 76500) loss: 2.308333\n",
      "(Epoch 35 / 100) train acc: 0.097000; val_acc: 0.094000\n",
      "(Iteration 26801 / 76500) loss: 2.308346\n",
      "(Iteration 26901 / 76500) loss: 2.308330\n",
      "(Iteration 27001 / 76500) loss: 2.308356\n",
      "(Iteration 27101 / 76500) loss: 2.308357\n",
      "(Iteration 27201 / 76500) loss: 2.308348\n",
      "(Iteration 27301 / 76500) loss: 2.308358\n",
      "(Iteration 27401 / 76500) loss: 2.308329\n",
      "(Iteration 27501 / 76500) loss: 2.308345\n",
      "(Epoch 36 / 100) train acc: 0.101000; val_acc: 0.094000\n",
      "(Iteration 27601 / 76500) loss: 2.308342\n",
      "(Iteration 27701 / 76500) loss: 2.308348\n",
      "(Iteration 27801 / 76500) loss: 2.308345\n",
      "(Iteration 27901 / 76500) loss: 2.308346\n",
      "(Iteration 28001 / 76500) loss: 2.308320\n",
      "(Iteration 28101 / 76500) loss: 2.308336\n",
      "(Iteration 28201 / 76500) loss: 2.308345\n",
      "(Iteration 28301 / 76500) loss: 2.308322\n",
      "(Epoch 37 / 100) train acc: 0.094000; val_acc: 0.094000\n",
      "(Iteration 28401 / 76500) loss: 2.308342\n",
      "(Iteration 28501 / 76500) loss: 2.308333\n",
      "(Iteration 28601 / 76500) loss: 2.308338\n",
      "(Iteration 28701 / 76500) loss: 2.308341\n",
      "(Iteration 28801 / 76500) loss: 2.308323\n",
      "(Iteration 28901 / 76500) loss: 2.308335\n",
      "(Iteration 29001 / 76500) loss: 2.308327\n",
      "(Epoch 38 / 100) train acc: 0.115000; val_acc: 0.094000\n",
      "(Iteration 29101 / 76500) loss: 2.308354\n",
      "(Iteration 29201 / 76500) loss: 2.308357\n",
      "(Iteration 29301 / 76500) loss: 2.308331\n",
      "(Iteration 29401 / 76500) loss: 2.308358\n",
      "(Iteration 29501 / 76500) loss: 2.308354\n",
      "(Iteration 29601 / 76500) loss: 2.308350\n",
      "(Iteration 29701 / 76500) loss: 2.308346\n",
      "(Iteration 29801 / 76500) loss: 2.308346\n",
      "(Epoch 39 / 100) train acc: 0.101000; val_acc: 0.094000\n",
      "(Iteration 29901 / 76500) loss: 2.308350\n",
      "(Iteration 30001 / 76500) loss: 2.308335\n",
      "(Iteration 30101 / 76500) loss: 2.308345\n",
      "(Iteration 30201 / 76500) loss: 2.308351\n",
      "(Iteration 30301 / 76500) loss: 2.308310\n",
      "(Iteration 30401 / 76500) loss: 2.308345\n",
      "(Iteration 30501 / 76500) loss: 2.308337\n",
      "(Epoch 40 / 100) train acc: 0.093000; val_acc: 0.094000\n",
      "(Iteration 30601 / 76500) loss: 2.308354\n",
      "(Iteration 30701 / 76500) loss: 2.308337\n",
      "(Iteration 30801 / 76500) loss: 2.308351\n",
      "(Iteration 30901 / 76500) loss: 2.308358\n",
      "(Iteration 31001 / 76500) loss: 2.308357\n",
      "(Iteration 31101 / 76500) loss: 2.308331\n",
      "(Iteration 31201 / 76500) loss: 2.308341\n",
      "(Iteration 31301 / 76500) loss: 2.308358\n",
      "(Epoch 41 / 100) train acc: 0.115000; val_acc: 0.094000\n",
      "(Iteration 31401 / 76500) loss: 2.308343\n",
      "(Iteration 31501 / 76500) loss: 2.308346\n",
      "(Iteration 31601 / 76500) loss: 2.308336\n",
      "(Iteration 31701 / 76500) loss: 2.308340\n",
      "(Iteration 31801 / 76500) loss: 2.308345\n",
      "(Iteration 31901 / 76500) loss: 2.308357\n",
      "(Iteration 32001 / 76500) loss: 2.308357\n",
      "(Iteration 32101 / 76500) loss: 2.308330\n",
      "(Epoch 42 / 100) train acc: 0.098000; val_acc: 0.094000\n",
      "(Iteration 32201 / 76500) loss: 2.308345\n",
      "(Iteration 32301 / 76500) loss: 2.308343\n",
      "(Iteration 32401 / 76500) loss: 2.308331\n",
      "(Iteration 32501 / 76500) loss: 2.308348\n",
      "(Iteration 32601 / 76500) loss: 2.308359\n",
      "(Iteration 32701 / 76500) loss: 2.308331\n",
      "(Iteration 32801 / 76500) loss: 2.308355\n",
      "(Epoch 43 / 100) train acc: 0.109000; val_acc: 0.094000\n",
      "(Iteration 32901 / 76500) loss: 2.308349\n",
      "(Iteration 33001 / 76500) loss: 2.308365\n",
      "(Iteration 33101 / 76500) loss: 2.308337\n",
      "(Iteration 33201 / 76500) loss: 2.308361\n",
      "(Iteration 33301 / 76500) loss: 2.308329\n",
      "(Iteration 33401 / 76500) loss: 2.308351\n",
      "(Iteration 33501 / 76500) loss: 2.308333\n",
      "(Iteration 33601 / 76500) loss: 2.308333\n",
      "(Epoch 44 / 100) train acc: 0.119000; val_acc: 0.094000\n",
      "(Iteration 33701 / 76500) loss: 2.308356\n",
      "(Iteration 33801 / 76500) loss: 2.308343\n",
      "(Iteration 33901 / 76500) loss: 2.308330\n",
      "(Iteration 34001 / 76500) loss: 2.308336\n",
      "(Iteration 34101 / 76500) loss: 2.308345\n",
      "(Iteration 34201 / 76500) loss: 2.308341\n",
      "(Iteration 34301 / 76500) loss: 2.308338\n",
      "(Iteration 34401 / 76500) loss: 2.308357\n",
      "(Epoch 45 / 100) train acc: 0.095000; val_acc: 0.094000\n",
      "(Iteration 34501 / 76500) loss: 2.308342\n",
      "(Iteration 34601 / 76500) loss: 2.308340\n",
      "(Iteration 34701 / 76500) loss: 2.308351\n",
      "(Iteration 34801 / 76500) loss: 2.308343\n",
      "(Iteration 34901 / 76500) loss: 2.308348\n",
      "(Iteration 35001 / 76500) loss: 2.308339\n",
      "(Iteration 35101 / 76500) loss: 2.308328\n",
      "(Epoch 46 / 100) train acc: 0.093000; val_acc: 0.094000\n",
      "(Iteration 35201 / 76500) loss: 2.308366\n",
      "(Iteration 35301 / 76500) loss: 2.308324\n",
      "(Iteration 35401 / 76500) loss: 2.308325\n",
      "(Iteration 35501 / 76500) loss: 2.308353\n",
      "(Iteration 35601 / 76500) loss: 2.308328\n",
      "(Iteration 35701 / 76500) loss: 2.308350\n",
      "(Iteration 35801 / 76500) loss: 2.308327\n",
      "(Iteration 35901 / 76500) loss: 2.308338\n",
      "(Epoch 47 / 100) train acc: 0.111000; val_acc: 0.094000\n",
      "(Iteration 36001 / 76500) loss: 2.308362\n",
      "(Iteration 36101 / 76500) loss: 2.308344\n",
      "(Iteration 36201 / 76500) loss: 2.308341\n",
      "(Iteration 36301 / 76500) loss: 2.308354\n",
      "(Iteration 36401 / 76500) loss: 2.308340\n",
      "(Iteration 36501 / 76500) loss: 2.308336\n",
      "(Iteration 36601 / 76500) loss: 2.308347\n",
      "(Iteration 36701 / 76500) loss: 2.308358\n",
      "(Epoch 48 / 100) train acc: 0.086000; val_acc: 0.094000\n",
      "(Iteration 36801 / 76500) loss: 2.308338\n",
      "(Iteration 36901 / 76500) loss: 2.308334\n",
      "(Iteration 37001 / 76500) loss: 2.308352\n",
      "(Iteration 37101 / 76500) loss: 2.308342\n",
      "(Iteration 37201 / 76500) loss: 2.308318\n",
      "(Iteration 37301 / 76500) loss: 2.308348\n",
      "(Iteration 37401 / 76500) loss: 2.308324\n",
      "(Epoch 49 / 100) train acc: 0.094000; val_acc: 0.094000\n",
      "(Iteration 37501 / 76500) loss: 2.308360\n",
      "(Iteration 37601 / 76500) loss: 2.308338\n",
      "(Iteration 37701 / 76500) loss: 2.308344\n",
      "(Iteration 37801 / 76500) loss: 2.308344\n",
      "(Iteration 37901 / 76500) loss: 2.308332\n",
      "(Iteration 38001 / 76500) loss: 2.308343\n",
      "(Iteration 38101 / 76500) loss: 2.308343\n",
      "(Iteration 38201 / 76500) loss: 2.308338\n",
      "(Epoch 50 / 100) train acc: 0.091000; val_acc: 0.094000\n",
      "(Iteration 38301 / 76500) loss: 2.308332\n",
      "(Iteration 38401 / 76500) loss: 2.308329\n",
      "(Iteration 38501 / 76500) loss: 2.308352\n",
      "(Iteration 38601 / 76500) loss: 2.308354\n",
      "(Iteration 38701 / 76500) loss: 2.308351\n",
      "(Iteration 38801 / 76500) loss: 2.308353\n",
      "(Iteration 38901 / 76500) loss: 2.308338\n",
      "(Iteration 39001 / 76500) loss: 2.308350\n",
      "(Epoch 51 / 100) train acc: 0.099000; val_acc: 0.094000\n",
      "(Iteration 39101 / 76500) loss: 2.308358\n",
      "(Iteration 39201 / 76500) loss: 2.308338\n",
      "(Iteration 39301 / 76500) loss: 2.308344\n",
      "(Iteration 39401 / 76500) loss: 2.308336\n",
      "(Iteration 39501 / 76500) loss: 2.308335\n",
      "(Iteration 39601 / 76500) loss: 2.308330\n",
      "(Iteration 39701 / 76500) loss: 2.308350\n",
      "(Epoch 52 / 100) train acc: 0.113000; val_acc: 0.094000\n",
      "(Iteration 39801 / 76500) loss: 2.308325\n",
      "(Iteration 39901 / 76500) loss: 2.308353\n",
      "(Iteration 40001 / 76500) loss: 2.308354\n",
      "(Iteration 40101 / 76500) loss: 2.308334\n",
      "(Iteration 40201 / 76500) loss: 2.308337\n",
      "(Iteration 40301 / 76500) loss: 2.308341\n",
      "(Iteration 40401 / 76500) loss: 2.308343\n",
      "(Iteration 40501 / 76500) loss: 2.308336\n",
      "(Epoch 53 / 100) train acc: 0.102000; val_acc: 0.094000\n",
      "(Iteration 40601 / 76500) loss: 2.308341\n",
      "(Iteration 40701 / 76500) loss: 2.308341\n",
      "(Iteration 40801 / 76500) loss: 2.308331\n",
      "(Iteration 40901 / 76500) loss: 2.308344\n",
      "(Iteration 41001 / 76500) loss: 2.308341\n",
      "(Iteration 41101 / 76500) loss: 2.308352\n",
      "(Iteration 41201 / 76500) loss: 2.308332\n",
      "(Iteration 41301 / 76500) loss: 2.308333\n",
      "(Epoch 54 / 100) train acc: 0.102000; val_acc: 0.094000\n",
      "(Iteration 41401 / 76500) loss: 2.308335\n",
      "(Iteration 41501 / 76500) loss: 2.308350\n",
      "(Iteration 41601 / 76500) loss: 2.308351\n",
      "(Iteration 41701 / 76500) loss: 2.308346\n",
      "(Iteration 41801 / 76500) loss: 2.308364\n",
      "(Iteration 41901 / 76500) loss: 2.308335\n",
      "(Iteration 42001 / 76500) loss: 2.308352\n",
      "(Epoch 55 / 100) train acc: 0.095000; val_acc: 0.094000\n",
      "(Iteration 42101 / 76500) loss: 2.308360\n",
      "(Iteration 42201 / 76500) loss: 2.308358\n",
      "(Iteration 42301 / 76500) loss: 2.308325\n",
      "(Iteration 42401 / 76500) loss: 2.308332\n",
      "(Iteration 42501 / 76500) loss: 2.308342\n",
      "(Iteration 42601 / 76500) loss: 2.308347\n",
      "(Iteration 42701 / 76500) loss: 2.308341\n",
      "(Iteration 42801 / 76500) loss: 2.308340\n",
      "(Epoch 56 / 100) train acc: 0.105000; val_acc: 0.094000\n",
      "(Iteration 42901 / 76500) loss: 2.308337\n",
      "(Iteration 43001 / 76500) loss: 2.308339\n",
      "(Iteration 43101 / 76500) loss: 2.308323\n",
      "(Iteration 43201 / 76500) loss: 2.308351\n",
      "(Iteration 43301 / 76500) loss: 2.308342\n",
      "(Iteration 43401 / 76500) loss: 2.308342\n",
      "(Iteration 43501 / 76500) loss: 2.308342\n",
      "(Iteration 43601 / 76500) loss: 2.308341\n",
      "(Epoch 57 / 100) train acc: 0.090000; val_acc: 0.094000\n",
      "(Iteration 43701 / 76500) loss: 2.308345\n",
      "(Iteration 43801 / 76500) loss: 2.308332\n",
      "(Iteration 43901 / 76500) loss: 2.308344\n",
      "(Iteration 44001 / 76500) loss: 2.308344\n",
      "(Iteration 44101 / 76500) loss: 2.308349\n",
      "(Iteration 44201 / 76500) loss: 2.308331\n",
      "(Iteration 44301 / 76500) loss: 2.308344\n",
      "(Epoch 58 / 100) train acc: 0.100000; val_acc: 0.094000\n",
      "(Iteration 44401 / 76500) loss: 2.308340\n",
      "(Iteration 44501 / 76500) loss: 2.308355\n",
      "(Iteration 44601 / 76500) loss: 2.308334\n",
      "(Iteration 44701 / 76500) loss: 2.308339\n",
      "(Iteration 44801 / 76500) loss: 2.308356\n",
      "(Iteration 44901 / 76500) loss: 2.308348\n",
      "(Iteration 45001 / 76500) loss: 2.308332\n",
      "(Iteration 45101 / 76500) loss: 2.308347\n",
      "(Epoch 59 / 100) train acc: 0.103000; val_acc: 0.094000\n",
      "(Iteration 45201 / 76500) loss: 2.308357\n",
      "(Iteration 45301 / 76500) loss: 2.308329\n",
      "(Iteration 45401 / 76500) loss: 2.308342\n",
      "(Iteration 45501 / 76500) loss: 2.308343\n",
      "(Iteration 45601 / 76500) loss: 2.308339\n",
      "(Iteration 45701 / 76500) loss: 2.308345\n",
      "(Iteration 45801 / 76500) loss: 2.308343\n",
      "(Epoch 60 / 100) train acc: 0.088000; val_acc: 0.094000\n",
      "(Iteration 45901 / 76500) loss: 2.308345\n",
      "(Iteration 46001 / 76500) loss: 2.308335\n",
      "(Iteration 46101 / 76500) loss: 2.308347\n",
      "(Iteration 46201 / 76500) loss: 2.308346\n",
      "(Iteration 46301 / 76500) loss: 2.308340\n",
      "(Iteration 46401 / 76500) loss: 2.308343\n",
      "(Iteration 46501 / 76500) loss: 2.308335\n",
      "(Iteration 46601 / 76500) loss: 2.308357\n",
      "(Epoch 61 / 100) train acc: 0.098000; val_acc: 0.094000\n",
      "(Iteration 46701 / 76500) loss: 2.308343\n",
      "(Iteration 46801 / 76500) loss: 2.308350\n",
      "(Iteration 46901 / 76500) loss: 2.308362\n",
      "(Iteration 47001 / 76500) loss: 2.308349\n",
      "(Iteration 47101 / 76500) loss: 2.308355\n",
      "(Iteration 47201 / 76500) loss: 2.308352\n",
      "(Iteration 47301 / 76500) loss: 2.308343\n",
      "(Iteration 47401 / 76500) loss: 2.308345\n",
      "(Epoch 62 / 100) train acc: 0.105000; val_acc: 0.094000\n",
      "(Iteration 47501 / 76500) loss: 2.308342\n",
      "(Iteration 47601 / 76500) loss: 2.308323\n",
      "(Iteration 47701 / 76500) loss: 2.308340\n",
      "(Iteration 47801 / 76500) loss: 2.308348\n",
      "(Iteration 47901 / 76500) loss: 2.308345\n",
      "(Iteration 48001 / 76500) loss: 2.308341\n",
      "(Iteration 48101 / 76500) loss: 2.308337\n",
      "(Epoch 63 / 100) train acc: 0.086000; val_acc: 0.094000\n",
      "(Iteration 48201 / 76500) loss: 2.308335\n",
      "(Iteration 48301 / 76500) loss: 2.308335\n",
      "(Iteration 48401 / 76500) loss: 2.308349\n",
      "(Iteration 48501 / 76500) loss: 2.308343\n",
      "(Iteration 48601 / 76500) loss: 2.308362\n",
      "(Iteration 48701 / 76500) loss: 2.308341\n",
      "(Iteration 48801 / 76500) loss: 2.308338\n",
      "(Iteration 48901 / 76500) loss: 2.308351\n",
      "(Epoch 64 / 100) train acc: 0.074000; val_acc: 0.094000\n",
      "(Iteration 49001 / 76500) loss: 2.308330\n",
      "(Iteration 49101 / 76500) loss: 2.308335\n",
      "(Iteration 49201 / 76500) loss: 2.308349\n",
      "(Iteration 49301 / 76500) loss: 2.308325\n",
      "(Iteration 49401 / 76500) loss: 2.308350\n",
      "(Iteration 49501 / 76500) loss: 2.308327\n",
      "(Iteration 49601 / 76500) loss: 2.308347\n",
      "(Iteration 49701 / 76500) loss: 2.308334\n",
      "(Epoch 65 / 100) train acc: 0.097000; val_acc: 0.094000\n",
      "(Iteration 49801 / 76500) loss: 2.308346\n",
      "(Iteration 49901 / 76500) loss: 2.308349\n",
      "(Iteration 50001 / 76500) loss: 2.308338\n",
      "(Iteration 50101 / 76500) loss: 2.308350\n",
      "(Iteration 50201 / 76500) loss: 2.308331\n",
      "(Iteration 50301 / 76500) loss: 2.308333\n",
      "(Iteration 50401 / 76500) loss: 2.308326\n",
      "(Epoch 66 / 100) train acc: 0.104000; val_acc: 0.094000\n",
      "(Iteration 50501 / 76500) loss: 2.308323\n",
      "(Iteration 50601 / 76500) loss: 2.308335\n",
      "(Iteration 50701 / 76500) loss: 2.308354\n",
      "(Iteration 50801 / 76500) loss: 2.308339\n",
      "(Iteration 50901 / 76500) loss: 2.308348\n",
      "(Iteration 51001 / 76500) loss: 2.308317\n",
      "(Iteration 51101 / 76500) loss: 2.308325\n",
      "(Iteration 51201 / 76500) loss: 2.308348\n",
      "(Epoch 67 / 100) train acc: 0.111000; val_acc: 0.094000\n",
      "(Iteration 51301 / 76500) loss: 2.308336\n",
      "(Iteration 51401 / 76500) loss: 2.308339\n",
      "(Iteration 51501 / 76500) loss: 2.308341\n",
      "(Iteration 51601 / 76500) loss: 2.308328\n",
      "(Iteration 51701 / 76500) loss: 2.308325\n",
      "(Iteration 51801 / 76500) loss: 2.308355\n",
      "(Iteration 51901 / 76500) loss: 2.308336\n",
      "(Iteration 52001 / 76500) loss: 2.308354\n",
      "(Epoch 68 / 100) train acc: 0.116000; val_acc: 0.094000\n",
      "(Iteration 52101 / 76500) loss: 2.308342\n",
      "(Iteration 52201 / 76500) loss: 2.308348\n",
      "(Iteration 52301 / 76500) loss: 2.308351\n",
      "(Iteration 52401 / 76500) loss: 2.308351\n",
      "(Iteration 52501 / 76500) loss: 2.308350\n",
      "(Iteration 52601 / 76500) loss: 2.308344\n",
      "(Iteration 52701 / 76500) loss: 2.308336\n",
      "(Epoch 69 / 100) train acc: 0.102000; val_acc: 0.094000\n",
      "(Iteration 52801 / 76500) loss: 2.308347\n",
      "(Iteration 52901 / 76500) loss: 2.308361\n",
      "(Iteration 53001 / 76500) loss: 2.308350\n",
      "(Iteration 53101 / 76500) loss: 2.308339\n",
      "(Iteration 53201 / 76500) loss: 2.308327\n",
      "(Iteration 53301 / 76500) loss: 2.308340\n",
      "(Iteration 53401 / 76500) loss: 2.308338\n",
      "(Iteration 53501 / 76500) loss: 2.308336\n",
      "(Epoch 70 / 100) train acc: 0.100000; val_acc: 0.094000\n",
      "(Iteration 53601 / 76500) loss: 2.308332\n",
      "(Iteration 53701 / 76500) loss: 2.308358\n",
      "(Iteration 53801 / 76500) loss: 2.308336\n",
      "(Iteration 53901 / 76500) loss: 2.308363\n",
      "(Iteration 54001 / 76500) loss: 2.308341\n",
      "(Iteration 54101 / 76500) loss: 2.308336\n",
      "(Iteration 54201 / 76500) loss: 2.308350\n",
      "(Iteration 54301 / 76500) loss: 2.308344\n",
      "(Epoch 71 / 100) train acc: 0.109000; val_acc: 0.094000\n",
      "(Iteration 54401 / 76500) loss: 2.308335\n",
      "(Iteration 54501 / 76500) loss: 2.308356\n",
      "(Iteration 54601 / 76500) loss: 2.308320\n",
      "(Iteration 54701 / 76500) loss: 2.308352\n",
      "(Iteration 54801 / 76500) loss: 2.308353\n",
      "(Iteration 54901 / 76500) loss: 2.308330\n",
      "(Iteration 55001 / 76500) loss: 2.308339\n",
      "(Epoch 72 / 100) train acc: 0.094000; val_acc: 0.094000\n",
      "(Iteration 55101 / 76500) loss: 2.308352\n",
      "(Iteration 55201 / 76500) loss: 2.308347\n",
      "(Iteration 55301 / 76500) loss: 2.308334\n",
      "(Iteration 55401 / 76500) loss: 2.308330\n",
      "(Iteration 55501 / 76500) loss: 2.308327\n",
      "(Iteration 55601 / 76500) loss: 2.308336\n",
      "(Iteration 55701 / 76500) loss: 2.308317\n",
      "(Iteration 55801 / 76500) loss: 2.308337\n",
      "(Epoch 73 / 100) train acc: 0.085000; val_acc: 0.094000\n",
      "(Iteration 55901 / 76500) loss: 2.308337\n",
      "(Iteration 56001 / 76500) loss: 2.308334\n",
      "(Iteration 56101 / 76500) loss: 2.308336\n",
      "(Iteration 56201 / 76500) loss: 2.308350\n",
      "(Iteration 56301 / 76500) loss: 2.308333\n",
      "(Iteration 56401 / 76500) loss: 2.308348\n",
      "(Iteration 56501 / 76500) loss: 2.308330\n",
      "(Iteration 56601 / 76500) loss: 2.308349\n",
      "(Epoch 74 / 100) train acc: 0.101000; val_acc: 0.094000\n",
      "(Iteration 56701 / 76500) loss: 2.308354\n",
      "(Iteration 56801 / 76500) loss: 2.308322\n",
      "(Iteration 56901 / 76500) loss: 2.308326\n",
      "(Iteration 57001 / 76500) loss: 2.308345\n",
      "(Iteration 57101 / 76500) loss: 2.308324\n",
      "(Iteration 57201 / 76500) loss: 2.308354\n",
      "(Iteration 57301 / 76500) loss: 2.308340\n",
      "(Epoch 75 / 100) train acc: 0.092000; val_acc: 0.094000\n",
      "(Iteration 57401 / 76500) loss: 2.308342\n",
      "(Iteration 57501 / 76500) loss: 2.308348\n",
      "(Iteration 57601 / 76500) loss: 2.308343\n",
      "(Iteration 57701 / 76500) loss: 2.308323\n",
      "(Iteration 57801 / 76500) loss: 2.308355\n",
      "(Iteration 57901 / 76500) loss: 2.308335\n",
      "(Iteration 58001 / 76500) loss: 2.308346\n",
      "(Iteration 58101 / 76500) loss: 2.308330\n",
      "(Epoch 76 / 100) train acc: 0.101000; val_acc: 0.094000\n",
      "(Iteration 58201 / 76500) loss: 2.308312\n",
      "(Iteration 58301 / 76500) loss: 2.308332\n",
      "(Iteration 58401 / 76500) loss: 2.308346\n",
      "(Iteration 58501 / 76500) loss: 2.308344\n",
      "(Iteration 58601 / 76500) loss: 2.308325\n",
      "(Iteration 58701 / 76500) loss: 2.308334\n",
      "(Iteration 58801 / 76500) loss: 2.308339\n",
      "(Iteration 58901 / 76500) loss: 2.308342\n",
      "(Epoch 77 / 100) train acc: 0.111000; val_acc: 0.094000\n",
      "(Iteration 59001 / 76500) loss: 2.308350\n",
      "(Iteration 59101 / 76500) loss: 2.308344\n",
      "(Iteration 59201 / 76500) loss: 2.308344\n",
      "(Iteration 59301 / 76500) loss: 2.308361\n",
      "(Iteration 59401 / 76500) loss: 2.308345\n",
      "(Iteration 59501 / 76500) loss: 2.308346\n",
      "(Iteration 59601 / 76500) loss: 2.308343\n",
      "(Epoch 78 / 100) train acc: 0.098000; val_acc: 0.094000\n",
      "(Iteration 59701 / 76500) loss: 2.308350\n",
      "(Iteration 59801 / 76500) loss: 2.308336\n",
      "(Iteration 59901 / 76500) loss: 2.308347\n",
      "(Iteration 60001 / 76500) loss: 2.308331\n",
      "(Iteration 60101 / 76500) loss: 2.308337\n",
      "(Iteration 60201 / 76500) loss: 2.308345\n",
      "(Iteration 60301 / 76500) loss: 2.308324\n",
      "(Iteration 60401 / 76500) loss: 2.308339\n",
      "(Epoch 79 / 100) train acc: 0.101000; val_acc: 0.094000\n",
      "(Iteration 60501 / 76500) loss: 2.308339\n",
      "(Iteration 60601 / 76500) loss: 2.308353\n",
      "(Iteration 60701 / 76500) loss: 2.308315\n",
      "(Iteration 60801 / 76500) loss: 2.308332\n",
      "(Iteration 60901 / 76500) loss: 2.308354\n",
      "(Iteration 61001 / 76500) loss: 2.308336\n",
      "(Iteration 61101 / 76500) loss: 2.308320\n",
      "(Epoch 80 / 100) train acc: 0.091000; val_acc: 0.094000\n",
      "(Iteration 61201 / 76500) loss: 2.308358\n",
      "(Iteration 61301 / 76500) loss: 2.308335\n",
      "(Iteration 61401 / 76500) loss: 2.308335\n",
      "(Iteration 61501 / 76500) loss: 2.308361\n",
      "(Iteration 61601 / 76500) loss: 2.308344\n",
      "(Iteration 61701 / 76500) loss: 2.308324\n",
      "(Iteration 61801 / 76500) loss: 2.308331\n",
      "(Iteration 61901 / 76500) loss: 2.308326\n",
      "(Epoch 81 / 100) train acc: 0.116000; val_acc: 0.094000\n",
      "(Iteration 62001 / 76500) loss: 2.308349\n",
      "(Iteration 62101 / 76500) loss: 2.308349\n",
      "(Iteration 62201 / 76500) loss: 2.308334\n",
      "(Iteration 62301 / 76500) loss: 2.308353\n",
      "(Iteration 62401 / 76500) loss: 2.308337\n",
      "(Iteration 62501 / 76500) loss: 2.308354\n",
      "(Iteration 62601 / 76500) loss: 2.308326\n",
      "(Iteration 62701 / 76500) loss: 2.308346\n",
      "(Epoch 82 / 100) train acc: 0.098000; val_acc: 0.094000\n",
      "(Iteration 62801 / 76500) loss: 2.308355\n",
      "(Iteration 62901 / 76500) loss: 2.308330\n",
      "(Iteration 63001 / 76500) loss: 2.308333\n",
      "(Iteration 63101 / 76500) loss: 2.308340\n",
      "(Iteration 63201 / 76500) loss: 2.308329\n",
      "(Iteration 63301 / 76500) loss: 2.308365\n",
      "(Iteration 63401 / 76500) loss: 2.308361\n",
      "(Epoch 83 / 100) train acc: 0.095000; val_acc: 0.094000\n",
      "(Iteration 63501 / 76500) loss: 2.308343\n",
      "(Iteration 63601 / 76500) loss: 2.308358\n",
      "(Iteration 63701 / 76500) loss: 2.308339\n",
      "(Iteration 63801 / 76500) loss: 2.308320\n",
      "(Iteration 63901 / 76500) loss: 2.308324\n",
      "(Iteration 64001 / 76500) loss: 2.308363\n",
      "(Iteration 64101 / 76500) loss: 2.308343\n",
      "(Iteration 64201 / 76500) loss: 2.308357\n",
      "(Epoch 84 / 100) train acc: 0.115000; val_acc: 0.094000\n",
      "(Iteration 64301 / 76500) loss: 2.308356\n",
      "(Iteration 64401 / 76500) loss: 2.308337\n",
      "(Iteration 64501 / 76500) loss: 2.308342\n",
      "(Iteration 64601 / 76500) loss: 2.308336\n",
      "(Iteration 64701 / 76500) loss: 2.308326\n",
      "(Iteration 64801 / 76500) loss: 2.308343\n",
      "(Iteration 64901 / 76500) loss: 2.308328\n",
      "(Iteration 65001 / 76500) loss: 2.308309\n",
      "(Epoch 85 / 100) train acc: 0.101000; val_acc: 0.094000\n",
      "(Iteration 65101 / 76500) loss: 2.308348\n",
      "(Iteration 65201 / 76500) loss: 2.308339\n",
      "(Iteration 65301 / 76500) loss: 2.308350\n",
      "(Iteration 65401 / 76500) loss: 2.308345\n",
      "(Iteration 65501 / 76500) loss: 2.308347\n",
      "(Iteration 65601 / 76500) loss: 2.308353\n",
      "(Iteration 65701 / 76500) loss: 2.308347\n",
      "(Epoch 86 / 100) train acc: 0.110000; val_acc: 0.094000\n",
      "(Iteration 65801 / 76500) loss: 2.308361\n",
      "(Iteration 65901 / 76500) loss: 2.308348\n",
      "(Iteration 66001 / 76500) loss: 2.308335\n",
      "(Iteration 66101 / 76500) loss: 2.308345\n",
      "(Iteration 66201 / 76500) loss: 2.308345\n",
      "(Iteration 66301 / 76500) loss: 2.308330\n",
      "(Iteration 66401 / 76500) loss: 2.308357\n",
      "(Iteration 66501 / 76500) loss: 2.308353\n",
      "(Epoch 87 / 100) train acc: 0.113000; val_acc: 0.094000\n",
      "(Iteration 66601 / 76500) loss: 2.308327\n",
      "(Iteration 66701 / 76500) loss: 2.308330\n",
      "(Iteration 66801 / 76500) loss: 2.308332\n",
      "(Iteration 66901 / 76500) loss: 2.308329\n",
      "(Iteration 67001 / 76500) loss: 2.308343\n",
      "(Iteration 67101 / 76500) loss: 2.308336\n",
      "(Iteration 67201 / 76500) loss: 2.308337\n",
      "(Iteration 67301 / 76500) loss: 2.308338\n",
      "(Epoch 88 / 100) train acc: 0.102000; val_acc: 0.094000\n",
      "(Iteration 67401 / 76500) loss: 2.308342\n",
      "(Iteration 67501 / 76500) loss: 2.308348\n",
      "(Iteration 67601 / 76500) loss: 2.308336\n",
      "(Iteration 67701 / 76500) loss: 2.308344\n",
      "(Iteration 67801 / 76500) loss: 2.308340\n",
      "(Iteration 67901 / 76500) loss: 2.308340\n",
      "(Iteration 68001 / 76500) loss: 2.308350\n",
      "(Epoch 89 / 100) train acc: 0.078000; val_acc: 0.094000\n",
      "(Iteration 68101 / 76500) loss: 2.308340\n",
      "(Iteration 68201 / 76500) loss: 2.308348\n",
      "(Iteration 68301 / 76500) loss: 2.308340\n",
      "(Iteration 68401 / 76500) loss: 2.308336\n",
      "(Iteration 68501 / 76500) loss: 2.308350\n",
      "(Iteration 68601 / 76500) loss: 2.308349\n",
      "(Iteration 68701 / 76500) loss: 2.308358\n",
      "(Iteration 68801 / 76500) loss: 2.308307\n",
      "(Epoch 90 / 100) train acc: 0.106000; val_acc: 0.094000\n",
      "(Iteration 68901 / 76500) loss: 2.308339\n",
      "(Iteration 69001 / 76500) loss: 2.308344\n",
      "(Iteration 69101 / 76500) loss: 2.308354\n",
      "(Iteration 69201 / 76500) loss: 2.308350\n",
      "(Iteration 69301 / 76500) loss: 2.308319\n",
      "(Iteration 69401 / 76500) loss: 2.308324\n",
      "(Iteration 69501 / 76500) loss: 2.308331\n",
      "(Iteration 69601 / 76500) loss: 2.308340\n",
      "(Epoch 91 / 100) train acc: 0.103000; val_acc: 0.094000\n",
      "(Iteration 69701 / 76500) loss: 2.308346\n",
      "(Iteration 69801 / 76500) loss: 2.308347\n",
      "(Iteration 69901 / 76500) loss: 2.308344\n",
      "(Iteration 70001 / 76500) loss: 2.308347\n",
      "(Iteration 70101 / 76500) loss: 2.308348\n",
      "(Iteration 70201 / 76500) loss: 2.308351\n",
      "(Iteration 70301 / 76500) loss: 2.308330\n",
      "(Epoch 92 / 100) train acc: 0.091000; val_acc: 0.094000\n",
      "(Iteration 70401 / 76500) loss: 2.308340\n",
      "(Iteration 70501 / 76500) loss: 2.308350\n",
      "(Iteration 70601 / 76500) loss: 2.308349\n",
      "(Iteration 70701 / 76500) loss: 2.308343\n",
      "(Iteration 70801 / 76500) loss: 2.308339\n",
      "(Iteration 70901 / 76500) loss: 2.308359\n",
      "(Iteration 71001 / 76500) loss: 2.308352\n",
      "(Iteration 71101 / 76500) loss: 2.308340\n",
      "(Epoch 93 / 100) train acc: 0.100000; val_acc: 0.094000\n",
      "(Iteration 71201 / 76500) loss: 2.308350\n",
      "(Iteration 71301 / 76500) loss: 2.308356\n",
      "(Iteration 71401 / 76500) loss: 2.308336\n",
      "(Iteration 71501 / 76500) loss: 2.308336\n",
      "(Iteration 71601 / 76500) loss: 2.308350\n",
      "(Iteration 71701 / 76500) loss: 2.308347\n",
      "(Iteration 71801 / 76500) loss: 2.308358\n",
      "(Iteration 71901 / 76500) loss: 2.308340\n",
      "(Epoch 94 / 100) train acc: 0.118000; val_acc: 0.094000\n",
      "(Iteration 72001 / 76500) loss: 2.308338\n",
      "(Iteration 72101 / 76500) loss: 2.308347\n",
      "(Iteration 72201 / 76500) loss: 2.308325\n",
      "(Iteration 72301 / 76500) loss: 2.308334\n",
      "(Iteration 72401 / 76500) loss: 2.308343\n",
      "(Iteration 72501 / 76500) loss: 2.308356\n",
      "(Iteration 72601 / 76500) loss: 2.308341\n",
      "(Epoch 95 / 100) train acc: 0.093000; val_acc: 0.094000\n",
      "(Iteration 72701 / 76500) loss: 2.308334\n",
      "(Iteration 72801 / 76500) loss: 2.308339\n",
      "(Iteration 72901 / 76500) loss: 2.308350\n",
      "(Iteration 73001 / 76500) loss: 2.308338\n",
      "(Iteration 73101 / 76500) loss: 2.308358\n",
      "(Iteration 73201 / 76500) loss: 2.308343\n",
      "(Iteration 73301 / 76500) loss: 2.308346\n",
      "(Iteration 73401 / 76500) loss: 2.308357\n",
      "(Epoch 96 / 100) train acc: 0.098000; val_acc: 0.094000\n",
      "(Iteration 73501 / 76500) loss: 2.308336\n",
      "(Iteration 73601 / 76500) loss: 2.308337\n",
      "(Iteration 73701 / 76500) loss: 2.308352\n",
      "(Iteration 73801 / 76500) loss: 2.308332\n",
      "(Iteration 73901 / 76500) loss: 2.308345\n",
      "(Iteration 74001 / 76500) loss: 2.308342\n",
      "(Iteration 74101 / 76500) loss: 2.308341\n",
      "(Iteration 74201 / 76500) loss: 2.308338\n",
      "(Epoch 97 / 100) train acc: 0.100000; val_acc: 0.094000\n",
      "(Iteration 74301 / 76500) loss: 2.308331\n",
      "(Iteration 74401 / 76500) loss: 2.308356\n",
      "(Iteration 74501 / 76500) loss: 2.308342\n",
      "(Iteration 74601 / 76500) loss: 2.308338\n",
      "(Iteration 74701 / 76500) loss: 2.308359\n",
      "(Iteration 74801 / 76500) loss: 2.308344\n",
      "(Iteration 74901 / 76500) loss: 2.308332\n",
      "(Epoch 98 / 100) train acc: 0.091000; val_acc: 0.094000\n",
      "(Iteration 75001 / 76500) loss: 2.308359\n",
      "(Iteration 75101 / 76500) loss: 2.308342\n",
      "(Iteration 75201 / 76500) loss: 2.308314\n",
      "(Iteration 75301 / 76500) loss: 2.308348\n",
      "(Iteration 75401 / 76500) loss: 2.308359\n",
      "(Iteration 75501 / 76500) loss: 2.308340\n",
      "(Iteration 75601 / 76500) loss: 2.308337\n",
      "(Iteration 75701 / 76500) loss: 2.308353\n",
      "(Epoch 99 / 100) train acc: 0.090000; val_acc: 0.094000\n",
      "(Iteration 75801 / 76500) loss: 2.308349\n",
      "(Iteration 75901 / 76500) loss: 2.308339\n",
      "(Iteration 76001 / 76500) loss: 2.308352\n",
      "(Iteration 76101 / 76500) loss: 2.308356\n",
      "(Iteration 76201 / 76500) loss: 2.308327\n",
      "(Iteration 76301 / 76500) loss: 2.308342\n",
      "(Iteration 76401 / 76500) loss: 2.308341\n",
      "(Epoch 100 / 100) train acc: 0.082000; val_acc: 0.094000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.308228\n",
      "(Epoch 0 / 100) train acc: 0.126000; val_acc: 0.122000\n",
      "(Iteration 101 / 38200) loss: 2.308218\n",
      "(Iteration 201 / 38200) loss: 2.308212\n",
      "(Iteration 301 / 38200) loss: 2.308231\n",
      "(Epoch 1 / 100) train acc: 0.129000; val_acc: 0.122000\n",
      "(Iteration 401 / 38200) loss: 2.308222\n",
      "(Iteration 501 / 38200) loss: 2.308221\n",
      "(Iteration 601 / 38200) loss: 2.308228\n",
      "(Iteration 701 / 38200) loss: 2.308242\n",
      "(Epoch 2 / 100) train acc: 0.136000; val_acc: 0.122000\n",
      "(Iteration 801 / 38200) loss: 2.308220\n",
      "(Iteration 901 / 38200) loss: 2.308226\n",
      "(Iteration 1001 / 38200) loss: 2.308216\n",
      "(Iteration 1101 / 38200) loss: 2.308219\n",
      "(Epoch 3 / 100) train acc: 0.114000; val_acc: 0.122000\n",
      "(Iteration 1201 / 38200) loss: 2.308229\n",
      "(Iteration 1301 / 38200) loss: 2.308220\n",
      "(Iteration 1401 / 38200) loss: 2.308227\n",
      "(Iteration 1501 / 38200) loss: 2.308234\n",
      "(Epoch 4 / 100) train acc: 0.118000; val_acc: 0.122000\n",
      "(Iteration 1601 / 38200) loss: 2.308215\n",
      "(Iteration 1701 / 38200) loss: 2.308232\n",
      "(Iteration 1801 / 38200) loss: 2.308235\n",
      "(Iteration 1901 / 38200) loss: 2.308226\n",
      "(Epoch 5 / 100) train acc: 0.144000; val_acc: 0.122000\n",
      "(Iteration 2001 / 38200) loss: 2.308222\n",
      "(Iteration 2101 / 38200) loss: 2.308225\n",
      "(Iteration 2201 / 38200) loss: 2.308228\n",
      "(Epoch 6 / 100) train acc: 0.115000; val_acc: 0.122000\n",
      "(Iteration 2301 / 38200) loss: 2.308229\n",
      "(Iteration 2401 / 38200) loss: 2.308224\n",
      "(Iteration 2501 / 38200) loss: 2.308227\n",
      "(Iteration 2601 / 38200) loss: 2.308220\n",
      "(Epoch 7 / 100) train acc: 0.129000; val_acc: 0.122000\n",
      "(Iteration 2701 / 38200) loss: 2.308229\n",
      "(Iteration 2801 / 38200) loss: 2.308211\n",
      "(Iteration 2901 / 38200) loss: 2.308231\n",
      "(Iteration 3001 / 38200) loss: 2.308216\n",
      "(Epoch 8 / 100) train acc: 0.137000; val_acc: 0.122000\n",
      "(Iteration 3101 / 38200) loss: 2.308220\n",
      "(Iteration 3201 / 38200) loss: 2.308226\n",
      "(Iteration 3301 / 38200) loss: 2.308224\n",
      "(Iteration 3401 / 38200) loss: 2.308221\n",
      "(Epoch 9 / 100) train acc: 0.128000; val_acc: 0.122000\n",
      "(Iteration 3501 / 38200) loss: 2.308213\n",
      "(Iteration 3601 / 38200) loss: 2.308211\n",
      "(Iteration 3701 / 38200) loss: 2.308227\n",
      "(Iteration 3801 / 38200) loss: 2.308224\n",
      "(Epoch 10 / 100) train acc: 0.115000; val_acc: 0.122000\n",
      "(Iteration 3901 / 38200) loss: 2.308228\n",
      "(Iteration 4001 / 38200) loss: 2.308222\n",
      "(Iteration 4101 / 38200) loss: 2.308230\n",
      "(Iteration 4201 / 38200) loss: 2.308232\n",
      "(Epoch 11 / 100) train acc: 0.133000; val_acc: 0.122000\n",
      "(Iteration 4301 / 38200) loss: 2.308220\n",
      "(Iteration 4401 / 38200) loss: 2.308230\n",
      "(Iteration 4501 / 38200) loss: 2.308237\n",
      "(Epoch 12 / 100) train acc: 0.100000; val_acc: 0.122000\n",
      "(Iteration 4601 / 38200) loss: 2.308234\n",
      "(Iteration 4701 / 38200) loss: 2.308212\n",
      "(Iteration 4801 / 38200) loss: 2.308221\n",
      "(Iteration 4901 / 38200) loss: 2.308222\n",
      "(Epoch 13 / 100) train acc: 0.112000; val_acc: 0.122000\n",
      "(Iteration 5001 / 38200) loss: 2.308231\n",
      "(Iteration 5101 / 38200) loss: 2.308217\n",
      "(Iteration 5201 / 38200) loss: 2.308217\n",
      "(Iteration 5301 / 38200) loss: 2.308226\n",
      "(Epoch 14 / 100) train acc: 0.124000; val_acc: 0.122000\n",
      "(Iteration 5401 / 38200) loss: 2.308231\n",
      "(Iteration 5501 / 38200) loss: 2.308222\n",
      "(Iteration 5601 / 38200) loss: 2.308222\n",
      "(Iteration 5701 / 38200) loss: 2.308220\n",
      "(Epoch 15 / 100) train acc: 0.120000; val_acc: 0.122000\n",
      "(Iteration 5801 / 38200) loss: 2.308213\n",
      "(Iteration 5901 / 38200) loss: 2.308230\n",
      "(Iteration 6001 / 38200) loss: 2.308215\n",
      "(Iteration 6101 / 38200) loss: 2.308227\n",
      "(Epoch 16 / 100) train acc: 0.121000; val_acc: 0.122000\n",
      "(Iteration 6201 / 38200) loss: 2.308213\n",
      "(Iteration 6301 / 38200) loss: 2.308217\n",
      "(Iteration 6401 / 38200) loss: 2.308223\n",
      "(Epoch 17 / 100) train acc: 0.133000; val_acc: 0.122000\n",
      "(Iteration 6501 / 38200) loss: 2.308217\n",
      "(Iteration 6601 / 38200) loss: 2.308219\n",
      "(Iteration 6701 / 38200) loss: 2.308213\n",
      "(Iteration 6801 / 38200) loss: 2.308224\n",
      "(Epoch 18 / 100) train acc: 0.115000; val_acc: 0.122000\n",
      "(Iteration 6901 / 38200) loss: 2.308227\n",
      "(Iteration 7001 / 38200) loss: 2.308221\n",
      "(Iteration 7101 / 38200) loss: 2.308226\n",
      "(Iteration 7201 / 38200) loss: 2.308213\n",
      "(Epoch 19 / 100) train acc: 0.130000; val_acc: 0.122000\n",
      "(Iteration 7301 / 38200) loss: 2.308213\n",
      "(Iteration 7401 / 38200) loss: 2.308221\n",
      "(Iteration 7501 / 38200) loss: 2.308237\n",
      "(Iteration 7601 / 38200) loss: 2.308210\n",
      "(Epoch 20 / 100) train acc: 0.129000; val_acc: 0.122000\n",
      "(Iteration 7701 / 38200) loss: 2.308218\n",
      "(Iteration 7801 / 38200) loss: 2.308232\n",
      "(Iteration 7901 / 38200) loss: 2.308233\n",
      "(Iteration 8001 / 38200) loss: 2.308225\n",
      "(Epoch 21 / 100) train acc: 0.115000; val_acc: 0.122000\n",
      "(Iteration 8101 / 38200) loss: 2.308221\n",
      "(Iteration 8201 / 38200) loss: 2.308220\n",
      "(Iteration 8301 / 38200) loss: 2.308229\n",
      "(Iteration 8401 / 38200) loss: 2.308210\n",
      "(Epoch 22 / 100) train acc: 0.136000; val_acc: 0.122000\n",
      "(Iteration 8501 / 38200) loss: 2.308229\n",
      "(Iteration 8601 / 38200) loss: 2.308220\n",
      "(Iteration 8701 / 38200) loss: 2.308225\n",
      "(Epoch 23 / 100) train acc: 0.124000; val_acc: 0.122000\n",
      "(Iteration 8801 / 38200) loss: 2.308232\n",
      "(Iteration 8901 / 38200) loss: 2.308225\n",
      "(Iteration 9001 / 38200) loss: 2.308212\n",
      "(Iteration 9101 / 38200) loss: 2.308228\n",
      "(Epoch 24 / 100) train acc: 0.147000; val_acc: 0.122000\n",
      "(Iteration 9201 / 38200) loss: 2.308230\n",
      "(Iteration 9301 / 38200) loss: 2.308231\n",
      "(Iteration 9401 / 38200) loss: 2.308227\n",
      "(Iteration 9501 / 38200) loss: 2.308233\n",
      "(Epoch 25 / 100) train acc: 0.142000; val_acc: 0.122000\n",
      "(Iteration 9601 / 38200) loss: 2.308224\n",
      "(Iteration 9701 / 38200) loss: 2.308218\n",
      "(Iteration 9801 / 38200) loss: 2.308227\n",
      "(Iteration 9901 / 38200) loss: 2.308221\n",
      "(Epoch 26 / 100) train acc: 0.131000; val_acc: 0.122000\n",
      "(Iteration 10001 / 38200) loss: 2.308231\n",
      "(Iteration 10101 / 38200) loss: 2.308224\n",
      "(Iteration 10201 / 38200) loss: 2.308233\n",
      "(Iteration 10301 / 38200) loss: 2.308212\n",
      "(Epoch 27 / 100) train acc: 0.122000; val_acc: 0.122000\n",
      "(Iteration 10401 / 38200) loss: 2.308226\n",
      "(Iteration 10501 / 38200) loss: 2.308221\n",
      "(Iteration 10601 / 38200) loss: 2.308223\n",
      "(Epoch 28 / 100) train acc: 0.141000; val_acc: 0.122000\n",
      "(Iteration 10701 / 38200) loss: 2.308226\n",
      "(Iteration 10801 / 38200) loss: 2.308225\n",
      "(Iteration 10901 / 38200) loss: 2.308217\n",
      "(Iteration 11001 / 38200) loss: 2.308226\n",
      "(Epoch 29 / 100) train acc: 0.138000; val_acc: 0.122000\n",
      "(Iteration 11101 / 38200) loss: 2.308221\n",
      "(Iteration 11201 / 38200) loss: 2.308224\n",
      "(Iteration 11301 / 38200) loss: 2.308238\n",
      "(Iteration 11401 / 38200) loss: 2.308227\n",
      "(Epoch 30 / 100) train acc: 0.135000; val_acc: 0.122000\n",
      "(Iteration 11501 / 38200) loss: 2.308212\n",
      "(Iteration 11601 / 38200) loss: 2.308219\n",
      "(Iteration 11701 / 38200) loss: 2.308210\n",
      "(Iteration 11801 / 38200) loss: 2.308206\n",
      "(Epoch 31 / 100) train acc: 0.130000; val_acc: 0.122000\n",
      "(Iteration 11901 / 38200) loss: 2.308228\n",
      "(Iteration 12001 / 38200) loss: 2.308227\n",
      "(Iteration 12101 / 38200) loss: 2.308220\n",
      "(Iteration 12201 / 38200) loss: 2.308224\n",
      "(Epoch 32 / 100) train acc: 0.126000; val_acc: 0.122000\n",
      "(Iteration 12301 / 38200) loss: 2.308229\n",
      "(Iteration 12401 / 38200) loss: 2.308229\n",
      "(Iteration 12501 / 38200) loss: 2.308219\n",
      "(Iteration 12601 / 38200) loss: 2.308238\n",
      "(Epoch 33 / 100) train acc: 0.117000; val_acc: 0.122000\n",
      "(Iteration 12701 / 38200) loss: 2.308230\n",
      "(Iteration 12801 / 38200) loss: 2.308227\n",
      "(Iteration 12901 / 38200) loss: 2.308220\n",
      "(Epoch 34 / 100) train acc: 0.124000; val_acc: 0.122000\n",
      "(Iteration 13001 / 38200) loss: 2.308235\n",
      "(Iteration 13101 / 38200) loss: 2.308215\n",
      "(Iteration 13201 / 38200) loss: 2.308227\n",
      "(Iteration 13301 / 38200) loss: 2.308230\n",
      "(Epoch 35 / 100) train acc: 0.130000; val_acc: 0.122000\n",
      "(Iteration 13401 / 38200) loss: 2.308220\n",
      "(Iteration 13501 / 38200) loss: 2.308227\n",
      "(Iteration 13601 / 38200) loss: 2.308203\n",
      "(Iteration 13701 / 38200) loss: 2.308213\n",
      "(Epoch 36 / 100) train acc: 0.122000; val_acc: 0.122000\n",
      "(Iteration 13801 / 38200) loss: 2.308227\n",
      "(Iteration 13901 / 38200) loss: 2.308231\n",
      "(Iteration 14001 / 38200) loss: 2.308214\n",
      "(Iteration 14101 / 38200) loss: 2.308239\n",
      "(Epoch 37 / 100) train acc: 0.138000; val_acc: 0.122000\n",
      "(Iteration 14201 / 38200) loss: 2.308214\n",
      "(Iteration 14301 / 38200) loss: 2.308221\n",
      "(Iteration 14401 / 38200) loss: 2.308224\n",
      "(Iteration 14501 / 38200) loss: 2.308227\n",
      "(Epoch 38 / 100) train acc: 0.126000; val_acc: 0.122000\n",
      "(Iteration 14601 / 38200) loss: 2.308205\n",
      "(Iteration 14701 / 38200) loss: 2.308224\n",
      "(Iteration 14801 / 38200) loss: 2.308229\n",
      "(Epoch 39 / 100) train acc: 0.124000; val_acc: 0.122000\n",
      "(Iteration 14901 / 38200) loss: 2.308231\n",
      "(Iteration 15001 / 38200) loss: 2.308217\n",
      "(Iteration 15101 / 38200) loss: 2.308214\n",
      "(Iteration 15201 / 38200) loss: 2.308230\n",
      "(Epoch 40 / 100) train acc: 0.121000; val_acc: 0.122000\n",
      "(Iteration 15301 / 38200) loss: 2.308224\n",
      "(Iteration 15401 / 38200) loss: 2.308227\n",
      "(Iteration 15501 / 38200) loss: 2.308224\n",
      "(Iteration 15601 / 38200) loss: 2.308225\n",
      "(Epoch 41 / 100) train acc: 0.115000; val_acc: 0.122000\n",
      "(Iteration 15701 / 38200) loss: 2.308225\n",
      "(Iteration 15801 / 38200) loss: 2.308214\n",
      "(Iteration 15901 / 38200) loss: 2.308214\n",
      "(Iteration 16001 / 38200) loss: 2.308213\n",
      "(Epoch 42 / 100) train acc: 0.130000; val_acc: 0.122000\n",
      "(Iteration 16101 / 38200) loss: 2.308223\n",
      "(Iteration 16201 / 38200) loss: 2.308233\n",
      "(Iteration 16301 / 38200) loss: 2.308220\n",
      "(Iteration 16401 / 38200) loss: 2.308233\n",
      "(Epoch 43 / 100) train acc: 0.138000; val_acc: 0.122000\n",
      "(Iteration 16501 / 38200) loss: 2.308230\n",
      "(Iteration 16601 / 38200) loss: 2.308215\n",
      "(Iteration 16701 / 38200) loss: 2.308228\n",
      "(Iteration 16801 / 38200) loss: 2.308234\n",
      "(Epoch 44 / 100) train acc: 0.140000; val_acc: 0.122000\n",
      "(Iteration 16901 / 38200) loss: 2.308222\n",
      "(Iteration 17001 / 38200) loss: 2.308221\n",
      "(Iteration 17101 / 38200) loss: 2.308232\n",
      "(Epoch 45 / 100) train acc: 0.136000; val_acc: 0.122000\n",
      "(Iteration 17201 / 38200) loss: 2.308245\n",
      "(Iteration 17301 / 38200) loss: 2.308225\n",
      "(Iteration 17401 / 38200) loss: 2.308223\n",
      "(Iteration 17501 / 38200) loss: 2.308223\n",
      "(Epoch 46 / 100) train acc: 0.112000; val_acc: 0.122000\n",
      "(Iteration 17601 / 38200) loss: 2.308217\n",
      "(Iteration 17701 / 38200) loss: 2.308220\n",
      "(Iteration 17801 / 38200) loss: 2.308220\n",
      "(Iteration 17901 / 38200) loss: 2.308219\n",
      "(Epoch 47 / 100) train acc: 0.133000; val_acc: 0.122000\n",
      "(Iteration 18001 / 38200) loss: 2.308233\n",
      "(Iteration 18101 / 38200) loss: 2.308218\n",
      "(Iteration 18201 / 38200) loss: 2.308210\n",
      "(Iteration 18301 / 38200) loss: 2.308227\n",
      "(Epoch 48 / 100) train acc: 0.122000; val_acc: 0.122000\n",
      "(Iteration 18401 / 38200) loss: 2.308235\n",
      "(Iteration 18501 / 38200) loss: 2.308225\n",
      "(Iteration 18601 / 38200) loss: 2.308233\n",
      "(Iteration 18701 / 38200) loss: 2.308224\n",
      "(Epoch 49 / 100) train acc: 0.131000; val_acc: 0.122000\n",
      "(Iteration 18801 / 38200) loss: 2.308225\n",
      "(Iteration 18901 / 38200) loss: 2.308229\n",
      "(Iteration 19001 / 38200) loss: 2.308222\n",
      "(Epoch 50 / 100) train acc: 0.124000; val_acc: 0.122000\n",
      "(Iteration 19101 / 38200) loss: 2.308223\n",
      "(Iteration 19201 / 38200) loss: 2.308220\n",
      "(Iteration 19301 / 38200) loss: 2.308225\n",
      "(Iteration 19401 / 38200) loss: 2.308224\n",
      "(Epoch 51 / 100) train acc: 0.138000; val_acc: 0.122000\n",
      "(Iteration 19501 / 38200) loss: 2.308221\n",
      "(Iteration 19601 / 38200) loss: 2.308221\n",
      "(Iteration 19701 / 38200) loss: 2.308224\n",
      "(Iteration 19801 / 38200) loss: 2.308228\n",
      "(Epoch 52 / 100) train acc: 0.136000; val_acc: 0.122000\n",
      "(Iteration 19901 / 38200) loss: 2.308228\n",
      "(Iteration 20001 / 38200) loss: 2.308229\n",
      "(Iteration 20101 / 38200) loss: 2.308230\n",
      "(Iteration 20201 / 38200) loss: 2.308228\n",
      "(Epoch 53 / 100) train acc: 0.140000; val_acc: 0.122000\n",
      "(Iteration 20301 / 38200) loss: 2.308222\n",
      "(Iteration 20401 / 38200) loss: 2.308221\n",
      "(Iteration 20501 / 38200) loss: 2.308218\n",
      "(Iteration 20601 / 38200) loss: 2.308224\n",
      "(Epoch 54 / 100) train acc: 0.124000; val_acc: 0.122000\n",
      "(Iteration 20701 / 38200) loss: 2.308222\n",
      "(Iteration 20801 / 38200) loss: 2.308224\n",
      "(Iteration 20901 / 38200) loss: 2.308220\n",
      "(Iteration 21001 / 38200) loss: 2.308229\n",
      "(Epoch 55 / 100) train acc: 0.141000; val_acc: 0.122000\n",
      "(Iteration 21101 / 38200) loss: 2.308221\n",
      "(Iteration 21201 / 38200) loss: 2.308228\n",
      "(Iteration 21301 / 38200) loss: 2.308222\n",
      "(Epoch 56 / 100) train acc: 0.137000; val_acc: 0.122000\n",
      "(Iteration 21401 / 38200) loss: 2.308231\n",
      "(Iteration 21501 / 38200) loss: 2.308212\n",
      "(Iteration 21601 / 38200) loss: 2.308216\n",
      "(Iteration 21701 / 38200) loss: 2.308215\n",
      "(Epoch 57 / 100) train acc: 0.124000; val_acc: 0.122000\n",
      "(Iteration 21801 / 38200) loss: 2.308224\n",
      "(Iteration 21901 / 38200) loss: 2.308234\n",
      "(Iteration 22001 / 38200) loss: 2.308210\n",
      "(Iteration 22101 / 38200) loss: 2.308221\n",
      "(Epoch 58 / 100) train acc: 0.122000; val_acc: 0.122000\n",
      "(Iteration 22201 / 38200) loss: 2.308232\n",
      "(Iteration 22301 / 38200) loss: 2.308227\n",
      "(Iteration 22401 / 38200) loss: 2.308224\n",
      "(Iteration 22501 / 38200) loss: 2.308220\n",
      "(Epoch 59 / 100) train acc: 0.133000; val_acc: 0.122000\n",
      "(Iteration 22601 / 38200) loss: 2.308232\n",
      "(Iteration 22701 / 38200) loss: 2.308211\n",
      "(Iteration 22801 / 38200) loss: 2.308220\n",
      "(Iteration 22901 / 38200) loss: 2.308223\n",
      "(Epoch 60 / 100) train acc: 0.134000; val_acc: 0.122000\n",
      "(Iteration 23001 / 38200) loss: 2.308221\n",
      "(Iteration 23101 / 38200) loss: 2.308242\n",
      "(Iteration 23201 / 38200) loss: 2.308219\n",
      "(Iteration 23301 / 38200) loss: 2.308227\n",
      "(Epoch 61 / 100) train acc: 0.139000; val_acc: 0.122000\n",
      "(Iteration 23401 / 38200) loss: 2.308228\n",
      "(Iteration 23501 / 38200) loss: 2.308230\n",
      "(Iteration 23601 / 38200) loss: 2.308217\n",
      "(Epoch 62 / 100) train acc: 0.127000; val_acc: 0.122000\n",
      "(Iteration 23701 / 38200) loss: 2.308216\n",
      "(Iteration 23801 / 38200) loss: 2.308227\n",
      "(Iteration 23901 / 38200) loss: 2.308227\n",
      "(Iteration 24001 / 38200) loss: 2.308217\n",
      "(Epoch 63 / 100) train acc: 0.156000; val_acc: 0.122000\n",
      "(Iteration 24101 / 38200) loss: 2.308235\n",
      "(Iteration 24201 / 38200) loss: 2.308229\n",
      "(Iteration 24301 / 38200) loss: 2.308232\n",
      "(Iteration 24401 / 38200) loss: 2.308235\n",
      "(Epoch 64 / 100) train acc: 0.124000; val_acc: 0.122000\n",
      "(Iteration 24501 / 38200) loss: 2.308216\n",
      "(Iteration 24601 / 38200) loss: 2.308222\n",
      "(Iteration 24701 / 38200) loss: 2.308228\n",
      "(Iteration 24801 / 38200) loss: 2.308227\n",
      "(Epoch 65 / 100) train acc: 0.121000; val_acc: 0.122000\n",
      "(Iteration 24901 / 38200) loss: 2.308213\n",
      "(Iteration 25001 / 38200) loss: 2.308225\n",
      "(Iteration 25101 / 38200) loss: 2.308222\n",
      "(Iteration 25201 / 38200) loss: 2.308213\n",
      "(Epoch 66 / 100) train acc: 0.133000; val_acc: 0.122000\n",
      "(Iteration 25301 / 38200) loss: 2.308220\n",
      "(Iteration 25401 / 38200) loss: 2.308218\n",
      "(Iteration 25501 / 38200) loss: 2.308216\n",
      "(Epoch 67 / 100) train acc: 0.139000; val_acc: 0.122000\n",
      "(Iteration 25601 / 38200) loss: 2.308229\n",
      "(Iteration 25701 / 38200) loss: 2.308212\n",
      "(Iteration 25801 / 38200) loss: 2.308218\n",
      "(Iteration 25901 / 38200) loss: 2.308228\n",
      "(Epoch 68 / 100) train acc: 0.137000; val_acc: 0.122000\n",
      "(Iteration 26001 / 38200) loss: 2.308233\n",
      "(Iteration 26101 / 38200) loss: 2.308226\n",
      "(Iteration 26201 / 38200) loss: 2.308220\n",
      "(Iteration 26301 / 38200) loss: 2.308215\n",
      "(Epoch 69 / 100) train acc: 0.136000; val_acc: 0.122000\n",
      "(Iteration 26401 / 38200) loss: 2.308231\n",
      "(Iteration 26501 / 38200) loss: 2.308218\n",
      "(Iteration 26601 / 38200) loss: 2.308216\n",
      "(Iteration 26701 / 38200) loss: 2.308223\n",
      "(Epoch 70 / 100) train acc: 0.163000; val_acc: 0.122000\n",
      "(Iteration 26801 / 38200) loss: 2.308222\n",
      "(Iteration 26901 / 38200) loss: 2.308224\n",
      "(Iteration 27001 / 38200) loss: 2.308220\n",
      "(Iteration 27101 / 38200) loss: 2.308223\n",
      "(Epoch 71 / 100) train acc: 0.139000; val_acc: 0.122000\n",
      "(Iteration 27201 / 38200) loss: 2.308232\n",
      "(Iteration 27301 / 38200) loss: 2.308222\n",
      "(Iteration 27401 / 38200) loss: 2.308224\n",
      "(Iteration 27501 / 38200) loss: 2.308225\n",
      "(Epoch 72 / 100) train acc: 0.138000; val_acc: 0.122000\n",
      "(Iteration 27601 / 38200) loss: 2.308218\n",
      "(Iteration 27701 / 38200) loss: 2.308234\n",
      "(Iteration 27801 / 38200) loss: 2.308215\n",
      "(Epoch 73 / 100) train acc: 0.132000; val_acc: 0.122000\n",
      "(Iteration 27901 / 38200) loss: 2.308219\n",
      "(Iteration 28001 / 38200) loss: 2.308217\n",
      "(Iteration 28101 / 38200) loss: 2.308226\n",
      "(Iteration 28201 / 38200) loss: 2.308227\n",
      "(Epoch 74 / 100) train acc: 0.167000; val_acc: 0.122000\n",
      "(Iteration 28301 / 38200) loss: 2.308221\n",
      "(Iteration 28401 / 38200) loss: 2.308231\n",
      "(Iteration 28501 / 38200) loss: 2.308228\n",
      "(Iteration 28601 / 38200) loss: 2.308210\n",
      "(Epoch 75 / 100) train acc: 0.133000; val_acc: 0.122000\n",
      "(Iteration 28701 / 38200) loss: 2.308218\n",
      "(Iteration 28801 / 38200) loss: 2.308214\n",
      "(Iteration 28901 / 38200) loss: 2.308210\n",
      "(Iteration 29001 / 38200) loss: 2.308225\n",
      "(Epoch 76 / 100) train acc: 0.150000; val_acc: 0.122000\n",
      "(Iteration 29101 / 38200) loss: 2.308223\n",
      "(Iteration 29201 / 38200) loss: 2.308230\n",
      "(Iteration 29301 / 38200) loss: 2.308222\n",
      "(Iteration 29401 / 38200) loss: 2.308213\n",
      "(Epoch 77 / 100) train acc: 0.131000; val_acc: 0.122000\n",
      "(Iteration 29501 / 38200) loss: 2.308212\n",
      "(Iteration 29601 / 38200) loss: 2.308221\n",
      "(Iteration 29701 / 38200) loss: 2.308222\n",
      "(Epoch 78 / 100) train acc: 0.126000; val_acc: 0.122000\n",
      "(Iteration 29801 / 38200) loss: 2.308232\n",
      "(Iteration 29901 / 38200) loss: 2.308218\n",
      "(Iteration 30001 / 38200) loss: 2.308224\n",
      "(Iteration 30101 / 38200) loss: 2.308228\n",
      "(Epoch 79 / 100) train acc: 0.140000; val_acc: 0.122000\n",
      "(Iteration 30201 / 38200) loss: 2.308238\n",
      "(Iteration 30301 / 38200) loss: 2.308229\n",
      "(Iteration 30401 / 38200) loss: 2.308232\n",
      "(Iteration 30501 / 38200) loss: 2.308220\n",
      "(Epoch 80 / 100) train acc: 0.136000; val_acc: 0.122000\n",
      "(Iteration 30601 / 38200) loss: 2.308225\n",
      "(Iteration 30701 / 38200) loss: 2.308225\n",
      "(Iteration 30801 / 38200) loss: 2.308232\n",
      "(Iteration 30901 / 38200) loss: 2.308219\n",
      "(Epoch 81 / 100) train acc: 0.134000; val_acc: 0.122000\n",
      "(Iteration 31001 / 38200) loss: 2.308219\n",
      "(Iteration 31101 / 38200) loss: 2.308232\n",
      "(Iteration 31201 / 38200) loss: 2.308225\n",
      "(Iteration 31301 / 38200) loss: 2.308221\n",
      "(Epoch 82 / 100) train acc: 0.145000; val_acc: 0.122000\n",
      "(Iteration 31401 / 38200) loss: 2.308229\n",
      "(Iteration 31501 / 38200) loss: 2.308224\n",
      "(Iteration 31601 / 38200) loss: 2.308220\n",
      "(Iteration 31701 / 38200) loss: 2.308211\n",
      "(Epoch 83 / 100) train acc: 0.107000; val_acc: 0.122000\n",
      "(Iteration 31801 / 38200) loss: 2.308226\n",
      "(Iteration 31901 / 38200) loss: 2.308217\n",
      "(Iteration 32001 / 38200) loss: 2.308215\n",
      "(Epoch 84 / 100) train acc: 0.125000; val_acc: 0.122000\n",
      "(Iteration 32101 / 38200) loss: 2.308216\n",
      "(Iteration 32201 / 38200) loss: 2.308236\n",
      "(Iteration 32301 / 38200) loss: 2.308215\n",
      "(Iteration 32401 / 38200) loss: 2.308215\n",
      "(Epoch 85 / 100) train acc: 0.131000; val_acc: 0.122000\n",
      "(Iteration 32501 / 38200) loss: 2.308219\n",
      "(Iteration 32601 / 38200) loss: 2.308211\n",
      "(Iteration 32701 / 38200) loss: 2.308224\n",
      "(Iteration 32801 / 38200) loss: 2.308219\n",
      "(Epoch 86 / 100) train acc: 0.128000; val_acc: 0.122000\n",
      "(Iteration 32901 / 38200) loss: 2.308225\n",
      "(Iteration 33001 / 38200) loss: 2.308229\n",
      "(Iteration 33101 / 38200) loss: 2.308231\n",
      "(Iteration 33201 / 38200) loss: 2.308227\n",
      "(Epoch 87 / 100) train acc: 0.131000; val_acc: 0.122000\n",
      "(Iteration 33301 / 38200) loss: 2.308229\n",
      "(Iteration 33401 / 38200) loss: 2.308224\n",
      "(Iteration 33501 / 38200) loss: 2.308231\n",
      "(Iteration 33601 / 38200) loss: 2.308227\n",
      "(Epoch 88 / 100) train acc: 0.138000; val_acc: 0.122000\n",
      "(Iteration 33701 / 38200) loss: 2.308221\n",
      "(Iteration 33801 / 38200) loss: 2.308227\n",
      "(Iteration 33901 / 38200) loss: 2.308221\n",
      "(Epoch 89 / 100) train acc: 0.126000; val_acc: 0.122000\n",
      "(Iteration 34001 / 38200) loss: 2.308218\n",
      "(Iteration 34101 / 38200) loss: 2.308222\n",
      "(Iteration 34201 / 38200) loss: 2.308223\n",
      "(Iteration 34301 / 38200) loss: 2.308223\n",
      "(Epoch 90 / 100) train acc: 0.136000; val_acc: 0.122000\n",
      "(Iteration 34401 / 38200) loss: 2.308228\n",
      "(Iteration 34501 / 38200) loss: 2.308225\n",
      "(Iteration 34601 / 38200) loss: 2.308224\n",
      "(Iteration 34701 / 38200) loss: 2.308228\n",
      "(Epoch 91 / 100) train acc: 0.135000; val_acc: 0.122000\n",
      "(Iteration 34801 / 38200) loss: 2.308212\n",
      "(Iteration 34901 / 38200) loss: 2.308220\n",
      "(Iteration 35001 / 38200) loss: 2.308222\n",
      "(Iteration 35101 / 38200) loss: 2.308229\n",
      "(Epoch 92 / 100) train acc: 0.148000; val_acc: 0.122000\n",
      "(Iteration 35201 / 38200) loss: 2.308216\n",
      "(Iteration 35301 / 38200) loss: 2.308228\n",
      "(Iteration 35401 / 38200) loss: 2.308217\n",
      "(Iteration 35501 / 38200) loss: 2.308222\n",
      "(Epoch 93 / 100) train acc: 0.130000; val_acc: 0.122000\n",
      "(Iteration 35601 / 38200) loss: 2.308235\n",
      "(Iteration 35701 / 38200) loss: 2.308216\n",
      "(Iteration 35801 / 38200) loss: 2.308220\n",
      "(Iteration 35901 / 38200) loss: 2.308217\n",
      "(Epoch 94 / 100) train acc: 0.148000; val_acc: 0.122000\n",
      "(Iteration 36001 / 38200) loss: 2.308230\n",
      "(Iteration 36101 / 38200) loss: 2.308223\n",
      "(Iteration 36201 / 38200) loss: 2.308213\n",
      "(Epoch 95 / 100) train acc: 0.134000; val_acc: 0.122000\n",
      "(Iteration 36301 / 38200) loss: 2.308218\n",
      "(Iteration 36401 / 38200) loss: 2.308225\n",
      "(Iteration 36501 / 38200) loss: 2.308225\n",
      "(Iteration 36601 / 38200) loss: 2.308222\n",
      "(Epoch 96 / 100) train acc: 0.135000; val_acc: 0.122000\n",
      "(Iteration 36701 / 38200) loss: 2.308232\n",
      "(Iteration 36801 / 38200) loss: 2.308229\n",
      "(Iteration 36901 / 38200) loss: 2.308225\n",
      "(Iteration 37001 / 38200) loss: 2.308220\n",
      "(Epoch 97 / 100) train acc: 0.144000; val_acc: 0.122000\n",
      "(Iteration 37101 / 38200) loss: 2.308239\n",
      "(Iteration 37201 / 38200) loss: 2.308242\n",
      "(Iteration 37301 / 38200) loss: 2.308232\n",
      "(Iteration 37401 / 38200) loss: 2.308234\n",
      "(Epoch 98 / 100) train acc: 0.116000; val_acc: 0.122000\n",
      "(Iteration 37501 / 38200) loss: 2.308229\n",
      "(Iteration 37601 / 38200) loss: 2.308204\n",
      "(Iteration 37701 / 38200) loss: 2.308206\n",
      "(Iteration 37801 / 38200) loss: 2.308223\n",
      "(Epoch 99 / 100) train acc: 0.134000; val_acc: 0.122000\n",
      "(Iteration 37901 / 38200) loss: 2.308239\n",
      "(Iteration 38001 / 38200) loss: 2.308235\n",
      "(Iteration 38101 / 38200) loss: 2.308220\n",
      "(Epoch 100 / 100) train acc: 0.131000; val_acc: 0.122000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.308382\n",
      "(Epoch 0 / 100) train acc: 0.100000; val_acc: 0.102000\n",
      "(Iteration 101 / 76500) loss: 2.308391\n",
      "(Iteration 201 / 76500) loss: 2.308396\n",
      "(Iteration 301 / 76500) loss: 2.308374\n",
      "(Iteration 401 / 76500) loss: 2.308391\n",
      "(Iteration 501 / 76500) loss: 2.308384\n",
      "(Iteration 601 / 76500) loss: 2.308396\n",
      "(Iteration 701 / 76500) loss: 2.308367\n",
      "(Epoch 1 / 100) train acc: 0.073000; val_acc: 0.103000\n",
      "(Iteration 801 / 76500) loss: 2.308403\n",
      "(Iteration 901 / 76500) loss: 2.308403\n",
      "(Iteration 1001 / 76500) loss: 2.308376\n",
      "(Iteration 1101 / 76500) loss: 2.308383\n",
      "(Iteration 1201 / 76500) loss: 2.308390\n",
      "(Iteration 1301 / 76500) loss: 2.308369\n",
      "(Iteration 1401 / 76500) loss: 2.308383\n",
      "(Iteration 1501 / 76500) loss: 2.308376\n",
      "(Epoch 2 / 100) train acc: 0.077000; val_acc: 0.103000\n",
      "(Iteration 1601 / 76500) loss: 2.308389\n",
      "(Iteration 1701 / 76500) loss: 2.308373\n",
      "(Iteration 1801 / 76500) loss: 2.308391\n",
      "(Iteration 1901 / 76500) loss: 2.308380\n",
      "(Iteration 2001 / 76500) loss: 2.308364\n",
      "(Iteration 2101 / 76500) loss: 2.308368\n",
      "(Iteration 2201 / 76500) loss: 2.308380\n",
      "(Epoch 3 / 100) train acc: 0.106000; val_acc: 0.104000\n",
      "(Iteration 2301 / 76500) loss: 2.308386\n",
      "(Iteration 2401 / 76500) loss: 2.308385\n",
      "(Iteration 2501 / 76500) loss: 2.308391\n",
      "(Iteration 2601 / 76500) loss: 2.308358\n",
      "(Iteration 2701 / 76500) loss: 2.308388\n",
      "(Iteration 2801 / 76500) loss: 2.308391\n",
      "(Iteration 2901 / 76500) loss: 2.308364\n",
      "(Iteration 3001 / 76500) loss: 2.308382\n",
      "(Epoch 4 / 100) train acc: 0.094000; val_acc: 0.104000\n",
      "(Iteration 3101 / 76500) loss: 2.308377\n",
      "(Iteration 3201 / 76500) loss: 2.308397\n",
      "(Iteration 3301 / 76500) loss: 2.308379\n",
      "(Iteration 3401 / 76500) loss: 2.308375\n",
      "(Iteration 3501 / 76500) loss: 2.308393\n",
      "(Iteration 3601 / 76500) loss: 2.308374\n",
      "(Iteration 3701 / 76500) loss: 2.308378\n",
      "(Iteration 3801 / 76500) loss: 2.308384\n",
      "(Epoch 5 / 100) train acc: 0.078000; val_acc: 0.103000\n",
      "(Iteration 3901 / 76500) loss: 2.308378\n",
      "(Iteration 4001 / 76500) loss: 2.308377\n",
      "(Iteration 4101 / 76500) loss: 2.308360\n",
      "(Iteration 4201 / 76500) loss: 2.308368\n",
      "(Iteration 4301 / 76500) loss: 2.308382\n",
      "(Iteration 4401 / 76500) loss: 2.308359\n",
      "(Iteration 4501 / 76500) loss: 2.308370\n",
      "(Epoch 6 / 100) train acc: 0.087000; val_acc: 0.103000\n",
      "(Iteration 4601 / 76500) loss: 2.308377\n",
      "(Iteration 4701 / 76500) loss: 2.308360\n",
      "(Iteration 4801 / 76500) loss: 2.308371\n",
      "(Iteration 4901 / 76500) loss: 2.308374\n",
      "(Iteration 5001 / 76500) loss: 2.308394\n",
      "(Iteration 5101 / 76500) loss: 2.308377\n",
      "(Iteration 5201 / 76500) loss: 2.308385\n",
      "(Iteration 5301 / 76500) loss: 2.308387\n",
      "(Epoch 7 / 100) train acc: 0.080000; val_acc: 0.103000\n",
      "(Iteration 5401 / 76500) loss: 2.308376\n",
      "(Iteration 5501 / 76500) loss: 2.308393\n",
      "(Iteration 5601 / 76500) loss: 2.308382\n",
      "(Iteration 5701 / 76500) loss: 2.308401\n",
      "(Iteration 5801 / 76500) loss: 2.308381\n",
      "(Iteration 5901 / 76500) loss: 2.308406\n",
      "(Iteration 6001 / 76500) loss: 2.308372\n",
      "(Iteration 6101 / 76500) loss: 2.308383\n",
      "(Epoch 8 / 100) train acc: 0.100000; val_acc: 0.103000\n",
      "(Iteration 6201 / 76500) loss: 2.308383\n",
      "(Iteration 6301 / 76500) loss: 2.308377\n",
      "(Iteration 6401 / 76500) loss: 2.308370\n",
      "(Iteration 6501 / 76500) loss: 2.308377\n",
      "(Iteration 6601 / 76500) loss: 2.308374\n",
      "(Iteration 6701 / 76500) loss: 2.308373\n",
      "(Iteration 6801 / 76500) loss: 2.308384\n",
      "(Epoch 9 / 100) train acc: 0.099000; val_acc: 0.103000\n",
      "(Iteration 6901 / 76500) loss: 2.308382\n",
      "(Iteration 7001 / 76500) loss: 2.308387\n",
      "(Iteration 7101 / 76500) loss: 2.308390\n",
      "(Iteration 7201 / 76500) loss: 2.308376\n",
      "(Iteration 7301 / 76500) loss: 2.308390\n",
      "(Iteration 7401 / 76500) loss: 2.308383\n",
      "(Iteration 7501 / 76500) loss: 2.308391\n",
      "(Iteration 7601 / 76500) loss: 2.308369\n",
      "(Epoch 10 / 100) train acc: 0.102000; val_acc: 0.103000\n",
      "(Iteration 7701 / 76500) loss: 2.308349\n",
      "(Iteration 7801 / 76500) loss: 2.308383\n",
      "(Iteration 7901 / 76500) loss: 2.308383\n",
      "(Iteration 8001 / 76500) loss: 2.308360\n",
      "(Iteration 8101 / 76500) loss: 2.308385\n",
      "(Iteration 8201 / 76500) loss: 2.308376\n",
      "(Iteration 8301 / 76500) loss: 2.308369\n",
      "(Iteration 8401 / 76500) loss: 2.308389\n",
      "(Epoch 11 / 100) train acc: 0.095000; val_acc: 0.103000\n",
      "(Iteration 8501 / 76500) loss: 2.308374\n",
      "(Iteration 8601 / 76500) loss: 2.308356\n",
      "(Iteration 8701 / 76500) loss: 2.308376\n",
      "(Iteration 8801 / 76500) loss: 2.308387\n",
      "(Iteration 8901 / 76500) loss: 2.308370\n",
      "(Iteration 9001 / 76500) loss: 2.308392\n",
      "(Iteration 9101 / 76500) loss: 2.308371\n",
      "(Epoch 12 / 100) train acc: 0.091000; val_acc: 0.102000\n",
      "(Iteration 9201 / 76500) loss: 2.308378\n",
      "(Iteration 9301 / 76500) loss: 2.308384\n",
      "(Iteration 9401 / 76500) loss: 2.308352\n",
      "(Iteration 9501 / 76500) loss: 2.308371\n",
      "(Iteration 9601 / 76500) loss: 2.308370\n",
      "(Iteration 9701 / 76500) loss: 2.308368\n",
      "(Iteration 9801 / 76500) loss: 2.308358\n",
      "(Iteration 9901 / 76500) loss: 2.308375\n",
      "(Epoch 13 / 100) train acc: 0.104000; val_acc: 0.102000\n",
      "(Iteration 10001 / 76500) loss: 2.308388\n",
      "(Iteration 10101 / 76500) loss: 2.308370\n",
      "(Iteration 10201 / 76500) loss: 2.308384\n",
      "(Iteration 10301 / 76500) loss: 2.308379\n",
      "(Iteration 10401 / 76500) loss: 2.308355\n",
      "(Iteration 10501 / 76500) loss: 2.308379\n",
      "(Iteration 10601 / 76500) loss: 2.308380\n",
      "(Iteration 10701 / 76500) loss: 2.308404\n",
      "(Epoch 14 / 100) train acc: 0.094000; val_acc: 0.102000\n",
      "(Iteration 10801 / 76500) loss: 2.308373\n",
      "(Iteration 10901 / 76500) loss: 2.308355\n",
      "(Iteration 11001 / 76500) loss: 2.308367\n",
      "(Iteration 11101 / 76500) loss: 2.308372\n",
      "(Iteration 11201 / 76500) loss: 2.308374\n",
      "(Iteration 11301 / 76500) loss: 2.308361\n",
      "(Iteration 11401 / 76500) loss: 2.308380\n",
      "(Epoch 15 / 100) train acc: 0.105000; val_acc: 0.102000\n",
      "(Iteration 11501 / 76500) loss: 2.308379\n",
      "(Iteration 11601 / 76500) loss: 2.308386\n",
      "(Iteration 11701 / 76500) loss: 2.308375\n",
      "(Iteration 11801 / 76500) loss: 2.308382\n",
      "(Iteration 11901 / 76500) loss: 2.308383\n",
      "(Iteration 12001 / 76500) loss: 2.308384\n",
      "(Iteration 12101 / 76500) loss: 2.308399\n",
      "(Iteration 12201 / 76500) loss: 2.308380\n",
      "(Epoch 16 / 100) train acc: 0.079000; val_acc: 0.102000\n",
      "(Iteration 12301 / 76500) loss: 2.308396\n",
      "(Iteration 12401 / 76500) loss: 2.308370\n",
      "(Iteration 12501 / 76500) loss: 2.308397\n",
      "(Iteration 12601 / 76500) loss: 2.308392\n",
      "(Iteration 12701 / 76500) loss: 2.308388\n",
      "(Iteration 12801 / 76500) loss: 2.308344\n",
      "(Iteration 12901 / 76500) loss: 2.308381\n",
      "(Iteration 13001 / 76500) loss: 2.308362\n",
      "(Epoch 17 / 100) train acc: 0.096000; val_acc: 0.102000\n",
      "(Iteration 13101 / 76500) loss: 2.308354\n",
      "(Iteration 13201 / 76500) loss: 2.308382\n",
      "(Iteration 13301 / 76500) loss: 2.308361\n",
      "(Iteration 13401 / 76500) loss: 2.308359\n",
      "(Iteration 13501 / 76500) loss: 2.308374\n",
      "(Iteration 13601 / 76500) loss: 2.308365\n",
      "(Iteration 13701 / 76500) loss: 2.308382\n",
      "(Epoch 18 / 100) train acc: 0.094000; val_acc: 0.102000\n",
      "(Iteration 13801 / 76500) loss: 2.308392\n",
      "(Iteration 13901 / 76500) loss: 2.308397\n",
      "(Iteration 14001 / 76500) loss: 2.308389\n",
      "(Iteration 14101 / 76500) loss: 2.308382\n",
      "(Iteration 14201 / 76500) loss: 2.308381\n",
      "(Iteration 14301 / 76500) loss: 2.308390\n",
      "(Iteration 14401 / 76500) loss: 2.308368\n",
      "(Iteration 14501 / 76500) loss: 2.308369\n",
      "(Epoch 19 / 100) train acc: 0.089000; val_acc: 0.102000\n",
      "(Iteration 14601 / 76500) loss: 2.308368\n",
      "(Iteration 14701 / 76500) loss: 2.308363\n",
      "(Iteration 14801 / 76500) loss: 2.308370\n",
      "(Iteration 14901 / 76500) loss: 2.308373\n",
      "(Iteration 15001 / 76500) loss: 2.308385\n",
      "(Iteration 15101 / 76500) loss: 2.308398\n",
      "(Iteration 15201 / 76500) loss: 2.308360\n",
      "(Epoch 20 / 100) train acc: 0.109000; val_acc: 0.102000\n",
      "(Iteration 15301 / 76500) loss: 2.308371\n",
      "(Iteration 15401 / 76500) loss: 2.308376\n",
      "(Iteration 15501 / 76500) loss: 2.308362\n",
      "(Iteration 15601 / 76500) loss: 2.308373\n",
      "(Iteration 15701 / 76500) loss: 2.308346\n",
      "(Iteration 15801 / 76500) loss: 2.308378\n",
      "(Iteration 15901 / 76500) loss: 2.308362\n",
      "(Iteration 16001 / 76500) loss: 2.308395\n",
      "(Epoch 21 / 100) train acc: 0.109000; val_acc: 0.102000\n",
      "(Iteration 16101 / 76500) loss: 2.308385\n",
      "(Iteration 16201 / 76500) loss: 2.308374\n",
      "(Iteration 16301 / 76500) loss: 2.308388\n",
      "(Iteration 16401 / 76500) loss: 2.308372\n",
      "(Iteration 16501 / 76500) loss: 2.308392\n",
      "(Iteration 16601 / 76500) loss: 2.308373\n",
      "(Iteration 16701 / 76500) loss: 2.308370\n",
      "(Iteration 16801 / 76500) loss: 2.308388\n",
      "(Epoch 22 / 100) train acc: 0.095000; val_acc: 0.102000\n",
      "(Iteration 16901 / 76500) loss: 2.308376\n",
      "(Iteration 17001 / 76500) loss: 2.308371\n",
      "(Iteration 17101 / 76500) loss: 2.308368\n",
      "(Iteration 17201 / 76500) loss: 2.308342\n",
      "(Iteration 17301 / 76500) loss: 2.308369\n",
      "(Iteration 17401 / 76500) loss: 2.308384\n",
      "(Iteration 17501 / 76500) loss: 2.308377\n",
      "(Epoch 23 / 100) train acc: 0.104000; val_acc: 0.102000\n",
      "(Iteration 17601 / 76500) loss: 2.308356\n",
      "(Iteration 17701 / 76500) loss: 2.308368\n",
      "(Iteration 17801 / 76500) loss: 2.308371\n",
      "(Iteration 17901 / 76500) loss: 2.308370\n",
      "(Iteration 18001 / 76500) loss: 2.308382\n",
      "(Iteration 18101 / 76500) loss: 2.308354\n",
      "(Iteration 18201 / 76500) loss: 2.308367\n",
      "(Iteration 18301 / 76500) loss: 2.308380\n",
      "(Epoch 24 / 100) train acc: 0.079000; val_acc: 0.102000\n",
      "(Iteration 18401 / 76500) loss: 2.308370\n",
      "(Iteration 18501 / 76500) loss: 2.308382\n",
      "(Iteration 18601 / 76500) loss: 2.308377\n",
      "(Iteration 18701 / 76500) loss: 2.308370\n",
      "(Iteration 18801 / 76500) loss: 2.308364\n",
      "(Iteration 18901 / 76500) loss: 2.308362\n",
      "(Iteration 19001 / 76500) loss: 2.308376\n",
      "(Iteration 19101 / 76500) loss: 2.308384\n",
      "(Epoch 25 / 100) train acc: 0.106000; val_acc: 0.102000\n",
      "(Iteration 19201 / 76500) loss: 2.308369\n",
      "(Iteration 19301 / 76500) loss: 2.308384\n",
      "(Iteration 19401 / 76500) loss: 2.308384\n",
      "(Iteration 19501 / 76500) loss: 2.308370\n",
      "(Iteration 19601 / 76500) loss: 2.308358\n",
      "(Iteration 19701 / 76500) loss: 2.308386\n",
      "(Iteration 19801 / 76500) loss: 2.308364\n",
      "(Epoch 26 / 100) train acc: 0.110000; val_acc: 0.102000\n",
      "(Iteration 19901 / 76500) loss: 2.308372\n",
      "(Iteration 20001 / 76500) loss: 2.308367\n",
      "(Iteration 20101 / 76500) loss: 2.308378\n",
      "(Iteration 20201 / 76500) loss: 2.308361\n",
      "(Iteration 20301 / 76500) loss: 2.308377\n",
      "(Iteration 20401 / 76500) loss: 2.308360\n",
      "(Iteration 20501 / 76500) loss: 2.308356\n",
      "(Iteration 20601 / 76500) loss: 2.308366\n",
      "(Epoch 27 / 100) train acc: 0.085000; val_acc: 0.102000\n",
      "(Iteration 20701 / 76500) loss: 2.308373\n",
      "(Iteration 20801 / 76500) loss: 2.308354\n",
      "(Iteration 20901 / 76500) loss: 2.308381\n",
      "(Iteration 21001 / 76500) loss: 2.308376\n",
      "(Iteration 21101 / 76500) loss: 2.308371\n",
      "(Iteration 21201 / 76500) loss: 2.308373\n",
      "(Iteration 21301 / 76500) loss: 2.308371\n",
      "(Iteration 21401 / 76500) loss: 2.308391\n",
      "(Epoch 28 / 100) train acc: 0.093000; val_acc: 0.102000\n",
      "(Iteration 21501 / 76500) loss: 2.308379\n",
      "(Iteration 21601 / 76500) loss: 2.308377\n",
      "(Iteration 21701 / 76500) loss: 2.308378\n",
      "(Iteration 21801 / 76500) loss: 2.308356\n",
      "(Iteration 21901 / 76500) loss: 2.308355\n",
      "(Iteration 22001 / 76500) loss: 2.308380\n",
      "(Iteration 22101 / 76500) loss: 2.308367\n",
      "(Epoch 29 / 100) train acc: 0.107000; val_acc: 0.102000\n",
      "(Iteration 22201 / 76500) loss: 2.308372\n",
      "(Iteration 22301 / 76500) loss: 2.308384\n",
      "(Iteration 22401 / 76500) loss: 2.308358\n",
      "(Iteration 22501 / 76500) loss: 2.308364\n",
      "(Iteration 22601 / 76500) loss: 2.308384\n",
      "(Iteration 22701 / 76500) loss: 2.308361\n",
      "(Iteration 22801 / 76500) loss: 2.308388\n",
      "(Iteration 22901 / 76500) loss: 2.308361\n",
      "(Epoch 30 / 100) train acc: 0.098000; val_acc: 0.102000\n",
      "(Iteration 23001 / 76500) loss: 2.308364\n",
      "(Iteration 23101 / 76500) loss: 2.308377\n",
      "(Iteration 23201 / 76500) loss: 2.308381\n",
      "(Iteration 23301 / 76500) loss: 2.308380\n",
      "(Iteration 23401 / 76500) loss: 2.308359\n",
      "(Iteration 23501 / 76500) loss: 2.308368\n",
      "(Iteration 23601 / 76500) loss: 2.308361\n",
      "(Iteration 23701 / 76500) loss: 2.308381\n",
      "(Epoch 31 / 100) train acc: 0.092000; val_acc: 0.102000\n",
      "(Iteration 23801 / 76500) loss: 2.308365\n",
      "(Iteration 23901 / 76500) loss: 2.308378\n",
      "(Iteration 24001 / 76500) loss: 2.308366\n",
      "(Iteration 24101 / 76500) loss: 2.308363\n",
      "(Iteration 24201 / 76500) loss: 2.308370\n",
      "(Iteration 24301 / 76500) loss: 2.308387\n",
      "(Iteration 24401 / 76500) loss: 2.308368\n",
      "(Epoch 32 / 100) train acc: 0.096000; val_acc: 0.102000\n",
      "(Iteration 24501 / 76500) loss: 2.308367\n",
      "(Iteration 24601 / 76500) loss: 2.308371\n",
      "(Iteration 24701 / 76500) loss: 2.308374\n",
      "(Iteration 24801 / 76500) loss: 2.308367\n",
      "(Iteration 24901 / 76500) loss: 2.308356\n",
      "(Iteration 25001 / 76500) loss: 2.308366\n",
      "(Iteration 25101 / 76500) loss: 2.308352\n",
      "(Iteration 25201 / 76500) loss: 2.308374\n",
      "(Epoch 33 / 100) train acc: 0.081000; val_acc: 0.102000\n",
      "(Iteration 25301 / 76500) loss: 2.308379\n",
      "(Iteration 25401 / 76500) loss: 2.308367\n",
      "(Iteration 25501 / 76500) loss: 2.308377\n",
      "(Iteration 25601 / 76500) loss: 2.308385\n",
      "(Iteration 25701 / 76500) loss: 2.308365\n",
      "(Iteration 25801 / 76500) loss: 2.308355\n",
      "(Iteration 25901 / 76500) loss: 2.308394\n",
      "(Iteration 26001 / 76500) loss: 2.308367\n",
      "(Epoch 34 / 100) train acc: 0.098000; val_acc: 0.102000\n",
      "(Iteration 26101 / 76500) loss: 2.308370\n",
      "(Iteration 26201 / 76500) loss: 2.308394\n",
      "(Iteration 26301 / 76500) loss: 2.308383\n",
      "(Iteration 26401 / 76500) loss: 2.308355\n",
      "(Iteration 26501 / 76500) loss: 2.308373\n",
      "(Iteration 26601 / 76500) loss: 2.308376\n",
      "(Iteration 26701 / 76500) loss: 2.308367\n",
      "(Epoch 35 / 100) train acc: 0.102000; val_acc: 0.102000\n",
      "(Iteration 26801 / 76500) loss: 2.308364\n",
      "(Iteration 26901 / 76500) loss: 2.308380\n",
      "(Iteration 27001 / 76500) loss: 2.308383\n",
      "(Iteration 27101 / 76500) loss: 2.308367\n",
      "(Iteration 27201 / 76500) loss: 2.308383\n",
      "(Iteration 27301 / 76500) loss: 2.308380\n",
      "(Iteration 27401 / 76500) loss: 2.308381\n",
      "(Iteration 27501 / 76500) loss: 2.308381\n",
      "(Epoch 36 / 100) train acc: 0.103000; val_acc: 0.102000\n",
      "(Iteration 27601 / 76500) loss: 2.308370\n",
      "(Iteration 27701 / 76500) loss: 2.308390\n",
      "(Iteration 27801 / 76500) loss: 2.308375\n",
      "(Iteration 27901 / 76500) loss: 2.308373\n",
      "(Iteration 28001 / 76500) loss: 2.308377\n",
      "(Iteration 28101 / 76500) loss: 2.308371\n",
      "(Iteration 28201 / 76500) loss: 2.308379\n",
      "(Iteration 28301 / 76500) loss: 2.308376\n",
      "(Epoch 37 / 100) train acc: 0.096000; val_acc: 0.102000\n",
      "(Iteration 28401 / 76500) loss: 2.308380\n",
      "(Iteration 28501 / 76500) loss: 2.308391\n",
      "(Iteration 28601 / 76500) loss: 2.308358\n",
      "(Iteration 28701 / 76500) loss: 2.308373\n",
      "(Iteration 28801 / 76500) loss: 2.308365\n",
      "(Iteration 28901 / 76500) loss: 2.308380\n",
      "(Iteration 29001 / 76500) loss: 2.308388\n",
      "(Epoch 38 / 100) train acc: 0.100000; val_acc: 0.102000\n",
      "(Iteration 29101 / 76500) loss: 2.308377\n",
      "(Iteration 29201 / 76500) loss: 2.308363\n",
      "(Iteration 29301 / 76500) loss: 2.308382\n",
      "(Iteration 29401 / 76500) loss: 2.308372\n",
      "(Iteration 29501 / 76500) loss: 2.308379\n",
      "(Iteration 29601 / 76500) loss: 2.308378\n",
      "(Iteration 29701 / 76500) loss: 2.308366\n",
      "(Iteration 29801 / 76500) loss: 2.308371\n",
      "(Epoch 39 / 100) train acc: 0.099000; val_acc: 0.102000\n",
      "(Iteration 29901 / 76500) loss: 2.308360\n",
      "(Iteration 30001 / 76500) loss: 2.308368\n",
      "(Iteration 30101 / 76500) loss: 2.308374\n",
      "(Iteration 30201 / 76500) loss: 2.308375\n",
      "(Iteration 30301 / 76500) loss: 2.308380\n",
      "(Iteration 30401 / 76500) loss: 2.308378\n",
      "(Iteration 30501 / 76500) loss: 2.308372\n",
      "(Epoch 40 / 100) train acc: 0.100000; val_acc: 0.102000\n",
      "(Iteration 30601 / 76500) loss: 2.308387\n",
      "(Iteration 30701 / 76500) loss: 2.308381\n",
      "(Iteration 30801 / 76500) loss: 2.308374\n",
      "(Iteration 30901 / 76500) loss: 2.308354\n",
      "(Iteration 31001 / 76500) loss: 2.308374\n",
      "(Iteration 31101 / 76500) loss: 2.308369\n",
      "(Iteration 31201 / 76500) loss: 2.308345\n",
      "(Iteration 31301 / 76500) loss: 2.308359\n",
      "(Epoch 41 / 100) train acc: 0.096000; val_acc: 0.102000\n",
      "(Iteration 31401 / 76500) loss: 2.308379\n",
      "(Iteration 31501 / 76500) loss: 2.308380\n",
      "(Iteration 31601 / 76500) loss: 2.308354\n",
      "(Iteration 31701 / 76500) loss: 2.308389\n",
      "(Iteration 31801 / 76500) loss: 2.308375\n",
      "(Iteration 31901 / 76500) loss: 2.308347\n",
      "(Iteration 32001 / 76500) loss: 2.308366\n",
      "(Iteration 32101 / 76500) loss: 2.308370\n",
      "(Epoch 42 / 100) train acc: 0.112000; val_acc: 0.103000\n",
      "(Iteration 32201 / 76500) loss: 2.308371\n",
      "(Iteration 32301 / 76500) loss: 2.308371\n",
      "(Iteration 32401 / 76500) loss: 2.308370\n",
      "(Iteration 32501 / 76500) loss: 2.308394\n",
      "(Iteration 32601 / 76500) loss: 2.308317\n",
      "(Iteration 32701 / 76500) loss: 2.308389\n",
      "(Iteration 32801 / 76500) loss: 2.308385\n",
      "(Epoch 43 / 100) train acc: 0.086000; val_acc: 0.102000\n",
      "(Iteration 32901 / 76500) loss: 2.308377\n",
      "(Iteration 33001 / 76500) loss: 2.308360\n",
      "(Iteration 33101 / 76500) loss: 2.308374\n",
      "(Iteration 33201 / 76500) loss: 2.308365\n",
      "(Iteration 33301 / 76500) loss: 2.308370\n",
      "(Iteration 33401 / 76500) loss: 2.308361\n",
      "(Iteration 33501 / 76500) loss: 2.308378\n",
      "(Iteration 33601 / 76500) loss: 2.308352\n",
      "(Epoch 44 / 100) train acc: 0.084000; val_acc: 0.102000\n",
      "(Iteration 33701 / 76500) loss: 2.308381\n",
      "(Iteration 33801 / 76500) loss: 2.308357\n",
      "(Iteration 33901 / 76500) loss: 2.308386\n",
      "(Iteration 34001 / 76500) loss: 2.308368\n",
      "(Iteration 34101 / 76500) loss: 2.308364\n",
      "(Iteration 34201 / 76500) loss: 2.308382\n",
      "(Iteration 34301 / 76500) loss: 2.308368\n",
      "(Iteration 34401 / 76500) loss: 2.308383\n",
      "(Epoch 45 / 100) train acc: 0.116000; val_acc: 0.102000\n",
      "(Iteration 34501 / 76500) loss: 2.308383\n",
      "(Iteration 34601 / 76500) loss: 2.308368\n",
      "(Iteration 34701 / 76500) loss: 2.308372\n",
      "(Iteration 34801 / 76500) loss: 2.308350\n",
      "(Iteration 34901 / 76500) loss: 2.308401\n",
      "(Iteration 35001 / 76500) loss: 2.308367\n",
      "(Iteration 35101 / 76500) loss: 2.308376\n",
      "(Epoch 46 / 100) train acc: 0.108000; val_acc: 0.102000\n",
      "(Iteration 35201 / 76500) loss: 2.308371\n",
      "(Iteration 35301 / 76500) loss: 2.308366\n",
      "(Iteration 35401 / 76500) loss: 2.308372\n",
      "(Iteration 35501 / 76500) loss: 2.308368\n",
      "(Iteration 35601 / 76500) loss: 2.308380\n",
      "(Iteration 35701 / 76500) loss: 2.308366\n",
      "(Iteration 35801 / 76500) loss: 2.308387\n",
      "(Iteration 35901 / 76500) loss: 2.308368\n",
      "(Epoch 47 / 100) train acc: 0.100000; val_acc: 0.103000\n",
      "(Iteration 36001 / 76500) loss: 2.308353\n",
      "(Iteration 36101 / 76500) loss: 2.308384\n",
      "(Iteration 36201 / 76500) loss: 2.308374\n",
      "(Iteration 36301 / 76500) loss: 2.308376\n",
      "(Iteration 36401 / 76500) loss: 2.308373\n",
      "(Iteration 36501 / 76500) loss: 2.308362\n",
      "(Iteration 36601 / 76500) loss: 2.308374\n",
      "(Iteration 36701 / 76500) loss: 2.308348\n",
      "(Epoch 48 / 100) train acc: 0.079000; val_acc: 0.103000\n",
      "(Iteration 36801 / 76500) loss: 2.308360\n",
      "(Iteration 36901 / 76500) loss: 2.308345\n",
      "(Iteration 37001 / 76500) loss: 2.308389\n",
      "(Iteration 37101 / 76500) loss: 2.308361\n",
      "(Iteration 37201 / 76500) loss: 2.308371\n",
      "(Iteration 37301 / 76500) loss: 2.308376\n",
      "(Iteration 37401 / 76500) loss: 2.308362\n",
      "(Epoch 49 / 100) train acc: 0.091000; val_acc: 0.103000\n",
      "(Iteration 37501 / 76500) loss: 2.308366\n",
      "(Iteration 37601 / 76500) loss: 2.308351\n",
      "(Iteration 37701 / 76500) loss: 2.308403\n",
      "(Iteration 37801 / 76500) loss: 2.308371\n",
      "(Iteration 37901 / 76500) loss: 2.308392\n",
      "(Iteration 38001 / 76500) loss: 2.308363\n",
      "(Iteration 38101 / 76500) loss: 2.308360\n",
      "(Iteration 38201 / 76500) loss: 2.308372\n",
      "(Epoch 50 / 100) train acc: 0.101000; val_acc: 0.102000\n",
      "(Iteration 38301 / 76500) loss: 2.308353\n",
      "(Iteration 38401 / 76500) loss: 2.308373\n",
      "(Iteration 38501 / 76500) loss: 2.308376\n",
      "(Iteration 38601 / 76500) loss: 2.308380\n",
      "(Iteration 38701 / 76500) loss: 2.308350\n",
      "(Iteration 38801 / 76500) loss: 2.308384\n",
      "(Iteration 38901 / 76500) loss: 2.308377\n",
      "(Iteration 39001 / 76500) loss: 2.308377\n",
      "(Epoch 51 / 100) train acc: 0.095000; val_acc: 0.103000\n",
      "(Iteration 39101 / 76500) loss: 2.308370\n",
      "(Iteration 39201 / 76500) loss: 2.308348\n",
      "(Iteration 39301 / 76500) loss: 2.308372\n",
      "(Iteration 39401 / 76500) loss: 2.308362\n",
      "(Iteration 39501 / 76500) loss: 2.308361\n",
      "(Iteration 39601 / 76500) loss: 2.308384\n",
      "(Iteration 39701 / 76500) loss: 2.308380\n",
      "(Epoch 52 / 100) train acc: 0.094000; val_acc: 0.102000\n",
      "(Iteration 39801 / 76500) loss: 2.308378\n",
      "(Iteration 39901 / 76500) loss: 2.308355\n",
      "(Iteration 40001 / 76500) loss: 2.308366\n",
      "(Iteration 40101 / 76500) loss: 2.308381\n",
      "(Iteration 40201 / 76500) loss: 2.308375\n",
      "(Iteration 40301 / 76500) loss: 2.308363\n",
      "(Iteration 40401 / 76500) loss: 2.308349\n",
      "(Iteration 40501 / 76500) loss: 2.308383\n",
      "(Epoch 53 / 100) train acc: 0.088000; val_acc: 0.103000\n",
      "(Iteration 40601 / 76500) loss: 2.308352\n",
      "(Iteration 40701 / 76500) loss: 2.308365\n",
      "(Iteration 40801 / 76500) loss: 2.308365\n",
      "(Iteration 40901 / 76500) loss: 2.308367\n",
      "(Iteration 41001 / 76500) loss: 2.308371\n",
      "(Iteration 41101 / 76500) loss: 2.308377\n",
      "(Iteration 41201 / 76500) loss: 2.308377\n",
      "(Iteration 41301 / 76500) loss: 2.308379\n",
      "(Epoch 54 / 100) train acc: 0.086000; val_acc: 0.103000\n",
      "(Iteration 41401 / 76500) loss: 2.308347\n",
      "(Iteration 41501 / 76500) loss: 2.308371\n",
      "(Iteration 41601 / 76500) loss: 2.308371\n",
      "(Iteration 41701 / 76500) loss: 2.308372\n",
      "(Iteration 41801 / 76500) loss: 2.308383\n",
      "(Iteration 41901 / 76500) loss: 2.308362\n",
      "(Iteration 42001 / 76500) loss: 2.308385\n",
      "(Epoch 55 / 100) train acc: 0.090000; val_acc: 0.103000\n",
      "(Iteration 42101 / 76500) loss: 2.308364\n",
      "(Iteration 42201 / 76500) loss: 2.308343\n",
      "(Iteration 42301 / 76500) loss: 2.308375\n",
      "(Iteration 42401 / 76500) loss: 2.308351\n",
      "(Iteration 42501 / 76500) loss: 2.308366\n",
      "(Iteration 42601 / 76500) loss: 2.308374\n",
      "(Iteration 42701 / 76500) loss: 2.308374\n",
      "(Iteration 42801 / 76500) loss: 2.308378\n",
      "(Epoch 56 / 100) train acc: 0.094000; val_acc: 0.103000\n",
      "(Iteration 42901 / 76500) loss: 2.308367\n",
      "(Iteration 43001 / 76500) loss: 2.308367\n",
      "(Iteration 43101 / 76500) loss: 2.308364\n",
      "(Iteration 43201 / 76500) loss: 2.308361\n",
      "(Iteration 43301 / 76500) loss: 2.308382\n",
      "(Iteration 43401 / 76500) loss: 2.308364\n",
      "(Iteration 43501 / 76500) loss: 2.308353\n",
      "(Iteration 43601 / 76500) loss: 2.308374\n",
      "(Epoch 57 / 100) train acc: 0.091000; val_acc: 0.103000\n",
      "(Iteration 43701 / 76500) loss: 2.308378\n",
      "(Iteration 43801 / 76500) loss: 2.308388\n",
      "(Iteration 43901 / 76500) loss: 2.308384\n",
      "(Iteration 44001 / 76500) loss: 2.308381\n",
      "(Iteration 44101 / 76500) loss: 2.308366\n",
      "(Iteration 44201 / 76500) loss: 2.308361\n",
      "(Iteration 44301 / 76500) loss: 2.308358\n",
      "(Epoch 58 / 100) train acc: 0.094000; val_acc: 0.103000\n",
      "(Iteration 44401 / 76500) loss: 2.308373\n",
      "(Iteration 44501 / 76500) loss: 2.308341\n",
      "(Iteration 44601 / 76500) loss: 2.308350\n",
      "(Iteration 44701 / 76500) loss: 2.308370\n",
      "(Iteration 44801 / 76500) loss: 2.308372\n",
      "(Iteration 44901 / 76500) loss: 2.308380\n",
      "(Iteration 45001 / 76500) loss: 2.308372\n",
      "(Iteration 45101 / 76500) loss: 2.308357\n",
      "(Epoch 59 / 100) train acc: 0.110000; val_acc: 0.103000\n",
      "(Iteration 45201 / 76500) loss: 2.308376\n",
      "(Iteration 45301 / 76500) loss: 2.308356\n",
      "(Iteration 45401 / 76500) loss: 2.308384\n",
      "(Iteration 45501 / 76500) loss: 2.308372\n",
      "(Iteration 45601 / 76500) loss: 2.308383\n",
      "(Iteration 45701 / 76500) loss: 2.308382\n",
      "(Iteration 45801 / 76500) loss: 2.308375\n",
      "(Epoch 60 / 100) train acc: 0.100000; val_acc: 0.103000\n",
      "(Iteration 45901 / 76500) loss: 2.308384\n",
      "(Iteration 46001 / 76500) loss: 2.308365\n",
      "(Iteration 46101 / 76500) loss: 2.308366\n",
      "(Iteration 46201 / 76500) loss: 2.308364\n",
      "(Iteration 46301 / 76500) loss: 2.308370\n",
      "(Iteration 46401 / 76500) loss: 2.308374\n",
      "(Iteration 46501 / 76500) loss: 2.308371\n",
      "(Iteration 46601 / 76500) loss: 2.308364\n",
      "(Epoch 61 / 100) train acc: 0.087000; val_acc: 0.103000\n",
      "(Iteration 46701 / 76500) loss: 2.308354\n",
      "(Iteration 46801 / 76500) loss: 2.308366\n",
      "(Iteration 46901 / 76500) loss: 2.308369\n",
      "(Iteration 47001 / 76500) loss: 2.308382\n",
      "(Iteration 47101 / 76500) loss: 2.308373\n",
      "(Iteration 47201 / 76500) loss: 2.308383\n",
      "(Iteration 47301 / 76500) loss: 2.308360\n",
      "(Iteration 47401 / 76500) loss: 2.308379\n",
      "(Epoch 62 / 100) train acc: 0.099000; val_acc: 0.103000\n",
      "(Iteration 47501 / 76500) loss: 2.308384\n",
      "(Iteration 47601 / 76500) loss: 2.308378\n",
      "(Iteration 47701 / 76500) loss: 2.308369\n",
      "(Iteration 47801 / 76500) loss: 2.308387\n",
      "(Iteration 47901 / 76500) loss: 2.308368\n",
      "(Iteration 48001 / 76500) loss: 2.308374\n",
      "(Iteration 48101 / 76500) loss: 2.308355\n",
      "(Epoch 63 / 100) train acc: 0.095000; val_acc: 0.103000\n",
      "(Iteration 48201 / 76500) loss: 2.308380\n",
      "(Iteration 48301 / 76500) loss: 2.308378\n",
      "(Iteration 48401 / 76500) loss: 2.308383\n",
      "(Iteration 48501 / 76500) loss: 2.308355\n",
      "(Iteration 48601 / 76500) loss: 2.308367\n",
      "(Iteration 48701 / 76500) loss: 2.308376\n",
      "(Iteration 48801 / 76500) loss: 2.308376\n",
      "(Iteration 48901 / 76500) loss: 2.308383\n",
      "(Epoch 64 / 100) train acc: 0.088000; val_acc: 0.103000\n",
      "(Iteration 49001 / 76500) loss: 2.308368\n",
      "(Iteration 49101 / 76500) loss: 2.308349\n",
      "(Iteration 49201 / 76500) loss: 2.308373\n",
      "(Iteration 49301 / 76500) loss: 2.308371\n",
      "(Iteration 49401 / 76500) loss: 2.308369\n",
      "(Iteration 49501 / 76500) loss: 2.308365\n",
      "(Iteration 49601 / 76500) loss: 2.308366\n",
      "(Iteration 49701 / 76500) loss: 2.308341\n",
      "(Epoch 65 / 100) train acc: 0.093000; val_acc: 0.103000\n",
      "(Iteration 49801 / 76500) loss: 2.308367\n",
      "(Iteration 49901 / 76500) loss: 2.308385\n",
      "(Iteration 50001 / 76500) loss: 2.308385\n",
      "(Iteration 50101 / 76500) loss: 2.308392\n",
      "(Iteration 50201 / 76500) loss: 2.308370\n",
      "(Iteration 50301 / 76500) loss: 2.308380\n",
      "(Iteration 50401 / 76500) loss: 2.308387\n",
      "(Epoch 66 / 100) train acc: 0.082000; val_acc: 0.103000\n",
      "(Iteration 50501 / 76500) loss: 2.308377\n",
      "(Iteration 50601 / 76500) loss: 2.308377\n",
      "(Iteration 50701 / 76500) loss: 2.308376\n",
      "(Iteration 50801 / 76500) loss: 2.308374\n",
      "(Iteration 50901 / 76500) loss: 2.308355\n",
      "(Iteration 51001 / 76500) loss: 2.308361\n",
      "(Iteration 51101 / 76500) loss: 2.308367\n",
      "(Iteration 51201 / 76500) loss: 2.308353\n",
      "(Epoch 67 / 100) train acc: 0.079000; val_acc: 0.103000\n",
      "(Iteration 51301 / 76500) loss: 2.308375\n",
      "(Iteration 51401 / 76500) loss: 2.308372\n",
      "(Iteration 51501 / 76500) loss: 2.308369\n",
      "(Iteration 51601 / 76500) loss: 2.308364\n",
      "(Iteration 51701 / 76500) loss: 2.308360\n",
      "(Iteration 51801 / 76500) loss: 2.308378\n",
      "(Iteration 51901 / 76500) loss: 2.308382\n",
      "(Iteration 52001 / 76500) loss: 2.308371\n",
      "(Epoch 68 / 100) train acc: 0.095000; val_acc: 0.103000\n",
      "(Iteration 52101 / 76500) loss: 2.308357\n",
      "(Iteration 52201 / 76500) loss: 2.308344\n",
      "(Iteration 52301 / 76500) loss: 2.308360\n",
      "(Iteration 52401 / 76500) loss: 2.308351\n",
      "(Iteration 52501 / 76500) loss: 2.308375\n",
      "(Iteration 52601 / 76500) loss: 2.308376\n",
      "(Iteration 52701 / 76500) loss: 2.308387\n",
      "(Epoch 69 / 100) train acc: 0.089000; val_acc: 0.103000\n",
      "(Iteration 52801 / 76500) loss: 2.308351\n",
      "(Iteration 52901 / 76500) loss: 2.308359\n",
      "(Iteration 53001 / 76500) loss: 2.308391\n",
      "(Iteration 53101 / 76500) loss: 2.308371\n",
      "(Iteration 53201 / 76500) loss: 2.308368\n",
      "(Iteration 53301 / 76500) loss: 2.308379\n",
      "(Iteration 53401 / 76500) loss: 2.308370\n",
      "(Iteration 53501 / 76500) loss: 2.308367\n",
      "(Epoch 70 / 100) train acc: 0.099000; val_acc: 0.103000\n",
      "(Iteration 53601 / 76500) loss: 2.308358\n",
      "(Iteration 53701 / 76500) loss: 2.308372\n",
      "(Iteration 53801 / 76500) loss: 2.308376\n",
      "(Iteration 53901 / 76500) loss: 2.308373\n",
      "(Iteration 54001 / 76500) loss: 2.308355\n",
      "(Iteration 54101 / 76500) loss: 2.308368\n",
      "(Iteration 54201 / 76500) loss: 2.308384\n",
      "(Iteration 54301 / 76500) loss: 2.308360\n",
      "(Epoch 71 / 100) train acc: 0.094000; val_acc: 0.103000\n",
      "(Iteration 54401 / 76500) loss: 2.308360\n",
      "(Iteration 54501 / 76500) loss: 2.308385\n",
      "(Iteration 54601 / 76500) loss: 2.308363\n",
      "(Iteration 54701 / 76500) loss: 2.308356\n",
      "(Iteration 54801 / 76500) loss: 2.308389\n",
      "(Iteration 54901 / 76500) loss: 2.308358\n",
      "(Iteration 55001 / 76500) loss: 2.308361\n",
      "(Epoch 72 / 100) train acc: 0.080000; val_acc: 0.103000\n",
      "(Iteration 55101 / 76500) loss: 2.308385\n",
      "(Iteration 55201 / 76500) loss: 2.308383\n",
      "(Iteration 55301 / 76500) loss: 2.308379\n",
      "(Iteration 55401 / 76500) loss: 2.308370\n",
      "(Iteration 55501 / 76500) loss: 2.308369\n",
      "(Iteration 55601 / 76500) loss: 2.308366\n",
      "(Iteration 55701 / 76500) loss: 2.308368\n",
      "(Iteration 55801 / 76500) loss: 2.308350\n",
      "(Epoch 73 / 100) train acc: 0.106000; val_acc: 0.103000\n",
      "(Iteration 55901 / 76500) loss: 2.308378\n",
      "(Iteration 56001 / 76500) loss: 2.308383\n",
      "(Iteration 56101 / 76500) loss: 2.308359\n",
      "(Iteration 56201 / 76500) loss: 2.308380\n",
      "(Iteration 56301 / 76500) loss: 2.308350\n",
      "(Iteration 56401 / 76500) loss: 2.308360\n",
      "(Iteration 56501 / 76500) loss: 2.308365\n",
      "(Iteration 56601 / 76500) loss: 2.308375\n",
      "(Epoch 74 / 100) train acc: 0.102000; val_acc: 0.103000\n",
      "(Iteration 56701 / 76500) loss: 2.308364\n",
      "(Iteration 56801 / 76500) loss: 2.308356\n",
      "(Iteration 56901 / 76500) loss: 2.308365\n",
      "(Iteration 57001 / 76500) loss: 2.308367\n",
      "(Iteration 57101 / 76500) loss: 2.308372\n",
      "(Iteration 57201 / 76500) loss: 2.308373\n",
      "(Iteration 57301 / 76500) loss: 2.308356\n",
      "(Epoch 75 / 100) train acc: 0.088000; val_acc: 0.103000\n",
      "(Iteration 57401 / 76500) loss: 2.308375\n",
      "(Iteration 57501 / 76500) loss: 2.308370\n",
      "(Iteration 57601 / 76500) loss: 2.308372\n",
      "(Iteration 57701 / 76500) loss: 2.308388\n",
      "(Iteration 57801 / 76500) loss: 2.308358\n",
      "(Iteration 57901 / 76500) loss: 2.308357\n",
      "(Iteration 58001 / 76500) loss: 2.308362\n",
      "(Iteration 58101 / 76500) loss: 2.308381\n",
      "(Epoch 76 / 100) train acc: 0.107000; val_acc: 0.103000\n",
      "(Iteration 58201 / 76500) loss: 2.308363\n",
      "(Iteration 58301 / 76500) loss: 2.308368\n",
      "(Iteration 58401 / 76500) loss: 2.308377\n",
      "(Iteration 58501 / 76500) loss: 2.308378\n",
      "(Iteration 58601 / 76500) loss: 2.308360\n",
      "(Iteration 58701 / 76500) loss: 2.308370\n",
      "(Iteration 58801 / 76500) loss: 2.308366\n",
      "(Iteration 58901 / 76500) loss: 2.308359\n",
      "(Epoch 77 / 100) train acc: 0.085000; val_acc: 0.103000\n",
      "(Iteration 59001 / 76500) loss: 2.308385\n",
      "(Iteration 59101 / 76500) loss: 2.308377\n",
      "(Iteration 59201 / 76500) loss: 2.308362\n",
      "(Iteration 59301 / 76500) loss: 2.308345\n",
      "(Iteration 59401 / 76500) loss: 2.308381\n",
      "(Iteration 59501 / 76500) loss: 2.308355\n",
      "(Iteration 59601 / 76500) loss: 2.308381\n",
      "(Epoch 78 / 100) train acc: 0.101000; val_acc: 0.103000\n",
      "(Iteration 59701 / 76500) loss: 2.308375\n",
      "(Iteration 59801 / 76500) loss: 2.308362\n",
      "(Iteration 59901 / 76500) loss: 2.308361\n",
      "(Iteration 60001 / 76500) loss: 2.308369\n",
      "(Iteration 60101 / 76500) loss: 2.308372\n",
      "(Iteration 60201 / 76500) loss: 2.308377\n",
      "(Iteration 60301 / 76500) loss: 2.308391\n",
      "(Iteration 60401 / 76500) loss: 2.308374\n",
      "(Epoch 79 / 100) train acc: 0.072000; val_acc: 0.103000\n",
      "(Iteration 60501 / 76500) loss: 2.308367\n",
      "(Iteration 60601 / 76500) loss: 2.308380\n",
      "(Iteration 60701 / 76500) loss: 2.308358\n",
      "(Iteration 60801 / 76500) loss: 2.308378\n",
      "(Iteration 60901 / 76500) loss: 2.308383\n",
      "(Iteration 61001 / 76500) loss: 2.308353\n",
      "(Iteration 61101 / 76500) loss: 2.308365\n",
      "(Epoch 80 / 100) train acc: 0.097000; val_acc: 0.103000\n",
      "(Iteration 61201 / 76500) loss: 2.308375\n",
      "(Iteration 61301 / 76500) loss: 2.308371\n",
      "(Iteration 61401 / 76500) loss: 2.308379\n",
      "(Iteration 61501 / 76500) loss: 2.308380\n",
      "(Iteration 61601 / 76500) loss: 2.308342\n",
      "(Iteration 61701 / 76500) loss: 2.308366\n",
      "(Iteration 61801 / 76500) loss: 2.308377\n",
      "(Iteration 61901 / 76500) loss: 2.308354\n",
      "(Epoch 81 / 100) train acc: 0.102000; val_acc: 0.103000\n",
      "(Iteration 62001 / 76500) loss: 2.308374\n",
      "(Iteration 62101 / 76500) loss: 2.308391\n",
      "(Iteration 62201 / 76500) loss: 2.308376\n",
      "(Iteration 62301 / 76500) loss: 2.308380\n",
      "(Iteration 62401 / 76500) loss: 2.308362\n",
      "(Iteration 62501 / 76500) loss: 2.308381\n",
      "(Iteration 62601 / 76500) loss: 2.308378\n",
      "(Iteration 62701 / 76500) loss: 2.308360\n",
      "(Epoch 82 / 100) train acc: 0.128000; val_acc: 0.103000\n",
      "(Iteration 62801 / 76500) loss: 2.308366\n",
      "(Iteration 62901 / 76500) loss: 2.308372\n",
      "(Iteration 63001 / 76500) loss: 2.308368\n",
      "(Iteration 63101 / 76500) loss: 2.308380\n",
      "(Iteration 63201 / 76500) loss: 2.308344\n",
      "(Iteration 63301 / 76500) loss: 2.308351\n",
      "(Iteration 63401 / 76500) loss: 2.308371\n",
      "(Epoch 83 / 100) train acc: 0.109000; val_acc: 0.103000\n",
      "(Iteration 63501 / 76500) loss: 2.308370\n",
      "(Iteration 63601 / 76500) loss: 2.308374\n",
      "(Iteration 63701 / 76500) loss: 2.308379\n",
      "(Iteration 63801 / 76500) loss: 2.308363\n",
      "(Iteration 63901 / 76500) loss: 2.308369\n",
      "(Iteration 64001 / 76500) loss: 2.308383\n",
      "(Iteration 64101 / 76500) loss: 2.308366\n",
      "(Iteration 64201 / 76500) loss: 2.308375\n",
      "(Epoch 84 / 100) train acc: 0.094000; val_acc: 0.103000\n",
      "(Iteration 64301 / 76500) loss: 2.308392\n",
      "(Iteration 64401 / 76500) loss: 2.308381\n",
      "(Iteration 64501 / 76500) loss: 2.308383\n",
      "(Iteration 64601 / 76500) loss: 2.308377\n",
      "(Iteration 64701 / 76500) loss: 2.308372\n",
      "(Iteration 64801 / 76500) loss: 2.308372\n",
      "(Iteration 64901 / 76500) loss: 2.308382\n",
      "(Iteration 65001 / 76500) loss: 2.308374\n",
      "(Epoch 85 / 100) train acc: 0.098000; val_acc: 0.103000\n",
      "(Iteration 65101 / 76500) loss: 2.308380\n",
      "(Iteration 65201 / 76500) loss: 2.308356\n",
      "(Iteration 65301 / 76500) loss: 2.308364\n",
      "(Iteration 65401 / 76500) loss: 2.308392\n",
      "(Iteration 65501 / 76500) loss: 2.308363\n",
      "(Iteration 65601 / 76500) loss: 2.308367\n",
      "(Iteration 65701 / 76500) loss: 2.308360\n",
      "(Epoch 86 / 100) train acc: 0.077000; val_acc: 0.103000\n",
      "(Iteration 65801 / 76500) loss: 2.308374\n",
      "(Iteration 65901 / 76500) loss: 2.308363\n",
      "(Iteration 66001 / 76500) loss: 2.308348\n",
      "(Iteration 66101 / 76500) loss: 2.308354\n",
      "(Iteration 66201 / 76500) loss: 2.308363\n",
      "(Iteration 66301 / 76500) loss: 2.308363\n",
      "(Iteration 66401 / 76500) loss: 2.308353\n",
      "(Iteration 66501 / 76500) loss: 2.308358\n",
      "(Epoch 87 / 100) train acc: 0.081000; val_acc: 0.103000\n",
      "(Iteration 66601 / 76500) loss: 2.308370\n",
      "(Iteration 66701 / 76500) loss: 2.308388\n",
      "(Iteration 66801 / 76500) loss: 2.308366\n",
      "(Iteration 66901 / 76500) loss: 2.308345\n",
      "(Iteration 67001 / 76500) loss: 2.308393\n",
      "(Iteration 67101 / 76500) loss: 2.308339\n",
      "(Iteration 67201 / 76500) loss: 2.308349\n",
      "(Iteration 67301 / 76500) loss: 2.308367\n",
      "(Epoch 88 / 100) train acc: 0.098000; val_acc: 0.103000\n",
      "(Iteration 67401 / 76500) loss: 2.308376\n",
      "(Iteration 67501 / 76500) loss: 2.308387\n",
      "(Iteration 67601 / 76500) loss: 2.308376\n",
      "(Iteration 67701 / 76500) loss: 2.308353\n",
      "(Iteration 67801 / 76500) loss: 2.308375\n",
      "(Iteration 67901 / 76500) loss: 2.308370\n",
      "(Iteration 68001 / 76500) loss: 2.308366\n",
      "(Epoch 89 / 100) train acc: 0.086000; val_acc: 0.103000\n",
      "(Iteration 68101 / 76500) loss: 2.308377\n",
      "(Iteration 68201 / 76500) loss: 2.308381\n",
      "(Iteration 68301 / 76500) loss: 2.308369\n",
      "(Iteration 68401 / 76500) loss: 2.308367\n",
      "(Iteration 68501 / 76500) loss: 2.308381\n",
      "(Iteration 68601 / 76500) loss: 2.308367\n",
      "(Iteration 68701 / 76500) loss: 2.308385\n",
      "(Iteration 68801 / 76500) loss: 2.308358\n",
      "(Epoch 90 / 100) train acc: 0.117000; val_acc: 0.103000\n",
      "(Iteration 68901 / 76500) loss: 2.308376\n",
      "(Iteration 69001 / 76500) loss: 2.308363\n",
      "(Iteration 69101 / 76500) loss: 2.308381\n",
      "(Iteration 69201 / 76500) loss: 2.308359\n",
      "(Iteration 69301 / 76500) loss: 2.308357\n",
      "(Iteration 69401 / 76500) loss: 2.308364\n",
      "(Iteration 69501 / 76500) loss: 2.308359\n",
      "(Iteration 69601 / 76500) loss: 2.308369\n",
      "(Epoch 91 / 100) train acc: 0.088000; val_acc: 0.103000\n",
      "(Iteration 69701 / 76500) loss: 2.308385\n",
      "(Iteration 69801 / 76500) loss: 2.308387\n",
      "(Iteration 69901 / 76500) loss: 2.308374\n",
      "(Iteration 70001 / 76500) loss: 2.308371\n",
      "(Iteration 70101 / 76500) loss: 2.308363\n",
      "(Iteration 70201 / 76500) loss: 2.308358\n",
      "(Iteration 70301 / 76500) loss: 2.308365\n",
      "(Epoch 92 / 100) train acc: 0.096000; val_acc: 0.103000\n",
      "(Iteration 70401 / 76500) loss: 2.308374\n",
      "(Iteration 70501 / 76500) loss: 2.308371\n",
      "(Iteration 70601 / 76500) loss: 2.308365\n",
      "(Iteration 70701 / 76500) loss: 2.308376\n",
      "(Iteration 70801 / 76500) loss: 2.308379\n",
      "(Iteration 70901 / 76500) loss: 2.308353\n",
      "(Iteration 71001 / 76500) loss: 2.308355\n",
      "(Iteration 71101 / 76500) loss: 2.308357\n",
      "(Epoch 93 / 100) train acc: 0.088000; val_acc: 0.103000\n",
      "(Iteration 71201 / 76500) loss: 2.308372\n",
      "(Iteration 71301 / 76500) loss: 2.308374\n",
      "(Iteration 71401 / 76500) loss: 2.308360\n",
      "(Iteration 71501 / 76500) loss: 2.308363\n",
      "(Iteration 71601 / 76500) loss: 2.308372\n",
      "(Iteration 71701 / 76500) loss: 2.308356\n",
      "(Iteration 71801 / 76500) loss: 2.308373\n",
      "(Iteration 71901 / 76500) loss: 2.308359\n",
      "(Epoch 94 / 100) train acc: 0.104000; val_acc: 0.103000\n",
      "(Iteration 72001 / 76500) loss: 2.308372\n",
      "(Iteration 72101 / 76500) loss: 2.308373\n",
      "(Iteration 72201 / 76500) loss: 2.308356\n",
      "(Iteration 72301 / 76500) loss: 2.308363\n",
      "(Iteration 72401 / 76500) loss: 2.308370\n",
      "(Iteration 72501 / 76500) loss: 2.308363\n",
      "(Iteration 72601 / 76500) loss: 2.308366\n",
      "(Epoch 95 / 100) train acc: 0.099000; val_acc: 0.103000\n",
      "(Iteration 72701 / 76500) loss: 2.308368\n",
      "(Iteration 72801 / 76500) loss: 2.308383\n",
      "(Iteration 72901 / 76500) loss: 2.308368\n",
      "(Iteration 73001 / 76500) loss: 2.308371\n",
      "(Iteration 73101 / 76500) loss: 2.308388\n",
      "(Iteration 73201 / 76500) loss: 2.308364\n",
      "(Iteration 73301 / 76500) loss: 2.308372\n",
      "(Iteration 73401 / 76500) loss: 2.308361\n",
      "(Epoch 96 / 100) train acc: 0.087000; val_acc: 0.103000\n",
      "(Iteration 73501 / 76500) loss: 2.308352\n",
      "(Iteration 73601 / 76500) loss: 2.308378\n",
      "(Iteration 73701 / 76500) loss: 2.308374\n",
      "(Iteration 73801 / 76500) loss: 2.308350\n",
      "(Iteration 73901 / 76500) loss: 2.308366\n",
      "(Iteration 74001 / 76500) loss: 2.308375\n",
      "(Iteration 74101 / 76500) loss: 2.308385\n",
      "(Iteration 74201 / 76500) loss: 2.308381\n",
      "(Epoch 97 / 100) train acc: 0.088000; val_acc: 0.103000\n",
      "(Iteration 74301 / 76500) loss: 2.308353\n",
      "(Iteration 74401 / 76500) loss: 2.308360\n",
      "(Iteration 74501 / 76500) loss: 2.308366\n",
      "(Iteration 74601 / 76500) loss: 2.308356\n",
      "(Iteration 74701 / 76500) loss: 2.308358\n",
      "(Iteration 74801 / 76500) loss: 2.308370\n",
      "(Iteration 74901 / 76500) loss: 2.308375\n",
      "(Epoch 98 / 100) train acc: 0.108000; val_acc: 0.103000\n",
      "(Iteration 75001 / 76500) loss: 2.308363\n",
      "(Iteration 75101 / 76500) loss: 2.308380\n",
      "(Iteration 75201 / 76500) loss: 2.308364\n",
      "(Iteration 75301 / 76500) loss: 2.308363\n",
      "(Iteration 75401 / 76500) loss: 2.308382\n",
      "(Iteration 75501 / 76500) loss: 2.308342\n",
      "(Iteration 75601 / 76500) loss: 2.308365\n",
      "(Iteration 75701 / 76500) loss: 2.308351\n",
      "(Epoch 99 / 100) train acc: 0.078000; val_acc: 0.103000\n",
      "(Iteration 75801 / 76500) loss: 2.308351\n",
      "(Iteration 75901 / 76500) loss: 2.308366\n",
      "(Iteration 76001 / 76500) loss: 2.308377\n",
      "(Iteration 76101 / 76500) loss: 2.308371\n",
      "(Iteration 76201 / 76500) loss: 2.308384\n",
      "(Iteration 76301 / 76500) loss: 2.308361\n",
      "(Iteration 76401 / 76500) loss: 2.308377\n",
      "(Epoch 100 / 100) train acc: 0.088000; val_acc: 0.103000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 1e-07, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.308201\n",
      "(Epoch 0 / 100) train acc: 0.115000; val_acc: 0.128000\n",
      "(Iteration 101 / 38200) loss: 2.308214\n",
      "(Iteration 201 / 38200) loss: 2.308201\n",
      "(Iteration 301 / 38200) loss: 2.308196\n",
      "(Epoch 1 / 100) train acc: 0.136000; val_acc: 0.128000\n",
      "(Iteration 401 / 38200) loss: 2.308218\n",
      "(Iteration 501 / 38200) loss: 2.308198\n",
      "(Iteration 601 / 38200) loss: 2.308202\n",
      "(Iteration 701 / 38200) loss: 2.308207\n",
      "(Epoch 2 / 100) train acc: 0.117000; val_acc: 0.128000\n",
      "(Iteration 801 / 38200) loss: 2.308202\n",
      "(Iteration 901 / 38200) loss: 2.308212\n",
      "(Iteration 1001 / 38200) loss: 2.308200\n",
      "(Iteration 1101 / 38200) loss: 2.308220\n",
      "(Epoch 3 / 100) train acc: 0.128000; val_acc: 0.128000\n",
      "(Iteration 1201 / 38200) loss: 2.308210\n",
      "(Iteration 1301 / 38200) loss: 2.308195\n",
      "(Iteration 1401 / 38200) loss: 2.308216\n",
      "(Iteration 1501 / 38200) loss: 2.308216\n",
      "(Epoch 4 / 100) train acc: 0.116000; val_acc: 0.128000\n",
      "(Iteration 1601 / 38200) loss: 2.308224\n",
      "(Iteration 1701 / 38200) loss: 2.308212\n",
      "(Iteration 1801 / 38200) loss: 2.308200\n",
      "(Iteration 1901 / 38200) loss: 2.308195\n",
      "(Epoch 5 / 100) train acc: 0.122000; val_acc: 0.128000\n",
      "(Iteration 2001 / 38200) loss: 2.308202\n",
      "(Iteration 2101 / 38200) loss: 2.308211\n",
      "(Iteration 2201 / 38200) loss: 2.308185\n",
      "(Epoch 6 / 100) train acc: 0.112000; val_acc: 0.128000\n",
      "(Iteration 2301 / 38200) loss: 2.308187\n",
      "(Iteration 2401 / 38200) loss: 2.308202\n",
      "(Iteration 2501 / 38200) loss: 2.308194\n",
      "(Iteration 2601 / 38200) loss: 2.308206\n",
      "(Epoch 7 / 100) train acc: 0.114000; val_acc: 0.128000\n",
      "(Iteration 2701 / 38200) loss: 2.308199\n",
      "(Iteration 2801 / 38200) loss: 2.308208\n",
      "(Iteration 2901 / 38200) loss: 2.308210\n",
      "(Iteration 3001 / 38200) loss: 2.308202\n",
      "(Epoch 8 / 100) train acc: 0.121000; val_acc: 0.128000\n",
      "(Iteration 3101 / 38200) loss: 2.308219\n",
      "(Iteration 3201 / 38200) loss: 2.308207\n",
      "(Iteration 3301 / 38200) loss: 2.308211\n",
      "(Iteration 3401 / 38200) loss: 2.308210\n",
      "(Epoch 9 / 100) train acc: 0.129000; val_acc: 0.128000\n",
      "(Iteration 3501 / 38200) loss: 2.308207\n",
      "(Iteration 3601 / 38200) loss: 2.308211\n",
      "(Iteration 3701 / 38200) loss: 2.308211\n",
      "(Iteration 3801 / 38200) loss: 2.308201\n",
      "(Epoch 10 / 100) train acc: 0.119000; val_acc: 0.128000\n",
      "(Iteration 3901 / 38200) loss: 2.308202\n",
      "(Iteration 4001 / 38200) loss: 2.308212\n",
      "(Iteration 4101 / 38200) loss: 2.308204\n",
      "(Iteration 4201 / 38200) loss: 2.308204\n",
      "(Epoch 11 / 100) train acc: 0.126000; val_acc: 0.128000\n",
      "(Iteration 4301 / 38200) loss: 2.308197\n",
      "(Iteration 4401 / 38200) loss: 2.308201\n",
      "(Iteration 4501 / 38200) loss: 2.308204\n",
      "(Epoch 12 / 100) train acc: 0.149000; val_acc: 0.128000\n",
      "(Iteration 4601 / 38200) loss: 2.308204\n",
      "(Iteration 4701 / 38200) loss: 2.308200\n",
      "(Iteration 4801 / 38200) loss: 2.308202\n",
      "(Iteration 4901 / 38200) loss: 2.308196\n",
      "(Epoch 13 / 100) train acc: 0.117000; val_acc: 0.128000\n",
      "(Iteration 5001 / 38200) loss: 2.308187\n",
      "(Iteration 5101 / 38200) loss: 2.308195\n",
      "(Iteration 5201 / 38200) loss: 2.308196\n",
      "(Iteration 5301 / 38200) loss: 2.308197\n",
      "(Epoch 14 / 100) train acc: 0.119000; val_acc: 0.128000\n",
      "(Iteration 5401 / 38200) loss: 2.308196\n",
      "(Iteration 5501 / 38200) loss: 2.308191\n",
      "(Iteration 5601 / 38200) loss: 2.308204\n",
      "(Iteration 5701 / 38200) loss: 2.308207\n",
      "(Epoch 15 / 100) train acc: 0.121000; val_acc: 0.128000\n",
      "(Iteration 5801 / 38200) loss: 2.308177\n",
      "(Iteration 5901 / 38200) loss: 2.308205\n",
      "(Iteration 6001 / 38200) loss: 2.308204\n",
      "(Iteration 6101 / 38200) loss: 2.308194\n",
      "(Epoch 16 / 100) train acc: 0.131000; val_acc: 0.128000\n",
      "(Iteration 6201 / 38200) loss: 2.308203\n",
      "(Iteration 6301 / 38200) loss: 2.308198\n",
      "(Iteration 6401 / 38200) loss: 2.308191\n",
      "(Epoch 17 / 100) train acc: 0.113000; val_acc: 0.128000\n",
      "(Iteration 6501 / 38200) loss: 2.308208\n",
      "(Iteration 6601 / 38200) loss: 2.308209\n",
      "(Iteration 6701 / 38200) loss: 2.308190\n",
      "(Iteration 6801 / 38200) loss: 2.308194\n",
      "(Epoch 18 / 100) train acc: 0.119000; val_acc: 0.128000\n",
      "(Iteration 6901 / 38200) loss: 2.308205\n",
      "(Iteration 7001 / 38200) loss: 2.308216\n",
      "(Iteration 7101 / 38200) loss: 2.308194\n",
      "(Iteration 7201 / 38200) loss: 2.308202\n",
      "(Epoch 19 / 100) train acc: 0.118000; val_acc: 0.128000\n",
      "(Iteration 7301 / 38200) loss: 2.308207\n",
      "(Iteration 7401 / 38200) loss: 2.308209\n",
      "(Iteration 7501 / 38200) loss: 2.308205\n",
      "(Iteration 7601 / 38200) loss: 2.308205\n",
      "(Epoch 20 / 100) train acc: 0.125000; val_acc: 0.128000\n",
      "(Iteration 7701 / 38200) loss: 2.308196\n",
      "(Iteration 7801 / 38200) loss: 2.308173\n",
      "(Iteration 7901 / 38200) loss: 2.308194\n",
      "(Iteration 8001 / 38200) loss: 2.308195\n",
      "(Epoch 21 / 100) train acc: 0.110000; val_acc: 0.129000\n",
      "(Iteration 8101 / 38200) loss: 2.308211\n",
      "(Iteration 8201 / 38200) loss: 2.308201\n",
      "(Iteration 8301 / 38200) loss: 2.308217\n",
      "(Iteration 8401 / 38200) loss: 2.308198\n",
      "(Epoch 22 / 100) train acc: 0.131000; val_acc: 0.129000\n",
      "(Iteration 8501 / 38200) loss: 2.308198\n",
      "(Iteration 8601 / 38200) loss: 2.308202\n",
      "(Iteration 8701 / 38200) loss: 2.308200\n",
      "(Epoch 23 / 100) train acc: 0.112000; val_acc: 0.129000\n",
      "(Iteration 8801 / 38200) loss: 2.308214\n",
      "(Iteration 8901 / 38200) loss: 2.308203\n",
      "(Iteration 9001 / 38200) loss: 2.308198\n",
      "(Iteration 9101 / 38200) loss: 2.308206\n",
      "(Epoch 24 / 100) train acc: 0.113000; val_acc: 0.129000\n",
      "(Iteration 9201 / 38200) loss: 2.308195\n",
      "(Iteration 9301 / 38200) loss: 2.308184\n",
      "(Iteration 9401 / 38200) loss: 2.308210\n",
      "(Iteration 9501 / 38200) loss: 2.308187\n",
      "(Epoch 25 / 100) train acc: 0.129000; val_acc: 0.129000\n",
      "(Iteration 9601 / 38200) loss: 2.308193\n",
      "(Iteration 9701 / 38200) loss: 2.308191\n",
      "(Iteration 9801 / 38200) loss: 2.308181\n",
      "(Iteration 9901 / 38200) loss: 2.308203\n",
      "(Epoch 26 / 100) train acc: 0.109000; val_acc: 0.129000\n",
      "(Iteration 10001 / 38200) loss: 2.308186\n",
      "(Iteration 10101 / 38200) loss: 2.308201\n",
      "(Iteration 10201 / 38200) loss: 2.308197\n",
      "(Iteration 10301 / 38200) loss: 2.308194\n",
      "(Epoch 27 / 100) train acc: 0.118000; val_acc: 0.129000\n",
      "(Iteration 10401 / 38200) loss: 2.308193\n",
      "(Iteration 10501 / 38200) loss: 2.308205\n",
      "(Iteration 10601 / 38200) loss: 2.308201\n",
      "(Epoch 28 / 100) train acc: 0.112000; val_acc: 0.128000\n",
      "(Iteration 10701 / 38200) loss: 2.308209\n",
      "(Iteration 10801 / 38200) loss: 2.308214\n",
      "(Iteration 10901 / 38200) loss: 2.308190\n",
      "(Iteration 11001 / 38200) loss: 2.308200\n",
      "(Epoch 29 / 100) train acc: 0.133000; val_acc: 0.129000\n",
      "(Iteration 11101 / 38200) loss: 2.308203\n",
      "(Iteration 11201 / 38200) loss: 2.308200\n",
      "(Iteration 11301 / 38200) loss: 2.308189\n",
      "(Iteration 11401 / 38200) loss: 2.308199\n",
      "(Epoch 30 / 100) train acc: 0.126000; val_acc: 0.128000\n",
      "(Iteration 11501 / 38200) loss: 2.308210\n",
      "(Iteration 11601 / 38200) loss: 2.308203\n",
      "(Iteration 11701 / 38200) loss: 2.308201\n",
      "(Iteration 11801 / 38200) loss: 2.308194\n",
      "(Epoch 31 / 100) train acc: 0.128000; val_acc: 0.128000\n",
      "(Iteration 11901 / 38200) loss: 2.308197\n",
      "(Iteration 12001 / 38200) loss: 2.308206\n",
      "(Iteration 12101 / 38200) loss: 2.308200\n",
      "(Iteration 12201 / 38200) loss: 2.308196\n",
      "(Epoch 32 / 100) train acc: 0.123000; val_acc: 0.128000\n",
      "(Iteration 12301 / 38200) loss: 2.308198\n",
      "(Iteration 12401 / 38200) loss: 2.308191\n",
      "(Iteration 12501 / 38200) loss: 2.308203\n",
      "(Iteration 12601 / 38200) loss: 2.308201\n",
      "(Epoch 33 / 100) train acc: 0.105000; val_acc: 0.128000\n",
      "(Iteration 12701 / 38200) loss: 2.308198\n",
      "(Iteration 12801 / 38200) loss: 2.308210\n",
      "(Iteration 12901 / 38200) loss: 2.308194\n",
      "(Epoch 34 / 100) train acc: 0.129000; val_acc: 0.128000\n",
      "(Iteration 13001 / 38200) loss: 2.308200\n",
      "(Iteration 13101 / 38200) loss: 2.308184\n",
      "(Iteration 13201 / 38200) loss: 2.308210\n",
      "(Iteration 13301 / 38200) loss: 2.308199\n",
      "(Epoch 35 / 100) train acc: 0.120000; val_acc: 0.128000\n",
      "(Iteration 13401 / 38200) loss: 2.308210\n",
      "(Iteration 13501 / 38200) loss: 2.308210\n",
      "(Iteration 13601 / 38200) loss: 2.308194\n",
      "(Iteration 13701 / 38200) loss: 2.308201\n",
      "(Epoch 36 / 100) train acc: 0.118000; val_acc: 0.128000\n",
      "(Iteration 13801 / 38200) loss: 2.308197\n",
      "(Iteration 13901 / 38200) loss: 2.308210\n",
      "(Iteration 14001 / 38200) loss: 2.308211\n",
      "(Iteration 14101 / 38200) loss: 2.308210\n",
      "(Epoch 37 / 100) train acc: 0.139000; val_acc: 0.128000\n",
      "(Iteration 14201 / 38200) loss: 2.308205\n",
      "(Iteration 14301 / 38200) loss: 2.308213\n",
      "(Iteration 14401 / 38200) loss: 2.308194\n",
      "(Iteration 14501 / 38200) loss: 2.308212\n",
      "(Epoch 38 / 100) train acc: 0.112000; val_acc: 0.128000\n",
      "(Iteration 14601 / 38200) loss: 2.308200\n",
      "(Iteration 14701 / 38200) loss: 2.308185\n",
      "(Iteration 14801 / 38200) loss: 2.308218\n",
      "(Epoch 39 / 100) train acc: 0.119000; val_acc: 0.129000\n",
      "(Iteration 14901 / 38200) loss: 2.308200\n",
      "(Iteration 15001 / 38200) loss: 2.308200\n",
      "(Iteration 15101 / 38200) loss: 2.308191\n",
      "(Iteration 15201 / 38200) loss: 2.308185\n",
      "(Epoch 40 / 100) train acc: 0.116000; val_acc: 0.129000\n",
      "(Iteration 15301 / 38200) loss: 2.308192\n",
      "(Iteration 15401 / 38200) loss: 2.308184\n",
      "(Iteration 15501 / 38200) loss: 2.308191\n",
      "(Iteration 15601 / 38200) loss: 2.308190\n",
      "(Epoch 41 / 100) train acc: 0.133000; val_acc: 0.129000\n",
      "(Iteration 15701 / 38200) loss: 2.308200\n",
      "(Iteration 15801 / 38200) loss: 2.308188\n",
      "(Iteration 15901 / 38200) loss: 2.308202\n",
      "(Iteration 16001 / 38200) loss: 2.308192\n",
      "(Epoch 42 / 100) train acc: 0.131000; val_acc: 0.129000\n",
      "(Iteration 16101 / 38200) loss: 2.308185\n",
      "(Iteration 16201 / 38200) loss: 2.308201\n",
      "(Iteration 16301 / 38200) loss: 2.308204\n",
      "(Iteration 16401 / 38200) loss: 2.308196\n",
      "(Epoch 43 / 100) train acc: 0.105000; val_acc: 0.129000\n",
      "(Iteration 16501 / 38200) loss: 2.308185\n",
      "(Iteration 16601 / 38200) loss: 2.308198\n",
      "(Iteration 16701 / 38200) loss: 2.308181\n",
      "(Iteration 16801 / 38200) loss: 2.308202\n",
      "(Epoch 44 / 100) train acc: 0.130000; val_acc: 0.129000\n",
      "(Iteration 16901 / 38200) loss: 2.308196\n",
      "(Iteration 17001 / 38200) loss: 2.308191\n",
      "(Iteration 17101 / 38200) loss: 2.308203\n",
      "(Epoch 45 / 100) train acc: 0.134000; val_acc: 0.129000\n",
      "(Iteration 17201 / 38200) loss: 2.308197\n",
      "(Iteration 17301 / 38200) loss: 2.308208\n",
      "(Iteration 17401 / 38200) loss: 2.308213\n",
      "(Iteration 17501 / 38200) loss: 2.308208\n",
      "(Epoch 46 / 100) train acc: 0.138000; val_acc: 0.129000\n",
      "(Iteration 17601 / 38200) loss: 2.308201\n",
      "(Iteration 17701 / 38200) loss: 2.308198\n",
      "(Iteration 17801 / 38200) loss: 2.308207\n",
      "(Iteration 17901 / 38200) loss: 2.308185\n",
      "(Epoch 47 / 100) train acc: 0.110000; val_acc: 0.129000\n",
      "(Iteration 18001 / 38200) loss: 2.308198\n",
      "(Iteration 18101 / 38200) loss: 2.308202\n",
      "(Iteration 18201 / 38200) loss: 2.308196\n",
      "(Iteration 18301 / 38200) loss: 2.308215\n",
      "(Epoch 48 / 100) train acc: 0.108000; val_acc: 0.129000\n",
      "(Iteration 18401 / 38200) loss: 2.308208\n",
      "(Iteration 18501 / 38200) loss: 2.308206\n",
      "(Iteration 18601 / 38200) loss: 2.308212\n",
      "(Iteration 18701 / 38200) loss: 2.308189\n",
      "(Epoch 49 / 100) train acc: 0.112000; val_acc: 0.129000\n",
      "(Iteration 18801 / 38200) loss: 2.308199\n",
      "(Iteration 18901 / 38200) loss: 2.308183\n",
      "(Iteration 19001 / 38200) loss: 2.308214\n",
      "(Epoch 50 / 100) train acc: 0.120000; val_acc: 0.129000\n",
      "(Iteration 19101 / 38200) loss: 2.308210\n",
      "(Iteration 19201 / 38200) loss: 2.308210\n",
      "(Iteration 19301 / 38200) loss: 2.308179\n",
      "(Iteration 19401 / 38200) loss: 2.308200\n",
      "(Epoch 51 / 100) train acc: 0.113000; val_acc: 0.129000\n",
      "(Iteration 19501 / 38200) loss: 2.308203\n",
      "(Iteration 19601 / 38200) loss: 2.308200\n",
      "(Iteration 19701 / 38200) loss: 2.308225\n",
      "(Iteration 19801 / 38200) loss: 2.308205\n",
      "(Epoch 52 / 100) train acc: 0.114000; val_acc: 0.129000\n",
      "(Iteration 19901 / 38200) loss: 2.308193\n",
      "(Iteration 20001 / 38200) loss: 2.308198\n",
      "(Iteration 20101 / 38200) loss: 2.308200\n",
      "(Iteration 20201 / 38200) loss: 2.308196\n",
      "(Epoch 53 / 100) train acc: 0.116000; val_acc: 0.129000\n",
      "(Iteration 20301 / 38200) loss: 2.308215\n",
      "(Iteration 20401 / 38200) loss: 2.308184\n",
      "(Iteration 20501 / 38200) loss: 2.308204\n",
      "(Iteration 20601 / 38200) loss: 2.308207\n",
      "(Epoch 54 / 100) train acc: 0.123000; val_acc: 0.129000\n",
      "(Iteration 20701 / 38200) loss: 2.308199\n",
      "(Iteration 20801 / 38200) loss: 2.308207\n",
      "(Iteration 20901 / 38200) loss: 2.308201\n",
      "(Iteration 21001 / 38200) loss: 2.308208\n",
      "(Epoch 55 / 100) train acc: 0.117000; val_acc: 0.129000\n",
      "(Iteration 21101 / 38200) loss: 2.308203\n",
      "(Iteration 21201 / 38200) loss: 2.308200\n",
      "(Iteration 21301 / 38200) loss: 2.308199\n",
      "(Epoch 56 / 100) train acc: 0.120000; val_acc: 0.129000\n",
      "(Iteration 21401 / 38200) loss: 2.308195\n",
      "(Iteration 21501 / 38200) loss: 2.308205\n",
      "(Iteration 21601 / 38200) loss: 2.308203\n",
      "(Iteration 21701 / 38200) loss: 2.308186\n",
      "(Epoch 57 / 100) train acc: 0.133000; val_acc: 0.129000\n",
      "(Iteration 21801 / 38200) loss: 2.308205\n",
      "(Iteration 21901 / 38200) loss: 2.308198\n",
      "(Iteration 22001 / 38200) loss: 2.308207\n",
      "(Iteration 22101 / 38200) loss: 2.308202\n",
      "(Epoch 58 / 100) train acc: 0.138000; val_acc: 0.129000\n",
      "(Iteration 22201 / 38200) loss: 2.308200\n",
      "(Iteration 22301 / 38200) loss: 2.308184\n",
      "(Iteration 22401 / 38200) loss: 2.308190\n",
      "(Iteration 22501 / 38200) loss: 2.308205\n",
      "(Epoch 59 / 100) train acc: 0.123000; val_acc: 0.129000\n",
      "(Iteration 22601 / 38200) loss: 2.308202\n",
      "(Iteration 22701 / 38200) loss: 2.308189\n",
      "(Iteration 22801 / 38200) loss: 2.308196\n",
      "(Iteration 22901 / 38200) loss: 2.308174\n",
      "(Epoch 60 / 100) train acc: 0.128000; val_acc: 0.129000\n",
      "(Iteration 23001 / 38200) loss: 2.308190\n",
      "(Iteration 23101 / 38200) loss: 2.308192\n",
      "(Iteration 23201 / 38200) loss: 2.308196\n",
      "(Iteration 23301 / 38200) loss: 2.308194\n",
      "(Epoch 61 / 100) train acc: 0.114000; val_acc: 0.129000\n",
      "(Iteration 23401 / 38200) loss: 2.308184\n",
      "(Iteration 23501 / 38200) loss: 2.308216\n",
      "(Iteration 23601 / 38200) loss: 2.308202\n",
      "(Epoch 62 / 100) train acc: 0.123000; val_acc: 0.129000\n",
      "(Iteration 23701 / 38200) loss: 2.308193\n",
      "(Iteration 23801 / 38200) loss: 2.308202\n",
      "(Iteration 23901 / 38200) loss: 2.308204\n",
      "(Iteration 24001 / 38200) loss: 2.308211\n",
      "(Epoch 63 / 100) train acc: 0.133000; val_acc: 0.129000\n",
      "(Iteration 24101 / 38200) loss: 2.308189\n",
      "(Iteration 24201 / 38200) loss: 2.308190\n",
      "(Iteration 24301 / 38200) loss: 2.308189\n",
      "(Iteration 24401 / 38200) loss: 2.308209\n",
      "(Epoch 64 / 100) train acc: 0.121000; val_acc: 0.129000\n",
      "(Iteration 24501 / 38200) loss: 2.308207\n",
      "(Iteration 24601 / 38200) loss: 2.308218\n",
      "(Iteration 24701 / 38200) loss: 2.308187\n",
      "(Iteration 24801 / 38200) loss: 2.308205\n",
      "(Epoch 65 / 100) train acc: 0.107000; val_acc: 0.129000\n",
      "(Iteration 24901 / 38200) loss: 2.308196\n",
      "(Iteration 25001 / 38200) loss: 2.308204\n",
      "(Iteration 25101 / 38200) loss: 2.308211\n",
      "(Iteration 25201 / 38200) loss: 2.308200\n",
      "(Epoch 66 / 100) train acc: 0.106000; val_acc: 0.129000\n",
      "(Iteration 25301 / 38200) loss: 2.308196\n",
      "(Iteration 25401 / 38200) loss: 2.308197\n",
      "(Iteration 25501 / 38200) loss: 2.308211\n",
      "(Epoch 67 / 100) train acc: 0.138000; val_acc: 0.129000\n",
      "(Iteration 25601 / 38200) loss: 2.308204\n",
      "(Iteration 25701 / 38200) loss: 2.308203\n",
      "(Iteration 25801 / 38200) loss: 2.308198\n",
      "(Iteration 25901 / 38200) loss: 2.308179\n",
      "(Epoch 68 / 100) train acc: 0.129000; val_acc: 0.129000\n",
      "(Iteration 26001 / 38200) loss: 2.308189\n",
      "(Iteration 26101 / 38200) loss: 2.308198\n",
      "(Iteration 26201 / 38200) loss: 2.308194\n",
      "(Iteration 26301 / 38200) loss: 2.308212\n",
      "(Epoch 69 / 100) train acc: 0.120000; val_acc: 0.129000\n",
      "(Iteration 26401 / 38200) loss: 2.308213\n",
      "(Iteration 26501 / 38200) loss: 2.308212\n",
      "(Iteration 26601 / 38200) loss: 2.308185\n",
      "(Iteration 26701 / 38200) loss: 2.308200\n",
      "(Epoch 70 / 100) train acc: 0.129000; val_acc: 0.129000\n",
      "(Iteration 26801 / 38200) loss: 2.308203\n",
      "(Iteration 26901 / 38200) loss: 2.308206\n",
      "(Iteration 27001 / 38200) loss: 2.308188\n",
      "(Iteration 27101 / 38200) loss: 2.308195\n",
      "(Epoch 71 / 100) train acc: 0.110000; val_acc: 0.129000\n",
      "(Iteration 27201 / 38200) loss: 2.308198\n",
      "(Iteration 27301 / 38200) loss: 2.308195\n",
      "(Iteration 27401 / 38200) loss: 2.308203\n",
      "(Iteration 27501 / 38200) loss: 2.308213\n",
      "(Epoch 72 / 100) train acc: 0.103000; val_acc: 0.129000\n",
      "(Iteration 27601 / 38200) loss: 2.308195\n",
      "(Iteration 27701 / 38200) loss: 2.308200\n",
      "(Iteration 27801 / 38200) loss: 2.308194\n",
      "(Epoch 73 / 100) train acc: 0.122000; val_acc: 0.129000\n",
      "(Iteration 27901 / 38200) loss: 2.308203\n",
      "(Iteration 28001 / 38200) loss: 2.308198\n",
      "(Iteration 28101 / 38200) loss: 2.308210\n",
      "(Iteration 28201 / 38200) loss: 2.308183\n",
      "(Epoch 74 / 100) train acc: 0.126000; val_acc: 0.129000\n",
      "(Iteration 28301 / 38200) loss: 2.308195\n",
      "(Iteration 28401 / 38200) loss: 2.308202\n",
      "(Iteration 28501 / 38200) loss: 2.308201\n",
      "(Iteration 28601 / 38200) loss: 2.308205\n",
      "(Epoch 75 / 100) train acc: 0.120000; val_acc: 0.129000\n",
      "(Iteration 28701 / 38200) loss: 2.308199\n",
      "(Iteration 28801 / 38200) loss: 2.308196\n",
      "(Iteration 28901 / 38200) loss: 2.308216\n",
      "(Iteration 29001 / 38200) loss: 2.308188\n",
      "(Epoch 76 / 100) train acc: 0.126000; val_acc: 0.129000\n",
      "(Iteration 29101 / 38200) loss: 2.308205\n",
      "(Iteration 29201 / 38200) loss: 2.308207\n",
      "(Iteration 29301 / 38200) loss: 2.308197\n",
      "(Iteration 29401 / 38200) loss: 2.308204\n",
      "(Epoch 77 / 100) train acc: 0.117000; val_acc: 0.129000\n",
      "(Iteration 29501 / 38200) loss: 2.308202\n",
      "(Iteration 29601 / 38200) loss: 2.308192\n",
      "(Iteration 29701 / 38200) loss: 2.308207\n",
      "(Epoch 78 / 100) train acc: 0.111000; val_acc: 0.129000\n",
      "(Iteration 29801 / 38200) loss: 2.308194\n",
      "(Iteration 29901 / 38200) loss: 2.308204\n",
      "(Iteration 30001 / 38200) loss: 2.308216\n",
      "(Iteration 30101 / 38200) loss: 2.308201\n",
      "(Epoch 79 / 100) train acc: 0.125000; val_acc: 0.129000\n",
      "(Iteration 30201 / 38200) loss: 2.308207\n",
      "(Iteration 30301 / 38200) loss: 2.308195\n",
      "(Iteration 30401 / 38200) loss: 2.308197\n",
      "(Iteration 30501 / 38200) loss: 2.308189\n",
      "(Epoch 80 / 100) train acc: 0.125000; val_acc: 0.129000\n",
      "(Iteration 30601 / 38200) loss: 2.308206\n",
      "(Iteration 30701 / 38200) loss: 2.308203\n",
      "(Iteration 30801 / 38200) loss: 2.308204\n",
      "(Iteration 30901 / 38200) loss: 2.308208\n",
      "(Epoch 81 / 100) train acc: 0.110000; val_acc: 0.129000\n",
      "(Iteration 31001 / 38200) loss: 2.308203\n",
      "(Iteration 31101 / 38200) loss: 2.308192\n",
      "(Iteration 31201 / 38200) loss: 2.308194\n",
      "(Iteration 31301 / 38200) loss: 2.308184\n",
      "(Epoch 82 / 100) train acc: 0.111000; val_acc: 0.129000\n",
      "(Iteration 31401 / 38200) loss: 2.308207\n",
      "(Iteration 31501 / 38200) loss: 2.308199\n",
      "(Iteration 31601 / 38200) loss: 2.308188\n",
      "(Iteration 31701 / 38200) loss: 2.308207\n",
      "(Epoch 83 / 100) train acc: 0.124000; val_acc: 0.129000\n",
      "(Iteration 31801 / 38200) loss: 2.308190\n",
      "(Iteration 31901 / 38200) loss: 2.308200\n",
      "(Iteration 32001 / 38200) loss: 2.308191\n",
      "(Epoch 84 / 100) train acc: 0.113000; val_acc: 0.129000\n",
      "(Iteration 32101 / 38200) loss: 2.308204\n",
      "(Iteration 32201 / 38200) loss: 2.308201\n",
      "(Iteration 32301 / 38200) loss: 2.308209\n",
      "(Iteration 32401 / 38200) loss: 2.308209\n",
      "(Epoch 85 / 100) train acc: 0.118000; val_acc: 0.129000\n",
      "(Iteration 32501 / 38200) loss: 2.308201\n",
      "(Iteration 32601 / 38200) loss: 2.308188\n",
      "(Iteration 32701 / 38200) loss: 2.308192\n",
      "(Iteration 32801 / 38200) loss: 2.308196\n",
      "(Epoch 86 / 100) train acc: 0.108000; val_acc: 0.129000\n",
      "(Iteration 32901 / 38200) loss: 2.308184\n",
      "(Iteration 33001 / 38200) loss: 2.308199\n",
      "(Iteration 33101 / 38200) loss: 2.308196\n",
      "(Iteration 33201 / 38200) loss: 2.308202\n",
      "(Epoch 87 / 100) train acc: 0.125000; val_acc: 0.129000\n",
      "(Iteration 33301 / 38200) loss: 2.308199\n",
      "(Iteration 33401 / 38200) loss: 2.308210\n",
      "(Iteration 33501 / 38200) loss: 2.308195\n",
      "(Iteration 33601 / 38200) loss: 2.308188\n",
      "(Epoch 88 / 100) train acc: 0.126000; val_acc: 0.129000\n",
      "(Iteration 33701 / 38200) loss: 2.308201\n",
      "(Iteration 33801 / 38200) loss: 2.308211\n",
      "(Iteration 33901 / 38200) loss: 2.308185\n",
      "(Epoch 89 / 100) train acc: 0.123000; val_acc: 0.129000\n",
      "(Iteration 34001 / 38200) loss: 2.308196\n",
      "(Iteration 34101 / 38200) loss: 2.308196\n",
      "(Iteration 34201 / 38200) loss: 2.308202\n",
      "(Iteration 34301 / 38200) loss: 2.308204\n",
      "(Epoch 90 / 100) train acc: 0.101000; val_acc: 0.129000\n",
      "(Iteration 34401 / 38200) loss: 2.308199\n",
      "(Iteration 34501 / 38200) loss: 2.308201\n",
      "(Iteration 34601 / 38200) loss: 2.308196\n",
      "(Iteration 34701 / 38200) loss: 2.308200\n",
      "(Epoch 91 / 100) train acc: 0.120000; val_acc: 0.129000\n",
      "(Iteration 34801 / 38200) loss: 2.308203\n",
      "(Iteration 34901 / 38200) loss: 2.308195\n",
      "(Iteration 35001 / 38200) loss: 2.308208\n",
      "(Iteration 35101 / 38200) loss: 2.308194\n",
      "(Epoch 92 / 100) train acc: 0.130000; val_acc: 0.129000\n",
      "(Iteration 35201 / 38200) loss: 2.308204\n",
      "(Iteration 35301 / 38200) loss: 2.308184\n",
      "(Iteration 35401 / 38200) loss: 2.308204\n",
      "(Iteration 35501 / 38200) loss: 2.308196\n",
      "(Epoch 93 / 100) train acc: 0.102000; val_acc: 0.129000\n",
      "(Iteration 35601 / 38200) loss: 2.308188\n",
      "(Iteration 35701 / 38200) loss: 2.308201\n",
      "(Iteration 35801 / 38200) loss: 2.308218\n",
      "(Iteration 35901 / 38200) loss: 2.308200\n",
      "(Epoch 94 / 100) train acc: 0.126000; val_acc: 0.129000\n",
      "(Iteration 36001 / 38200) loss: 2.308192\n",
      "(Iteration 36101 / 38200) loss: 2.308209\n",
      "(Iteration 36201 / 38200) loss: 2.308198\n",
      "(Epoch 95 / 100) train acc: 0.141000; val_acc: 0.129000\n",
      "(Iteration 36301 / 38200) loss: 2.308207\n",
      "(Iteration 36401 / 38200) loss: 2.308196\n",
      "(Iteration 36501 / 38200) loss: 2.308194\n",
      "(Iteration 36601 / 38200) loss: 2.308184\n",
      "(Epoch 96 / 100) train acc: 0.113000; val_acc: 0.129000\n",
      "(Iteration 36701 / 38200) loss: 2.308196\n",
      "(Iteration 36801 / 38200) loss: 2.308202\n",
      "(Iteration 36901 / 38200) loss: 2.308188\n",
      "(Iteration 37001 / 38200) loss: 2.308198\n",
      "(Epoch 97 / 100) train acc: 0.125000; val_acc: 0.129000\n",
      "(Iteration 37101 / 38200) loss: 2.308199\n",
      "(Iteration 37201 / 38200) loss: 2.308211\n",
      "(Iteration 37301 / 38200) loss: 2.308217\n",
      "(Iteration 37401 / 38200) loss: 2.308197\n",
      "(Epoch 98 / 100) train acc: 0.115000; val_acc: 0.129000\n",
      "(Iteration 37501 / 38200) loss: 2.308205\n",
      "(Iteration 37601 / 38200) loss: 2.308205\n",
      "(Iteration 37701 / 38200) loss: 2.308192\n",
      "(Iteration 37801 / 38200) loss: 2.308198\n",
      "(Epoch 99 / 100) train acc: 0.119000; val_acc: 0.129000\n",
      "(Iteration 37901 / 38200) loss: 2.308204\n",
      "(Iteration 38001 / 38200) loss: 2.308194\n",
      "(Iteration 38101 / 38200) loss: 2.308194\n",
      "(Epoch 100 / 100) train acc: 0.126000; val_acc: 0.129000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.306673\n",
      "(Epoch 0 / 80) train acc: 0.117000; val_acc: 0.124000\n",
      "(Iteration 101 / 61200) loss: 2.306650\n",
      "(Iteration 201 / 61200) loss: 2.306604\n",
      "(Iteration 301 / 61200) loss: 2.306545\n",
      "(Iteration 401 / 61200) loss: 2.306535\n",
      "(Iteration 501 / 61200) loss: 2.306477\n",
      "(Iteration 601 / 61200) loss: 2.306435\n",
      "(Iteration 701 / 61200) loss: 2.306406\n",
      "(Epoch 1 / 80) train acc: 0.106000; val_acc: 0.107000\n",
      "(Iteration 801 / 61200) loss: 2.306370\n",
      "(Iteration 901 / 61200) loss: 2.306306\n",
      "(Iteration 1001 / 61200) loss: 2.306292\n",
      "(Iteration 1101 / 61200) loss: 2.306259\n",
      "(Iteration 1201 / 61200) loss: 2.306225\n",
      "(Iteration 1301 / 61200) loss: 2.306205\n",
      "(Iteration 1401 / 61200) loss: 2.306159\n",
      "(Iteration 1501 / 61200) loss: 2.306144\n",
      "(Epoch 2 / 80) train acc: 0.113000; val_acc: 0.117000\n",
      "(Iteration 1601 / 61200) loss: 2.306096\n",
      "(Iteration 1701 / 61200) loss: 2.306089\n",
      "(Iteration 1801 / 61200) loss: 2.306026\n",
      "(Iteration 1901 / 61200) loss: 2.306029\n",
      "(Iteration 2001 / 61200) loss: 2.305982\n",
      "(Iteration 2101 / 61200) loss: 2.305961\n",
      "(Iteration 2201 / 61200) loss: 2.305913\n",
      "(Epoch 3 / 80) train acc: 0.112000; val_acc: 0.102000\n",
      "(Iteration 2301 / 61200) loss: 2.305902\n",
      "(Iteration 2401 / 61200) loss: 2.305871\n",
      "(Iteration 2501 / 61200) loss: 2.305877\n",
      "(Iteration 2601 / 61200) loss: 2.305827\n",
      "(Iteration 2701 / 61200) loss: 2.305855\n",
      "(Iteration 2801 / 61200) loss: 2.305788\n",
      "(Iteration 2901 / 61200) loss: 2.305746\n",
      "(Iteration 3001 / 61200) loss: 2.305736\n",
      "(Epoch 4 / 80) train acc: 0.106000; val_acc: 0.108000\n",
      "(Iteration 3101 / 61200) loss: 2.305702\n",
      "(Iteration 3201 / 61200) loss: 2.305661\n",
      "(Iteration 3301 / 61200) loss: 2.305683\n",
      "(Iteration 3401 / 61200) loss: 2.305647\n",
      "(Iteration 3501 / 61200) loss: 2.305635\n",
      "(Iteration 3601 / 61200) loss: 2.305619\n",
      "(Iteration 3701 / 61200) loss: 2.305619\n",
      "(Iteration 3801 / 61200) loss: 2.305599\n",
      "(Epoch 5 / 80) train acc: 0.120000; val_acc: 0.131000\n",
      "(Iteration 3901 / 61200) loss: 2.305578\n",
      "(Iteration 4001 / 61200) loss: 2.305534\n",
      "(Iteration 4101 / 61200) loss: 2.305498\n",
      "(Iteration 4201 / 61200) loss: 2.305494\n",
      "(Iteration 4301 / 61200) loss: 2.305490\n",
      "(Iteration 4401 / 61200) loss: 2.305446\n",
      "(Iteration 4501 / 61200) loss: 2.305465\n",
      "(Epoch 6 / 80) train acc: 0.140000; val_acc: 0.132000\n",
      "(Iteration 4601 / 61200) loss: 2.305428\n",
      "(Iteration 4701 / 61200) loss: 2.305419\n",
      "(Iteration 4801 / 61200) loss: 2.305420\n",
      "(Iteration 4901 / 61200) loss: 2.305392\n",
      "(Iteration 5001 / 61200) loss: 2.305331\n",
      "(Iteration 5101 / 61200) loss: 2.305354\n",
      "(Iteration 5201 / 61200) loss: 2.305315\n",
      "(Iteration 5301 / 61200) loss: 2.305301\n",
      "(Epoch 7 / 80) train acc: 0.113000; val_acc: 0.113000\n",
      "(Iteration 5401 / 61200) loss: 2.305297\n",
      "(Iteration 5501 / 61200) loss: 2.305308\n",
      "(Iteration 5601 / 61200) loss: 2.305313\n",
      "(Iteration 5701 / 61200) loss: 2.305304\n",
      "(Iteration 5801 / 61200) loss: 2.305219\n",
      "(Iteration 5901 / 61200) loss: 2.305257\n",
      "(Iteration 6001 / 61200) loss: 2.305260\n",
      "(Iteration 6101 / 61200) loss: 2.305197\n",
      "(Epoch 8 / 80) train acc: 0.122000; val_acc: 0.116000\n",
      "(Iteration 6201 / 61200) loss: 2.305247\n",
      "(Iteration 6301 / 61200) loss: 2.305205\n",
      "(Iteration 6401 / 61200) loss: 2.305212\n",
      "(Iteration 6501 / 61200) loss: 2.305169\n",
      "(Iteration 6601 / 61200) loss: 2.305166\n",
      "(Iteration 6701 / 61200) loss: 2.305184\n",
      "(Iteration 6801 / 61200) loss: 2.305129\n",
      "(Epoch 9 / 80) train acc: 0.131000; val_acc: 0.131000\n",
      "(Iteration 6901 / 61200) loss: 2.305136\n",
      "(Iteration 7001 / 61200) loss: 2.305109\n",
      "(Iteration 7101 / 61200) loss: 2.305131\n",
      "(Iteration 7201 / 61200) loss: 2.305123\n",
      "(Iteration 7301 / 61200) loss: 2.305070\n",
      "(Iteration 7401 / 61200) loss: 2.305059\n",
      "(Iteration 7501 / 61200) loss: 2.305111\n",
      "(Iteration 7601 / 61200) loss: 2.305053\n",
      "(Epoch 10 / 80) train acc: 0.124000; val_acc: 0.120000\n",
      "(Iteration 7701 / 61200) loss: 2.305085\n",
      "(Iteration 7801 / 61200) loss: 2.305075\n",
      "(Iteration 7901 / 61200) loss: 2.305046\n",
      "(Iteration 8001 / 61200) loss: 2.305039\n",
      "(Iteration 8101 / 61200) loss: 2.305030\n",
      "(Iteration 8201 / 61200) loss: 2.305033\n",
      "(Iteration 8301 / 61200) loss: 2.305035\n",
      "(Iteration 8401 / 61200) loss: 2.305052\n",
      "(Epoch 11 / 80) train acc: 0.101000; val_acc: 0.103000\n",
      "(Iteration 8501 / 61200) loss: 2.304972\n",
      "(Iteration 8601 / 61200) loss: 2.304989\n",
      "(Iteration 8701 / 61200) loss: 2.305020\n",
      "(Iteration 8801 / 61200) loss: 2.304936\n",
      "(Iteration 8901 / 61200) loss: 2.304991\n",
      "(Iteration 9001 / 61200) loss: 2.304952\n",
      "(Iteration 9101 / 61200) loss: 2.304997\n",
      "(Epoch 12 / 80) train acc: 0.106000; val_acc: 0.095000\n",
      "(Iteration 9201 / 61200) loss: 2.304968\n",
      "(Iteration 9301 / 61200) loss: 2.304899\n",
      "(Iteration 9401 / 61200) loss: 2.304951\n",
      "(Iteration 9501 / 61200) loss: 2.304953\n",
      "(Iteration 9601 / 61200) loss: 2.304943\n",
      "(Iteration 9701 / 61200) loss: 2.304905\n",
      "(Iteration 9801 / 61200) loss: 2.304893\n",
      "(Iteration 9901 / 61200) loss: 2.304871\n",
      "(Epoch 13 / 80) train acc: 0.097000; val_acc: 0.097000\n",
      "(Iteration 10001 / 61200) loss: 2.304875\n",
      "(Iteration 10101 / 61200) loss: 2.304947\n",
      "(Iteration 10201 / 61200) loss: 2.304904\n",
      "(Iteration 10301 / 61200) loss: 2.304878\n",
      "(Iteration 10401 / 61200) loss: 2.304877\n",
      "(Iteration 10501 / 61200) loss: 2.304821\n",
      "(Iteration 10601 / 61200) loss: 2.304859\n",
      "(Iteration 10701 / 61200) loss: 2.304854\n",
      "(Epoch 14 / 80) train acc: 0.126000; val_acc: 0.106000\n",
      "(Iteration 10801 / 61200) loss: 2.304844\n",
      "(Iteration 10901 / 61200) loss: 2.304846\n",
      "(Iteration 11001 / 61200) loss: 2.304823\n",
      "(Iteration 11101 / 61200) loss: 2.304805\n",
      "(Iteration 11201 / 61200) loss: 2.304848\n",
      "(Iteration 11301 / 61200) loss: 2.304808\n",
      "(Iteration 11401 / 61200) loss: 2.304763\n",
      "(Epoch 15 / 80) train acc: 0.104000; val_acc: 0.110000\n",
      "(Iteration 11501 / 61200) loss: 2.304844\n",
      "(Iteration 11601 / 61200) loss: 2.304828\n",
      "(Iteration 11701 / 61200) loss: 2.304768\n",
      "(Iteration 11801 / 61200) loss: 2.304782\n",
      "(Iteration 11901 / 61200) loss: 2.304814\n",
      "(Iteration 12001 / 61200) loss: 2.304741\n",
      "(Iteration 12101 / 61200) loss: 2.304770\n",
      "(Iteration 12201 / 61200) loss: 2.304781\n",
      "(Epoch 16 / 80) train acc: 0.120000; val_acc: 0.095000\n",
      "(Iteration 12301 / 61200) loss: 2.304732\n",
      "(Iteration 12401 / 61200) loss: 2.304773\n",
      "(Iteration 12501 / 61200) loss: 2.304758\n",
      "(Iteration 12601 / 61200) loss: 2.304759\n",
      "(Iteration 12701 / 61200) loss: 2.304725\n",
      "(Iteration 12801 / 61200) loss: 2.304775\n",
      "(Iteration 12901 / 61200) loss: 2.304770\n",
      "(Iteration 13001 / 61200) loss: 2.304741\n",
      "(Epoch 17 / 80) train acc: 0.103000; val_acc: 0.092000\n",
      "(Iteration 13101 / 61200) loss: 2.304728\n",
      "(Iteration 13201 / 61200) loss: 2.304743\n",
      "(Iteration 13301 / 61200) loss: 2.304748\n",
      "(Iteration 13401 / 61200) loss: 2.304745\n",
      "(Iteration 13501 / 61200) loss: 2.304704\n",
      "(Iteration 13601 / 61200) loss: 2.304737\n",
      "(Iteration 13701 / 61200) loss: 2.304743\n",
      "(Epoch 18 / 80) train acc: 0.087000; val_acc: 0.092000\n",
      "(Iteration 13801 / 61200) loss: 2.304705\n",
      "(Iteration 13901 / 61200) loss: 2.304687\n",
      "(Iteration 14001 / 61200) loss: 2.304719\n",
      "(Iteration 14101 / 61200) loss: 2.304703\n",
      "(Iteration 14201 / 61200) loss: 2.304716\n",
      "(Iteration 14301 / 61200) loss: 2.304708\n",
      "(Iteration 14401 / 61200) loss: 2.304724\n",
      "(Iteration 14501 / 61200) loss: 2.304680\n",
      "(Epoch 19 / 80) train acc: 0.118000; val_acc: 0.092000\n",
      "(Iteration 14601 / 61200) loss: 2.304670\n",
      "(Iteration 14701 / 61200) loss: 2.304638\n",
      "(Iteration 14801 / 61200) loss: 2.304695\n",
      "(Iteration 14901 / 61200) loss: 2.304695\n",
      "(Iteration 15001 / 61200) loss: 2.304672\n",
      "(Iteration 15101 / 61200) loss: 2.304714\n",
      "(Iteration 15201 / 61200) loss: 2.304682\n",
      "(Epoch 20 / 80) train acc: 0.091000; val_acc: 0.092000\n",
      "(Iteration 15301 / 61200) loss: 2.304657\n",
      "(Iteration 15401 / 61200) loss: 2.304670\n",
      "(Iteration 15501 / 61200) loss: 2.304647\n",
      "(Iteration 15601 / 61200) loss: 2.304659\n",
      "(Iteration 15701 / 61200) loss: 2.304676\n",
      "(Iteration 15801 / 61200) loss: 2.304660\n",
      "(Iteration 15901 / 61200) loss: 2.304677\n",
      "(Iteration 16001 / 61200) loss: 2.304637\n",
      "(Epoch 21 / 80) train acc: 0.106000; val_acc: 0.092000\n",
      "(Iteration 16101 / 61200) loss: 2.304679\n",
      "(Iteration 16201 / 61200) loss: 2.304637\n",
      "(Iteration 16301 / 61200) loss: 2.304677\n",
      "(Iteration 16401 / 61200) loss: 2.304649\n",
      "(Iteration 16501 / 61200) loss: 2.304610\n",
      "(Iteration 16601 / 61200) loss: 2.304602\n",
      "(Iteration 16701 / 61200) loss: 2.304639\n",
      "(Iteration 16801 / 61200) loss: 2.304598\n",
      "(Epoch 22 / 80) train acc: 0.101000; val_acc: 0.091000\n",
      "(Iteration 16901 / 61200) loss: 2.304637\n",
      "(Iteration 17001 / 61200) loss: 2.304632\n",
      "(Iteration 17101 / 61200) loss: 2.304637\n",
      "(Iteration 17201 / 61200) loss: 2.304661\n",
      "(Iteration 17301 / 61200) loss: 2.304630\n",
      "(Iteration 17401 / 61200) loss: 2.304576\n",
      "(Iteration 17501 / 61200) loss: 2.304622\n",
      "(Epoch 23 / 80) train acc: 0.109000; val_acc: 0.090000\n",
      "(Iteration 17601 / 61200) loss: 2.304613\n",
      "(Iteration 17701 / 61200) loss: 2.304607\n",
      "(Iteration 17801 / 61200) loss: 2.304634\n",
      "(Iteration 17901 / 61200) loss: 2.304623\n",
      "(Iteration 18001 / 61200) loss: 2.304574\n",
      "(Iteration 18101 / 61200) loss: 2.304620\n",
      "(Iteration 18201 / 61200) loss: 2.304569\n",
      "(Iteration 18301 / 61200) loss: 2.304612\n",
      "(Epoch 24 / 80) train acc: 0.099000; val_acc: 0.090000\n",
      "(Iteration 18401 / 61200) loss: 2.304601\n",
      "(Iteration 18501 / 61200) loss: 2.304622\n",
      "(Iteration 18601 / 61200) loss: 2.304610\n",
      "(Iteration 18701 / 61200) loss: 2.304577\n",
      "(Iteration 18801 / 61200) loss: 2.304591\n",
      "(Iteration 18901 / 61200) loss: 2.304571\n",
      "(Iteration 19001 / 61200) loss: 2.304623\n",
      "(Iteration 19101 / 61200) loss: 2.304579\n",
      "(Epoch 25 / 80) train acc: 0.123000; val_acc: 0.090000\n",
      "(Iteration 19201 / 61200) loss: 2.304567\n",
      "(Iteration 19301 / 61200) loss: 2.304587\n",
      "(Iteration 19401 / 61200) loss: 2.304657\n",
      "(Iteration 19501 / 61200) loss: 2.304575\n",
      "(Iteration 19601 / 61200) loss: 2.304586\n",
      "(Iteration 19701 / 61200) loss: 2.304639\n",
      "(Iteration 19801 / 61200) loss: 2.304578\n",
      "(Epoch 26 / 80) train acc: 0.080000; val_acc: 0.090000\n",
      "(Iteration 19901 / 61200) loss: 2.304568\n",
      "(Iteration 20001 / 61200) loss: 2.304595\n",
      "(Iteration 20101 / 61200) loss: 2.304579\n",
      "(Iteration 20201 / 61200) loss: 2.304601\n",
      "(Iteration 20301 / 61200) loss: 2.304596\n",
      "(Iteration 20401 / 61200) loss: 2.304588\n",
      "(Iteration 20501 / 61200) loss: 2.304548\n",
      "(Iteration 20601 / 61200) loss: 2.304607\n",
      "(Epoch 27 / 80) train acc: 0.081000; val_acc: 0.090000\n",
      "(Iteration 20701 / 61200) loss: 2.304596\n",
      "(Iteration 20801 / 61200) loss: 2.304568\n",
      "(Iteration 20901 / 61200) loss: 2.304568\n",
      "(Iteration 21001 / 61200) loss: 2.304532\n",
      "(Iteration 21101 / 61200) loss: 2.304544\n",
      "(Iteration 21201 / 61200) loss: 2.304544\n",
      "(Iteration 21301 / 61200) loss: 2.304516\n",
      "(Iteration 21401 / 61200) loss: 2.304564\n",
      "(Epoch 28 / 80) train acc: 0.115000; val_acc: 0.091000\n",
      "(Iteration 21501 / 61200) loss: 2.304587\n",
      "(Iteration 21601 / 61200) loss: 2.304534\n",
      "(Iteration 21701 / 61200) loss: 2.304553\n",
      "(Iteration 21801 / 61200) loss: 2.304532\n",
      "(Iteration 21901 / 61200) loss: 2.304510\n",
      "(Iteration 22001 / 61200) loss: 2.304536\n",
      "(Iteration 22101 / 61200) loss: 2.304572\n",
      "(Epoch 29 / 80) train acc: 0.093000; val_acc: 0.090000\n",
      "(Iteration 22201 / 61200) loss: 2.304536\n",
      "(Iteration 22301 / 61200) loss: 2.304533\n",
      "(Iteration 22401 / 61200) loss: 2.304512\n",
      "(Iteration 22501 / 61200) loss: 2.304549\n",
      "(Iteration 22601 / 61200) loss: 2.304564\n",
      "(Iteration 22701 / 61200) loss: 2.304589\n",
      "(Iteration 22801 / 61200) loss: 2.304501\n",
      "(Iteration 22901 / 61200) loss: 2.304540\n",
      "(Epoch 30 / 80) train acc: 0.097000; val_acc: 0.090000\n",
      "(Iteration 23001 / 61200) loss: 2.304555\n",
      "(Iteration 23101 / 61200) loss: 2.304550\n",
      "(Iteration 23201 / 61200) loss: 2.304577\n",
      "(Iteration 23301 / 61200) loss: 2.304548\n",
      "(Iteration 23401 / 61200) loss: 2.304546\n",
      "(Iteration 23501 / 61200) loss: 2.304566\n",
      "(Iteration 23601 / 61200) loss: 2.304542\n",
      "(Iteration 23701 / 61200) loss: 2.304517\n",
      "(Epoch 31 / 80) train acc: 0.099000; val_acc: 0.090000\n",
      "(Iteration 23801 / 61200) loss: 2.304483\n",
      "(Iteration 23901 / 61200) loss: 2.304489\n",
      "(Iteration 24001 / 61200) loss: 2.304531\n",
      "(Iteration 24101 / 61200) loss: 2.304534\n",
      "(Iteration 24201 / 61200) loss: 2.304551\n",
      "(Iteration 24301 / 61200) loss: 2.304532\n",
      "(Iteration 24401 / 61200) loss: 2.304539\n",
      "(Epoch 32 / 80) train acc: 0.103000; val_acc: 0.090000\n",
      "(Iteration 24501 / 61200) loss: 2.304568\n",
      "(Iteration 24601 / 61200) loss: 2.304570\n",
      "(Iteration 24701 / 61200) loss: 2.304568\n",
      "(Iteration 24801 / 61200) loss: 2.304538\n",
      "(Iteration 24901 / 61200) loss: 2.304561\n",
      "(Iteration 25001 / 61200) loss: 2.304526\n",
      "(Iteration 25101 / 61200) loss: 2.304523\n",
      "(Iteration 25201 / 61200) loss: 2.304534\n",
      "(Epoch 33 / 80) train acc: 0.080000; val_acc: 0.090000\n",
      "(Iteration 25301 / 61200) loss: 2.304526\n",
      "(Iteration 25401 / 61200) loss: 2.304501\n",
      "(Iteration 25501 / 61200) loss: 2.304540\n",
      "(Iteration 25601 / 61200) loss: 2.304546\n",
      "(Iteration 25701 / 61200) loss: 2.304505\n",
      "(Iteration 25801 / 61200) loss: 2.304566\n",
      "(Iteration 25901 / 61200) loss: 2.304514\n",
      "(Iteration 26001 / 61200) loss: 2.304533\n",
      "(Epoch 34 / 80) train acc: 0.108000; val_acc: 0.090000\n",
      "(Iteration 26101 / 61200) loss: 2.304547\n",
      "(Iteration 26201 / 61200) loss: 2.304566\n",
      "(Iteration 26301 / 61200) loss: 2.304528\n",
      "(Iteration 26401 / 61200) loss: 2.304502\n",
      "(Iteration 26501 / 61200) loss: 2.304483\n",
      "(Iteration 26601 / 61200) loss: 2.304492\n",
      "(Iteration 26701 / 61200) loss: 2.304530\n",
      "(Epoch 35 / 80) train acc: 0.099000; val_acc: 0.090000\n",
      "(Iteration 26801 / 61200) loss: 2.304537\n",
      "(Iteration 26901 / 61200) loss: 2.304526\n",
      "(Iteration 27001 / 61200) loss: 2.304493\n",
      "(Iteration 27101 / 61200) loss: 2.304466\n",
      "(Iteration 27201 / 61200) loss: 2.304544\n",
      "(Iteration 27301 / 61200) loss: 2.304494\n",
      "(Iteration 27401 / 61200) loss: 2.304538\n",
      "(Iteration 27501 / 61200) loss: 2.304504\n",
      "(Epoch 36 / 80) train acc: 0.103000; val_acc: 0.090000\n",
      "(Iteration 27601 / 61200) loss: 2.304521\n",
      "(Iteration 27701 / 61200) loss: 2.304510\n",
      "(Iteration 27801 / 61200) loss: 2.304544\n",
      "(Iteration 27901 / 61200) loss: 2.304450\n",
      "(Iteration 28001 / 61200) loss: 2.304527\n",
      "(Iteration 28101 / 61200) loss: 2.304513\n",
      "(Iteration 28201 / 61200) loss: 2.304542\n",
      "(Iteration 28301 / 61200) loss: 2.304496\n",
      "(Epoch 37 / 80) train acc: 0.106000; val_acc: 0.090000\n",
      "(Iteration 28401 / 61200) loss: 2.304490\n",
      "(Iteration 28501 / 61200) loss: 2.304483\n",
      "(Iteration 28601 / 61200) loss: 2.304495\n",
      "(Iteration 28701 / 61200) loss: 2.304547\n",
      "(Iteration 28801 / 61200) loss: 2.304521\n",
      "(Iteration 28901 / 61200) loss: 2.304519\n",
      "(Iteration 29001 / 61200) loss: 2.304537\n",
      "(Epoch 38 / 80) train acc: 0.102000; val_acc: 0.090000\n",
      "(Iteration 29101 / 61200) loss: 2.304492\n",
      "(Iteration 29201 / 61200) loss: 2.304522\n",
      "(Iteration 29301 / 61200) loss: 2.304509\n",
      "(Iteration 29401 / 61200) loss: 2.304522\n",
      "(Iteration 29501 / 61200) loss: 2.304499\n",
      "(Iteration 29601 / 61200) loss: 2.304487\n",
      "(Iteration 29701 / 61200) loss: 2.304519\n",
      "(Iteration 29801 / 61200) loss: 2.304486\n",
      "(Epoch 39 / 80) train acc: 0.102000; val_acc: 0.090000\n",
      "(Iteration 29901 / 61200) loss: 2.304526\n",
      "(Iteration 30001 / 61200) loss: 2.304513\n",
      "(Iteration 30101 / 61200) loss: 2.304508\n",
      "(Iteration 30201 / 61200) loss: 2.304483\n",
      "(Iteration 30301 / 61200) loss: 2.304475\n",
      "(Iteration 30401 / 61200) loss: 2.304512\n",
      "(Iteration 30501 / 61200) loss: 2.304502\n",
      "(Epoch 40 / 80) train acc: 0.113000; val_acc: 0.090000\n",
      "(Iteration 30601 / 61200) loss: 2.304507\n",
      "(Iteration 30701 / 61200) loss: 2.304452\n",
      "(Iteration 30801 / 61200) loss: 2.304537\n",
      "(Iteration 30901 / 61200) loss: 2.304506\n",
      "(Iteration 31001 / 61200) loss: 2.304494\n",
      "(Iteration 31101 / 61200) loss: 2.304509\n",
      "(Iteration 31201 / 61200) loss: 2.304473\n",
      "(Iteration 31301 / 61200) loss: 2.304462\n",
      "(Epoch 41 / 80) train acc: 0.112000; val_acc: 0.090000\n",
      "(Iteration 31401 / 61200) loss: 2.304536\n",
      "(Iteration 31501 / 61200) loss: 2.304523\n",
      "(Iteration 31601 / 61200) loss: 2.304489\n",
      "(Iteration 31701 / 61200) loss: 2.304526\n",
      "(Iteration 31801 / 61200) loss: 2.304536\n",
      "(Iteration 31901 / 61200) loss: 2.304479\n",
      "(Iteration 32001 / 61200) loss: 2.304490\n",
      "(Iteration 32101 / 61200) loss: 2.304490\n",
      "(Epoch 42 / 80) train acc: 0.093000; val_acc: 0.090000\n",
      "(Iteration 32201 / 61200) loss: 2.304491\n",
      "(Iteration 32301 / 61200) loss: 2.304465\n",
      "(Iteration 32401 / 61200) loss: 2.304527\n",
      "(Iteration 32501 / 61200) loss: 2.304491\n",
      "(Iteration 32601 / 61200) loss: 2.304474\n",
      "(Iteration 32701 / 61200) loss: 2.304523\n",
      "(Iteration 32801 / 61200) loss: 2.304519\n",
      "(Epoch 43 / 80) train acc: 0.096000; val_acc: 0.090000\n",
      "(Iteration 32901 / 61200) loss: 2.304466\n",
      "(Iteration 33001 / 61200) loss: 2.304485\n",
      "(Iteration 33101 / 61200) loss: 2.304505\n",
      "(Iteration 33201 / 61200) loss: 2.304514\n",
      "(Iteration 33301 / 61200) loss: 2.304532\n",
      "(Iteration 33401 / 61200) loss: 2.304495\n",
      "(Iteration 33501 / 61200) loss: 2.304505\n",
      "(Iteration 33601 / 61200) loss: 2.304481\n",
      "(Epoch 44 / 80) train acc: 0.106000; val_acc: 0.090000\n",
      "(Iteration 33701 / 61200) loss: 2.304514\n",
      "(Iteration 33801 / 61200) loss: 2.304463\n",
      "(Iteration 33901 / 61200) loss: 2.304534\n",
      "(Iteration 34001 / 61200) loss: 2.304473\n",
      "(Iteration 34101 / 61200) loss: 2.304537\n",
      "(Iteration 34201 / 61200) loss: 2.304478\n",
      "(Iteration 34301 / 61200) loss: 2.304469\n",
      "(Iteration 34401 / 61200) loss: 2.304470\n",
      "(Epoch 45 / 80) train acc: 0.104000; val_acc: 0.090000\n",
      "(Iteration 34501 / 61200) loss: 2.304458\n",
      "(Iteration 34601 / 61200) loss: 2.304470\n",
      "(Iteration 34701 / 61200) loss: 2.304471\n",
      "(Iteration 34801 / 61200) loss: 2.304476\n",
      "(Iteration 34901 / 61200) loss: 2.304487\n",
      "(Iteration 35001 / 61200) loss: 2.304446\n",
      "(Iteration 35101 / 61200) loss: 2.304472\n",
      "(Epoch 46 / 80) train acc: 0.091000; val_acc: 0.090000\n",
      "(Iteration 35201 / 61200) loss: 2.304536\n",
      "(Iteration 35301 / 61200) loss: 2.304502\n",
      "(Iteration 35401 / 61200) loss: 2.304562\n",
      "(Iteration 35501 / 61200) loss: 2.304502\n",
      "(Iteration 35601 / 61200) loss: 2.304482\n",
      "(Iteration 35701 / 61200) loss: 2.304521\n",
      "(Iteration 35801 / 61200) loss: 2.304518\n",
      "(Iteration 35901 / 61200) loss: 2.304499\n",
      "(Epoch 47 / 80) train acc: 0.088000; val_acc: 0.090000\n",
      "(Iteration 36001 / 61200) loss: 2.304500\n",
      "(Iteration 36101 / 61200) loss: 2.304481\n",
      "(Iteration 36201 / 61200) loss: 2.304453\n",
      "(Iteration 36301 / 61200) loss: 2.304473\n",
      "(Iteration 36401 / 61200) loss: 2.304500\n",
      "(Iteration 36501 / 61200) loss: 2.304487\n",
      "(Iteration 36601 / 61200) loss: 2.304496\n",
      "(Iteration 36701 / 61200) loss: 2.304496\n",
      "(Epoch 48 / 80) train acc: 0.112000; val_acc: 0.090000\n",
      "(Iteration 36801 / 61200) loss: 2.304544\n",
      "(Iteration 36901 / 61200) loss: 2.304473\n",
      "(Iteration 37001 / 61200) loss: 2.304483\n",
      "(Iteration 37101 / 61200) loss: 2.304484\n",
      "(Iteration 37201 / 61200) loss: 2.304448\n",
      "(Iteration 37301 / 61200) loss: 2.304513\n",
      "(Iteration 37401 / 61200) loss: 2.304529\n",
      "(Epoch 49 / 80) train acc: 0.120000; val_acc: 0.090000\n",
      "(Iteration 37501 / 61200) loss: 2.304474\n",
      "(Iteration 37601 / 61200) loss: 2.304471\n",
      "(Iteration 37701 / 61200) loss: 2.304475\n",
      "(Iteration 37801 / 61200) loss: 2.304485\n",
      "(Iteration 37901 / 61200) loss: 2.304473\n",
      "(Iteration 38001 / 61200) loss: 2.304470\n",
      "(Iteration 38101 / 61200) loss: 2.304488\n",
      "(Iteration 38201 / 61200) loss: 2.304455\n",
      "(Epoch 50 / 80) train acc: 0.097000; val_acc: 0.090000\n",
      "(Iteration 38301 / 61200) loss: 2.304509\n",
      "(Iteration 38401 / 61200) loss: 2.304472\n",
      "(Iteration 38501 / 61200) loss: 2.304477\n",
      "(Iteration 38601 / 61200) loss: 2.304485\n",
      "(Iteration 38701 / 61200) loss: 2.304498\n",
      "(Iteration 38801 / 61200) loss: 2.304534\n",
      "(Iteration 38901 / 61200) loss: 2.304520\n",
      "(Iteration 39001 / 61200) loss: 2.304458\n",
      "(Epoch 51 / 80) train acc: 0.104000; val_acc: 0.090000\n",
      "(Iteration 39101 / 61200) loss: 2.304476\n",
      "(Iteration 39201 / 61200) loss: 2.304452\n",
      "(Iteration 39301 / 61200) loss: 2.304493\n",
      "(Iteration 39401 / 61200) loss: 2.304525\n",
      "(Iteration 39501 / 61200) loss: 2.304489\n",
      "(Iteration 39601 / 61200) loss: 2.304457\n",
      "(Iteration 39701 / 61200) loss: 2.304478\n",
      "(Epoch 52 / 80) train acc: 0.102000; val_acc: 0.090000\n",
      "(Iteration 39801 / 61200) loss: 2.304501\n",
      "(Iteration 39901 / 61200) loss: 2.304518\n",
      "(Iteration 40001 / 61200) loss: 2.304468\n",
      "(Iteration 40101 / 61200) loss: 2.304515\n",
      "(Iteration 40201 / 61200) loss: 2.304443\n",
      "(Iteration 40301 / 61200) loss: 2.304439\n",
      "(Iteration 40401 / 61200) loss: 2.304518\n",
      "(Iteration 40501 / 61200) loss: 2.304485\n",
      "(Epoch 53 / 80) train acc: 0.115000; val_acc: 0.090000\n",
      "(Iteration 40601 / 61200) loss: 2.304484\n",
      "(Iteration 40701 / 61200) loss: 2.304455\n",
      "(Iteration 40801 / 61200) loss: 2.304501\n",
      "(Iteration 40901 / 61200) loss: 2.304497\n",
      "(Iteration 41001 / 61200) loss: 2.304502\n",
      "(Iteration 41101 / 61200) loss: 2.304463\n",
      "(Iteration 41201 / 61200) loss: 2.304448\n",
      "(Iteration 41301 / 61200) loss: 2.304480\n",
      "(Epoch 54 / 80) train acc: 0.085000; val_acc: 0.090000\n",
      "(Iteration 41401 / 61200) loss: 2.304487\n",
      "(Iteration 41501 / 61200) loss: 2.304521\n",
      "(Iteration 41601 / 61200) loss: 2.304485\n",
      "(Iteration 41701 / 61200) loss: 2.304451\n",
      "(Iteration 41801 / 61200) loss: 2.304454\n",
      "(Iteration 41901 / 61200) loss: 2.304489\n",
      "(Iteration 42001 / 61200) loss: 2.304477\n",
      "(Epoch 55 / 80) train acc: 0.092000; val_acc: 0.090000\n",
      "(Iteration 42101 / 61200) loss: 2.304457\n",
      "(Iteration 42201 / 61200) loss: 2.304468\n",
      "(Iteration 42301 / 61200) loss: 2.304465\n",
      "(Iteration 42401 / 61200) loss: 2.304442\n",
      "(Iteration 42501 / 61200) loss: 2.304488\n",
      "(Iteration 42601 / 61200) loss: 2.304470\n",
      "(Iteration 42701 / 61200) loss: 2.304521\n",
      "(Iteration 42801 / 61200) loss: 2.304486\n",
      "(Epoch 56 / 80) train acc: 0.096000; val_acc: 0.090000\n",
      "(Iteration 42901 / 61200) loss: 2.304474\n",
      "(Iteration 43001 / 61200) loss: 2.304498\n",
      "(Iteration 43101 / 61200) loss: 2.304505\n",
      "(Iteration 43201 / 61200) loss: 2.304502\n",
      "(Iteration 43301 / 61200) loss: 2.304461\n",
      "(Iteration 43401 / 61200) loss: 2.304465\n",
      "(Iteration 43501 / 61200) loss: 2.304472\n",
      "(Iteration 43601 / 61200) loss: 2.304505\n",
      "(Epoch 57 / 80) train acc: 0.092000; val_acc: 0.090000\n",
      "(Iteration 43701 / 61200) loss: 2.304486\n",
      "(Iteration 43801 / 61200) loss: 2.304464\n",
      "(Iteration 43901 / 61200) loss: 2.304506\n",
      "(Iteration 44001 / 61200) loss: 2.304502\n",
      "(Iteration 44101 / 61200) loss: 2.304494\n",
      "(Iteration 44201 / 61200) loss: 2.304445\n",
      "(Iteration 44301 / 61200) loss: 2.304489\n",
      "(Epoch 58 / 80) train acc: 0.106000; val_acc: 0.090000\n",
      "(Iteration 44401 / 61200) loss: 2.304493\n",
      "(Iteration 44501 / 61200) loss: 2.304481\n",
      "(Iteration 44601 / 61200) loss: 2.304468\n",
      "(Iteration 44701 / 61200) loss: 2.304477\n",
      "(Iteration 44801 / 61200) loss: 2.304504\n",
      "(Iteration 44901 / 61200) loss: 2.304486\n",
      "(Iteration 45001 / 61200) loss: 2.304522\n",
      "(Iteration 45101 / 61200) loss: 2.304429\n",
      "(Epoch 59 / 80) train acc: 0.084000; val_acc: 0.090000\n",
      "(Iteration 45201 / 61200) loss: 2.304487\n",
      "(Iteration 45301 / 61200) loss: 2.304480\n",
      "(Iteration 45401 / 61200) loss: 2.304481\n",
      "(Iteration 45501 / 61200) loss: 2.304426\n",
      "(Iteration 45601 / 61200) loss: 2.304475\n",
      "(Iteration 45701 / 61200) loss: 2.304484\n",
      "(Iteration 45801 / 61200) loss: 2.304485\n",
      "(Epoch 60 / 80) train acc: 0.103000; val_acc: 0.090000\n",
      "(Iteration 45901 / 61200) loss: 2.304483\n",
      "(Iteration 46001 / 61200) loss: 2.304490\n",
      "(Iteration 46101 / 61200) loss: 2.304476\n",
      "(Iteration 46201 / 61200) loss: 2.304461\n",
      "(Iteration 46301 / 61200) loss: 2.304513\n",
      "(Iteration 46401 / 61200) loss: 2.304474\n",
      "(Iteration 46501 / 61200) loss: 2.304475\n",
      "(Iteration 46601 / 61200) loss: 2.304485\n",
      "(Epoch 61 / 80) train acc: 0.092000; val_acc: 0.090000\n",
      "(Iteration 46701 / 61200) loss: 2.304484\n",
      "(Iteration 46801 / 61200) loss: 2.304474\n",
      "(Iteration 46901 / 61200) loss: 2.304471\n",
      "(Iteration 47001 / 61200) loss: 2.304452\n",
      "(Iteration 47101 / 61200) loss: 2.304475\n",
      "(Iteration 47201 / 61200) loss: 2.304477\n",
      "(Iteration 47301 / 61200) loss: 2.304470\n",
      "(Iteration 47401 / 61200) loss: 2.304518\n",
      "(Epoch 62 / 80) train acc: 0.117000; val_acc: 0.090000\n",
      "(Iteration 47501 / 61200) loss: 2.304453\n",
      "(Iteration 47601 / 61200) loss: 2.304436\n",
      "(Iteration 47701 / 61200) loss: 2.304467\n",
      "(Iteration 47801 / 61200) loss: 2.304489\n",
      "(Iteration 47901 / 61200) loss: 2.304470\n",
      "(Iteration 48001 / 61200) loss: 2.304504\n",
      "(Iteration 48101 / 61200) loss: 2.304509\n",
      "(Epoch 63 / 80) train acc: 0.097000; val_acc: 0.090000\n",
      "(Iteration 48201 / 61200) loss: 2.304498\n",
      "(Iteration 48301 / 61200) loss: 2.304460\n",
      "(Iteration 48401 / 61200) loss: 2.304485\n",
      "(Iteration 48501 / 61200) loss: 2.304470\n",
      "(Iteration 48601 / 61200) loss: 2.304514\n",
      "(Iteration 48701 / 61200) loss: 2.304488\n",
      "(Iteration 48801 / 61200) loss: 2.304467\n",
      "(Iteration 48901 / 61200) loss: 2.304490\n",
      "(Epoch 64 / 80) train acc: 0.099000; val_acc: 0.090000\n",
      "(Iteration 49001 / 61200) loss: 2.304490\n",
      "(Iteration 49101 / 61200) loss: 2.304511\n",
      "(Iteration 49201 / 61200) loss: 2.304516\n",
      "(Iteration 49301 / 61200) loss: 2.304481\n",
      "(Iteration 49401 / 61200) loss: 2.304482\n",
      "(Iteration 49501 / 61200) loss: 2.304465\n",
      "(Iteration 49601 / 61200) loss: 2.304475\n",
      "(Iteration 49701 / 61200) loss: 2.304498\n",
      "(Epoch 65 / 80) train acc: 0.095000; val_acc: 0.090000\n",
      "(Iteration 49801 / 61200) loss: 2.304496\n",
      "(Iteration 49901 / 61200) loss: 2.304535\n",
      "(Iteration 50001 / 61200) loss: 2.304493\n",
      "(Iteration 50101 / 61200) loss: 2.304472\n",
      "(Iteration 50201 / 61200) loss: 2.304459\n",
      "(Iteration 50301 / 61200) loss: 2.304487\n",
      "(Iteration 50401 / 61200) loss: 2.304463\n",
      "(Epoch 66 / 80) train acc: 0.103000; val_acc: 0.090000\n",
      "(Iteration 50501 / 61200) loss: 2.304473\n",
      "(Iteration 50601 / 61200) loss: 2.304486\n",
      "(Iteration 50701 / 61200) loss: 2.304450\n",
      "(Iteration 50801 / 61200) loss: 2.304485\n",
      "(Iteration 50901 / 61200) loss: 2.304521\n",
      "(Iteration 51001 / 61200) loss: 2.304473\n",
      "(Iteration 51101 / 61200) loss: 2.304473\n",
      "(Iteration 51201 / 61200) loss: 2.304505\n",
      "(Epoch 67 / 80) train acc: 0.094000; val_acc: 0.090000\n",
      "(Iteration 51301 / 61200) loss: 2.304511\n",
      "(Iteration 51401 / 61200) loss: 2.304487\n",
      "(Iteration 51501 / 61200) loss: 2.304501\n",
      "(Iteration 51601 / 61200) loss: 2.304452\n",
      "(Iteration 51701 / 61200) loss: 2.304488\n",
      "(Iteration 51801 / 61200) loss: 2.304531\n",
      "(Iteration 51901 / 61200) loss: 2.304518\n",
      "(Iteration 52001 / 61200) loss: 2.304542\n",
      "(Epoch 68 / 80) train acc: 0.121000; val_acc: 0.090000\n",
      "(Iteration 52101 / 61200) loss: 2.304488\n",
      "(Iteration 52201 / 61200) loss: 2.304466\n",
      "(Iteration 52301 / 61200) loss: 2.304490\n",
      "(Iteration 52401 / 61200) loss: 2.304487\n",
      "(Iteration 52501 / 61200) loss: 2.304473\n",
      "(Iteration 52601 / 61200) loss: 2.304470\n",
      "(Iteration 52701 / 61200) loss: 2.304468\n",
      "(Epoch 69 / 80) train acc: 0.109000; val_acc: 0.090000\n",
      "(Iteration 52801 / 61200) loss: 2.304420\n",
      "(Iteration 52901 / 61200) loss: 2.304509\n",
      "(Iteration 53001 / 61200) loss: 2.304510\n",
      "(Iteration 53101 / 61200) loss: 2.304512\n",
      "(Iteration 53201 / 61200) loss: 2.304443\n",
      "(Iteration 53301 / 61200) loss: 2.304453\n",
      "(Iteration 53401 / 61200) loss: 2.304500\n",
      "(Iteration 53501 / 61200) loss: 2.304503\n",
      "(Epoch 70 / 80) train acc: 0.119000; val_acc: 0.090000\n",
      "(Iteration 53601 / 61200) loss: 2.304448\n",
      "(Iteration 53701 / 61200) loss: 2.304468\n",
      "(Iteration 53801 / 61200) loss: 2.304498\n",
      "(Iteration 53901 / 61200) loss: 2.304513\n",
      "(Iteration 54001 / 61200) loss: 2.304502\n",
      "(Iteration 54101 / 61200) loss: 2.304495\n",
      "(Iteration 54201 / 61200) loss: 2.304486\n",
      "(Iteration 54301 / 61200) loss: 2.304503\n",
      "(Epoch 71 / 80) train acc: 0.095000; val_acc: 0.090000\n",
      "(Iteration 54401 / 61200) loss: 2.304478\n",
      "(Iteration 54501 / 61200) loss: 2.304487\n",
      "(Iteration 54601 / 61200) loss: 2.304447\n",
      "(Iteration 54701 / 61200) loss: 2.304494\n",
      "(Iteration 54801 / 61200) loss: 2.304487\n",
      "(Iteration 54901 / 61200) loss: 2.304441\n",
      "(Iteration 55001 / 61200) loss: 2.304488\n",
      "(Epoch 72 / 80) train acc: 0.093000; val_acc: 0.090000\n",
      "(Iteration 55101 / 61200) loss: 2.304479\n",
      "(Iteration 55201 / 61200) loss: 2.304524\n",
      "(Iteration 55301 / 61200) loss: 2.304489\n",
      "(Iteration 55401 / 61200) loss: 2.304471\n",
      "(Iteration 55501 / 61200) loss: 2.304465\n",
      "(Iteration 55601 / 61200) loss: 2.304518\n",
      "(Iteration 55701 / 61200) loss: 2.304504\n",
      "(Iteration 55801 / 61200) loss: 2.304502\n",
      "(Epoch 73 / 80) train acc: 0.090000; val_acc: 0.090000\n",
      "(Iteration 55901 / 61200) loss: 2.304490\n",
      "(Iteration 56001 / 61200) loss: 2.304467\n",
      "(Iteration 56101 / 61200) loss: 2.304467\n",
      "(Iteration 56201 / 61200) loss: 2.304478\n",
      "(Iteration 56301 / 61200) loss: 2.304496\n",
      "(Iteration 56401 / 61200) loss: 2.304501\n",
      "(Iteration 56501 / 61200) loss: 2.304481\n",
      "(Iteration 56601 / 61200) loss: 2.304475\n",
      "(Epoch 74 / 80) train acc: 0.103000; val_acc: 0.090000\n",
      "(Iteration 56701 / 61200) loss: 2.304498\n",
      "(Iteration 56801 / 61200) loss: 2.304502\n",
      "(Iteration 56901 / 61200) loss: 2.304490\n",
      "(Iteration 57001 / 61200) loss: 2.304489\n",
      "(Iteration 57101 / 61200) loss: 2.304480\n",
      "(Iteration 57201 / 61200) loss: 2.304494\n",
      "(Iteration 57301 / 61200) loss: 2.304465\n",
      "(Epoch 75 / 80) train acc: 0.108000; val_acc: 0.090000\n",
      "(Iteration 57401 / 61200) loss: 2.304489\n",
      "(Iteration 57501 / 61200) loss: 2.304454\n",
      "(Iteration 57601 / 61200) loss: 2.304466\n",
      "(Iteration 57701 / 61200) loss: 2.304473\n",
      "(Iteration 57801 / 61200) loss: 2.304492\n",
      "(Iteration 57901 / 61200) loss: 2.304514\n",
      "(Iteration 58001 / 61200) loss: 2.304519\n",
      "(Iteration 58101 / 61200) loss: 2.304527\n",
      "(Epoch 76 / 80) train acc: 0.110000; val_acc: 0.090000\n",
      "(Iteration 58201 / 61200) loss: 2.304495\n",
      "(Iteration 58301 / 61200) loss: 2.304459\n",
      "(Iteration 58401 / 61200) loss: 2.304480\n",
      "(Iteration 58501 / 61200) loss: 2.304455\n",
      "(Iteration 58601 / 61200) loss: 2.304470\n",
      "(Iteration 58701 / 61200) loss: 2.304444\n",
      "(Iteration 58801 / 61200) loss: 2.304513\n",
      "(Iteration 58901 / 61200) loss: 2.304507\n",
      "(Epoch 77 / 80) train acc: 0.101000; val_acc: 0.090000\n",
      "(Iteration 59001 / 61200) loss: 2.304496\n",
      "(Iteration 59101 / 61200) loss: 2.304513\n",
      "(Iteration 59201 / 61200) loss: 2.304470\n",
      "(Iteration 59301 / 61200) loss: 2.304477\n",
      "(Iteration 59401 / 61200) loss: 2.304500\n",
      "(Iteration 59501 / 61200) loss: 2.304522\n",
      "(Iteration 59601 / 61200) loss: 2.304503\n",
      "(Epoch 78 / 80) train acc: 0.108000; val_acc: 0.090000\n",
      "(Iteration 59701 / 61200) loss: 2.304472\n",
      "(Iteration 59801 / 61200) loss: 2.304463\n",
      "(Iteration 59901 / 61200) loss: 2.304482\n",
      "(Iteration 60001 / 61200) loss: 2.304456\n",
      "(Iteration 60101 / 61200) loss: 2.304504\n",
      "(Iteration 60201 / 61200) loss: 2.304497\n",
      "(Iteration 60301 / 61200) loss: 2.304538\n",
      "(Iteration 60401 / 61200) loss: 2.304519\n",
      "(Epoch 79 / 80) train acc: 0.105000; val_acc: 0.090000\n",
      "(Iteration 60501 / 61200) loss: 2.304511\n",
      "(Iteration 60601 / 61200) loss: 2.304540\n",
      "(Iteration 60701 / 61200) loss: 2.304473\n",
      "(Iteration 60801 / 61200) loss: 2.304539\n",
      "(Iteration 60901 / 61200) loss: 2.304473\n",
      "(Iteration 61001 / 61200) loss: 2.304461\n",
      "(Iteration 61101 / 61200) loss: 2.304477\n",
      "(Epoch 80 / 80) train acc: 0.109000; val_acc: 0.090000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.306653\n",
      "(Epoch 0 / 80) train acc: 0.083000; val_acc: 0.096000\n",
      "(Iteration 101 / 30560) loss: 2.306610\n",
      "(Iteration 201 / 30560) loss: 2.306570\n",
      "(Iteration 301 / 30560) loss: 2.306537\n",
      "(Epoch 1 / 80) train acc: 0.109000; val_acc: 0.085000\n",
      "(Iteration 401 / 30560) loss: 2.306512\n",
      "(Iteration 501 / 30560) loss: 2.306464\n",
      "(Iteration 601 / 30560) loss: 2.306425\n",
      "(Iteration 701 / 30560) loss: 2.306398\n",
      "(Epoch 2 / 80) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 801 / 30560) loss: 2.306373\n",
      "(Iteration 901 / 30560) loss: 2.306304\n",
      "(Iteration 1001 / 30560) loss: 2.306295\n",
      "(Iteration 1101 / 30560) loss: 2.306259\n",
      "(Epoch 3 / 80) train acc: 0.115000; val_acc: 0.081000\n",
      "(Iteration 1201 / 30560) loss: 2.306258\n",
      "(Iteration 1301 / 30560) loss: 2.306204\n",
      "(Iteration 1401 / 30560) loss: 2.306174\n",
      "(Iteration 1501 / 30560) loss: 2.306150\n",
      "(Epoch 4 / 80) train acc: 0.116000; val_acc: 0.090000\n",
      "(Iteration 1601 / 30560) loss: 2.306153\n",
      "(Iteration 1701 / 30560) loss: 2.306103\n",
      "(Iteration 1801 / 30560) loss: 2.306089\n",
      "(Iteration 1901 / 30560) loss: 2.306044\n",
      "(Epoch 5 / 80) train acc: 0.093000; val_acc: 0.083000\n",
      "(Iteration 2001 / 30560) loss: 2.306060\n",
      "(Iteration 2101 / 30560) loss: 2.306019\n",
      "(Iteration 2201 / 30560) loss: 2.306013\n",
      "(Epoch 6 / 80) train acc: 0.098000; val_acc: 0.084000\n",
      "(Iteration 2301 / 30560) loss: 2.305973\n",
      "(Iteration 2401 / 30560) loss: 2.305972\n",
      "(Iteration 2501 / 30560) loss: 2.305931\n",
      "(Iteration 2601 / 30560) loss: 2.305964\n",
      "(Epoch 7 / 80) train acc: 0.106000; val_acc: 0.082000\n",
      "(Iteration 2701 / 30560) loss: 2.305915\n",
      "(Iteration 2801 / 30560) loss: 2.305889\n",
      "(Iteration 2901 / 30560) loss: 2.305869\n",
      "(Iteration 3001 / 30560) loss: 2.305862\n",
      "(Epoch 8 / 80) train acc: 0.118000; val_acc: 0.087000\n",
      "(Iteration 3101 / 30560) loss: 2.305854\n",
      "(Iteration 3201 / 30560) loss: 2.305850\n",
      "(Iteration 3301 / 30560) loss: 2.305817\n",
      "(Iteration 3401 / 30560) loss: 2.305821\n",
      "(Epoch 9 / 80) train acc: 0.097000; val_acc: 0.085000\n",
      "(Iteration 3501 / 30560) loss: 2.305795\n",
      "(Iteration 3601 / 30560) loss: 2.305791\n",
      "(Iteration 3701 / 30560) loss: 2.305760\n",
      "(Iteration 3801 / 30560) loss: 2.305760\n",
      "(Epoch 10 / 80) train acc: 0.113000; val_acc: 0.085000\n",
      "(Iteration 3901 / 30560) loss: 2.305764\n",
      "(Iteration 4001 / 30560) loss: 2.305739\n",
      "(Iteration 4101 / 30560) loss: 2.305706\n",
      "(Iteration 4201 / 30560) loss: 2.305723\n",
      "(Epoch 11 / 80) train acc: 0.093000; val_acc: 0.084000\n",
      "(Iteration 4301 / 30560) loss: 2.305721\n",
      "(Iteration 4401 / 30560) loss: 2.305703\n",
      "(Iteration 4501 / 30560) loss: 2.305656\n",
      "(Epoch 12 / 80) train acc: 0.118000; val_acc: 0.085000\n",
      "(Iteration 4601 / 30560) loss: 2.305677\n",
      "(Iteration 4701 / 30560) loss: 2.305676\n",
      "(Iteration 4801 / 30560) loss: 2.305680\n",
      "(Iteration 4901 / 30560) loss: 2.305668\n",
      "(Epoch 13 / 80) train acc: 0.100000; val_acc: 0.083000\n",
      "(Iteration 5001 / 30560) loss: 2.305630\n",
      "(Iteration 5101 / 30560) loss: 2.305612\n",
      "(Iteration 5201 / 30560) loss: 2.305609\n",
      "(Iteration 5301 / 30560) loss: 2.305630\n",
      "(Epoch 14 / 80) train acc: 0.110000; val_acc: 0.083000\n",
      "(Iteration 5401 / 30560) loss: 2.305616\n",
      "(Iteration 5501 / 30560) loss: 2.305593\n",
      "(Iteration 5601 / 30560) loss: 2.305576\n",
      "(Iteration 5701 / 30560) loss: 2.305563\n",
      "(Epoch 15 / 80) train acc: 0.090000; val_acc: 0.083000\n",
      "(Iteration 5801 / 30560) loss: 2.305592\n",
      "(Iteration 5901 / 30560) loss: 2.305580\n",
      "(Iteration 6001 / 30560) loss: 2.305554\n",
      "(Iteration 6101 / 30560) loss: 2.305557\n",
      "(Epoch 16 / 80) train acc: 0.120000; val_acc: 0.085000\n",
      "(Iteration 6201 / 30560) loss: 2.305566\n",
      "(Iteration 6301 / 30560) loss: 2.305545\n",
      "(Iteration 6401 / 30560) loss: 2.305562\n",
      "(Epoch 17 / 80) train acc: 0.097000; val_acc: 0.083000\n",
      "(Iteration 6501 / 30560) loss: 2.305553\n",
      "(Iteration 6601 / 30560) loss: 2.305537\n",
      "(Iteration 6701 / 30560) loss: 2.305509\n",
      "(Iteration 6801 / 30560) loss: 2.305503\n",
      "(Epoch 18 / 80) train acc: 0.099000; val_acc: 0.084000\n",
      "(Iteration 6901 / 30560) loss: 2.305520\n",
      "(Iteration 7001 / 30560) loss: 2.305513\n",
      "(Iteration 7101 / 30560) loss: 2.305509\n",
      "(Iteration 7201 / 30560) loss: 2.305523\n",
      "(Epoch 19 / 80) train acc: 0.116000; val_acc: 0.087000\n",
      "(Iteration 7301 / 30560) loss: 2.305500\n",
      "(Iteration 7401 / 30560) loss: 2.305491\n",
      "(Iteration 7501 / 30560) loss: 2.305519\n",
      "(Iteration 7601 / 30560) loss: 2.305474\n",
      "(Epoch 20 / 80) train acc: 0.096000; val_acc: 0.086000\n",
      "(Iteration 7701 / 30560) loss: 2.305502\n",
      "(Iteration 7801 / 30560) loss: 2.305471\n",
      "(Iteration 7901 / 30560) loss: 2.305465\n",
      "(Iteration 8001 / 30560) loss: 2.305473\n",
      "(Epoch 21 / 80) train acc: 0.084000; val_acc: 0.087000\n",
      "(Iteration 8101 / 30560) loss: 2.305504\n",
      "(Iteration 8201 / 30560) loss: 2.305510\n",
      "(Iteration 8301 / 30560) loss: 2.305494\n",
      "(Iteration 8401 / 30560) loss: 2.305465\n",
      "(Epoch 22 / 80) train acc: 0.093000; val_acc: 0.085000\n",
      "(Iteration 8501 / 30560) loss: 2.305460\n",
      "(Iteration 8601 / 30560) loss: 2.305471\n",
      "(Iteration 8701 / 30560) loss: 2.305440\n",
      "(Epoch 23 / 80) train acc: 0.126000; val_acc: 0.086000\n",
      "(Iteration 8801 / 30560) loss: 2.305441\n",
      "(Iteration 8901 / 30560) loss: 2.305447\n",
      "(Iteration 9001 / 30560) loss: 2.305434\n",
      "(Iteration 9101 / 30560) loss: 2.305470\n",
      "(Epoch 24 / 80) train acc: 0.105000; val_acc: 0.086000\n",
      "(Iteration 9201 / 30560) loss: 2.305473\n",
      "(Iteration 9301 / 30560) loss: 2.305457\n",
      "(Iteration 9401 / 30560) loss: 2.305453\n",
      "(Iteration 9501 / 30560) loss: 2.305431\n",
      "(Epoch 25 / 80) train acc: 0.092000; val_acc: 0.085000\n",
      "(Iteration 9601 / 30560) loss: 2.305443\n",
      "(Iteration 9701 / 30560) loss: 2.305431\n",
      "(Iteration 9801 / 30560) loss: 2.305440\n",
      "(Iteration 9901 / 30560) loss: 2.305427\n",
      "(Epoch 26 / 80) train acc: 0.091000; val_acc: 0.084000\n",
      "(Iteration 10001 / 30560) loss: 2.305419\n",
      "(Iteration 10101 / 30560) loss: 2.305406\n",
      "(Iteration 10201 / 30560) loss: 2.305436\n",
      "(Iteration 10301 / 30560) loss: 2.305392\n",
      "(Epoch 27 / 80) train acc: 0.098000; val_acc: 0.085000\n",
      "(Iteration 10401 / 30560) loss: 2.305432\n",
      "(Iteration 10501 / 30560) loss: 2.305417\n",
      "(Iteration 10601 / 30560) loss: 2.305425\n",
      "(Epoch 28 / 80) train acc: 0.113000; val_acc: 0.086000\n",
      "(Iteration 10701 / 30560) loss: 2.305408\n",
      "(Iteration 10801 / 30560) loss: 2.305422\n",
      "(Iteration 10901 / 30560) loss: 2.305432\n",
      "(Iteration 11001 / 30560) loss: 2.305407\n",
      "(Epoch 29 / 80) train acc: 0.105000; val_acc: 0.084000\n",
      "(Iteration 11101 / 30560) loss: 2.305430\n",
      "(Iteration 11201 / 30560) loss: 2.305414\n",
      "(Iteration 11301 / 30560) loss: 2.305402\n",
      "(Iteration 11401 / 30560) loss: 2.305402\n",
      "(Epoch 30 / 80) train acc: 0.107000; val_acc: 0.084000\n",
      "(Iteration 11501 / 30560) loss: 2.305389\n",
      "(Iteration 11601 / 30560) loss: 2.305401\n",
      "(Iteration 11701 / 30560) loss: 2.305399\n",
      "(Iteration 11801 / 30560) loss: 2.305404\n",
      "(Epoch 31 / 80) train acc: 0.119000; val_acc: 0.084000\n",
      "(Iteration 11901 / 30560) loss: 2.305425\n",
      "(Iteration 12001 / 30560) loss: 2.305417\n",
      "(Iteration 12101 / 30560) loss: 2.305376\n",
      "(Iteration 12201 / 30560) loss: 2.305370\n",
      "(Epoch 32 / 80) train acc: 0.102000; val_acc: 0.084000\n",
      "(Iteration 12301 / 30560) loss: 2.305418\n",
      "(Iteration 12401 / 30560) loss: 2.305393\n",
      "(Iteration 12501 / 30560) loss: 2.305395\n",
      "(Iteration 12601 / 30560) loss: 2.305382\n",
      "(Epoch 33 / 80) train acc: 0.099000; val_acc: 0.084000\n",
      "(Iteration 12701 / 30560) loss: 2.305387\n",
      "(Iteration 12801 / 30560) loss: 2.305396\n",
      "(Iteration 12901 / 30560) loss: 2.305404\n",
      "(Epoch 34 / 80) train acc: 0.107000; val_acc: 0.086000\n",
      "(Iteration 13001 / 30560) loss: 2.305382\n",
      "(Iteration 13101 / 30560) loss: 2.305396\n",
      "(Iteration 13201 / 30560) loss: 2.305376\n",
      "(Iteration 13301 / 30560) loss: 2.305376\n",
      "(Epoch 35 / 80) train acc: 0.106000; val_acc: 0.086000\n",
      "(Iteration 13401 / 30560) loss: 2.305369\n",
      "(Iteration 13501 / 30560) loss: 2.305389\n",
      "(Iteration 13601 / 30560) loss: 2.305390\n",
      "(Iteration 13701 / 30560) loss: 2.305402\n",
      "(Epoch 36 / 80) train acc: 0.094000; val_acc: 0.084000\n",
      "(Iteration 13801 / 30560) loss: 2.305386\n",
      "(Iteration 13901 / 30560) loss: 2.305394\n",
      "(Iteration 14001 / 30560) loss: 2.305380\n",
      "(Iteration 14101 / 30560) loss: 2.305373\n",
      "(Epoch 37 / 80) train acc: 0.119000; val_acc: 0.084000\n",
      "(Iteration 14201 / 30560) loss: 2.305380\n",
      "(Iteration 14301 / 30560) loss: 2.305376\n",
      "(Iteration 14401 / 30560) loss: 2.305361\n",
      "(Iteration 14501 / 30560) loss: 2.305370\n",
      "(Epoch 38 / 80) train acc: 0.110000; val_acc: 0.084000\n",
      "(Iteration 14601 / 30560) loss: 2.305360\n",
      "(Iteration 14701 / 30560) loss: 2.305395\n",
      "(Iteration 14801 / 30560) loss: 2.305364\n",
      "(Epoch 39 / 80) train acc: 0.096000; val_acc: 0.085000\n",
      "(Iteration 14901 / 30560) loss: 2.305407\n",
      "(Iteration 15001 / 30560) loss: 2.305382\n",
      "(Iteration 15101 / 30560) loss: 2.305354\n",
      "(Iteration 15201 / 30560) loss: 2.305357\n",
      "(Epoch 40 / 80) train acc: 0.102000; val_acc: 0.085000\n",
      "(Iteration 15301 / 30560) loss: 2.305393\n",
      "(Iteration 15401 / 30560) loss: 2.305373\n",
      "(Iteration 15501 / 30560) loss: 2.305367\n",
      "(Iteration 15601 / 30560) loss: 2.305368\n",
      "(Epoch 41 / 80) train acc: 0.106000; val_acc: 0.086000\n",
      "(Iteration 15701 / 30560) loss: 2.305368\n",
      "(Iteration 15801 / 30560) loss: 2.305367\n",
      "(Iteration 15901 / 30560) loss: 2.305370\n",
      "(Iteration 16001 / 30560) loss: 2.305350\n",
      "(Epoch 42 / 80) train acc: 0.100000; val_acc: 0.086000\n",
      "(Iteration 16101 / 30560) loss: 2.305372\n",
      "(Iteration 16201 / 30560) loss: 2.305358\n",
      "(Iteration 16301 / 30560) loss: 2.305348\n",
      "(Iteration 16401 / 30560) loss: 2.305378\n",
      "(Epoch 43 / 80) train acc: 0.081000; val_acc: 0.086000\n",
      "(Iteration 16501 / 30560) loss: 2.305370\n",
      "(Iteration 16601 / 30560) loss: 2.305371\n",
      "(Iteration 16701 / 30560) loss: 2.305376\n",
      "(Iteration 16801 / 30560) loss: 2.305369\n",
      "(Epoch 44 / 80) train acc: 0.105000; val_acc: 0.086000\n",
      "(Iteration 16901 / 30560) loss: 2.305396\n",
      "(Iteration 17001 / 30560) loss: 2.305378\n",
      "(Iteration 17101 / 30560) loss: 2.305363\n",
      "(Epoch 45 / 80) train acc: 0.101000; val_acc: 0.086000\n",
      "(Iteration 17201 / 30560) loss: 2.305356\n",
      "(Iteration 17301 / 30560) loss: 2.305375\n",
      "(Iteration 17401 / 30560) loss: 2.305351\n",
      "(Iteration 17501 / 30560) loss: 2.305382\n",
      "(Epoch 46 / 80) train acc: 0.106000; val_acc: 0.086000\n",
      "(Iteration 17601 / 30560) loss: 2.305394\n",
      "(Iteration 17701 / 30560) loss: 2.305365\n",
      "(Iteration 17801 / 30560) loss: 2.305366\n",
      "(Iteration 17901 / 30560) loss: 2.305343\n",
      "(Epoch 47 / 80) train acc: 0.113000; val_acc: 0.086000\n",
      "(Iteration 18001 / 30560) loss: 2.305370\n",
      "(Iteration 18101 / 30560) loss: 2.305384\n",
      "(Iteration 18201 / 30560) loss: 2.305376\n",
      "(Iteration 18301 / 30560) loss: 2.305379\n",
      "(Epoch 48 / 80) train acc: 0.098000; val_acc: 0.086000\n",
      "(Iteration 18401 / 30560) loss: 2.305371\n",
      "(Iteration 18501 / 30560) loss: 2.305361\n",
      "(Iteration 18601 / 30560) loss: 2.305372\n",
      "(Iteration 18701 / 30560) loss: 2.305365\n",
      "(Epoch 49 / 80) train acc: 0.101000; val_acc: 0.086000\n",
      "(Iteration 18801 / 30560) loss: 2.305373\n",
      "(Iteration 18901 / 30560) loss: 2.305348\n",
      "(Iteration 19001 / 30560) loss: 2.305335\n",
      "(Epoch 50 / 80) train acc: 0.093000; val_acc: 0.086000\n",
      "(Iteration 19101 / 30560) loss: 2.305358\n",
      "(Iteration 19201 / 30560) loss: 2.305350\n",
      "(Iteration 19301 / 30560) loss: 2.305382\n",
      "(Iteration 19401 / 30560) loss: 2.305354\n",
      "(Epoch 51 / 80) train acc: 0.106000; val_acc: 0.086000\n",
      "(Iteration 19501 / 30560) loss: 2.305365\n",
      "(Iteration 19601 / 30560) loss: 2.305378\n",
      "(Iteration 19701 / 30560) loss: 2.305381\n",
      "(Iteration 19801 / 30560) loss: 2.305381\n",
      "(Epoch 52 / 80) train acc: 0.105000; val_acc: 0.086000\n",
      "(Iteration 19901 / 30560) loss: 2.305393\n",
      "(Iteration 20001 / 30560) loss: 2.305361\n",
      "(Iteration 20101 / 30560) loss: 2.305346\n",
      "(Iteration 20201 / 30560) loss: 2.305381\n",
      "(Epoch 53 / 80) train acc: 0.101000; val_acc: 0.086000\n",
      "(Iteration 20301 / 30560) loss: 2.305360\n",
      "(Iteration 20401 / 30560) loss: 2.305357\n",
      "(Iteration 20501 / 30560) loss: 2.305361\n",
      "(Iteration 20601 / 30560) loss: 2.305368\n",
      "(Epoch 54 / 80) train acc: 0.111000; val_acc: 0.086000\n",
      "(Iteration 20701 / 30560) loss: 2.305331\n",
      "(Iteration 20801 / 30560) loss: 2.305347\n",
      "(Iteration 20901 / 30560) loss: 2.305358\n",
      "(Iteration 21001 / 30560) loss: 2.305354\n",
      "(Epoch 55 / 80) train acc: 0.103000; val_acc: 0.086000\n",
      "(Iteration 21101 / 30560) loss: 2.305339\n",
      "(Iteration 21201 / 30560) loss: 2.305375\n",
      "(Iteration 21301 / 30560) loss: 2.305353\n",
      "(Epoch 56 / 80) train acc: 0.111000; val_acc: 0.086000\n",
      "(Iteration 21401 / 30560) loss: 2.305352\n",
      "(Iteration 21501 / 30560) loss: 2.305362\n",
      "(Iteration 21601 / 30560) loss: 2.305368\n",
      "(Iteration 21701 / 30560) loss: 2.305354\n",
      "(Epoch 57 / 80) train acc: 0.118000; val_acc: 0.086000\n",
      "(Iteration 21801 / 30560) loss: 2.305350\n",
      "(Iteration 21901 / 30560) loss: 2.305382\n",
      "(Iteration 22001 / 30560) loss: 2.305367\n",
      "(Iteration 22101 / 30560) loss: 2.305351\n",
      "(Epoch 58 / 80) train acc: 0.116000; val_acc: 0.086000\n",
      "(Iteration 22201 / 30560) loss: 2.305357\n",
      "(Iteration 22301 / 30560) loss: 2.305368\n",
      "(Iteration 22401 / 30560) loss: 2.305343\n",
      "(Iteration 22501 / 30560) loss: 2.305330\n",
      "(Epoch 59 / 80) train acc: 0.131000; val_acc: 0.086000\n",
      "(Iteration 22601 / 30560) loss: 2.305343\n",
      "(Iteration 22701 / 30560) loss: 2.305346\n",
      "(Iteration 22801 / 30560) loss: 2.305374\n",
      "(Iteration 22901 / 30560) loss: 2.305368\n",
      "(Epoch 60 / 80) train acc: 0.101000; val_acc: 0.086000\n",
      "(Iteration 23001 / 30560) loss: 2.305360\n",
      "(Iteration 23101 / 30560) loss: 2.305338\n",
      "(Iteration 23201 / 30560) loss: 2.305344\n",
      "(Iteration 23301 / 30560) loss: 2.305364\n",
      "(Epoch 61 / 80) train acc: 0.090000; val_acc: 0.086000\n",
      "(Iteration 23401 / 30560) loss: 2.305363\n",
      "(Iteration 23501 / 30560) loss: 2.305378\n",
      "(Iteration 23601 / 30560) loss: 2.305360\n",
      "(Epoch 62 / 80) train acc: 0.115000; val_acc: 0.086000\n",
      "(Iteration 23701 / 30560) loss: 2.305342\n",
      "(Iteration 23801 / 30560) loss: 2.305366\n",
      "(Iteration 23901 / 30560) loss: 2.305368\n",
      "(Iteration 24001 / 30560) loss: 2.305373\n",
      "(Epoch 63 / 80) train acc: 0.101000; val_acc: 0.086000\n",
      "(Iteration 24101 / 30560) loss: 2.305355\n",
      "(Iteration 24201 / 30560) loss: 2.305404\n",
      "(Iteration 24301 / 30560) loss: 2.305370\n",
      "(Iteration 24401 / 30560) loss: 2.305345\n",
      "(Epoch 64 / 80) train acc: 0.114000; val_acc: 0.086000\n",
      "(Iteration 24501 / 30560) loss: 2.305333\n",
      "(Iteration 24601 / 30560) loss: 2.305369\n",
      "(Iteration 24701 / 30560) loss: 2.305369\n",
      "(Iteration 24801 / 30560) loss: 2.305348\n",
      "(Epoch 65 / 80) train acc: 0.114000; val_acc: 0.086000\n",
      "(Iteration 24901 / 30560) loss: 2.305357\n",
      "(Iteration 25001 / 30560) loss: 2.305382\n",
      "(Iteration 25101 / 30560) loss: 2.305369\n",
      "(Iteration 25201 / 30560) loss: 2.305347\n",
      "(Epoch 66 / 80) train acc: 0.080000; val_acc: 0.086000\n",
      "(Iteration 25301 / 30560) loss: 2.305366\n",
      "(Iteration 25401 / 30560) loss: 2.305353\n",
      "(Iteration 25501 / 30560) loss: 2.305394\n",
      "(Epoch 67 / 80) train acc: 0.103000; val_acc: 0.086000\n",
      "(Iteration 25601 / 30560) loss: 2.305387\n",
      "(Iteration 25701 / 30560) loss: 2.305336\n",
      "(Iteration 25801 / 30560) loss: 2.305344\n",
      "(Iteration 25901 / 30560) loss: 2.305350\n",
      "(Epoch 68 / 80) train acc: 0.093000; val_acc: 0.086000\n",
      "(Iteration 26001 / 30560) loss: 2.305332\n",
      "(Iteration 26101 / 30560) loss: 2.305364\n",
      "(Iteration 26201 / 30560) loss: 2.305359\n",
      "(Iteration 26301 / 30560) loss: 2.305369\n",
      "(Epoch 69 / 80) train acc: 0.110000; val_acc: 0.086000\n",
      "(Iteration 26401 / 30560) loss: 2.305327\n",
      "(Iteration 26501 / 30560) loss: 2.305368\n",
      "(Iteration 26601 / 30560) loss: 2.305364\n",
      "(Iteration 26701 / 30560) loss: 2.305354\n",
      "(Epoch 70 / 80) train acc: 0.123000; val_acc: 0.086000\n",
      "(Iteration 26801 / 30560) loss: 2.305353\n",
      "(Iteration 26901 / 30560) loss: 2.305362\n",
      "(Iteration 27001 / 30560) loss: 2.305350\n",
      "(Iteration 27101 / 30560) loss: 2.305330\n",
      "(Epoch 71 / 80) train acc: 0.098000; val_acc: 0.086000\n",
      "(Iteration 27201 / 30560) loss: 2.305362\n",
      "(Iteration 27301 / 30560) loss: 2.305383\n",
      "(Iteration 27401 / 30560) loss: 2.305340\n",
      "(Iteration 27501 / 30560) loss: 2.305360\n",
      "(Epoch 72 / 80) train acc: 0.106000; val_acc: 0.086000\n",
      "(Iteration 27601 / 30560) loss: 2.305359\n",
      "(Iteration 27701 / 30560) loss: 2.305355\n",
      "(Iteration 27801 / 30560) loss: 2.305353\n",
      "(Epoch 73 / 80) train acc: 0.111000; val_acc: 0.086000\n",
      "(Iteration 27901 / 30560) loss: 2.305355\n",
      "(Iteration 28001 / 30560) loss: 2.305331\n",
      "(Iteration 28101 / 30560) loss: 2.305358\n",
      "(Iteration 28201 / 30560) loss: 2.305380\n",
      "(Epoch 74 / 80) train acc: 0.104000; val_acc: 0.086000\n",
      "(Iteration 28301 / 30560) loss: 2.305363\n",
      "(Iteration 28401 / 30560) loss: 2.305359\n",
      "(Iteration 28501 / 30560) loss: 2.305349\n",
      "(Iteration 28601 / 30560) loss: 2.305361\n",
      "(Epoch 75 / 80) train acc: 0.099000; val_acc: 0.086000\n",
      "(Iteration 28701 / 30560) loss: 2.305369\n",
      "(Iteration 28801 / 30560) loss: 2.305357\n",
      "(Iteration 28901 / 30560) loss: 2.305358\n",
      "(Iteration 29001 / 30560) loss: 2.305367\n",
      "(Epoch 76 / 80) train acc: 0.101000; val_acc: 0.086000\n",
      "(Iteration 29101 / 30560) loss: 2.305347\n",
      "(Iteration 29201 / 30560) loss: 2.305368\n",
      "(Iteration 29301 / 30560) loss: 2.305351\n",
      "(Iteration 29401 / 30560) loss: 2.305346\n",
      "(Epoch 77 / 80) train acc: 0.098000; val_acc: 0.086000\n",
      "(Iteration 29501 / 30560) loss: 2.305333\n",
      "(Iteration 29601 / 30560) loss: 2.305341\n",
      "(Iteration 29701 / 30560) loss: 2.305361\n",
      "(Epoch 78 / 80) train acc: 0.099000; val_acc: 0.086000\n",
      "(Iteration 29801 / 30560) loss: 2.305343\n",
      "(Iteration 29901 / 30560) loss: 2.305331\n",
      "(Iteration 30001 / 30560) loss: 2.305351\n",
      "(Iteration 30101 / 30560) loss: 2.305372\n",
      "(Epoch 79 / 80) train acc: 0.110000; val_acc: 0.086000\n",
      "(Iteration 30201 / 30560) loss: 2.305360\n",
      "(Iteration 30301 / 30560) loss: 2.305341\n",
      "(Iteration 30401 / 30560) loss: 2.305353\n",
      "(Iteration 30501 / 30560) loss: 2.305366\n",
      "(Epoch 80 / 80) train acc: 0.098000; val_acc: 0.086000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.306734\n",
      "(Epoch 0 / 80) train acc: 0.094000; val_acc: 0.090000\n",
      "(Iteration 101 / 61200) loss: 2.306691\n",
      "(Iteration 201 / 61200) loss: 2.306659\n",
      "(Iteration 301 / 61200) loss: 2.306640\n",
      "(Iteration 401 / 61200) loss: 2.306587\n",
      "(Iteration 501 / 61200) loss: 2.306553\n",
      "(Iteration 601 / 61200) loss: 2.306499\n",
      "(Iteration 701 / 61200) loss: 2.306459\n",
      "(Epoch 1 / 80) train acc: 0.105000; val_acc: 0.093000\n",
      "(Iteration 801 / 61200) loss: 2.306421\n",
      "(Iteration 901 / 61200) loss: 2.306375\n",
      "(Iteration 1001 / 61200) loss: 2.306347\n",
      "(Iteration 1101 / 61200) loss: 2.306311\n",
      "(Iteration 1201 / 61200) loss: 2.306288\n",
      "(Iteration 1301 / 61200) loss: 2.306190\n",
      "(Iteration 1401 / 61200) loss: 2.306215\n",
      "(Iteration 1501 / 61200) loss: 2.306162\n",
      "(Epoch 2 / 80) train acc: 0.132000; val_acc: 0.123000\n",
      "(Iteration 1601 / 61200) loss: 2.306136\n",
      "(Iteration 1701 / 61200) loss: 2.306066\n",
      "(Iteration 1801 / 61200) loss: 2.306045\n",
      "(Iteration 1901 / 61200) loss: 2.305995\n",
      "(Iteration 2001 / 61200) loss: 2.306084\n",
      "(Iteration 2101 / 61200) loss: 2.305930\n",
      "(Iteration 2201 / 61200) loss: 2.305974\n",
      "(Epoch 3 / 80) train acc: 0.105000; val_acc: 0.112000\n",
      "(Iteration 2301 / 61200) loss: 2.305902\n",
      "(Iteration 2401 / 61200) loss: 2.305925\n",
      "(Iteration 2501 / 61200) loss: 2.305893\n",
      "(Iteration 2601 / 61200) loss: 2.305879\n",
      "(Iteration 2701 / 61200) loss: 2.305807\n",
      "(Iteration 2801 / 61200) loss: 2.305784\n",
      "(Iteration 2901 / 61200) loss: 2.305773\n",
      "(Iteration 3001 / 61200) loss: 2.305754\n",
      "(Epoch 4 / 80) train acc: 0.133000; val_acc: 0.124000\n",
      "(Iteration 3101 / 61200) loss: 2.305704\n",
      "(Iteration 3201 / 61200) loss: 2.305640\n",
      "(Iteration 3301 / 61200) loss: 2.305674\n",
      "(Iteration 3401 / 61200) loss: 2.305603\n",
      "(Iteration 3501 / 61200) loss: 2.305522\n",
      "(Iteration 3601 / 61200) loss: 2.305549\n",
      "(Iteration 3701 / 61200) loss: 2.305532\n",
      "(Iteration 3801 / 61200) loss: 2.305513\n",
      "(Epoch 5 / 80) train acc: 0.118000; val_acc: 0.095000\n",
      "(Iteration 3901 / 61200) loss: 2.305476\n",
      "(Iteration 4001 / 61200) loss: 2.305473\n",
      "(Iteration 4101 / 61200) loss: 2.305482\n",
      "(Iteration 4201 / 61200) loss: 2.305401\n",
      "(Iteration 4301 / 61200) loss: 2.305402\n",
      "(Iteration 4401 / 61200) loss: 2.305344\n",
      "(Iteration 4501 / 61200) loss: 2.305352\n",
      "(Epoch 6 / 80) train acc: 0.153000; val_acc: 0.126000\n",
      "(Iteration 4601 / 61200) loss: 2.305256\n",
      "(Iteration 4701 / 61200) loss: 2.305329\n",
      "(Iteration 4801 / 61200) loss: 2.305201\n",
      "(Iteration 4901 / 61200) loss: 2.305246\n",
      "(Iteration 5001 / 61200) loss: 2.305259\n",
      "(Iteration 5101 / 61200) loss: 2.305167\n",
      "(Iteration 5201 / 61200) loss: 2.305200\n",
      "(Iteration 5301 / 61200) loss: 2.305165\n",
      "(Epoch 7 / 80) train acc: 0.110000; val_acc: 0.102000\n",
      "(Iteration 5401 / 61200) loss: 2.305147\n",
      "(Iteration 5501 / 61200) loss: 2.305091\n",
      "(Iteration 5601 / 61200) loss: 2.305164\n",
      "(Iteration 5701 / 61200) loss: 2.305182\n",
      "(Iteration 5801 / 61200) loss: 2.305138\n",
      "(Iteration 5901 / 61200) loss: 2.305101\n",
      "(Iteration 6001 / 61200) loss: 2.305012\n",
      "(Iteration 6101 / 61200) loss: 2.305054\n",
      "(Epoch 8 / 80) train acc: 0.100000; val_acc: 0.102000\n",
      "(Iteration 6201 / 61200) loss: 2.305030\n",
      "(Iteration 6301 / 61200) loss: 2.305014\n",
      "(Iteration 6401 / 61200) loss: 2.304980\n",
      "(Iteration 6501 / 61200) loss: 2.304919\n",
      "(Iteration 6601 / 61200) loss: 2.304892\n",
      "(Iteration 6701 / 61200) loss: 2.304965\n",
      "(Iteration 6801 / 61200) loss: 2.304918\n",
      "(Epoch 9 / 80) train acc: 0.086000; val_acc: 0.102000\n",
      "(Iteration 6901 / 61200) loss: 2.305035\n",
      "(Iteration 7001 / 61200) loss: 2.304897\n",
      "(Iteration 7101 / 61200) loss: 2.304895\n",
      "(Iteration 7201 / 61200) loss: 2.304894\n",
      "(Iteration 7301 / 61200) loss: 2.304906\n",
      "(Iteration 7401 / 61200) loss: 2.304858\n",
      "(Iteration 7501 / 61200) loss: 2.304898\n",
      "(Iteration 7601 / 61200) loss: 2.304845\n",
      "(Epoch 10 / 80) train acc: 0.091000; val_acc: 0.102000\n",
      "(Iteration 7701 / 61200) loss: 2.304742\n",
      "(Iteration 7801 / 61200) loss: 2.304773\n",
      "(Iteration 7901 / 61200) loss: 2.304864\n",
      "(Iteration 8001 / 61200) loss: 2.304765\n",
      "(Iteration 8101 / 61200) loss: 2.304781\n",
      "(Iteration 8201 / 61200) loss: 2.304823\n",
      "(Iteration 8301 / 61200) loss: 2.304756\n",
      "(Iteration 8401 / 61200) loss: 2.304667\n",
      "(Epoch 11 / 80) train acc: 0.097000; val_acc: 0.102000\n",
      "(Iteration 8501 / 61200) loss: 2.304632\n",
      "(Iteration 8601 / 61200) loss: 2.304642\n",
      "(Iteration 8701 / 61200) loss: 2.304720\n",
      "(Iteration 8801 / 61200) loss: 2.304723\n",
      "(Iteration 8901 / 61200) loss: 2.304623\n",
      "(Iteration 9001 / 61200) loss: 2.304626\n",
      "(Iteration 9101 / 61200) loss: 2.304601\n",
      "(Epoch 12 / 80) train acc: 0.104000; val_acc: 0.102000\n",
      "(Iteration 9201 / 61200) loss: 2.304700\n",
      "(Iteration 9301 / 61200) loss: 2.304608\n",
      "(Iteration 9401 / 61200) loss: 2.304601\n",
      "(Iteration 9501 / 61200) loss: 2.304648\n",
      "(Iteration 9601 / 61200) loss: 2.304643\n",
      "(Iteration 9701 / 61200) loss: 2.304543\n",
      "(Iteration 9801 / 61200) loss: 2.304552\n",
      "(Iteration 9901 / 61200) loss: 2.304486\n",
      "(Epoch 13 / 80) train acc: 0.101000; val_acc: 0.102000\n",
      "(Iteration 10001 / 61200) loss: 2.304526\n",
      "(Iteration 10101 / 61200) loss: 2.304583\n",
      "(Iteration 10201 / 61200) loss: 2.304611\n",
      "(Iteration 10301 / 61200) loss: 2.304561\n",
      "(Iteration 10401 / 61200) loss: 2.304504\n",
      "(Iteration 10501 / 61200) loss: 2.304473\n",
      "(Iteration 10601 / 61200) loss: 2.304457\n",
      "(Iteration 10701 / 61200) loss: 2.304460\n",
      "(Epoch 14 / 80) train acc: 0.101000; val_acc: 0.102000\n",
      "(Iteration 10801 / 61200) loss: 2.304461\n",
      "(Iteration 10901 / 61200) loss: 2.304448\n",
      "(Iteration 11001 / 61200) loss: 2.304398\n",
      "(Iteration 11101 / 61200) loss: 2.304409\n",
      "(Iteration 11201 / 61200) loss: 2.304477\n",
      "(Iteration 11301 / 61200) loss: 2.304373\n",
      "(Iteration 11401 / 61200) loss: 2.304424\n",
      "(Epoch 15 / 80) train acc: 0.109000; val_acc: 0.102000\n",
      "(Iteration 11501 / 61200) loss: 2.304423\n",
      "(Iteration 11601 / 61200) loss: 2.304418\n",
      "(Iteration 11701 / 61200) loss: 2.304341\n",
      "(Iteration 11801 / 61200) loss: 2.304298\n",
      "(Iteration 11901 / 61200) loss: 2.304367\n",
      "(Iteration 12001 / 61200) loss: 2.304366\n",
      "(Iteration 12101 / 61200) loss: 2.304370\n",
      "(Iteration 12201 / 61200) loss: 2.304316\n",
      "(Epoch 16 / 80) train acc: 0.104000; val_acc: 0.102000\n",
      "(Iteration 12301 / 61200) loss: 2.304318\n",
      "(Iteration 12401 / 61200) loss: 2.304348\n",
      "(Iteration 12501 / 61200) loss: 2.304346\n",
      "(Iteration 12601 / 61200) loss: 2.304294\n",
      "(Iteration 12701 / 61200) loss: 2.304307\n",
      "(Iteration 12801 / 61200) loss: 2.304249\n",
      "(Iteration 12901 / 61200) loss: 2.304345\n",
      "(Iteration 13001 / 61200) loss: 2.304306\n",
      "(Epoch 17 / 80) train acc: 0.103000; val_acc: 0.102000\n",
      "(Iteration 13101 / 61200) loss: 2.304253\n",
      "(Iteration 13201 / 61200) loss: 2.304253\n",
      "(Iteration 13301 / 61200) loss: 2.304280\n",
      "(Iteration 13401 / 61200) loss: 2.304229\n",
      "(Iteration 13501 / 61200) loss: 2.304184\n",
      "(Iteration 13601 / 61200) loss: 2.304246\n",
      "(Iteration 13701 / 61200) loss: 2.304274\n",
      "(Epoch 18 / 80) train acc: 0.097000; val_acc: 0.102000\n",
      "(Iteration 13801 / 61200) loss: 2.304300\n",
      "(Iteration 13901 / 61200) loss: 2.304195\n",
      "(Iteration 14001 / 61200) loss: 2.304160\n",
      "(Iteration 14101 / 61200) loss: 2.304272\n",
      "(Iteration 14201 / 61200) loss: 2.304227\n",
      "(Iteration 14301 / 61200) loss: 2.304284\n",
      "(Iteration 14401 / 61200) loss: 2.304126\n",
      "(Iteration 14501 / 61200) loss: 2.304196\n",
      "(Epoch 19 / 80) train acc: 0.098000; val_acc: 0.102000\n",
      "(Iteration 14601 / 61200) loss: 2.304220\n",
      "(Iteration 14701 / 61200) loss: 2.304201\n",
      "(Iteration 14801 / 61200) loss: 2.304145\n",
      "(Iteration 14901 / 61200) loss: 2.304196\n",
      "(Iteration 15001 / 61200) loss: 2.304168\n",
      "(Iteration 15101 / 61200) loss: 2.304169\n",
      "(Iteration 15201 / 61200) loss: 2.304137\n",
      "(Epoch 20 / 80) train acc: 0.099000; val_acc: 0.102000\n",
      "(Iteration 15301 / 61200) loss: 2.304192\n",
      "(Iteration 15401 / 61200) loss: 2.304192\n",
      "(Iteration 15501 / 61200) loss: 2.304088\n",
      "(Iteration 15601 / 61200) loss: 2.304150\n",
      "(Iteration 15701 / 61200) loss: 2.304178\n",
      "(Iteration 15801 / 61200) loss: 2.304046\n",
      "(Iteration 15901 / 61200) loss: 2.304033\n",
      "(Iteration 16001 / 61200) loss: 2.304140\n",
      "(Epoch 21 / 80) train acc: 0.096000; val_acc: 0.102000\n",
      "(Iteration 16101 / 61200) loss: 2.304043\n",
      "(Iteration 16201 / 61200) loss: 2.304077\n",
      "(Iteration 16301 / 61200) loss: 2.304070\n",
      "(Iteration 16401 / 61200) loss: 2.304078\n",
      "(Iteration 16501 / 61200) loss: 2.304027\n",
      "(Iteration 16601 / 61200) loss: 2.304064\n",
      "(Iteration 16701 / 61200) loss: 2.304087\n",
      "(Iteration 16801 / 61200) loss: 2.304116\n",
      "(Epoch 22 / 80) train acc: 0.110000; val_acc: 0.102000\n",
      "(Iteration 16901 / 61200) loss: 2.304045\n",
      "(Iteration 17001 / 61200) loss: 2.304102\n",
      "(Iteration 17101 / 61200) loss: 2.304022\n",
      "(Iteration 17201 / 61200) loss: 2.304089\n",
      "(Iteration 17301 / 61200) loss: 2.303960\n",
      "(Iteration 17401 / 61200) loss: 2.304100\n",
      "(Iteration 17501 / 61200) loss: 2.303970\n",
      "(Epoch 23 / 80) train acc: 0.078000; val_acc: 0.102000\n",
      "(Iteration 17601 / 61200) loss: 2.303997\n",
      "(Iteration 17701 / 61200) loss: 2.304026\n",
      "(Iteration 17801 / 61200) loss: 2.304031\n",
      "(Iteration 17901 / 61200) loss: 2.304038\n",
      "(Iteration 18001 / 61200) loss: 2.304032\n",
      "(Iteration 18101 / 61200) loss: 2.303993\n",
      "(Iteration 18201 / 61200) loss: 2.303966\n",
      "(Iteration 18301 / 61200) loss: 2.303957\n",
      "(Epoch 24 / 80) train acc: 0.108000; val_acc: 0.102000\n",
      "(Iteration 18401 / 61200) loss: 2.303908\n",
      "(Iteration 18501 / 61200) loss: 2.303952\n",
      "(Iteration 18601 / 61200) loss: 2.303997\n",
      "(Iteration 18701 / 61200) loss: 2.304038\n",
      "(Iteration 18801 / 61200) loss: 2.303957\n",
      "(Iteration 18901 / 61200) loss: 2.303908\n",
      "(Iteration 19001 / 61200) loss: 2.303948\n",
      "(Iteration 19101 / 61200) loss: 2.304005\n",
      "(Epoch 25 / 80) train acc: 0.110000; val_acc: 0.102000\n",
      "(Iteration 19201 / 61200) loss: 2.303874\n",
      "(Iteration 19301 / 61200) loss: 2.303873\n",
      "(Iteration 19401 / 61200) loss: 2.303983\n",
      "(Iteration 19501 / 61200) loss: 2.303989\n",
      "(Iteration 19601 / 61200) loss: 2.303920\n",
      "(Iteration 19701 / 61200) loss: 2.303931\n",
      "(Iteration 19801 / 61200) loss: 2.303949\n",
      "(Epoch 26 / 80) train acc: 0.098000; val_acc: 0.102000\n",
      "(Iteration 19901 / 61200) loss: 2.303930\n",
      "(Iteration 20001 / 61200) loss: 2.303912\n",
      "(Iteration 20101 / 61200) loss: 2.303965\n",
      "(Iteration 20201 / 61200) loss: 2.303867\n",
      "(Iteration 20301 / 61200) loss: 2.303941\n",
      "(Iteration 20401 / 61200) loss: 2.303920\n",
      "(Iteration 20501 / 61200) loss: 2.303793\n",
      "(Iteration 20601 / 61200) loss: 2.303917\n",
      "(Epoch 27 / 80) train acc: 0.083000; val_acc: 0.102000\n",
      "(Iteration 20701 / 61200) loss: 2.303943\n",
      "(Iteration 20801 / 61200) loss: 2.303870\n",
      "(Iteration 20901 / 61200) loss: 2.303955\n",
      "(Iteration 21001 / 61200) loss: 2.303938\n",
      "(Iteration 21101 / 61200) loss: 2.303812\n",
      "(Iteration 21201 / 61200) loss: 2.303830\n",
      "(Iteration 21301 / 61200) loss: 2.303788\n",
      "(Iteration 21401 / 61200) loss: 2.303819\n",
      "(Epoch 28 / 80) train acc: 0.108000; val_acc: 0.102000\n",
      "(Iteration 21501 / 61200) loss: 2.303819\n",
      "(Iteration 21601 / 61200) loss: 2.303805\n",
      "(Iteration 21701 / 61200) loss: 2.303870\n",
      "(Iteration 21801 / 61200) loss: 2.303786\n",
      "(Iteration 21901 / 61200) loss: 2.303824\n",
      "(Iteration 22001 / 61200) loss: 2.303977\n",
      "(Iteration 22101 / 61200) loss: 2.303762\n",
      "(Epoch 29 / 80) train acc: 0.097000; val_acc: 0.102000\n",
      "(Iteration 22201 / 61200) loss: 2.303942\n",
      "(Iteration 22301 / 61200) loss: 2.303841\n",
      "(Iteration 22401 / 61200) loss: 2.303878\n",
      "(Iteration 22501 / 61200) loss: 2.303823\n",
      "(Iteration 22601 / 61200) loss: 2.303915\n",
      "(Iteration 22701 / 61200) loss: 2.303804\n",
      "(Iteration 22801 / 61200) loss: 2.303926\n",
      "(Iteration 22901 / 61200) loss: 2.303873\n",
      "(Epoch 30 / 80) train acc: 0.094000; val_acc: 0.102000\n",
      "(Iteration 23001 / 61200) loss: 2.303815\n",
      "(Iteration 23101 / 61200) loss: 2.303827\n",
      "(Iteration 23201 / 61200) loss: 2.303795\n",
      "(Iteration 23301 / 61200) loss: 2.303892\n",
      "(Iteration 23401 / 61200) loss: 2.303863\n",
      "(Iteration 23501 / 61200) loss: 2.303797\n",
      "(Iteration 23601 / 61200) loss: 2.303812\n",
      "(Iteration 23701 / 61200) loss: 2.303814\n",
      "(Epoch 31 / 80) train acc: 0.089000; val_acc: 0.102000\n",
      "(Iteration 23801 / 61200) loss: 2.303736\n",
      "(Iteration 23901 / 61200) loss: 2.303807\n",
      "(Iteration 24001 / 61200) loss: 2.303824\n",
      "(Iteration 24101 / 61200) loss: 2.303726\n",
      "(Iteration 24201 / 61200) loss: 2.303778\n",
      "(Iteration 24301 / 61200) loss: 2.303759\n",
      "(Iteration 24401 / 61200) loss: 2.303823\n",
      "(Epoch 32 / 80) train acc: 0.109000; val_acc: 0.102000\n",
      "(Iteration 24501 / 61200) loss: 2.303819\n",
      "(Iteration 24601 / 61200) loss: 2.303756\n",
      "(Iteration 24701 / 61200) loss: 2.303776\n",
      "(Iteration 24801 / 61200) loss: 2.303749\n",
      "(Iteration 24901 / 61200) loss: 2.303752\n",
      "(Iteration 25001 / 61200) loss: 2.303792\n",
      "(Iteration 25101 / 61200) loss: 2.303725\n",
      "(Iteration 25201 / 61200) loss: 2.303703\n",
      "(Epoch 33 / 80) train acc: 0.100000; val_acc: 0.102000\n",
      "(Iteration 25301 / 61200) loss: 2.303849\n",
      "(Iteration 25401 / 61200) loss: 2.303755\n",
      "(Iteration 25501 / 61200) loss: 2.303720\n",
      "(Iteration 25601 / 61200) loss: 2.303800\n",
      "(Iteration 25701 / 61200) loss: 2.303747\n",
      "(Iteration 25801 / 61200) loss: 2.303692\n",
      "(Iteration 25901 / 61200) loss: 2.303729\n",
      "(Iteration 26001 / 61200) loss: 2.303746\n",
      "(Epoch 34 / 80) train acc: 0.098000; val_acc: 0.102000\n",
      "(Iteration 26101 / 61200) loss: 2.303688\n",
      "(Iteration 26201 / 61200) loss: 2.303740\n",
      "(Iteration 26301 / 61200) loss: 2.303716\n",
      "(Iteration 26401 / 61200) loss: 2.303741\n",
      "(Iteration 26501 / 61200) loss: 2.303823\n",
      "(Iteration 26601 / 61200) loss: 2.303778\n",
      "(Iteration 26701 / 61200) loss: 2.303709\n",
      "(Epoch 35 / 80) train acc: 0.120000; val_acc: 0.102000\n",
      "(Iteration 26801 / 61200) loss: 2.303718\n",
      "(Iteration 26901 / 61200) loss: 2.303805\n",
      "(Iteration 27001 / 61200) loss: 2.303750\n",
      "(Iteration 27101 / 61200) loss: 2.303773\n",
      "(Iteration 27201 / 61200) loss: 2.303725\n",
      "(Iteration 27301 / 61200) loss: 2.303777\n",
      "(Iteration 27401 / 61200) loss: 2.303794\n",
      "(Iteration 27501 / 61200) loss: 2.303754\n",
      "(Epoch 36 / 80) train acc: 0.100000; val_acc: 0.102000\n",
      "(Iteration 27601 / 61200) loss: 2.303700\n",
      "(Iteration 27701 / 61200) loss: 2.303766\n",
      "(Iteration 27801 / 61200) loss: 2.303715\n",
      "(Iteration 27901 / 61200) loss: 2.303723\n",
      "(Iteration 28001 / 61200) loss: 2.303699\n",
      "(Iteration 28101 / 61200) loss: 2.303702\n",
      "(Iteration 28201 / 61200) loss: 2.303655\n",
      "(Iteration 28301 / 61200) loss: 2.303696\n",
      "(Epoch 37 / 80) train acc: 0.103000; val_acc: 0.102000\n",
      "(Iteration 28401 / 61200) loss: 2.303751\n",
      "(Iteration 28501 / 61200) loss: 2.303669\n",
      "(Iteration 28601 / 61200) loss: 2.303711\n",
      "(Iteration 28701 / 61200) loss: 2.303732\n",
      "(Iteration 28801 / 61200) loss: 2.303713\n",
      "(Iteration 28901 / 61200) loss: 2.303703\n",
      "(Iteration 29001 / 61200) loss: 2.303711\n",
      "(Epoch 38 / 80) train acc: 0.096000; val_acc: 0.102000\n",
      "(Iteration 29101 / 61200) loss: 2.303724\n",
      "(Iteration 29201 / 61200) loss: 2.303712\n",
      "(Iteration 29301 / 61200) loss: 2.303694\n",
      "(Iteration 29401 / 61200) loss: 2.303635\n",
      "(Iteration 29501 / 61200) loss: 2.303737\n",
      "(Iteration 29601 / 61200) loss: 2.303657\n",
      "(Iteration 29701 / 61200) loss: 2.303725\n",
      "(Iteration 29801 / 61200) loss: 2.303728\n",
      "(Epoch 39 / 80) train acc: 0.088000; val_acc: 0.102000\n",
      "(Iteration 29901 / 61200) loss: 2.303721\n",
      "(Iteration 30001 / 61200) loss: 2.303669\n",
      "(Iteration 30101 / 61200) loss: 2.303699\n",
      "(Iteration 30201 / 61200) loss: 2.303655\n",
      "(Iteration 30301 / 61200) loss: 2.303691\n",
      "(Iteration 30401 / 61200) loss: 2.303690\n",
      "(Iteration 30501 / 61200) loss: 2.303691\n",
      "(Epoch 40 / 80) train acc: 0.103000; val_acc: 0.104000\n",
      "(Iteration 30601 / 61200) loss: 2.303684\n",
      "(Iteration 30701 / 61200) loss: 2.303641\n",
      "(Iteration 30801 / 61200) loss: 2.303588\n",
      "(Iteration 30901 / 61200) loss: 2.303595\n",
      "(Iteration 31001 / 61200) loss: 2.303657\n",
      "(Iteration 31101 / 61200) loss: 2.303705\n",
      "(Iteration 31201 / 61200) loss: 2.303664\n",
      "(Iteration 31301 / 61200) loss: 2.303671\n",
      "(Epoch 41 / 80) train acc: 0.105000; val_acc: 0.102000\n",
      "(Iteration 31401 / 61200) loss: 2.303633\n",
      "(Iteration 31501 / 61200) loss: 2.303620\n",
      "(Iteration 31601 / 61200) loss: 2.303561\n",
      "(Iteration 31701 / 61200) loss: 2.303633\n",
      "(Iteration 31801 / 61200) loss: 2.303706\n",
      "(Iteration 31901 / 61200) loss: 2.303627\n",
      "(Iteration 32001 / 61200) loss: 2.303639\n",
      "(Iteration 32101 / 61200) loss: 2.303603\n",
      "(Epoch 42 / 80) train acc: 0.114000; val_acc: 0.102000\n",
      "(Iteration 32201 / 61200) loss: 2.303653\n",
      "(Iteration 32301 / 61200) loss: 2.303649\n",
      "(Iteration 32401 / 61200) loss: 2.303706\n",
      "(Iteration 32501 / 61200) loss: 2.303608\n",
      "(Iteration 32601 / 61200) loss: 2.303636\n",
      "(Iteration 32701 / 61200) loss: 2.303580\n",
      "(Iteration 32801 / 61200) loss: 2.303574\n",
      "(Epoch 43 / 80) train acc: 0.115000; val_acc: 0.102000\n",
      "(Iteration 32901 / 61200) loss: 2.303584\n",
      "(Iteration 33001 / 61200) loss: 2.303686\n",
      "(Iteration 33101 / 61200) loss: 2.303682\n",
      "(Iteration 33201 / 61200) loss: 2.303635\n",
      "(Iteration 33301 / 61200) loss: 2.303582\n",
      "(Iteration 33401 / 61200) loss: 2.303600\n",
      "(Iteration 33501 / 61200) loss: 2.303685\n",
      "(Iteration 33601 / 61200) loss: 2.303620\n",
      "(Epoch 44 / 80) train acc: 0.108000; val_acc: 0.103000\n",
      "(Iteration 33701 / 61200) loss: 2.303645\n",
      "(Iteration 33801 / 61200) loss: 2.303643\n",
      "(Iteration 33901 / 61200) loss: 2.303701\n",
      "(Iteration 34001 / 61200) loss: 2.303647\n",
      "(Iteration 34101 / 61200) loss: 2.303576\n",
      "(Iteration 34201 / 61200) loss: 2.303713\n",
      "(Iteration 34301 / 61200) loss: 2.303673\n",
      "(Iteration 34401 / 61200) loss: 2.303581\n",
      "(Epoch 45 / 80) train acc: 0.112000; val_acc: 0.109000\n",
      "(Iteration 34501 / 61200) loss: 2.303564\n",
      "(Iteration 34601 / 61200) loss: 2.303635\n",
      "(Iteration 34701 / 61200) loss: 2.303583\n",
      "(Iteration 34801 / 61200) loss: 2.303692\n",
      "(Iteration 34901 / 61200) loss: 2.303628\n",
      "(Iteration 35001 / 61200) loss: 2.303586\n",
      "(Iteration 35101 / 61200) loss: 2.303653\n",
      "(Epoch 46 / 80) train acc: 0.107000; val_acc: 0.108000\n",
      "(Iteration 35201 / 61200) loss: 2.303627\n",
      "(Iteration 35301 / 61200) loss: 2.303630\n",
      "(Iteration 35401 / 61200) loss: 2.303677\n",
      "(Iteration 35501 / 61200) loss: 2.303524\n",
      "(Iteration 35601 / 61200) loss: 2.303589\n",
      "(Iteration 35701 / 61200) loss: 2.303621\n",
      "(Iteration 35801 / 61200) loss: 2.303584\n",
      "(Iteration 35901 / 61200) loss: 2.303630\n",
      "(Epoch 47 / 80) train acc: 0.120000; val_acc: 0.116000\n",
      "(Iteration 36001 / 61200) loss: 2.303707\n",
      "(Iteration 36101 / 61200) loss: 2.303615\n",
      "(Iteration 36201 / 61200) loss: 2.303535\n",
      "(Iteration 36301 / 61200) loss: 2.303567\n",
      "(Iteration 36401 / 61200) loss: 2.303573\n",
      "(Iteration 36501 / 61200) loss: 2.303607\n",
      "(Iteration 36601 / 61200) loss: 2.303621\n",
      "(Iteration 36701 / 61200) loss: 2.303585\n",
      "(Epoch 48 / 80) train acc: 0.125000; val_acc: 0.120000\n",
      "(Iteration 36801 / 61200) loss: 2.303628\n",
      "(Iteration 36901 / 61200) loss: 2.303670\n",
      "(Iteration 37001 / 61200) loss: 2.303550\n",
      "(Iteration 37101 / 61200) loss: 2.303624\n",
      "(Iteration 37201 / 61200) loss: 2.303601\n",
      "(Iteration 37301 / 61200) loss: 2.303607\n",
      "(Iteration 37401 / 61200) loss: 2.303629\n",
      "(Epoch 49 / 80) train acc: 0.101000; val_acc: 0.108000\n",
      "(Iteration 37501 / 61200) loss: 2.303575\n",
      "(Iteration 37601 / 61200) loss: 2.303597\n",
      "(Iteration 37701 / 61200) loss: 2.303582\n",
      "(Iteration 37801 / 61200) loss: 2.303591\n",
      "(Iteration 37901 / 61200) loss: 2.303510\n",
      "(Iteration 38001 / 61200) loss: 2.303606\n",
      "(Iteration 38101 / 61200) loss: 2.303706\n",
      "(Iteration 38201 / 61200) loss: 2.303595\n",
      "(Epoch 50 / 80) train acc: 0.103000; val_acc: 0.106000\n",
      "(Iteration 38301 / 61200) loss: 2.303586\n",
      "(Iteration 38401 / 61200) loss: 2.303597\n",
      "(Iteration 38501 / 61200) loss: 2.303606\n",
      "(Iteration 38601 / 61200) loss: 2.303571\n",
      "(Iteration 38701 / 61200) loss: 2.303610\n",
      "(Iteration 38801 / 61200) loss: 2.303601\n",
      "(Iteration 38901 / 61200) loss: 2.303599\n",
      "(Iteration 39001 / 61200) loss: 2.303597\n",
      "(Epoch 51 / 80) train acc: 0.121000; val_acc: 0.108000\n",
      "(Iteration 39101 / 61200) loss: 2.303586\n",
      "(Iteration 39201 / 61200) loss: 2.303560\n",
      "(Iteration 39301 / 61200) loss: 2.303608\n",
      "(Iteration 39401 / 61200) loss: 2.303648\n",
      "(Iteration 39501 / 61200) loss: 2.303624\n",
      "(Iteration 39601 / 61200) loss: 2.303635\n",
      "(Iteration 39701 / 61200) loss: 2.303558\n",
      "(Epoch 52 / 80) train acc: 0.109000; val_acc: 0.110000\n",
      "(Iteration 39801 / 61200) loss: 2.303583\n",
      "(Iteration 39901 / 61200) loss: 2.303624\n",
      "(Iteration 40001 / 61200) loss: 2.303599\n",
      "(Iteration 40101 / 61200) loss: 2.303605\n",
      "(Iteration 40201 / 61200) loss: 2.303507\n",
      "(Iteration 40301 / 61200) loss: 2.303674\n",
      "(Iteration 40401 / 61200) loss: 2.303489\n",
      "(Iteration 40501 / 61200) loss: 2.303566\n",
      "(Epoch 53 / 80) train acc: 0.125000; val_acc: 0.112000\n",
      "(Iteration 40601 / 61200) loss: 2.303568\n",
      "(Iteration 40701 / 61200) loss: 2.303596\n",
      "(Iteration 40801 / 61200) loss: 2.303539\n",
      "(Iteration 40901 / 61200) loss: 2.303586\n",
      "(Iteration 41001 / 61200) loss: 2.303587\n",
      "(Iteration 41101 / 61200) loss: 2.303620\n",
      "(Iteration 41201 / 61200) loss: 2.303513\n",
      "(Iteration 41301 / 61200) loss: 2.303434\n",
      "(Epoch 54 / 80) train acc: 0.127000; val_acc: 0.116000\n",
      "(Iteration 41401 / 61200) loss: 2.303488\n",
      "(Iteration 41501 / 61200) loss: 2.303523\n",
      "(Iteration 41601 / 61200) loss: 2.303600\n",
      "(Iteration 41701 / 61200) loss: 2.303588\n",
      "(Iteration 41801 / 61200) loss: 2.303561\n",
      "(Iteration 41901 / 61200) loss: 2.303559\n",
      "(Iteration 42001 / 61200) loss: 2.303524\n",
      "(Epoch 55 / 80) train acc: 0.120000; val_acc: 0.120000\n",
      "(Iteration 42101 / 61200) loss: 2.303610\n",
      "(Iteration 42201 / 61200) loss: 2.303521\n",
      "(Iteration 42301 / 61200) loss: 2.303510\n",
      "(Iteration 42401 / 61200) loss: 2.303583\n",
      "(Iteration 42501 / 61200) loss: 2.303525\n",
      "(Iteration 42601 / 61200) loss: 2.303472\n",
      "(Iteration 42701 / 61200) loss: 2.303556\n",
      "(Iteration 42801 / 61200) loss: 2.303553\n",
      "(Epoch 56 / 80) train acc: 0.152000; val_acc: 0.126000\n",
      "(Iteration 42901 / 61200) loss: 2.303616\n",
      "(Iteration 43001 / 61200) loss: 2.303563\n",
      "(Iteration 43101 / 61200) loss: 2.303573\n",
      "(Iteration 43201 / 61200) loss: 2.303571\n",
      "(Iteration 43301 / 61200) loss: 2.303607\n",
      "(Iteration 43401 / 61200) loss: 2.303635\n",
      "(Iteration 43501 / 61200) loss: 2.303570\n",
      "(Iteration 43601 / 61200) loss: 2.303513\n",
      "(Epoch 57 / 80) train acc: 0.127000; val_acc: 0.121000\n",
      "(Iteration 43701 / 61200) loss: 2.303550\n",
      "(Iteration 43801 / 61200) loss: 2.303621\n",
      "(Iteration 43901 / 61200) loss: 2.303531\n",
      "(Iteration 44001 / 61200) loss: 2.303634\n",
      "(Iteration 44101 / 61200) loss: 2.303663\n",
      "(Iteration 44201 / 61200) loss: 2.303565\n",
      "(Iteration 44301 / 61200) loss: 2.303555\n",
      "(Epoch 58 / 80) train acc: 0.147000; val_acc: 0.128000\n",
      "(Iteration 44401 / 61200) loss: 2.303532\n",
      "(Iteration 44501 / 61200) loss: 2.303546\n",
      "(Iteration 44601 / 61200) loss: 2.303541\n",
      "(Iteration 44701 / 61200) loss: 2.303553\n",
      "(Iteration 44801 / 61200) loss: 2.303565\n",
      "(Iteration 44901 / 61200) loss: 2.303546\n",
      "(Iteration 45001 / 61200) loss: 2.303530\n",
      "(Iteration 45101 / 61200) loss: 2.303590\n",
      "(Epoch 59 / 80) train acc: 0.113000; val_acc: 0.120000\n",
      "(Iteration 45201 / 61200) loss: 2.303593\n",
      "(Iteration 45301 / 61200) loss: 2.303551\n",
      "(Iteration 45401 / 61200) loss: 2.303594\n",
      "(Iteration 45501 / 61200) loss: 2.303589\n",
      "(Iteration 45601 / 61200) loss: 2.303510\n",
      "(Iteration 45701 / 61200) loss: 2.303487\n",
      "(Iteration 45801 / 61200) loss: 2.303469\n",
      "(Epoch 60 / 80) train acc: 0.121000; val_acc: 0.117000\n",
      "(Iteration 45901 / 61200) loss: 2.303603\n",
      "(Iteration 46001 / 61200) loss: 2.303530\n",
      "(Iteration 46101 / 61200) loss: 2.303563\n",
      "(Iteration 46201 / 61200) loss: 2.303612\n",
      "(Iteration 46301 / 61200) loss: 2.303505\n",
      "(Iteration 46401 / 61200) loss: 2.303482\n",
      "(Iteration 46501 / 61200) loss: 2.303497\n",
      "(Iteration 46601 / 61200) loss: 2.303579\n",
      "(Epoch 61 / 80) train acc: 0.114000; val_acc: 0.114000\n",
      "(Iteration 46701 / 61200) loss: 2.303565\n",
      "(Iteration 46801 / 61200) loss: 2.303536\n",
      "(Iteration 46901 / 61200) loss: 2.303551\n",
      "(Iteration 47001 / 61200) loss: 2.303480\n",
      "(Iteration 47101 / 61200) loss: 2.303547\n",
      "(Iteration 47201 / 61200) loss: 2.303567\n",
      "(Iteration 47301 / 61200) loss: 2.303659\n",
      "(Iteration 47401 / 61200) loss: 2.303527\n",
      "(Epoch 62 / 80) train acc: 0.114000; val_acc: 0.112000\n",
      "(Iteration 47501 / 61200) loss: 2.303652\n",
      "(Iteration 47601 / 61200) loss: 2.303515\n",
      "(Iteration 47701 / 61200) loss: 2.303457\n",
      "(Iteration 47801 / 61200) loss: 2.303596\n",
      "(Iteration 47901 / 61200) loss: 2.303524\n",
      "(Iteration 48001 / 61200) loss: 2.303523\n",
      "(Iteration 48101 / 61200) loss: 2.303561\n",
      "(Epoch 63 / 80) train acc: 0.125000; val_acc: 0.110000\n",
      "(Iteration 48201 / 61200) loss: 2.303511\n",
      "(Iteration 48301 / 61200) loss: 2.303568\n",
      "(Iteration 48401 / 61200) loss: 2.303489\n",
      "(Iteration 48501 / 61200) loss: 2.303478\n",
      "(Iteration 48601 / 61200) loss: 2.303536\n",
      "(Iteration 48701 / 61200) loss: 2.303533\n",
      "(Iteration 48801 / 61200) loss: 2.303575\n",
      "(Iteration 48901 / 61200) loss: 2.303530\n",
      "(Epoch 64 / 80) train acc: 0.101000; val_acc: 0.111000\n",
      "(Iteration 49001 / 61200) loss: 2.303510\n",
      "(Iteration 49101 / 61200) loss: 2.303546\n",
      "(Iteration 49201 / 61200) loss: 2.303528\n",
      "(Iteration 49301 / 61200) loss: 2.303472\n",
      "(Iteration 49401 / 61200) loss: 2.303593\n",
      "(Iteration 49501 / 61200) loss: 2.303573\n",
      "(Iteration 49601 / 61200) loss: 2.303580\n",
      "(Iteration 49701 / 61200) loss: 2.303563\n",
      "(Epoch 65 / 80) train acc: 0.126000; val_acc: 0.111000\n",
      "(Iteration 49801 / 61200) loss: 2.303504\n",
      "(Iteration 49901 / 61200) loss: 2.303509\n",
      "(Iteration 50001 / 61200) loss: 2.303593\n",
      "(Iteration 50101 / 61200) loss: 2.303498\n",
      "(Iteration 50201 / 61200) loss: 2.303498\n",
      "(Iteration 50301 / 61200) loss: 2.303535\n",
      "(Iteration 50401 / 61200) loss: 2.303478\n",
      "(Epoch 66 / 80) train acc: 0.120000; val_acc: 0.112000\n",
      "(Iteration 50501 / 61200) loss: 2.303489\n",
      "(Iteration 50601 / 61200) loss: 2.303650\n",
      "(Iteration 50701 / 61200) loss: 2.303498\n",
      "(Iteration 50801 / 61200) loss: 2.303444\n",
      "(Iteration 50901 / 61200) loss: 2.303580\n",
      "(Iteration 51001 / 61200) loss: 2.303532\n",
      "(Iteration 51101 / 61200) loss: 2.303536\n",
      "(Iteration 51201 / 61200) loss: 2.303436\n",
      "(Epoch 67 / 80) train acc: 0.110000; val_acc: 0.113000\n",
      "(Iteration 51301 / 61200) loss: 2.303543\n",
      "(Iteration 51401 / 61200) loss: 2.303468\n",
      "(Iteration 51501 / 61200) loss: 2.303452\n",
      "(Iteration 51601 / 61200) loss: 2.303672\n",
      "(Iteration 51701 / 61200) loss: 2.303524\n",
      "(Iteration 51801 / 61200) loss: 2.303460\n",
      "(Iteration 51901 / 61200) loss: 2.303501\n",
      "(Iteration 52001 / 61200) loss: 2.303471\n",
      "(Epoch 68 / 80) train acc: 0.123000; val_acc: 0.116000\n",
      "(Iteration 52101 / 61200) loss: 2.303516\n",
      "(Iteration 52201 / 61200) loss: 2.303520\n",
      "(Iteration 52301 / 61200) loss: 2.303574\n",
      "(Iteration 52401 / 61200) loss: 2.303495\n",
      "(Iteration 52501 / 61200) loss: 2.303419\n",
      "(Iteration 52601 / 61200) loss: 2.303561\n",
      "(Iteration 52701 / 61200) loss: 2.303515\n",
      "(Epoch 69 / 80) train acc: 0.115000; val_acc: 0.113000\n",
      "(Iteration 52801 / 61200) loss: 2.303529\n",
      "(Iteration 52901 / 61200) loss: 2.303523\n",
      "(Iteration 53001 / 61200) loss: 2.303583\n",
      "(Iteration 53101 / 61200) loss: 2.303500\n",
      "(Iteration 53201 / 61200) loss: 2.303554\n",
      "(Iteration 53301 / 61200) loss: 2.303528\n",
      "(Iteration 53401 / 61200) loss: 2.303410\n",
      "(Iteration 53501 / 61200) loss: 2.303634\n",
      "(Epoch 70 / 80) train acc: 0.135000; val_acc: 0.113000\n",
      "(Iteration 53601 / 61200) loss: 2.303508\n",
      "(Iteration 53701 / 61200) loss: 2.303489\n",
      "(Iteration 53801 / 61200) loss: 2.303500\n",
      "(Iteration 53901 / 61200) loss: 2.303629\n",
      "(Iteration 54001 / 61200) loss: 2.303475\n",
      "(Iteration 54101 / 61200) loss: 2.303571\n",
      "(Iteration 54201 / 61200) loss: 2.303595\n",
      "(Iteration 54301 / 61200) loss: 2.303535\n",
      "(Epoch 71 / 80) train acc: 0.101000; val_acc: 0.112000\n",
      "(Iteration 54401 / 61200) loss: 2.303506\n",
      "(Iteration 54501 / 61200) loss: 2.303418\n",
      "(Iteration 54601 / 61200) loss: 2.303532\n",
      "(Iteration 54701 / 61200) loss: 2.303477\n",
      "(Iteration 54801 / 61200) loss: 2.303444\n",
      "(Iteration 54901 / 61200) loss: 2.303465\n",
      "(Iteration 55001 / 61200) loss: 2.303581\n",
      "(Epoch 72 / 80) train acc: 0.123000; val_acc: 0.110000\n",
      "(Iteration 55101 / 61200) loss: 2.303532\n",
      "(Iteration 55201 / 61200) loss: 2.303500\n",
      "(Iteration 55301 / 61200) loss: 2.303478\n",
      "(Iteration 55401 / 61200) loss: 2.303545\n",
      "(Iteration 55501 / 61200) loss: 2.303573\n",
      "(Iteration 55601 / 61200) loss: 2.303509\n",
      "(Iteration 55701 / 61200) loss: 2.303435\n",
      "(Iteration 55801 / 61200) loss: 2.303539\n",
      "(Epoch 73 / 80) train acc: 0.100000; val_acc: 0.111000\n",
      "(Iteration 55901 / 61200) loss: 2.303533\n",
      "(Iteration 56001 / 61200) loss: 2.303446\n",
      "(Iteration 56101 / 61200) loss: 2.303417\n",
      "(Iteration 56201 / 61200) loss: 2.303534\n",
      "(Iteration 56301 / 61200) loss: 2.303574\n",
      "(Iteration 56401 / 61200) loss: 2.303510\n",
      "(Iteration 56501 / 61200) loss: 2.303427\n",
      "(Iteration 56601 / 61200) loss: 2.303547\n",
      "(Epoch 74 / 80) train acc: 0.110000; val_acc: 0.112000\n",
      "(Iteration 56701 / 61200) loss: 2.303472\n",
      "(Iteration 56801 / 61200) loss: 2.303543\n",
      "(Iteration 56901 / 61200) loss: 2.303492\n",
      "(Iteration 57001 / 61200) loss: 2.303546\n",
      "(Iteration 57101 / 61200) loss: 2.303507\n",
      "(Iteration 57201 / 61200) loss: 2.303580\n",
      "(Iteration 57301 / 61200) loss: 2.303456\n",
      "(Epoch 75 / 80) train acc: 0.117000; val_acc: 0.111000\n",
      "(Iteration 57401 / 61200) loss: 2.303496\n",
      "(Iteration 57501 / 61200) loss: 2.303543\n",
      "(Iteration 57601 / 61200) loss: 2.303506\n",
      "(Iteration 57701 / 61200) loss: 2.303495\n",
      "(Iteration 57801 / 61200) loss: 2.303592\n",
      "(Iteration 57901 / 61200) loss: 2.303590\n",
      "(Iteration 58001 / 61200) loss: 2.303585\n",
      "(Iteration 58101 / 61200) loss: 2.303576\n",
      "(Epoch 76 / 80) train acc: 0.111000; val_acc: 0.108000\n",
      "(Iteration 58201 / 61200) loss: 2.303480\n",
      "(Iteration 58301 / 61200) loss: 2.303487\n",
      "(Iteration 58401 / 61200) loss: 2.303561\n",
      "(Iteration 58501 / 61200) loss: 2.303490\n",
      "(Iteration 58601 / 61200) loss: 2.303386\n",
      "(Iteration 58701 / 61200) loss: 2.303610\n",
      "(Iteration 58801 / 61200) loss: 2.303480\n",
      "(Iteration 58901 / 61200) loss: 2.303474\n",
      "(Epoch 77 / 80) train acc: 0.118000; val_acc: 0.111000\n",
      "(Iteration 59001 / 61200) loss: 2.303531\n",
      "(Iteration 59101 / 61200) loss: 2.303494\n",
      "(Iteration 59201 / 61200) loss: 2.303549\n",
      "(Iteration 59301 / 61200) loss: 2.303480\n",
      "(Iteration 59401 / 61200) loss: 2.303438\n",
      "(Iteration 59501 / 61200) loss: 2.303457\n",
      "(Iteration 59601 / 61200) loss: 2.303538\n",
      "(Epoch 78 / 80) train acc: 0.112000; val_acc: 0.116000\n",
      "(Iteration 59701 / 61200) loss: 2.303479\n",
      "(Iteration 59801 / 61200) loss: 2.303484\n",
      "(Iteration 59901 / 61200) loss: 2.303535\n",
      "(Iteration 60001 / 61200) loss: 2.303585\n",
      "(Iteration 60101 / 61200) loss: 2.303436\n",
      "(Iteration 60201 / 61200) loss: 2.303472\n",
      "(Iteration 60301 / 61200) loss: 2.303583\n",
      "(Iteration 60401 / 61200) loss: 2.303455\n",
      "(Epoch 79 / 80) train acc: 0.137000; val_acc: 0.118000\n",
      "(Iteration 60501 / 61200) loss: 2.303562\n",
      "(Iteration 60601 / 61200) loss: 2.303546\n",
      "(Iteration 60701 / 61200) loss: 2.303527\n",
      "(Iteration 60801 / 61200) loss: 2.303483\n",
      "(Iteration 60901 / 61200) loss: 2.303472\n",
      "(Iteration 61001 / 61200) loss: 2.303578\n",
      "(Iteration 61101 / 61200) loss: 2.303483\n",
      "(Epoch 80 / 80) train acc: 0.125000; val_acc: 0.120000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.306716\n",
      "(Epoch 0 / 80) train acc: 0.088000; val_acc: 0.067000\n",
      "(Iteration 101 / 30560) loss: 2.306679\n",
      "(Iteration 201 / 30560) loss: 2.306636\n",
      "(Iteration 301 / 30560) loss: 2.306598\n",
      "(Epoch 1 / 80) train acc: 0.103000; val_acc: 0.089000\n",
      "(Iteration 401 / 30560) loss: 2.306589\n",
      "(Iteration 501 / 30560) loss: 2.306539\n",
      "(Iteration 601 / 30560) loss: 2.306477\n",
      "(Iteration 701 / 30560) loss: 2.306455\n",
      "(Epoch 2 / 80) train acc: 0.102000; val_acc: 0.095000\n",
      "(Iteration 801 / 30560) loss: 2.306402\n",
      "(Iteration 901 / 30560) loss: 2.306384\n",
      "(Iteration 1001 / 30560) loss: 2.306361\n",
      "(Iteration 1101 / 30560) loss: 2.306324\n",
      "(Epoch 3 / 80) train acc: 0.104000; val_acc: 0.090000\n",
      "(Iteration 1201 / 30560) loss: 2.306282\n",
      "(Iteration 1301 / 30560) loss: 2.306233\n",
      "(Iteration 1401 / 30560) loss: 2.306221\n",
      "(Iteration 1501 / 30560) loss: 2.306164\n",
      "(Epoch 4 / 80) train acc: 0.105000; val_acc: 0.086000\n",
      "(Iteration 1601 / 30560) loss: 2.306170\n",
      "(Iteration 1701 / 30560) loss: 2.306118\n",
      "(Iteration 1801 / 30560) loss: 2.306085\n",
      "(Iteration 1901 / 30560) loss: 2.306071\n",
      "(Epoch 5 / 80) train acc: 0.109000; val_acc: 0.095000\n",
      "(Iteration 2001 / 30560) loss: 2.306027\n",
      "(Iteration 2101 / 30560) loss: 2.305986\n",
      "(Iteration 2201 / 30560) loss: 2.305961\n",
      "(Epoch 6 / 80) train acc: 0.106000; val_acc: 0.090000\n",
      "(Iteration 2301 / 30560) loss: 2.305964\n",
      "(Iteration 2401 / 30560) loss: 2.305930\n",
      "(Iteration 2501 / 30560) loss: 2.305900\n",
      "(Iteration 2601 / 30560) loss: 2.305885\n",
      "(Epoch 7 / 80) train acc: 0.109000; val_acc: 0.090000\n",
      "(Iteration 2701 / 30560) loss: 2.305860\n",
      "(Iteration 2801 / 30560) loss: 2.305830\n",
      "(Iteration 2901 / 30560) loss: 2.305817\n",
      "(Iteration 3001 / 30560) loss: 2.305809\n",
      "(Epoch 8 / 80) train acc: 0.095000; val_acc: 0.097000\n",
      "(Iteration 3101 / 30560) loss: 2.305747\n",
      "(Iteration 3201 / 30560) loss: 2.305740\n",
      "(Iteration 3301 / 30560) loss: 2.305724\n",
      "(Iteration 3401 / 30560) loss: 2.305708\n",
      "(Epoch 9 / 80) train acc: 0.104000; val_acc: 0.097000\n",
      "(Iteration 3501 / 30560) loss: 2.305682\n",
      "(Iteration 3601 / 30560) loss: 2.305670\n",
      "(Iteration 3701 / 30560) loss: 2.305641\n",
      "(Iteration 3801 / 30560) loss: 2.305635\n",
      "(Epoch 10 / 80) train acc: 0.096000; val_acc: 0.096000\n",
      "(Iteration 3901 / 30560) loss: 2.305605\n",
      "(Iteration 4001 / 30560) loss: 2.305592\n",
      "(Iteration 4101 / 30560) loss: 2.305567\n",
      "(Iteration 4201 / 30560) loss: 2.305535\n",
      "(Epoch 11 / 80) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 4301 / 30560) loss: 2.305535\n",
      "(Iteration 4401 / 30560) loss: 2.305520\n",
      "(Iteration 4501 / 30560) loss: 2.305497\n",
      "(Epoch 12 / 80) train acc: 0.089000; val_acc: 0.090000\n",
      "(Iteration 4601 / 30560) loss: 2.305486\n",
      "(Iteration 4701 / 30560) loss: 2.305495\n",
      "(Iteration 4801 / 30560) loss: 2.305432\n",
      "(Iteration 4901 / 30560) loss: 2.305450\n",
      "(Epoch 13 / 80) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 5001 / 30560) loss: 2.305433\n",
      "(Iteration 5101 / 30560) loss: 2.305395\n",
      "(Iteration 5201 / 30560) loss: 2.305383\n",
      "(Iteration 5301 / 30560) loss: 2.305368\n",
      "(Epoch 14 / 80) train acc: 0.101000; val_acc: 0.080000\n",
      "(Iteration 5401 / 30560) loss: 2.305347\n",
      "(Iteration 5501 / 30560) loss: 2.305349\n",
      "(Iteration 5601 / 30560) loss: 2.305321\n",
      "(Iteration 5701 / 30560) loss: 2.305320\n",
      "(Epoch 15 / 80) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 5801 / 30560) loss: 2.305329\n",
      "(Iteration 5901 / 30560) loss: 2.305272\n",
      "(Iteration 6001 / 30560) loss: 2.305272\n",
      "(Iteration 6101 / 30560) loss: 2.305243\n",
      "(Epoch 16 / 80) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 6201 / 30560) loss: 2.305261\n",
      "(Iteration 6301 / 30560) loss: 2.305266\n",
      "(Iteration 6401 / 30560) loss: 2.305235\n",
      "(Epoch 17 / 80) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 6501 / 30560) loss: 2.305223\n",
      "(Iteration 6601 / 30560) loss: 2.305167\n",
      "(Iteration 6701 / 30560) loss: 2.305211\n",
      "(Iteration 6801 / 30560) loss: 2.305194\n",
      "(Epoch 18 / 80) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 6901 / 30560) loss: 2.305206\n",
      "(Iteration 7001 / 30560) loss: 2.305189\n",
      "(Iteration 7101 / 30560) loss: 2.305169\n",
      "(Iteration 7201 / 30560) loss: 2.305145\n",
      "(Epoch 19 / 80) train acc: 0.115000; val_acc: 0.080000\n",
      "(Iteration 7301 / 30560) loss: 2.305151\n",
      "(Iteration 7401 / 30560) loss: 2.305123\n",
      "(Iteration 7501 / 30560) loss: 2.305115\n",
      "(Iteration 7601 / 30560) loss: 2.305139\n",
      "(Epoch 20 / 80) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 7701 / 30560) loss: 2.305125\n",
      "(Iteration 7801 / 30560) loss: 2.305091\n",
      "(Iteration 7901 / 30560) loss: 2.305093\n",
      "(Iteration 8001 / 30560) loss: 2.305044\n",
      "(Epoch 21 / 80) train acc: 0.088000; val_acc: 0.083000\n",
      "(Iteration 8101 / 30560) loss: 2.305035\n",
      "(Iteration 8201 / 30560) loss: 2.305056\n",
      "(Iteration 8301 / 30560) loss: 2.305026\n",
      "(Iteration 8401 / 30560) loss: 2.305018\n",
      "(Epoch 22 / 80) train acc: 0.111000; val_acc: 0.080000\n",
      "(Iteration 8501 / 30560) loss: 2.305030\n",
      "(Iteration 8601 / 30560) loss: 2.305026\n",
      "(Iteration 8701 / 30560) loss: 2.305007\n",
      "(Epoch 23 / 80) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 8801 / 30560) loss: 2.305000\n",
      "(Iteration 8901 / 30560) loss: 2.305004\n",
      "(Iteration 9001 / 30560) loss: 2.304980\n",
      "(Iteration 9101 / 30560) loss: 2.304997\n",
      "(Epoch 24 / 80) train acc: 0.116000; val_acc: 0.079000\n",
      "(Iteration 9201 / 30560) loss: 2.304965\n",
      "(Iteration 9301 / 30560) loss: 2.304979\n",
      "(Iteration 9401 / 30560) loss: 2.304993\n",
      "(Iteration 9501 / 30560) loss: 2.304946\n",
      "(Epoch 25 / 80) train acc: 0.108000; val_acc: 0.080000\n",
      "(Iteration 9601 / 30560) loss: 2.304938\n",
      "(Iteration 9701 / 30560) loss: 2.304979\n",
      "(Iteration 9801 / 30560) loss: 2.304953\n",
      "(Iteration 9901 / 30560) loss: 2.304972\n",
      "(Epoch 26 / 80) train acc: 0.108000; val_acc: 0.080000\n",
      "(Iteration 10001 / 30560) loss: 2.304913\n",
      "(Iteration 10101 / 30560) loss: 2.304944\n",
      "(Iteration 10201 / 30560) loss: 2.304903\n",
      "(Iteration 10301 / 30560) loss: 2.304892\n",
      "(Epoch 27 / 80) train acc: 0.095000; val_acc: 0.080000\n",
      "(Iteration 10401 / 30560) loss: 2.304885\n",
      "(Iteration 10501 / 30560) loss: 2.304905\n",
      "(Iteration 10601 / 30560) loss: 2.304939\n",
      "(Epoch 28 / 80) train acc: 0.112000; val_acc: 0.080000\n",
      "(Iteration 10701 / 30560) loss: 2.304874\n",
      "(Iteration 10801 / 30560) loss: 2.304876\n",
      "(Iteration 10901 / 30560) loss: 2.304861\n",
      "(Iteration 11001 / 30560) loss: 2.304859\n",
      "(Epoch 29 / 80) train acc: 0.118000; val_acc: 0.079000\n",
      "(Iteration 11101 / 30560) loss: 2.304839\n",
      "(Iteration 11201 / 30560) loss: 2.304842\n",
      "(Iteration 11301 / 30560) loss: 2.304847\n",
      "(Iteration 11401 / 30560) loss: 2.304865\n",
      "(Epoch 30 / 80) train acc: 0.092000; val_acc: 0.079000\n",
      "(Iteration 11501 / 30560) loss: 2.304844\n",
      "(Iteration 11601 / 30560) loss: 2.304843\n",
      "(Iteration 11701 / 30560) loss: 2.304839\n",
      "(Iteration 11801 / 30560) loss: 2.304831\n",
      "(Epoch 31 / 80) train acc: 0.092000; val_acc: 0.079000\n",
      "(Iteration 11901 / 30560) loss: 2.304836\n",
      "(Iteration 12001 / 30560) loss: 2.304827\n",
      "(Iteration 12101 / 30560) loss: 2.304825\n",
      "(Iteration 12201 / 30560) loss: 2.304848\n",
      "(Epoch 32 / 80) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 12301 / 30560) loss: 2.304831\n",
      "(Iteration 12401 / 30560) loss: 2.304814\n",
      "(Iteration 12501 / 30560) loss: 2.304812\n",
      "(Iteration 12601 / 30560) loss: 2.304799\n",
      "(Epoch 33 / 80) train acc: 0.087000; val_acc: 0.079000\n",
      "(Iteration 12701 / 30560) loss: 2.304776\n",
      "(Iteration 12801 / 30560) loss: 2.304811\n",
      "(Iteration 12901 / 30560) loss: 2.304794\n",
      "(Epoch 34 / 80) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 13001 / 30560) loss: 2.304766\n",
      "(Iteration 13101 / 30560) loss: 2.304755\n",
      "(Iteration 13201 / 30560) loss: 2.304764\n",
      "(Iteration 13301 / 30560) loss: 2.304780\n",
      "(Epoch 35 / 80) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 13401 / 30560) loss: 2.304732\n",
      "(Iteration 13501 / 30560) loss: 2.304755\n",
      "(Iteration 13601 / 30560) loss: 2.304758\n",
      "(Iteration 13701 / 30560) loss: 2.304759\n",
      "(Epoch 36 / 80) train acc: 0.113000; val_acc: 0.079000\n",
      "(Iteration 13801 / 30560) loss: 2.304754\n",
      "(Iteration 13901 / 30560) loss: 2.304737\n",
      "(Iteration 14001 / 30560) loss: 2.304748\n",
      "(Iteration 14101 / 30560) loss: 2.304724\n",
      "(Epoch 37 / 80) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 14201 / 30560) loss: 2.304709\n",
      "(Iteration 14301 / 30560) loss: 2.304726\n",
      "(Iteration 14401 / 30560) loss: 2.304700\n",
      "(Iteration 14501 / 30560) loss: 2.304744\n",
      "(Epoch 38 / 80) train acc: 0.088000; val_acc: 0.079000\n",
      "(Iteration 14601 / 30560) loss: 2.304698\n",
      "(Iteration 14701 / 30560) loss: 2.304726\n",
      "(Iteration 14801 / 30560) loss: 2.304773\n",
      "(Epoch 39 / 80) train acc: 0.088000; val_acc: 0.079000\n",
      "(Iteration 14901 / 30560) loss: 2.304690\n",
      "(Iteration 15001 / 30560) loss: 2.304679\n",
      "(Iteration 15101 / 30560) loss: 2.304681\n",
      "(Iteration 15201 / 30560) loss: 2.304694\n",
      "(Epoch 40 / 80) train acc: 0.091000; val_acc: 0.079000\n",
      "(Iteration 15301 / 30560) loss: 2.304698\n",
      "(Iteration 15401 / 30560) loss: 2.304716\n",
      "(Iteration 15501 / 30560) loss: 2.304711\n",
      "(Iteration 15601 / 30560) loss: 2.304713\n",
      "(Epoch 41 / 80) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 15701 / 30560) loss: 2.304674\n",
      "(Iteration 15801 / 30560) loss: 2.304667\n",
      "(Iteration 15901 / 30560) loss: 2.304679\n",
      "(Iteration 16001 / 30560) loss: 2.304674\n",
      "(Epoch 42 / 80) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 16101 / 30560) loss: 2.304694\n",
      "(Iteration 16201 / 30560) loss: 2.304657\n",
      "(Iteration 16301 / 30560) loss: 2.304690\n",
      "(Iteration 16401 / 30560) loss: 2.304662\n",
      "(Epoch 43 / 80) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 16501 / 30560) loss: 2.304642\n",
      "(Iteration 16601 / 30560) loss: 2.304646\n",
      "(Iteration 16701 / 30560) loss: 2.304686\n",
      "(Iteration 16801 / 30560) loss: 2.304680\n",
      "(Epoch 44 / 80) train acc: 0.083000; val_acc: 0.079000\n",
      "(Iteration 16901 / 30560) loss: 2.304637\n",
      "(Iteration 17001 / 30560) loss: 2.304639\n",
      "(Iteration 17101 / 30560) loss: 2.304673\n",
      "(Epoch 45 / 80) train acc: 0.086000; val_acc: 0.079000\n",
      "(Iteration 17201 / 30560) loss: 2.304677\n",
      "(Iteration 17301 / 30560) loss: 2.304697\n",
      "(Iteration 17401 / 30560) loss: 2.304631\n",
      "(Iteration 17501 / 30560) loss: 2.304634\n",
      "(Epoch 46 / 80) train acc: 0.110000; val_acc: 0.079000\n",
      "(Iteration 17601 / 30560) loss: 2.304634\n",
      "(Iteration 17701 / 30560) loss: 2.304678\n",
      "(Iteration 17801 / 30560) loss: 2.304631\n",
      "(Iteration 17901 / 30560) loss: 2.304624\n",
      "(Epoch 47 / 80) train acc: 0.091000; val_acc: 0.079000\n",
      "(Iteration 18001 / 30560) loss: 2.304643\n",
      "(Iteration 18101 / 30560) loss: 2.304643\n",
      "(Iteration 18201 / 30560) loss: 2.304620\n",
      "(Iteration 18301 / 30560) loss: 2.304637\n",
      "(Epoch 48 / 80) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 18401 / 30560) loss: 2.304615\n",
      "(Iteration 18501 / 30560) loss: 2.304639\n",
      "(Iteration 18601 / 30560) loss: 2.304612\n",
      "(Iteration 18701 / 30560) loss: 2.304613\n",
      "(Epoch 49 / 80) train acc: 0.081000; val_acc: 0.079000\n",
      "(Iteration 18801 / 30560) loss: 2.304607\n",
      "(Iteration 18901 / 30560) loss: 2.304619\n",
      "(Iteration 19001 / 30560) loss: 2.304579\n",
      "(Epoch 50 / 80) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 19101 / 30560) loss: 2.304627\n",
      "(Iteration 19201 / 30560) loss: 2.304630\n",
      "(Iteration 19301 / 30560) loss: 2.304607\n",
      "(Iteration 19401 / 30560) loss: 2.304602\n",
      "(Epoch 51 / 80) train acc: 0.113000; val_acc: 0.079000\n",
      "(Iteration 19501 / 30560) loss: 2.304642\n",
      "(Iteration 19601 / 30560) loss: 2.304608\n",
      "(Iteration 19701 / 30560) loss: 2.304607\n",
      "(Iteration 19801 / 30560) loss: 2.304607\n",
      "(Epoch 52 / 80) train acc: 0.092000; val_acc: 0.079000\n",
      "(Iteration 19901 / 30560) loss: 2.304576\n",
      "(Iteration 20001 / 30560) loss: 2.304600\n",
      "(Iteration 20101 / 30560) loss: 2.304567\n",
      "(Iteration 20201 / 30560) loss: 2.304624\n",
      "(Epoch 53 / 80) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 20301 / 30560) loss: 2.304602\n",
      "(Iteration 20401 / 30560) loss: 2.304602\n",
      "(Iteration 20501 / 30560) loss: 2.304629\n",
      "(Iteration 20601 / 30560) loss: 2.304568\n",
      "(Epoch 54 / 80) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 20701 / 30560) loss: 2.304584\n",
      "(Iteration 20801 / 30560) loss: 2.304608\n",
      "(Iteration 20901 / 30560) loss: 2.304640\n",
      "(Iteration 21001 / 30560) loss: 2.304586\n",
      "(Epoch 55 / 80) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 21101 / 30560) loss: 2.304576\n",
      "(Iteration 21201 / 30560) loss: 2.304601\n",
      "(Iteration 21301 / 30560) loss: 2.304586\n",
      "(Epoch 56 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 21401 / 30560) loss: 2.304570\n",
      "(Iteration 21501 / 30560) loss: 2.304579\n",
      "(Iteration 21601 / 30560) loss: 2.304557\n",
      "(Iteration 21701 / 30560) loss: 2.304564\n",
      "(Epoch 57 / 80) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 21801 / 30560) loss: 2.304588\n",
      "(Iteration 21901 / 30560) loss: 2.304584\n",
      "(Iteration 22001 / 30560) loss: 2.304583\n",
      "(Iteration 22101 / 30560) loss: 2.304563\n",
      "(Epoch 58 / 80) train acc: 0.110000; val_acc: 0.079000\n",
      "(Iteration 22201 / 30560) loss: 2.304594\n",
      "(Iteration 22301 / 30560) loss: 2.304589\n",
      "(Iteration 22401 / 30560) loss: 2.304593\n",
      "(Iteration 22501 / 30560) loss: 2.304558\n",
      "(Epoch 59 / 80) train acc: 0.122000; val_acc: 0.079000\n",
      "(Iteration 22601 / 30560) loss: 2.304594\n",
      "(Iteration 22701 / 30560) loss: 2.304547\n",
      "(Iteration 22801 / 30560) loss: 2.304582\n",
      "(Iteration 22901 / 30560) loss: 2.304554\n",
      "(Epoch 60 / 80) train acc: 0.122000; val_acc: 0.079000\n",
      "(Iteration 23001 / 30560) loss: 2.304579\n",
      "(Iteration 23101 / 30560) loss: 2.304553\n",
      "(Iteration 23201 / 30560) loss: 2.304597\n",
      "(Iteration 23301 / 30560) loss: 2.304597\n",
      "(Epoch 61 / 80) train acc: 0.112000; val_acc: 0.079000\n",
      "(Iteration 23401 / 30560) loss: 2.304523\n",
      "(Iteration 23501 / 30560) loss: 2.304573\n",
      "(Iteration 23601 / 30560) loss: 2.304593\n",
      "(Epoch 62 / 80) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 23701 / 30560) loss: 2.304524\n",
      "(Iteration 23801 / 30560) loss: 2.304558\n",
      "(Iteration 23901 / 30560) loss: 2.304560\n",
      "(Iteration 24001 / 30560) loss: 2.304552\n",
      "(Epoch 63 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 24101 / 30560) loss: 2.304554\n",
      "(Iteration 24201 / 30560) loss: 2.304553\n",
      "(Iteration 24301 / 30560) loss: 2.304550\n",
      "(Iteration 24401 / 30560) loss: 2.304541\n",
      "(Epoch 64 / 80) train acc: 0.112000; val_acc: 0.079000\n",
      "(Iteration 24501 / 30560) loss: 2.304590\n",
      "(Iteration 24601 / 30560) loss: 2.304564\n",
      "(Iteration 24701 / 30560) loss: 2.304554\n",
      "(Iteration 24801 / 30560) loss: 2.304546\n",
      "(Epoch 65 / 80) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 24901 / 30560) loss: 2.304522\n",
      "(Iteration 25001 / 30560) loss: 2.304544\n",
      "(Iteration 25101 / 30560) loss: 2.304551\n",
      "(Iteration 25201 / 30560) loss: 2.304535\n",
      "(Epoch 66 / 80) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 25301 / 30560) loss: 2.304548\n",
      "(Iteration 25401 / 30560) loss: 2.304539\n",
      "(Iteration 25501 / 30560) loss: 2.304549\n",
      "(Epoch 67 / 80) train acc: 0.076000; val_acc: 0.079000\n",
      "(Iteration 25601 / 30560) loss: 2.304550\n",
      "(Iteration 25701 / 30560) loss: 2.304519\n",
      "(Iteration 25801 / 30560) loss: 2.304537\n",
      "(Iteration 25901 / 30560) loss: 2.304505\n",
      "(Epoch 68 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 26001 / 30560) loss: 2.304542\n",
      "(Iteration 26101 / 30560) loss: 2.304581\n",
      "(Iteration 26201 / 30560) loss: 2.304549\n",
      "(Iteration 26301 / 30560) loss: 2.304526\n",
      "(Epoch 69 / 80) train acc: 0.112000; val_acc: 0.079000\n",
      "(Iteration 26401 / 30560) loss: 2.304515\n",
      "(Iteration 26501 / 30560) loss: 2.304543\n",
      "(Iteration 26601 / 30560) loss: 2.304514\n",
      "(Iteration 26701 / 30560) loss: 2.304529\n",
      "(Epoch 70 / 80) train acc: 0.114000; val_acc: 0.079000\n",
      "(Iteration 26801 / 30560) loss: 2.304563\n",
      "(Iteration 26901 / 30560) loss: 2.304542\n",
      "(Iteration 27001 / 30560) loss: 2.304554\n",
      "(Iteration 27101 / 30560) loss: 2.304520\n",
      "(Epoch 71 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 27201 / 30560) loss: 2.304522\n",
      "(Iteration 27301 / 30560) loss: 2.304544\n",
      "(Iteration 27401 / 30560) loss: 2.304531\n",
      "(Iteration 27501 / 30560) loss: 2.304566\n",
      "(Epoch 72 / 80) train acc: 0.109000; val_acc: 0.079000\n",
      "(Iteration 27601 / 30560) loss: 2.304521\n",
      "(Iteration 27701 / 30560) loss: 2.304540\n",
      "(Iteration 27801 / 30560) loss: 2.304561\n",
      "(Epoch 73 / 80) train acc: 0.109000; val_acc: 0.079000\n",
      "(Iteration 27901 / 30560) loss: 2.304518\n",
      "(Iteration 28001 / 30560) loss: 2.304556\n",
      "(Iteration 28101 / 30560) loss: 2.304565\n",
      "(Iteration 28201 / 30560) loss: 2.304524\n",
      "(Epoch 74 / 80) train acc: 0.084000; val_acc: 0.079000\n",
      "(Iteration 28301 / 30560) loss: 2.304553\n",
      "(Iteration 28401 / 30560) loss: 2.304526\n",
      "(Iteration 28501 / 30560) loss: 2.304546\n",
      "(Iteration 28601 / 30560) loss: 2.304525\n",
      "(Epoch 75 / 80) train acc: 0.092000; val_acc: 0.079000\n",
      "(Iteration 28701 / 30560) loss: 2.304546\n",
      "(Iteration 28801 / 30560) loss: 2.304530\n",
      "(Iteration 28901 / 30560) loss: 2.304552\n",
      "(Iteration 29001 / 30560) loss: 2.304527\n",
      "(Epoch 76 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 29101 / 30560) loss: 2.304537\n",
      "(Iteration 29201 / 30560) loss: 2.304489\n",
      "(Iteration 29301 / 30560) loss: 2.304539\n",
      "(Iteration 29401 / 30560) loss: 2.304530\n",
      "(Epoch 77 / 80) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 29501 / 30560) loss: 2.304553\n",
      "(Iteration 29601 / 30560) loss: 2.304512\n",
      "(Iteration 29701 / 30560) loss: 2.304504\n",
      "(Epoch 78 / 80) train acc: 0.115000; val_acc: 0.079000\n",
      "(Iteration 29801 / 30560) loss: 2.304530\n",
      "(Iteration 29901 / 30560) loss: 2.304530\n",
      "(Iteration 30001 / 30560) loss: 2.304507\n",
      "(Iteration 30101 / 30560) loss: 2.304545\n",
      "(Epoch 79 / 80) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 30201 / 30560) loss: 2.304515\n",
      "(Iteration 30301 / 30560) loss: 2.304538\n",
      "(Iteration 30401 / 30560) loss: 2.304527\n",
      "(Iteration 30501 / 30560) loss: 2.304507\n",
      "(Epoch 80 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.308277\n",
      "(Epoch 0 / 80) train acc: 0.112000; val_acc: 0.127000\n",
      "(Iteration 101 / 61200) loss: 2.308196\n",
      "(Iteration 201 / 61200) loss: 2.308088\n",
      "(Iteration 301 / 61200) loss: 2.308033\n",
      "(Iteration 401 / 61200) loss: 2.307950\n",
      "(Iteration 501 / 61200) loss: 2.307894\n",
      "(Iteration 601 / 61200) loss: 2.307819\n",
      "(Iteration 701 / 61200) loss: 2.307738\n",
      "(Epoch 1 / 80) train acc: 0.109000; val_acc: 0.090000\n",
      "(Iteration 801 / 61200) loss: 2.307652\n",
      "(Iteration 901 / 61200) loss: 2.307606\n",
      "(Iteration 1001 / 61200) loss: 2.307543\n",
      "(Iteration 1101 / 61200) loss: 2.307454\n",
      "(Iteration 1201 / 61200) loss: 2.307379\n",
      "(Iteration 1301 / 61200) loss: 2.307289\n",
      "(Iteration 1401 / 61200) loss: 2.307234\n",
      "(Iteration 1501 / 61200) loss: 2.307185\n",
      "(Epoch 2 / 80) train acc: 0.103000; val_acc: 0.090000\n",
      "(Iteration 1601 / 61200) loss: 2.307155\n",
      "(Iteration 1701 / 61200) loss: 2.307141\n",
      "(Iteration 1801 / 61200) loss: 2.307070\n",
      "(Iteration 1901 / 61200) loss: 2.307039\n",
      "(Iteration 2001 / 61200) loss: 2.307028\n",
      "(Iteration 2101 / 61200) loss: 2.306921\n",
      "(Iteration 2201 / 61200) loss: 2.306851\n",
      "(Epoch 3 / 80) train acc: 0.083000; val_acc: 0.087000\n",
      "(Iteration 2301 / 61200) loss: 2.306797\n",
      "(Iteration 2401 / 61200) loss: 2.306832\n",
      "(Iteration 2501 / 61200) loss: 2.306792\n",
      "(Iteration 2601 / 61200) loss: 2.306690\n",
      "(Iteration 2701 / 61200) loss: 2.306671\n",
      "(Iteration 2801 / 61200) loss: 2.306592\n",
      "(Iteration 2901 / 61200) loss: 2.306593\n",
      "(Iteration 3001 / 61200) loss: 2.306486\n",
      "(Epoch 4 / 80) train acc: 0.118000; val_acc: 0.088000\n",
      "(Iteration 3101 / 61200) loss: 2.306497\n",
      "(Iteration 3201 / 61200) loss: 2.306443\n",
      "(Iteration 3301 / 61200) loss: 2.306404\n",
      "(Iteration 3401 / 61200) loss: 2.306446\n",
      "(Iteration 3501 / 61200) loss: 2.306366\n",
      "(Iteration 3601 / 61200) loss: 2.306331\n",
      "(Iteration 3701 / 61200) loss: 2.306261\n",
      "(Iteration 3801 / 61200) loss: 2.306295\n",
      "(Epoch 5 / 80) train acc: 0.104000; val_acc: 0.091000\n",
      "(Iteration 3901 / 61200) loss: 2.306239\n",
      "(Iteration 4001 / 61200) loss: 2.306155\n",
      "(Iteration 4101 / 61200) loss: 2.306118\n",
      "(Iteration 4201 / 61200) loss: 2.306137\n",
      "(Iteration 4301 / 61200) loss: 2.306079\n",
      "(Iteration 4401 / 61200) loss: 2.306065\n",
      "(Iteration 4501 / 61200) loss: 2.306081\n",
      "(Epoch 6 / 80) train acc: 0.112000; val_acc: 0.089000\n",
      "(Iteration 4601 / 61200) loss: 2.305989\n",
      "(Iteration 4701 / 61200) loss: 2.305980\n",
      "(Iteration 4801 / 61200) loss: 2.305955\n",
      "(Iteration 4901 / 61200) loss: 2.305925\n",
      "(Iteration 5001 / 61200) loss: 2.305917\n",
      "(Iteration 5101 / 61200) loss: 2.305888\n",
      "(Iteration 5201 / 61200) loss: 2.305810\n",
      "(Iteration 5301 / 61200) loss: 2.305867\n",
      "(Epoch 7 / 80) train acc: 0.104000; val_acc: 0.082000\n",
      "(Iteration 5401 / 61200) loss: 2.305861\n",
      "(Iteration 5501 / 61200) loss: 2.305784\n",
      "(Iteration 5601 / 61200) loss: 2.305698\n",
      "(Iteration 5701 / 61200) loss: 2.305752\n",
      "(Iteration 5801 / 61200) loss: 2.305743\n",
      "(Iteration 5901 / 61200) loss: 2.305704\n",
      "(Iteration 6001 / 61200) loss: 2.305611\n",
      "(Iteration 6101 / 61200) loss: 2.305591\n",
      "(Epoch 8 / 80) train acc: 0.085000; val_acc: 0.079000\n",
      "(Iteration 6201 / 61200) loss: 2.305604\n",
      "(Iteration 6301 / 61200) loss: 2.305654\n",
      "(Iteration 6401 / 61200) loss: 2.305525\n",
      "(Iteration 6501 / 61200) loss: 2.305585\n",
      "(Iteration 6601 / 61200) loss: 2.305596\n",
      "(Iteration 6701 / 61200) loss: 2.305563\n",
      "(Iteration 6801 / 61200) loss: 2.305521\n",
      "(Epoch 9 / 80) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 6901 / 61200) loss: 2.305568\n",
      "(Iteration 7001 / 61200) loss: 2.305468\n",
      "(Iteration 7101 / 61200) loss: 2.305474\n",
      "(Iteration 7201 / 61200) loss: 2.305501\n",
      "(Iteration 7301 / 61200) loss: 2.305483\n",
      "(Iteration 7401 / 61200) loss: 2.305456\n",
      "(Iteration 7501 / 61200) loss: 2.305470\n",
      "(Iteration 7601 / 61200) loss: 2.305427\n",
      "(Epoch 10 / 80) train acc: 0.093000; val_acc: 0.079000\n",
      "(Iteration 7701 / 61200) loss: 2.305394\n",
      "(Iteration 7801 / 61200) loss: 2.305352\n",
      "(Iteration 7901 / 61200) loss: 2.305425\n",
      "(Iteration 8001 / 61200) loss: 2.305363\n",
      "(Iteration 8101 / 61200) loss: 2.305336\n",
      "(Iteration 8201 / 61200) loss: 2.305361\n",
      "(Iteration 8301 / 61200) loss: 2.305322\n",
      "(Iteration 8401 / 61200) loss: 2.305277\n",
      "(Epoch 11 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 8501 / 61200) loss: 2.305307\n",
      "(Iteration 8601 / 61200) loss: 2.305288\n",
      "(Iteration 8701 / 61200) loss: 2.305299\n",
      "(Iteration 8801 / 61200) loss: 2.305286\n",
      "(Iteration 8901 / 61200) loss: 2.305316\n",
      "(Iteration 9001 / 61200) loss: 2.305309\n",
      "(Iteration 9101 / 61200) loss: 2.305212\n",
      "(Epoch 12 / 80) train acc: 0.092000; val_acc: 0.079000\n",
      "(Iteration 9201 / 61200) loss: 2.305184\n",
      "(Iteration 9301 / 61200) loss: 2.305175\n",
      "(Iteration 9401 / 61200) loss: 2.305229\n",
      "(Iteration 9501 / 61200) loss: 2.305132\n",
      "(Iteration 9601 / 61200) loss: 2.305181\n",
      "(Iteration 9701 / 61200) loss: 2.305091\n",
      "(Iteration 9801 / 61200) loss: 2.305142\n",
      "(Iteration 9901 / 61200) loss: 2.305099\n",
      "(Epoch 13 / 80) train acc: 0.080000; val_acc: 0.079000\n",
      "(Iteration 10001 / 61200) loss: 2.305206\n",
      "(Iteration 10101 / 61200) loss: 2.305109\n",
      "(Iteration 10201 / 61200) loss: 2.305115\n",
      "(Iteration 10301 / 61200) loss: 2.305051\n",
      "(Iteration 10401 / 61200) loss: 2.305084\n",
      "(Iteration 10501 / 61200) loss: 2.305050\n",
      "(Iteration 10601 / 61200) loss: 2.305084\n",
      "(Iteration 10701 / 61200) loss: 2.305067\n",
      "(Epoch 14 / 80) train acc: 0.092000; val_acc: 0.079000\n",
      "(Iteration 10801 / 61200) loss: 2.305070\n",
      "(Iteration 10901 / 61200) loss: 2.304986\n",
      "(Iteration 11001 / 61200) loss: 2.304984\n",
      "(Iteration 11101 / 61200) loss: 2.305048\n",
      "(Iteration 11201 / 61200) loss: 2.304959\n",
      "(Iteration 11301 / 61200) loss: 2.305044\n",
      "(Iteration 11401 / 61200) loss: 2.304933\n",
      "(Epoch 15 / 80) train acc: 0.093000; val_acc: 0.079000\n",
      "(Iteration 11501 / 61200) loss: 2.305057\n",
      "(Iteration 11601 / 61200) loss: 2.305033\n",
      "(Iteration 11701 / 61200) loss: 2.304967\n",
      "(Iteration 11801 / 61200) loss: 2.304878\n",
      "(Iteration 11901 / 61200) loss: 2.304998\n",
      "(Iteration 12001 / 61200) loss: 2.304927\n",
      "(Iteration 12101 / 61200) loss: 2.304970\n",
      "(Iteration 12201 / 61200) loss: 2.305061\n",
      "(Epoch 16 / 80) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 12301 / 61200) loss: 2.304943\n",
      "(Iteration 12401 / 61200) loss: 2.304933\n",
      "(Iteration 12501 / 61200) loss: 2.304939\n",
      "(Iteration 12601 / 61200) loss: 2.305009\n",
      "(Iteration 12701 / 61200) loss: 2.304921\n",
      "(Iteration 12801 / 61200) loss: 2.304927\n",
      "(Iteration 12901 / 61200) loss: 2.304892\n",
      "(Iteration 13001 / 61200) loss: 2.304932\n",
      "(Epoch 17 / 80) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 13101 / 61200) loss: 2.304883\n",
      "(Iteration 13201 / 61200) loss: 2.304842\n",
      "(Iteration 13301 / 61200) loss: 2.304927\n",
      "(Iteration 13401 / 61200) loss: 2.304841\n",
      "(Iteration 13501 / 61200) loss: 2.304935\n",
      "(Iteration 13601 / 61200) loss: 2.304952\n",
      "(Iteration 13701 / 61200) loss: 2.304906\n",
      "(Epoch 18 / 80) train acc: 0.082000; val_acc: 0.079000\n",
      "(Iteration 13801 / 61200) loss: 2.304849\n",
      "(Iteration 13901 / 61200) loss: 2.304860\n",
      "(Iteration 14001 / 61200) loss: 2.304816\n",
      "(Iteration 14101 / 61200) loss: 2.304882\n",
      "(Iteration 14201 / 61200) loss: 2.304888\n",
      "(Iteration 14301 / 61200) loss: 2.304781\n",
      "(Iteration 14401 / 61200) loss: 2.304862\n",
      "(Iteration 14501 / 61200) loss: 2.304811\n",
      "(Epoch 19 / 80) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 14601 / 61200) loss: 2.304824\n",
      "(Iteration 14701 / 61200) loss: 2.304926\n",
      "(Iteration 14801 / 61200) loss: 2.304870\n",
      "(Iteration 14901 / 61200) loss: 2.304827\n",
      "(Iteration 15001 / 61200) loss: 2.304822\n",
      "(Iteration 15101 / 61200) loss: 2.304813\n",
      "(Iteration 15201 / 61200) loss: 2.304762\n",
      "(Epoch 20 / 80) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 15301 / 61200) loss: 2.304774\n",
      "(Iteration 15401 / 61200) loss: 2.304783\n",
      "(Iteration 15501 / 61200) loss: 2.304803\n",
      "(Iteration 15601 / 61200) loss: 2.304809\n",
      "(Iteration 15701 / 61200) loss: 2.304781\n",
      "(Iteration 15801 / 61200) loss: 2.304845\n",
      "(Iteration 15901 / 61200) loss: 2.304825\n",
      "(Iteration 16001 / 61200) loss: 2.304808\n",
      "(Epoch 21 / 80) train acc: 0.117000; val_acc: 0.079000\n",
      "(Iteration 16101 / 61200) loss: 2.304745\n",
      "(Iteration 16201 / 61200) loss: 2.304708\n",
      "(Iteration 16301 / 61200) loss: 2.304794\n",
      "(Iteration 16401 / 61200) loss: 2.304735\n",
      "(Iteration 16501 / 61200) loss: 2.304718\n",
      "(Iteration 16601 / 61200) loss: 2.304775\n",
      "(Iteration 16701 / 61200) loss: 2.304757\n",
      "(Iteration 16801 / 61200) loss: 2.304745\n",
      "(Epoch 22 / 80) train acc: 0.088000; val_acc: 0.079000\n",
      "(Iteration 16901 / 61200) loss: 2.304842\n",
      "(Iteration 17001 / 61200) loss: 2.304743\n",
      "(Iteration 17101 / 61200) loss: 2.304705\n",
      "(Iteration 17201 / 61200) loss: 2.304672\n",
      "(Iteration 17301 / 61200) loss: 2.304738\n",
      "(Iteration 17401 / 61200) loss: 2.304725\n",
      "(Iteration 17501 / 61200) loss: 2.304757\n",
      "(Epoch 23 / 80) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 17601 / 61200) loss: 2.304819\n",
      "(Iteration 17701 / 61200) loss: 2.304726\n",
      "(Iteration 17801 / 61200) loss: 2.304683\n",
      "(Iteration 17901 / 61200) loss: 2.304729\n",
      "(Iteration 18001 / 61200) loss: 2.304658\n",
      "(Iteration 18101 / 61200) loss: 2.304730\n",
      "(Iteration 18201 / 61200) loss: 2.304683\n",
      "(Iteration 18301 / 61200) loss: 2.304763\n",
      "(Epoch 24 / 80) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 18401 / 61200) loss: 2.304683\n",
      "(Iteration 18501 / 61200) loss: 2.304670\n",
      "(Iteration 18601 / 61200) loss: 2.304695\n",
      "(Iteration 18701 / 61200) loss: 2.304754\n",
      "(Iteration 18801 / 61200) loss: 2.304670\n",
      "(Iteration 18901 / 61200) loss: 2.304623\n",
      "(Iteration 19001 / 61200) loss: 2.304749\n",
      "(Iteration 19101 / 61200) loss: 2.304647\n",
      "(Epoch 25 / 80) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 19201 / 61200) loss: 2.304720\n",
      "(Iteration 19301 / 61200) loss: 2.304654\n",
      "(Iteration 19401 / 61200) loss: 2.304661\n",
      "(Iteration 19501 / 61200) loss: 2.304705\n",
      "(Iteration 19601 / 61200) loss: 2.304661\n",
      "(Iteration 19701 / 61200) loss: 2.304670\n",
      "(Iteration 19801 / 61200) loss: 2.304628\n",
      "(Epoch 26 / 80) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 19901 / 61200) loss: 2.304633\n",
      "(Iteration 20001 / 61200) loss: 2.304625\n",
      "(Iteration 20101 / 61200) loss: 2.304681\n",
      "(Iteration 20201 / 61200) loss: 2.304626\n",
      "(Iteration 20301 / 61200) loss: 2.304678\n",
      "(Iteration 20401 / 61200) loss: 2.304644\n",
      "(Iteration 20501 / 61200) loss: 2.304646\n",
      "(Iteration 20601 / 61200) loss: 2.304623\n",
      "(Epoch 27 / 80) train acc: 0.088000; val_acc: 0.079000\n",
      "(Iteration 20701 / 61200) loss: 2.304661\n",
      "(Iteration 20801 / 61200) loss: 2.304617\n",
      "(Iteration 20901 / 61200) loss: 2.304619\n",
      "(Iteration 21001 / 61200) loss: 2.304575\n",
      "(Iteration 21101 / 61200) loss: 2.304690\n",
      "(Iteration 21201 / 61200) loss: 2.304609\n",
      "(Iteration 21301 / 61200) loss: 2.304611\n",
      "(Iteration 21401 / 61200) loss: 2.304706\n",
      "(Epoch 28 / 80) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 21501 / 61200) loss: 2.304589\n",
      "(Iteration 21601 / 61200) loss: 2.304650\n",
      "(Iteration 21701 / 61200) loss: 2.304639\n",
      "(Iteration 21801 / 61200) loss: 2.304667\n",
      "(Iteration 21901 / 61200) loss: 2.304648\n",
      "(Iteration 22001 / 61200) loss: 2.304655\n",
      "(Iteration 22101 / 61200) loss: 2.304569\n",
      "(Epoch 29 / 80) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 22201 / 61200) loss: 2.304626\n",
      "(Iteration 22301 / 61200) loss: 2.304631\n",
      "(Iteration 22401 / 61200) loss: 2.304619\n",
      "(Iteration 22501 / 61200) loss: 2.304546\n",
      "(Iteration 22601 / 61200) loss: 2.304631\n",
      "(Iteration 22701 / 61200) loss: 2.304671\n",
      "(Iteration 22801 / 61200) loss: 2.304677\n",
      "(Iteration 22901 / 61200) loss: 2.304632\n",
      "(Epoch 30 / 80) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 23001 / 61200) loss: 2.304623\n",
      "(Iteration 23101 / 61200) loss: 2.304619\n",
      "(Iteration 23201 / 61200) loss: 2.304690\n",
      "(Iteration 23301 / 61200) loss: 2.304633\n",
      "(Iteration 23401 / 61200) loss: 2.304654\n",
      "(Iteration 23501 / 61200) loss: 2.304669\n",
      "(Iteration 23601 / 61200) loss: 2.304640\n",
      "(Iteration 23701 / 61200) loss: 2.304583\n",
      "(Epoch 31 / 80) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 23801 / 61200) loss: 2.304584\n",
      "(Iteration 23901 / 61200) loss: 2.304631\n",
      "(Iteration 24001 / 61200) loss: 2.304653\n",
      "(Iteration 24101 / 61200) loss: 2.304674\n",
      "(Iteration 24201 / 61200) loss: 2.304610\n",
      "(Iteration 24301 / 61200) loss: 2.304514\n",
      "(Iteration 24401 / 61200) loss: 2.304631\n",
      "(Epoch 32 / 80) train acc: 0.112000; val_acc: 0.079000\n",
      "(Iteration 24501 / 61200) loss: 2.304610\n",
      "(Iteration 24601 / 61200) loss: 2.304646\n",
      "(Iteration 24701 / 61200) loss: 2.304591\n",
      "(Iteration 24801 / 61200) loss: 2.304648\n",
      "(Iteration 24901 / 61200) loss: 2.304578\n",
      "(Iteration 25001 / 61200) loss: 2.304615\n",
      "(Iteration 25101 / 61200) loss: 2.304693\n",
      "(Iteration 25201 / 61200) loss: 2.304660\n",
      "(Epoch 33 / 80) train acc: 0.109000; val_acc: 0.079000\n",
      "(Iteration 25301 / 61200) loss: 2.304649\n",
      "(Iteration 25401 / 61200) loss: 2.304638\n",
      "(Iteration 25501 / 61200) loss: 2.304628\n",
      "(Iteration 25601 / 61200) loss: 2.304636\n",
      "(Iteration 25701 / 61200) loss: 2.304624\n",
      "(Iteration 25801 / 61200) loss: 2.304580\n",
      "(Iteration 25901 / 61200) loss: 2.304570\n",
      "(Iteration 26001 / 61200) loss: 2.304563\n",
      "(Epoch 34 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 26101 / 61200) loss: 2.304628\n",
      "(Iteration 26201 / 61200) loss: 2.304607\n",
      "(Iteration 26301 / 61200) loss: 2.304556\n",
      "(Iteration 26401 / 61200) loss: 2.304544\n",
      "(Iteration 26501 / 61200) loss: 2.304597\n",
      "(Iteration 26601 / 61200) loss: 2.304629\n",
      "(Iteration 26701 / 61200) loss: 2.304568\n",
      "(Epoch 35 / 80) train acc: 0.109000; val_acc: 0.079000\n",
      "(Iteration 26801 / 61200) loss: 2.304585\n",
      "(Iteration 26901 / 61200) loss: 2.304582\n",
      "(Iteration 27001 / 61200) loss: 2.304622\n",
      "(Iteration 27101 / 61200) loss: 2.304575\n",
      "(Iteration 27201 / 61200) loss: 2.304520\n",
      "(Iteration 27301 / 61200) loss: 2.304644\n",
      "(Iteration 27401 / 61200) loss: 2.304607\n",
      "(Iteration 27501 / 61200) loss: 2.304524\n",
      "(Epoch 36 / 80) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 27601 / 61200) loss: 2.304569\n",
      "(Iteration 27701 / 61200) loss: 2.304557\n",
      "(Iteration 27801 / 61200) loss: 2.304621\n",
      "(Iteration 27901 / 61200) loss: 2.304571\n",
      "(Iteration 28001 / 61200) loss: 2.304496\n",
      "(Iteration 28101 / 61200) loss: 2.304549\n",
      "(Iteration 28201 / 61200) loss: 2.304588\n",
      "(Iteration 28301 / 61200) loss: 2.304509\n",
      "(Epoch 37 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 28401 / 61200) loss: 2.304526\n",
      "(Iteration 28501 / 61200) loss: 2.304577\n",
      "(Iteration 28601 / 61200) loss: 2.304600\n",
      "(Iteration 28701 / 61200) loss: 2.304511\n",
      "(Iteration 28801 / 61200) loss: 2.304625\n",
      "(Iteration 28901 / 61200) loss: 2.304568\n",
      "(Iteration 29001 / 61200) loss: 2.304525\n",
      "(Epoch 38 / 80) train acc: 0.087000; val_acc: 0.079000\n",
      "(Iteration 29101 / 61200) loss: 2.304509\n",
      "(Iteration 29201 / 61200) loss: 2.304536\n",
      "(Iteration 29301 / 61200) loss: 2.304576\n",
      "(Iteration 29401 / 61200) loss: 2.304535\n",
      "(Iteration 29501 / 61200) loss: 2.304533\n",
      "(Iteration 29601 / 61200) loss: 2.304657\n",
      "(Iteration 29701 / 61200) loss: 2.304602\n",
      "(Iteration 29801 / 61200) loss: 2.304577\n",
      "(Epoch 39 / 80) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 29901 / 61200) loss: 2.304566\n",
      "(Iteration 30001 / 61200) loss: 2.304588\n",
      "(Iteration 30101 / 61200) loss: 2.304584\n",
      "(Iteration 30201 / 61200) loss: 2.304587\n",
      "(Iteration 30301 / 61200) loss: 2.304548\n",
      "(Iteration 30401 / 61200) loss: 2.304620\n",
      "(Iteration 30501 / 61200) loss: 2.304497\n",
      "(Epoch 40 / 80) train acc: 0.122000; val_acc: 0.079000\n",
      "(Iteration 30601 / 61200) loss: 2.304592\n",
      "(Iteration 30701 / 61200) loss: 2.304549\n",
      "(Iteration 30801 / 61200) loss: 2.304571\n",
      "(Iteration 30901 / 61200) loss: 2.304573\n",
      "(Iteration 31001 / 61200) loss: 2.304539\n",
      "(Iteration 31101 / 61200) loss: 2.304490\n",
      "(Iteration 31201 / 61200) loss: 2.304551\n",
      "(Iteration 31301 / 61200) loss: 2.304567\n",
      "(Epoch 41 / 80) train acc: 0.087000; val_acc: 0.079000\n",
      "(Iteration 31401 / 61200) loss: 2.304592\n",
      "(Iteration 31501 / 61200) loss: 2.304544\n",
      "(Iteration 31601 / 61200) loss: 2.304610\n",
      "(Iteration 31701 / 61200) loss: 2.304493\n",
      "(Iteration 31801 / 61200) loss: 2.304549\n",
      "(Iteration 31901 / 61200) loss: 2.304580\n",
      "(Iteration 32001 / 61200) loss: 2.304527\n",
      "(Iteration 32101 / 61200) loss: 2.304565\n",
      "(Epoch 42 / 80) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 32201 / 61200) loss: 2.304503\n",
      "(Iteration 32301 / 61200) loss: 2.304528\n",
      "(Iteration 32401 / 61200) loss: 2.304539\n",
      "(Iteration 32501 / 61200) loss: 2.304524\n",
      "(Iteration 32601 / 61200) loss: 2.304529\n",
      "(Iteration 32701 / 61200) loss: 2.304465\n",
      "(Iteration 32801 / 61200) loss: 2.304580\n",
      "(Epoch 43 / 80) train acc: 0.089000; val_acc: 0.079000\n",
      "(Iteration 32901 / 61200) loss: 2.304559\n",
      "(Iteration 33001 / 61200) loss: 2.304506\n",
      "(Iteration 33101 / 61200) loss: 2.304593\n",
      "(Iteration 33201 / 61200) loss: 2.304689\n",
      "(Iteration 33301 / 61200) loss: 2.304481\n",
      "(Iteration 33401 / 61200) loss: 2.304532\n",
      "(Iteration 33501 / 61200) loss: 2.304517\n",
      "(Iteration 33601 / 61200) loss: 2.304513\n",
      "(Epoch 44 / 80) train acc: 0.083000; val_acc: 0.079000\n",
      "(Iteration 33701 / 61200) loss: 2.304522\n",
      "(Iteration 33801 / 61200) loss: 2.304537\n",
      "(Iteration 33901 / 61200) loss: 2.304567\n",
      "(Iteration 34001 / 61200) loss: 2.304518\n",
      "(Iteration 34101 / 61200) loss: 2.304488\n",
      "(Iteration 34201 / 61200) loss: 2.304581\n",
      "(Iteration 34301 / 61200) loss: 2.304548\n",
      "(Iteration 34401 / 61200) loss: 2.304568\n",
      "(Epoch 45 / 80) train acc: 0.079000; val_acc: 0.079000\n",
      "(Iteration 34501 / 61200) loss: 2.304498\n",
      "(Iteration 34601 / 61200) loss: 2.304553\n",
      "(Iteration 34701 / 61200) loss: 2.304591\n",
      "(Iteration 34801 / 61200) loss: 2.304593\n",
      "(Iteration 34901 / 61200) loss: 2.304562\n",
      "(Iteration 35001 / 61200) loss: 2.304534\n",
      "(Iteration 35101 / 61200) loss: 2.304558\n",
      "(Epoch 46 / 80) train acc: 0.090000; val_acc: 0.079000\n",
      "(Iteration 35201 / 61200) loss: 2.304520\n",
      "(Iteration 35301 / 61200) loss: 2.304491\n",
      "(Iteration 35401 / 61200) loss: 2.304611\n",
      "(Iteration 35501 / 61200) loss: 2.304576\n",
      "(Iteration 35601 / 61200) loss: 2.304457\n",
      "(Iteration 35701 / 61200) loss: 2.304469\n",
      "(Iteration 35801 / 61200) loss: 2.304609\n",
      "(Iteration 35901 / 61200) loss: 2.304463\n",
      "(Epoch 47 / 80) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 36001 / 61200) loss: 2.304558\n",
      "(Iteration 36101 / 61200) loss: 2.304445\n",
      "(Iteration 36201 / 61200) loss: 2.304495\n",
      "(Iteration 36301 / 61200) loss: 2.304645\n",
      "(Iteration 36401 / 61200) loss: 2.304610\n",
      "(Iteration 36501 / 61200) loss: 2.304559\n",
      "(Iteration 36601 / 61200) loss: 2.304501\n",
      "(Iteration 36701 / 61200) loss: 2.304499\n",
      "(Epoch 48 / 80) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 36801 / 61200) loss: 2.304509\n",
      "(Iteration 36901 / 61200) loss: 2.304513\n",
      "(Iteration 37001 / 61200) loss: 2.304497\n",
      "(Iteration 37101 / 61200) loss: 2.304557\n",
      "(Iteration 37201 / 61200) loss: 2.304526\n",
      "(Iteration 37301 / 61200) loss: 2.304523\n",
      "(Iteration 37401 / 61200) loss: 2.304580\n",
      "(Epoch 49 / 80) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 37501 / 61200) loss: 2.304606\n",
      "(Iteration 37601 / 61200) loss: 2.304505\n",
      "(Iteration 37701 / 61200) loss: 2.304634\n",
      "(Iteration 37801 / 61200) loss: 2.304504\n",
      "(Iteration 37901 / 61200) loss: 2.304577\n",
      "(Iteration 38001 / 61200) loss: 2.304543\n",
      "(Iteration 38101 / 61200) loss: 2.304586\n",
      "(Iteration 38201 / 61200) loss: 2.304567\n",
      "(Epoch 50 / 80) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 38301 / 61200) loss: 2.304514\n",
      "(Iteration 38401 / 61200) loss: 2.304489\n",
      "(Iteration 38501 / 61200) loss: 2.304416\n",
      "(Iteration 38601 / 61200) loss: 2.304441\n",
      "(Iteration 38701 / 61200) loss: 2.304602\n",
      "(Iteration 38801 / 61200) loss: 2.304534\n",
      "(Iteration 38901 / 61200) loss: 2.304449\n",
      "(Iteration 39001 / 61200) loss: 2.304518\n",
      "(Epoch 51 / 80) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 39101 / 61200) loss: 2.304560\n",
      "(Iteration 39201 / 61200) loss: 2.304501\n",
      "(Iteration 39301 / 61200) loss: 2.304551\n",
      "(Iteration 39401 / 61200) loss: 2.304466\n",
      "(Iteration 39501 / 61200) loss: 2.304512\n",
      "(Iteration 39601 / 61200) loss: 2.304630\n",
      "(Iteration 39701 / 61200) loss: 2.304488\n",
      "(Epoch 52 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 39801 / 61200) loss: 2.304587\n",
      "(Iteration 39901 / 61200) loss: 2.304520\n",
      "(Iteration 40001 / 61200) loss: 2.304592\n",
      "(Iteration 40101 / 61200) loss: 2.304491\n",
      "(Iteration 40201 / 61200) loss: 2.304573\n",
      "(Iteration 40301 / 61200) loss: 2.304500\n",
      "(Iteration 40401 / 61200) loss: 2.304517\n",
      "(Iteration 40501 / 61200) loss: 2.304609\n",
      "(Epoch 53 / 80) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 40601 / 61200) loss: 2.304468\n",
      "(Iteration 40701 / 61200) loss: 2.304525\n",
      "(Iteration 40801 / 61200) loss: 2.304529\n",
      "(Iteration 40901 / 61200) loss: 2.304483\n",
      "(Iteration 41001 / 61200) loss: 2.304581\n",
      "(Iteration 41101 / 61200) loss: 2.304513\n",
      "(Iteration 41201 / 61200) loss: 2.304514\n",
      "(Iteration 41301 / 61200) loss: 2.304587\n",
      "(Epoch 54 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 41401 / 61200) loss: 2.304453\n",
      "(Iteration 41501 / 61200) loss: 2.304529\n",
      "(Iteration 41601 / 61200) loss: 2.304528\n",
      "(Iteration 41701 / 61200) loss: 2.304545\n",
      "(Iteration 41801 / 61200) loss: 2.304512\n",
      "(Iteration 41901 / 61200) loss: 2.304558\n",
      "(Iteration 42001 / 61200) loss: 2.304456\n",
      "(Epoch 55 / 80) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 42101 / 61200) loss: 2.304561\n",
      "(Iteration 42201 / 61200) loss: 2.304506\n",
      "(Iteration 42301 / 61200) loss: 2.304565\n",
      "(Iteration 42401 / 61200) loss: 2.304461\n",
      "(Iteration 42501 / 61200) loss: 2.304470\n",
      "(Iteration 42601 / 61200) loss: 2.304498\n",
      "(Iteration 42701 / 61200) loss: 2.304534\n",
      "(Iteration 42801 / 61200) loss: 2.304569\n",
      "(Epoch 56 / 80) train acc: 0.086000; val_acc: 0.079000\n",
      "(Iteration 42901 / 61200) loss: 2.304545\n",
      "(Iteration 43001 / 61200) loss: 2.304525\n",
      "(Iteration 43101 / 61200) loss: 2.304532\n",
      "(Iteration 43201 / 61200) loss: 2.304505\n",
      "(Iteration 43301 / 61200) loss: 2.304548\n",
      "(Iteration 43401 / 61200) loss: 2.304485\n",
      "(Iteration 43501 / 61200) loss: 2.304542\n",
      "(Iteration 43601 / 61200) loss: 2.304524\n",
      "(Epoch 57 / 80) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 43701 / 61200) loss: 2.304509\n",
      "(Iteration 43801 / 61200) loss: 2.304526\n",
      "(Iteration 43901 / 61200) loss: 2.304436\n",
      "(Iteration 44001 / 61200) loss: 2.304513\n",
      "(Iteration 44101 / 61200) loss: 2.304437\n",
      "(Iteration 44201 / 61200) loss: 2.304476\n",
      "(Iteration 44301 / 61200) loss: 2.304536\n",
      "(Epoch 58 / 80) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 44401 / 61200) loss: 2.304512\n",
      "(Iteration 44501 / 61200) loss: 2.304485\n",
      "(Iteration 44601 / 61200) loss: 2.304534\n",
      "(Iteration 44701 / 61200) loss: 2.304578\n",
      "(Iteration 44801 / 61200) loss: 2.304572\n",
      "(Iteration 44901 / 61200) loss: 2.304459\n",
      "(Iteration 45001 / 61200) loss: 2.304508\n",
      "(Iteration 45101 / 61200) loss: 2.304619\n",
      "(Epoch 59 / 80) train acc: 0.110000; val_acc: 0.079000\n",
      "(Iteration 45201 / 61200) loss: 2.304597\n",
      "(Iteration 45301 / 61200) loss: 2.304519\n",
      "(Iteration 45401 / 61200) loss: 2.304485\n",
      "(Iteration 45501 / 61200) loss: 2.304466\n",
      "(Iteration 45601 / 61200) loss: 2.304514\n",
      "(Iteration 45701 / 61200) loss: 2.304568\n",
      "(Iteration 45801 / 61200) loss: 2.304522\n",
      "(Epoch 60 / 80) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 45901 / 61200) loss: 2.304433\n",
      "(Iteration 46001 / 61200) loss: 2.304490\n",
      "(Iteration 46101 / 61200) loss: 2.304404\n",
      "(Iteration 46201 / 61200) loss: 2.304540\n",
      "(Iteration 46301 / 61200) loss: 2.304568\n",
      "(Iteration 46401 / 61200) loss: 2.304496\n",
      "(Iteration 46501 / 61200) loss: 2.304472\n",
      "(Iteration 46601 / 61200) loss: 2.304593\n",
      "(Epoch 61 / 80) train acc: 0.117000; val_acc: 0.079000\n",
      "(Iteration 46701 / 61200) loss: 2.304572\n",
      "(Iteration 46801 / 61200) loss: 2.304541\n",
      "(Iteration 46901 / 61200) loss: 2.304546\n",
      "(Iteration 47001 / 61200) loss: 2.304582\n",
      "(Iteration 47101 / 61200) loss: 2.304583\n",
      "(Iteration 47201 / 61200) loss: 2.304566\n",
      "(Iteration 47301 / 61200) loss: 2.304590\n",
      "(Iteration 47401 / 61200) loss: 2.304532\n",
      "(Epoch 62 / 80) train acc: 0.109000; val_acc: 0.079000\n",
      "(Iteration 47501 / 61200) loss: 2.304567\n",
      "(Iteration 47601 / 61200) loss: 2.304489\n",
      "(Iteration 47701 / 61200) loss: 2.304484\n",
      "(Iteration 47801 / 61200) loss: 2.304533\n",
      "(Iteration 47901 / 61200) loss: 2.304525\n",
      "(Iteration 48001 / 61200) loss: 2.304544\n",
      "(Iteration 48101 / 61200) loss: 2.304638\n",
      "(Epoch 63 / 80) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 48201 / 61200) loss: 2.304476\n",
      "(Iteration 48301 / 61200) loss: 2.304535\n",
      "(Iteration 48401 / 61200) loss: 2.304479\n",
      "(Iteration 48501 / 61200) loss: 2.304538\n",
      "(Iteration 48601 / 61200) loss: 2.304589\n",
      "(Iteration 48701 / 61200) loss: 2.304476\n",
      "(Iteration 48801 / 61200) loss: 2.304524\n",
      "(Iteration 48901 / 61200) loss: 2.304502\n",
      "(Epoch 64 / 80) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 49001 / 61200) loss: 2.304571\n",
      "(Iteration 49101 / 61200) loss: 2.304455\n",
      "(Iteration 49201 / 61200) loss: 2.304523\n",
      "(Iteration 49301 / 61200) loss: 2.304536\n",
      "(Iteration 49401 / 61200) loss: 2.304512\n",
      "(Iteration 49501 / 61200) loss: 2.304576\n",
      "(Iteration 49601 / 61200) loss: 2.304493\n",
      "(Iteration 49701 / 61200) loss: 2.304535\n",
      "(Epoch 65 / 80) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 49801 / 61200) loss: 2.304607\n",
      "(Iteration 49901 / 61200) loss: 2.304577\n",
      "(Iteration 50001 / 61200) loss: 2.304432\n",
      "(Iteration 50101 / 61200) loss: 2.304516\n",
      "(Iteration 50201 / 61200) loss: 2.304526\n",
      "(Iteration 50301 / 61200) loss: 2.304540\n",
      "(Iteration 50401 / 61200) loss: 2.304480\n",
      "(Epoch 66 / 80) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 50501 / 61200) loss: 2.304469\n",
      "(Iteration 50601 / 61200) loss: 2.304493\n",
      "(Iteration 50701 / 61200) loss: 2.304469\n",
      "(Iteration 50801 / 61200) loss: 2.304491\n",
      "(Iteration 50901 / 61200) loss: 2.304472\n",
      "(Iteration 51001 / 61200) loss: 2.304567\n",
      "(Iteration 51101 / 61200) loss: 2.304486\n",
      "(Iteration 51201 / 61200) loss: 2.304575\n",
      "(Epoch 67 / 80) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 51301 / 61200) loss: 2.304504\n",
      "(Iteration 51401 / 61200) loss: 2.304412\n",
      "(Iteration 51501 / 61200) loss: 2.304514\n",
      "(Iteration 51601 / 61200) loss: 2.304631\n",
      "(Iteration 51701 / 61200) loss: 2.304562\n",
      "(Iteration 51801 / 61200) loss: 2.304503\n",
      "(Iteration 51901 / 61200) loss: 2.304651\n",
      "(Iteration 52001 / 61200) loss: 2.304471\n",
      "(Epoch 68 / 80) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 52101 / 61200) loss: 2.304496\n",
      "(Iteration 52201 / 61200) loss: 2.304438\n",
      "(Iteration 52301 / 61200) loss: 2.304551\n",
      "(Iteration 52401 / 61200) loss: 2.304539\n",
      "(Iteration 52501 / 61200) loss: 2.304537\n",
      "(Iteration 52601 / 61200) loss: 2.304471\n",
      "(Iteration 52701 / 61200) loss: 2.304483\n",
      "(Epoch 69 / 80) train acc: 0.121000; val_acc: 0.079000\n",
      "(Iteration 52801 / 61200) loss: 2.304445\n",
      "(Iteration 52901 / 61200) loss: 2.304554\n",
      "(Iteration 53001 / 61200) loss: 2.304493\n",
      "(Iteration 53101 / 61200) loss: 2.304444\n",
      "(Iteration 53201 / 61200) loss: 2.304530\n",
      "(Iteration 53301 / 61200) loss: 2.304577\n",
      "(Iteration 53401 / 61200) loss: 2.304495\n",
      "(Iteration 53501 / 61200) loss: 2.304494\n",
      "(Epoch 70 / 80) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 53601 / 61200) loss: 2.304500\n",
      "(Iteration 53701 / 61200) loss: 2.304552\n",
      "(Iteration 53801 / 61200) loss: 2.304469\n",
      "(Iteration 53901 / 61200) loss: 2.304544\n",
      "(Iteration 54001 / 61200) loss: 2.304497\n",
      "(Iteration 54101 / 61200) loss: 2.304584\n",
      "(Iteration 54201 / 61200) loss: 2.304512\n",
      "(Iteration 54301 / 61200) loss: 2.304598\n",
      "(Epoch 71 / 80) train acc: 0.116000; val_acc: 0.079000\n",
      "(Iteration 54401 / 61200) loss: 2.304574\n",
      "(Iteration 54501 / 61200) loss: 2.304496\n",
      "(Iteration 54601 / 61200) loss: 2.304479\n",
      "(Iteration 54701 / 61200) loss: 2.304477\n",
      "(Iteration 54801 / 61200) loss: 2.304572\n",
      "(Iteration 54901 / 61200) loss: 2.304564\n",
      "(Iteration 55001 / 61200) loss: 2.304535\n",
      "(Epoch 72 / 80) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 55101 / 61200) loss: 2.304511\n",
      "(Iteration 55201 / 61200) loss: 2.304587\n",
      "(Iteration 55301 / 61200) loss: 2.304553\n",
      "(Iteration 55401 / 61200) loss: 2.304592\n",
      "(Iteration 55501 / 61200) loss: 2.304526\n",
      "(Iteration 55601 / 61200) loss: 2.304576\n",
      "(Iteration 55701 / 61200) loss: 2.304607\n",
      "(Iteration 55801 / 61200) loss: 2.304449\n",
      "(Epoch 73 / 80) train acc: 0.092000; val_acc: 0.079000\n",
      "(Iteration 55901 / 61200) loss: 2.304500\n",
      "(Iteration 56001 / 61200) loss: 2.304459\n",
      "(Iteration 56101 / 61200) loss: 2.304577\n",
      "(Iteration 56201 / 61200) loss: 2.304493\n",
      "(Iteration 56301 / 61200) loss: 2.304543\n",
      "(Iteration 56401 / 61200) loss: 2.304491\n",
      "(Iteration 56501 / 61200) loss: 2.304565\n",
      "(Iteration 56601 / 61200) loss: 2.304470\n",
      "(Epoch 74 / 80) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 56701 / 61200) loss: 2.304555\n",
      "(Iteration 56801 / 61200) loss: 2.304496\n",
      "(Iteration 56901 / 61200) loss: 2.304603\n",
      "(Iteration 57001 / 61200) loss: 2.304531\n",
      "(Iteration 57101 / 61200) loss: 2.304418\n",
      "(Iteration 57201 / 61200) loss: 2.304499\n",
      "(Iteration 57301 / 61200) loss: 2.304509\n",
      "(Epoch 75 / 80) train acc: 0.110000; val_acc: 0.079000\n",
      "(Iteration 57401 / 61200) loss: 2.304479\n",
      "(Iteration 57501 / 61200) loss: 2.304446\n",
      "(Iteration 57601 / 61200) loss: 2.304493\n",
      "(Iteration 57701 / 61200) loss: 2.304546\n",
      "(Iteration 57801 / 61200) loss: 2.304517\n",
      "(Iteration 57901 / 61200) loss: 2.304556\n",
      "(Iteration 58001 / 61200) loss: 2.304490\n",
      "(Iteration 58101 / 61200) loss: 2.304501\n",
      "(Epoch 76 / 80) train acc: 0.091000; val_acc: 0.079000\n",
      "(Iteration 58201 / 61200) loss: 2.304593\n",
      "(Iteration 58301 / 61200) loss: 2.304489\n",
      "(Iteration 58401 / 61200) loss: 2.304540\n",
      "(Iteration 58501 / 61200) loss: 2.304534\n",
      "(Iteration 58601 / 61200) loss: 2.304610\n",
      "(Iteration 58701 / 61200) loss: 2.304397\n",
      "(Iteration 58801 / 61200) loss: 2.304555\n",
      "(Iteration 58901 / 61200) loss: 2.304535\n",
      "(Epoch 77 / 80) train acc: 0.116000; val_acc: 0.079000\n",
      "(Iteration 59001 / 61200) loss: 2.304448\n",
      "(Iteration 59101 / 61200) loss: 2.304563\n",
      "(Iteration 59201 / 61200) loss: 2.304579\n",
      "(Iteration 59301 / 61200) loss: 2.304542\n",
      "(Iteration 59401 / 61200) loss: 2.304469\n",
      "(Iteration 59501 / 61200) loss: 2.304641\n",
      "(Iteration 59601 / 61200) loss: 2.304504\n",
      "(Epoch 78 / 80) train acc: 0.085000; val_acc: 0.079000\n",
      "(Iteration 59701 / 61200) loss: 2.304589\n",
      "(Iteration 59801 / 61200) loss: 2.304532\n",
      "(Iteration 59901 / 61200) loss: 2.304477\n",
      "(Iteration 60001 / 61200) loss: 2.304464\n",
      "(Iteration 60101 / 61200) loss: 2.304482\n",
      "(Iteration 60201 / 61200) loss: 2.304573\n",
      "(Iteration 60301 / 61200) loss: 2.304540\n",
      "(Iteration 60401 / 61200) loss: 2.304482\n",
      "(Epoch 79 / 80) train acc: 0.084000; val_acc: 0.079000\n",
      "(Iteration 60501 / 61200) loss: 2.304481\n",
      "(Iteration 60601 / 61200) loss: 2.304482\n",
      "(Iteration 60701 / 61200) loss: 2.304556\n",
      "(Iteration 60801 / 61200) loss: 2.304522\n",
      "(Iteration 60901 / 61200) loss: 2.304531\n",
      "(Iteration 61001 / 61200) loss: 2.304529\n",
      "(Iteration 61101 / 61200) loss: 2.304483\n",
      "(Epoch 80 / 80) train acc: 0.100000; val_acc: 0.079000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.308362\n",
      "(Epoch 0 / 80) train acc: 0.082000; val_acc: 0.082000\n",
      "(Iteration 101 / 30560) loss: 2.308287\n",
      "(Iteration 201 / 30560) loss: 2.308203\n",
      "(Iteration 301 / 30560) loss: 2.308117\n",
      "(Epoch 1 / 80) train acc: 0.101000; val_acc: 0.083000\n",
      "(Iteration 401 / 30560) loss: 2.308034\n",
      "(Iteration 501 / 30560) loss: 2.307967\n",
      "(Iteration 601 / 30560) loss: 2.307905\n",
      "(Iteration 701 / 30560) loss: 2.307830\n",
      "(Epoch 2 / 80) train acc: 0.132000; val_acc: 0.085000\n",
      "(Iteration 801 / 30560) loss: 2.307776\n",
      "(Iteration 901 / 30560) loss: 2.307714\n",
      "(Iteration 1001 / 30560) loss: 2.307685\n",
      "(Iteration 1101 / 30560) loss: 2.307608\n",
      "(Epoch 3 / 80) train acc: 0.114000; val_acc: 0.079000\n",
      "(Iteration 1201 / 30560) loss: 2.307537\n",
      "(Iteration 1301 / 30560) loss: 2.307484\n",
      "(Iteration 1401 / 30560) loss: 2.307461\n",
      "(Iteration 1501 / 30560) loss: 2.307396\n",
      "(Epoch 4 / 80) train acc: 0.108000; val_acc: 0.089000\n",
      "(Iteration 1601 / 30560) loss: 2.307353\n",
      "(Iteration 1701 / 30560) loss: 2.307307\n",
      "(Iteration 1801 / 30560) loss: 2.307255\n",
      "(Iteration 1901 / 30560) loss: 2.307254\n",
      "(Epoch 5 / 80) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 2001 / 30560) loss: 2.307178\n",
      "(Iteration 2101 / 30560) loss: 2.307130\n",
      "(Iteration 2201 / 30560) loss: 2.307108\n",
      "(Epoch 6 / 80) train acc: 0.102000; val_acc: 0.081000\n",
      "(Iteration 2301 / 30560) loss: 2.307060\n",
      "(Iteration 2401 / 30560) loss: 2.307049\n",
      "(Iteration 2501 / 30560) loss: 2.306998\n",
      "(Iteration 2601 / 30560) loss: 2.306971\n",
      "(Epoch 7 / 80) train acc: 0.100000; val_acc: 0.083000\n",
      "(Iteration 2701 / 30560) loss: 2.306922\n",
      "(Iteration 2801 / 30560) loss: 2.306913\n",
      "(Iteration 2901 / 30560) loss: 2.306881\n",
      "(Iteration 3001 / 30560) loss: 2.306847\n",
      "(Epoch 8 / 80) train acc: 0.098000; val_acc: 0.085000\n",
      "(Iteration 3101 / 30560) loss: 2.306826\n",
      "(Iteration 3201 / 30560) loss: 2.306778\n",
      "(Iteration 3301 / 30560) loss: 2.306773\n",
      "(Iteration 3401 / 30560) loss: 2.306741\n",
      "(Epoch 9 / 80) train acc: 0.093000; val_acc: 0.083000\n",
      "(Iteration 3501 / 30560) loss: 2.306729\n",
      "(Iteration 3601 / 30560) loss: 2.306710\n",
      "(Iteration 3701 / 30560) loss: 2.306668\n",
      "(Iteration 3801 / 30560) loss: 2.306648\n",
      "(Epoch 10 / 80) train acc: 0.095000; val_acc: 0.081000\n",
      "(Iteration 3901 / 30560) loss: 2.306656\n",
      "(Iteration 4001 / 30560) loss: 2.306636\n",
      "(Iteration 4101 / 30560) loss: 2.306615\n",
      "(Iteration 4201 / 30560) loss: 2.306585\n",
      "(Epoch 11 / 80) train acc: 0.096000; val_acc: 0.083000\n",
      "(Iteration 4301 / 30560) loss: 2.306555\n",
      "(Iteration 4401 / 30560) loss: 2.306524\n",
      "(Iteration 4501 / 30560) loss: 2.306511\n",
      "(Epoch 12 / 80) train acc: 0.095000; val_acc: 0.083000\n",
      "(Iteration 4601 / 30560) loss: 2.306522\n",
      "(Iteration 4701 / 30560) loss: 2.306489\n",
      "(Iteration 4801 / 30560) loss: 2.306459\n",
      "(Iteration 4901 / 30560) loss: 2.306466\n",
      "(Epoch 13 / 80) train acc: 0.102000; val_acc: 0.080000\n",
      "(Iteration 5001 / 30560) loss: 2.306447\n",
      "(Iteration 5101 / 30560) loss: 2.306416\n",
      "(Iteration 5201 / 30560) loss: 2.306424\n",
      "(Iteration 5301 / 30560) loss: 2.306373\n",
      "(Epoch 14 / 80) train acc: 0.118000; val_acc: 0.083000\n",
      "(Iteration 5401 / 30560) loss: 2.306389\n",
      "(Iteration 5501 / 30560) loss: 2.306395\n",
      "(Iteration 5601 / 30560) loss: 2.306387\n",
      "(Iteration 5701 / 30560) loss: 2.306337\n",
      "(Epoch 15 / 80) train acc: 0.105000; val_acc: 0.083000\n",
      "(Iteration 5801 / 30560) loss: 2.306359\n",
      "(Iteration 5901 / 30560) loss: 2.306365\n",
      "(Iteration 6001 / 30560) loss: 2.306317\n",
      "(Iteration 6101 / 30560) loss: 2.306316\n",
      "(Epoch 16 / 80) train acc: 0.111000; val_acc: 0.085000\n",
      "(Iteration 6201 / 30560) loss: 2.306312\n",
      "(Iteration 6301 / 30560) loss: 2.306312\n",
      "(Iteration 6401 / 30560) loss: 2.306280\n",
      "(Epoch 17 / 80) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 6501 / 30560) loss: 2.306268\n",
      "(Iteration 6601 / 30560) loss: 2.306259\n",
      "(Iteration 6701 / 30560) loss: 2.306260\n",
      "(Iteration 6801 / 30560) loss: 2.306242\n",
      "(Epoch 18 / 80) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 6901 / 30560) loss: 2.306238\n",
      "(Iteration 7001 / 30560) loss: 2.306196\n",
      "(Iteration 7101 / 30560) loss: 2.306267\n",
      "(Iteration 7201 / 30560) loss: 2.306230\n",
      "(Epoch 19 / 80) train acc: 0.113000; val_acc: 0.083000\n",
      "(Iteration 7301 / 30560) loss: 2.306189\n",
      "(Iteration 7401 / 30560) loss: 2.306175\n",
      "(Iteration 7501 / 30560) loss: 2.306160\n",
      "(Iteration 7601 / 30560) loss: 2.306206\n",
      "(Epoch 20 / 80) train acc: 0.104000; val_acc: 0.086000\n",
      "(Iteration 7701 / 30560) loss: 2.306192\n",
      "(Iteration 7801 / 30560) loss: 2.306178\n",
      "(Iteration 7901 / 30560) loss: 2.306153\n",
      "(Iteration 8001 / 30560) loss: 2.306174\n",
      "(Epoch 21 / 80) train acc: 0.107000; val_acc: 0.085000\n",
      "(Iteration 8101 / 30560) loss: 2.306166\n",
      "(Iteration 8201 / 30560) loss: 2.306125\n",
      "(Iteration 8301 / 30560) loss: 2.306159\n",
      "(Iteration 8401 / 30560) loss: 2.306143\n",
      "(Epoch 22 / 80) train acc: 0.089000; val_acc: 0.086000\n",
      "(Iteration 8501 / 30560) loss: 2.306121\n",
      "(Iteration 8601 / 30560) loss: 2.306128\n",
      "(Iteration 8701 / 30560) loss: 2.306141\n",
      "(Epoch 23 / 80) train acc: 0.109000; val_acc: 0.085000\n",
      "(Iteration 8801 / 30560) loss: 2.306139\n",
      "(Iteration 8901 / 30560) loss: 2.306121\n",
      "(Iteration 9001 / 30560) loss: 2.306118\n",
      "(Iteration 9101 / 30560) loss: 2.306113\n",
      "(Epoch 24 / 80) train acc: 0.084000; val_acc: 0.085000\n",
      "(Iteration 9201 / 30560) loss: 2.306126\n",
      "(Iteration 9301 / 30560) loss: 2.306109\n",
      "(Iteration 9401 / 30560) loss: 2.306078\n",
      "(Iteration 9501 / 30560) loss: 2.306106\n",
      "(Epoch 25 / 80) train acc: 0.096000; val_acc: 0.085000\n",
      "(Iteration 9601 / 30560) loss: 2.306101\n",
      "(Iteration 9701 / 30560) loss: 2.306060\n",
      "(Iteration 9801 / 30560) loss: 2.306082\n",
      "(Iteration 9901 / 30560) loss: 2.306099\n",
      "(Epoch 26 / 80) train acc: 0.097000; val_acc: 0.086000\n",
      "(Iteration 10001 / 30560) loss: 2.306091\n",
      "(Iteration 10101 / 30560) loss: 2.306075\n",
      "(Iteration 10201 / 30560) loss: 2.306102\n",
      "(Iteration 10301 / 30560) loss: 2.306039\n",
      "(Epoch 27 / 80) train acc: 0.114000; val_acc: 0.085000\n",
      "(Iteration 10401 / 30560) loss: 2.306071\n",
      "(Iteration 10501 / 30560) loss: 2.306068\n",
      "(Iteration 10601 / 30560) loss: 2.306072\n",
      "(Epoch 28 / 80) train acc: 0.108000; val_acc: 0.084000\n",
      "(Iteration 10701 / 30560) loss: 2.306051\n",
      "(Iteration 10801 / 30560) loss: 2.306036\n",
      "(Iteration 10901 / 30560) loss: 2.306048\n",
      "(Iteration 11001 / 30560) loss: 2.306060\n",
      "(Epoch 29 / 80) train acc: 0.097000; val_acc: 0.085000\n",
      "(Iteration 11101 / 30560) loss: 2.306038\n",
      "(Iteration 11201 / 30560) loss: 2.306055\n",
      "(Iteration 11301 / 30560) loss: 2.306050\n",
      "(Iteration 11401 / 30560) loss: 2.306058\n",
      "(Epoch 30 / 80) train acc: 0.107000; val_acc: 0.087000\n",
      "(Iteration 11501 / 30560) loss: 2.306032\n",
      "(Iteration 11601 / 30560) loss: 2.306059\n",
      "(Iteration 11701 / 30560) loss: 2.306015\n",
      "(Iteration 11801 / 30560) loss: 2.306032\n",
      "(Epoch 31 / 80) train acc: 0.108000; val_acc: 0.086000\n",
      "(Iteration 11901 / 30560) loss: 2.306011\n",
      "(Iteration 12001 / 30560) loss: 2.305996\n",
      "(Iteration 12101 / 30560) loss: 2.306010\n",
      "(Iteration 12201 / 30560) loss: 2.306044\n",
      "(Epoch 32 / 80) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 12301 / 30560) loss: 2.306031\n",
      "(Iteration 12401 / 30560) loss: 2.305994\n",
      "(Iteration 12501 / 30560) loss: 2.306005\n",
      "(Iteration 12601 / 30560) loss: 2.306012\n",
      "(Epoch 33 / 80) train acc: 0.106000; val_acc: 0.086000\n",
      "(Iteration 12701 / 30560) loss: 2.306038\n",
      "(Iteration 12801 / 30560) loss: 2.305997\n",
      "(Iteration 12901 / 30560) loss: 2.306026\n",
      "(Epoch 34 / 80) train acc: 0.106000; val_acc: 0.085000\n",
      "(Iteration 13001 / 30560) loss: 2.305991\n",
      "(Iteration 13101 / 30560) loss: 2.305990\n",
      "(Iteration 13201 / 30560) loss: 2.306011\n",
      "(Iteration 13301 / 30560) loss: 2.306017\n",
      "(Epoch 35 / 80) train acc: 0.116000; val_acc: 0.085000\n",
      "(Iteration 13401 / 30560) loss: 2.305997\n",
      "(Iteration 13501 / 30560) loss: 2.306015\n",
      "(Iteration 13601 / 30560) loss: 2.306005\n",
      "(Iteration 13701 / 30560) loss: 2.306004\n",
      "(Epoch 36 / 80) train acc: 0.112000; val_acc: 0.086000\n",
      "(Iteration 13801 / 30560) loss: 2.306010\n",
      "(Iteration 13901 / 30560) loss: 2.305979\n",
      "(Iteration 14001 / 30560) loss: 2.305978\n",
      "(Iteration 14101 / 30560) loss: 2.305961\n",
      "(Epoch 37 / 80) train acc: 0.083000; val_acc: 0.084000\n",
      "(Iteration 14201 / 30560) loss: 2.306009\n",
      "(Iteration 14301 / 30560) loss: 2.306014\n",
      "(Iteration 14401 / 30560) loss: 2.305968\n",
      "(Iteration 14501 / 30560) loss: 2.305976\n",
      "(Epoch 38 / 80) train acc: 0.112000; val_acc: 0.084000\n",
      "(Iteration 14601 / 30560) loss: 2.306006\n",
      "(Iteration 14701 / 30560) loss: 2.305969\n",
      "(Iteration 14801 / 30560) loss: 2.305987\n",
      "(Epoch 39 / 80) train acc: 0.097000; val_acc: 0.084000\n",
      "(Iteration 14901 / 30560) loss: 2.305969\n",
      "(Iteration 15001 / 30560) loss: 2.305967\n",
      "(Iteration 15101 / 30560) loss: 2.305967\n",
      "(Iteration 15201 / 30560) loss: 2.305994\n",
      "(Epoch 40 / 80) train acc: 0.114000; val_acc: 0.084000\n",
      "(Iteration 15301 / 30560) loss: 2.306003\n",
      "(Iteration 15401 / 30560) loss: 2.305996\n",
      "(Iteration 15501 / 30560) loss: 2.305968\n",
      "(Iteration 15601 / 30560) loss: 2.305992\n",
      "(Epoch 41 / 80) train acc: 0.116000; val_acc: 0.084000\n",
      "(Iteration 15701 / 30560) loss: 2.306004\n",
      "(Iteration 15801 / 30560) loss: 2.305983\n",
      "(Iteration 15901 / 30560) loss: 2.305955\n",
      "(Iteration 16001 / 30560) loss: 2.305975\n",
      "(Epoch 42 / 80) train acc: 0.116000; val_acc: 0.084000\n",
      "(Iteration 16101 / 30560) loss: 2.305979\n",
      "(Iteration 16201 / 30560) loss: 2.305988\n",
      "(Iteration 16301 / 30560) loss: 2.305966\n",
      "(Iteration 16401 / 30560) loss: 2.305990\n",
      "(Epoch 43 / 80) train acc: 0.115000; val_acc: 0.084000\n",
      "(Iteration 16501 / 30560) loss: 2.305983\n",
      "(Iteration 16601 / 30560) loss: 2.305938\n",
      "(Iteration 16701 / 30560) loss: 2.305950\n",
      "(Iteration 16801 / 30560) loss: 2.305962\n",
      "(Epoch 44 / 80) train acc: 0.098000; val_acc: 0.084000\n",
      "(Iteration 16901 / 30560) loss: 2.305976\n",
      "(Iteration 17001 / 30560) loss: 2.305990\n",
      "(Iteration 17101 / 30560) loss: 2.306029\n",
      "(Epoch 45 / 80) train acc: 0.096000; val_acc: 0.084000\n",
      "(Iteration 17201 / 30560) loss: 2.305992\n",
      "(Iteration 17301 / 30560) loss: 2.305968\n",
      "(Iteration 17401 / 30560) loss: 2.305989\n",
      "(Iteration 17501 / 30560) loss: 2.305975\n",
      "(Epoch 46 / 80) train acc: 0.117000; val_acc: 0.084000\n",
      "(Iteration 17601 / 30560) loss: 2.305968\n",
      "(Iteration 17701 / 30560) loss: 2.305951\n",
      "(Iteration 17801 / 30560) loss: 2.305985\n",
      "(Iteration 17901 / 30560) loss: 2.305959\n",
      "(Epoch 47 / 80) train acc: 0.105000; val_acc: 0.084000\n",
      "(Iteration 18001 / 30560) loss: 2.305981\n",
      "(Iteration 18101 / 30560) loss: 2.305995\n",
      "(Iteration 18201 / 30560) loss: 2.305961\n",
      "(Iteration 18301 / 30560) loss: 2.305969\n",
      "(Epoch 48 / 80) train acc: 0.106000; val_acc: 0.084000\n",
      "(Iteration 18401 / 30560) loss: 2.305992\n",
      "(Iteration 18501 / 30560) loss: 2.305946\n",
      "(Iteration 18601 / 30560) loss: 2.305978\n",
      "(Iteration 18701 / 30560) loss: 2.305991\n",
      "(Epoch 49 / 80) train acc: 0.109000; val_acc: 0.084000\n",
      "(Iteration 18801 / 30560) loss: 2.305973\n",
      "(Iteration 18901 / 30560) loss: 2.306010\n",
      "(Iteration 19001 / 30560) loss: 2.305964\n",
      "(Epoch 50 / 80) train acc: 0.096000; val_acc: 0.084000\n",
      "(Iteration 19101 / 30560) loss: 2.305992\n",
      "(Iteration 19201 / 30560) loss: 2.305982\n",
      "(Iteration 19301 / 30560) loss: 2.306005\n",
      "(Iteration 19401 / 30560) loss: 2.305972\n",
      "(Epoch 51 / 80) train acc: 0.098000; val_acc: 0.084000\n",
      "(Iteration 19501 / 30560) loss: 2.305986\n",
      "(Iteration 19601 / 30560) loss: 2.305985\n",
      "(Iteration 19701 / 30560) loss: 2.305976\n",
      "(Iteration 19801 / 30560) loss: 2.305954\n",
      "(Epoch 52 / 80) train acc: 0.104000; val_acc: 0.084000\n",
      "(Iteration 19901 / 30560) loss: 2.305966\n",
      "(Iteration 20001 / 30560) loss: 2.305975\n",
      "(Iteration 20101 / 30560) loss: 2.305975\n",
      "(Iteration 20201 / 30560) loss: 2.305964\n",
      "(Epoch 53 / 80) train acc: 0.107000; val_acc: 0.084000\n",
      "(Iteration 20301 / 30560) loss: 2.305961\n",
      "(Iteration 20401 / 30560) loss: 2.305972\n",
      "(Iteration 20501 / 30560) loss: 2.305958\n",
      "(Iteration 20601 / 30560) loss: 2.305976\n",
      "(Epoch 54 / 80) train acc: 0.107000; val_acc: 0.084000\n",
      "(Iteration 20701 / 30560) loss: 2.305985\n",
      "(Iteration 20801 / 30560) loss: 2.305933\n",
      "(Iteration 20901 / 30560) loss: 2.305945\n",
      "(Iteration 21001 / 30560) loss: 2.305929\n",
      "(Epoch 55 / 80) train acc: 0.104000; val_acc: 0.084000\n",
      "(Iteration 21101 / 30560) loss: 2.305968\n",
      "(Iteration 21201 / 30560) loss: 2.305954\n",
      "(Iteration 21301 / 30560) loss: 2.305949\n",
      "(Epoch 56 / 80) train acc: 0.125000; val_acc: 0.084000\n",
      "(Iteration 21401 / 30560) loss: 2.305950\n",
      "(Iteration 21501 / 30560) loss: 2.305987\n",
      "(Iteration 21601 / 30560) loss: 2.305977\n",
      "(Iteration 21701 / 30560) loss: 2.305948\n",
      "(Epoch 57 / 80) train acc: 0.091000; val_acc: 0.084000\n",
      "(Iteration 21801 / 30560) loss: 2.305980\n",
      "(Iteration 21901 / 30560) loss: 2.305967\n",
      "(Iteration 22001 / 30560) loss: 2.305963\n",
      "(Iteration 22101 / 30560) loss: 2.305968\n",
      "(Epoch 58 / 80) train acc: 0.110000; val_acc: 0.084000\n",
      "(Iteration 22201 / 30560) loss: 2.305965\n",
      "(Iteration 22301 / 30560) loss: 2.305959\n",
      "(Iteration 22401 / 30560) loss: 2.305972\n",
      "(Iteration 22501 / 30560) loss: 2.305936\n",
      "(Epoch 59 / 80) train acc: 0.094000; val_acc: 0.084000\n",
      "(Iteration 22601 / 30560) loss: 2.305977\n",
      "(Iteration 22701 / 30560) loss: 2.305950\n",
      "(Iteration 22801 / 30560) loss: 2.305940\n",
      "(Iteration 22901 / 30560) loss: 2.305957\n",
      "(Epoch 60 / 80) train acc: 0.104000; val_acc: 0.084000\n",
      "(Iteration 23001 / 30560) loss: 2.305974\n",
      "(Iteration 23101 / 30560) loss: 2.305939\n",
      "(Iteration 23201 / 30560) loss: 2.305979\n",
      "(Iteration 23301 / 30560) loss: 2.305957\n",
      "(Epoch 61 / 80) train acc: 0.086000; val_acc: 0.084000\n",
      "(Iteration 23401 / 30560) loss: 2.305935\n",
      "(Iteration 23501 / 30560) loss: 2.305977\n",
      "(Iteration 23601 / 30560) loss: 2.305966\n",
      "(Epoch 62 / 80) train acc: 0.096000; val_acc: 0.084000\n",
      "(Iteration 23701 / 30560) loss: 2.305980\n",
      "(Iteration 23801 / 30560) loss: 2.305954\n",
      "(Iteration 23901 / 30560) loss: 2.305947\n",
      "(Iteration 24001 / 30560) loss: 2.305974\n",
      "(Epoch 63 / 80) train acc: 0.109000; val_acc: 0.084000\n",
      "(Iteration 24101 / 30560) loss: 2.305973\n",
      "(Iteration 24201 / 30560) loss: 2.305957\n",
      "(Iteration 24301 / 30560) loss: 2.305953\n",
      "(Iteration 24401 / 30560) loss: 2.305960\n",
      "(Epoch 64 / 80) train acc: 0.100000; val_acc: 0.084000\n",
      "(Iteration 24501 / 30560) loss: 2.305949\n",
      "(Iteration 24601 / 30560) loss: 2.305983\n",
      "(Iteration 24701 / 30560) loss: 2.305944\n",
      "(Iteration 24801 / 30560) loss: 2.305963\n",
      "(Epoch 65 / 80) train acc: 0.097000; val_acc: 0.084000\n",
      "(Iteration 24901 / 30560) loss: 2.305962\n",
      "(Iteration 25001 / 30560) loss: 2.305978\n",
      "(Iteration 25101 / 30560) loss: 2.305976\n",
      "(Iteration 25201 / 30560) loss: 2.305938\n",
      "(Epoch 66 / 80) train acc: 0.113000; val_acc: 0.084000\n",
      "(Iteration 25301 / 30560) loss: 2.305979\n",
      "(Iteration 25401 / 30560) loss: 2.305963\n",
      "(Iteration 25501 / 30560) loss: 2.305980\n",
      "(Epoch 67 / 80) train acc: 0.106000; val_acc: 0.084000\n",
      "(Iteration 25601 / 30560) loss: 2.305947\n",
      "(Iteration 25701 / 30560) loss: 2.305958\n",
      "(Iteration 25801 / 30560) loss: 2.305963\n",
      "(Iteration 25901 / 30560) loss: 2.306020\n",
      "(Epoch 68 / 80) train acc: 0.106000; val_acc: 0.084000\n",
      "(Iteration 26001 / 30560) loss: 2.305954\n",
      "(Iteration 26101 / 30560) loss: 2.305968\n",
      "(Iteration 26201 / 30560) loss: 2.305924\n",
      "(Iteration 26301 / 30560) loss: 2.305956\n",
      "(Epoch 69 / 80) train acc: 0.106000; val_acc: 0.084000\n",
      "(Iteration 26401 / 30560) loss: 2.305962\n",
      "(Iteration 26501 / 30560) loss: 2.305949\n",
      "(Iteration 26601 / 30560) loss: 2.305946\n",
      "(Iteration 26701 / 30560) loss: 2.305944\n",
      "(Epoch 70 / 80) train acc: 0.117000; val_acc: 0.084000\n",
      "(Iteration 26801 / 30560) loss: 2.305970\n",
      "(Iteration 26901 / 30560) loss: 2.305951\n",
      "(Iteration 27001 / 30560) loss: 2.305976\n",
      "(Iteration 27101 / 30560) loss: 2.305956\n",
      "(Epoch 71 / 80) train acc: 0.116000; val_acc: 0.084000\n",
      "(Iteration 27201 / 30560) loss: 2.305943\n",
      "(Iteration 27301 / 30560) loss: 2.305951\n",
      "(Iteration 27401 / 30560) loss: 2.305948\n",
      "(Iteration 27501 / 30560) loss: 2.305966\n",
      "(Epoch 72 / 80) train acc: 0.111000; val_acc: 0.084000\n",
      "(Iteration 27601 / 30560) loss: 2.305958\n",
      "(Iteration 27701 / 30560) loss: 2.305938\n",
      "(Iteration 27801 / 30560) loss: 2.305946\n",
      "(Epoch 73 / 80) train acc: 0.123000; val_acc: 0.084000\n",
      "(Iteration 27901 / 30560) loss: 2.305949\n",
      "(Iteration 28001 / 30560) loss: 2.305934\n",
      "(Iteration 28101 / 30560) loss: 2.305926\n",
      "(Iteration 28201 / 30560) loss: 2.305954\n",
      "(Epoch 74 / 80) train acc: 0.120000; val_acc: 0.084000\n",
      "(Iteration 28301 / 30560) loss: 2.305965\n",
      "(Iteration 28401 / 30560) loss: 2.305950\n",
      "(Iteration 28501 / 30560) loss: 2.305982\n",
      "(Iteration 28601 / 30560) loss: 2.305960\n",
      "(Epoch 75 / 80) train acc: 0.098000; val_acc: 0.084000\n",
      "(Iteration 28701 / 30560) loss: 2.305965\n",
      "(Iteration 28801 / 30560) loss: 2.305941\n",
      "(Iteration 28901 / 30560) loss: 2.305973\n",
      "(Iteration 29001 / 30560) loss: 2.305970\n",
      "(Epoch 76 / 80) train acc: 0.106000; val_acc: 0.084000\n",
      "(Iteration 29101 / 30560) loss: 2.305972\n",
      "(Iteration 29201 / 30560) loss: 2.305969\n",
      "(Iteration 29301 / 30560) loss: 2.305982\n",
      "(Iteration 29401 / 30560) loss: 2.305961\n",
      "(Epoch 77 / 80) train acc: 0.094000; val_acc: 0.084000\n",
      "(Iteration 29501 / 30560) loss: 2.305965\n",
      "(Iteration 29601 / 30560) loss: 2.305964\n",
      "(Iteration 29701 / 30560) loss: 2.305968\n",
      "(Epoch 78 / 80) train acc: 0.122000; val_acc: 0.084000\n",
      "(Iteration 29801 / 30560) loss: 2.305939\n",
      "(Iteration 29901 / 30560) loss: 2.305981\n",
      "(Iteration 30001 / 30560) loss: 2.305969\n",
      "(Iteration 30101 / 30560) loss: 2.305953\n",
      "(Epoch 79 / 80) train acc: 0.097000; val_acc: 0.084000\n",
      "(Iteration 30201 / 30560) loss: 2.306006\n",
      "(Iteration 30301 / 30560) loss: 2.305951\n",
      "(Iteration 30401 / 30560) loss: 2.305971\n",
      "(Iteration 30501 / 30560) loss: 2.305950\n",
      "(Epoch 80 / 80) train acc: 0.102000; val_acc: 0.084000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 61200) loss: 2.308355\n",
      "(Epoch 0 / 80) train acc: 0.114000; val_acc: 0.102000\n",
      "(Iteration 101 / 61200) loss: 2.308260\n",
      "(Iteration 201 / 61200) loss: 2.308194\n",
      "(Iteration 301 / 61200) loss: 2.308108\n",
      "(Iteration 401 / 61200) loss: 2.308043\n",
      "(Iteration 501 / 61200) loss: 2.307992\n",
      "(Iteration 601 / 61200) loss: 2.307881\n",
      "(Iteration 701 / 61200) loss: 2.307839\n",
      "(Epoch 1 / 80) train acc: 0.100000; val_acc: 0.095000\n",
      "(Iteration 801 / 61200) loss: 2.307753\n",
      "(Iteration 901 / 61200) loss: 2.307642\n",
      "(Iteration 1001 / 61200) loss: 2.307563\n",
      "(Iteration 1101 / 61200) loss: 2.307554\n",
      "(Iteration 1201 / 61200) loss: 2.307453\n",
      "(Iteration 1301 / 61200) loss: 2.307410\n",
      "(Iteration 1401 / 61200) loss: 2.307355\n",
      "(Iteration 1501 / 61200) loss: 2.307271\n",
      "(Epoch 2 / 80) train acc: 0.090000; val_acc: 0.106000\n",
      "(Iteration 1601 / 61200) loss: 2.307202\n",
      "(Iteration 1701 / 61200) loss: 2.307172\n",
      "(Iteration 1801 / 61200) loss: 2.307071\n",
      "(Iteration 1901 / 61200) loss: 2.307018\n",
      "(Iteration 2001 / 61200) loss: 2.307004\n",
      "(Iteration 2101 / 61200) loss: 2.306933\n",
      "(Iteration 2201 / 61200) loss: 2.306881\n",
      "(Epoch 3 / 80) train acc: 0.093000; val_acc: 0.117000\n",
      "(Iteration 2301 / 61200) loss: 2.306860\n",
      "(Iteration 2401 / 61200) loss: 2.306796\n",
      "(Iteration 2501 / 61200) loss: 2.306689\n",
      "(Iteration 2601 / 61200) loss: 2.306678\n",
      "(Iteration 2701 / 61200) loss: 2.306609\n",
      "(Iteration 2801 / 61200) loss: 2.306554\n",
      "(Iteration 2901 / 61200) loss: 2.306525\n",
      "(Iteration 3001 / 61200) loss: 2.306448\n",
      "(Epoch 4 / 80) train acc: 0.094000; val_acc: 0.115000\n",
      "(Iteration 3101 / 61200) loss: 2.306482\n",
      "(Iteration 3201 / 61200) loss: 2.306378\n",
      "(Iteration 3301 / 61200) loss: 2.306349\n",
      "(Iteration 3401 / 61200) loss: 2.306304\n",
      "(Iteration 3501 / 61200) loss: 2.306302\n",
      "(Iteration 3601 / 61200) loss: 2.306214\n",
      "(Iteration 3701 / 61200) loss: 2.306185\n",
      "(Iteration 3801 / 61200) loss: 2.306210\n",
      "(Epoch 5 / 80) train acc: 0.092000; val_acc: 0.119000\n",
      "(Iteration 3901 / 61200) loss: 2.306086\n",
      "(Iteration 4001 / 61200) loss: 2.306083\n",
      "(Iteration 4101 / 61200) loss: 2.306011\n",
      "(Iteration 4201 / 61200) loss: 2.305943\n",
      "(Iteration 4301 / 61200) loss: 2.305950\n",
      "(Iteration 4401 / 61200) loss: 2.305876\n",
      "(Iteration 4501 / 61200) loss: 2.305829\n",
      "(Epoch 6 / 80) train acc: 0.128000; val_acc: 0.148000\n",
      "(Iteration 4601 / 61200) loss: 2.305842\n",
      "(Iteration 4701 / 61200) loss: 2.305796\n",
      "(Iteration 4801 / 61200) loss: 2.305844\n",
      "(Iteration 4901 / 61200) loss: 2.305755\n",
      "(Iteration 5001 / 61200) loss: 2.305707\n",
      "(Iteration 5101 / 61200) loss: 2.305695\n",
      "(Iteration 5201 / 61200) loss: 2.305635\n",
      "(Iteration 5301 / 61200) loss: 2.305630\n",
      "(Epoch 7 / 80) train acc: 0.113000; val_acc: 0.116000\n",
      "(Iteration 5401 / 61200) loss: 2.305539\n",
      "(Iteration 5501 / 61200) loss: 2.305618\n",
      "(Iteration 5601 / 61200) loss: 2.305592\n",
      "(Iteration 5701 / 61200) loss: 2.305497\n",
      "(Iteration 5801 / 61200) loss: 2.305473\n",
      "(Iteration 5901 / 61200) loss: 2.305451\n",
      "(Iteration 6001 / 61200) loss: 2.305385\n",
      "(Iteration 6101 / 61200) loss: 2.305398\n",
      "(Epoch 8 / 80) train acc: 0.135000; val_acc: 0.142000\n",
      "(Iteration 6201 / 61200) loss: 2.305322\n",
      "(Iteration 6301 / 61200) loss: 2.305375\n",
      "(Iteration 6401 / 61200) loss: 2.305395\n",
      "(Iteration 6501 / 61200) loss: 2.305357\n",
      "(Iteration 6601 / 61200) loss: 2.305209\n",
      "(Iteration 6701 / 61200) loss: 2.305217\n",
      "(Iteration 6801 / 61200) loss: 2.305251\n",
      "(Epoch 9 / 80) train acc: 0.105000; val_acc: 0.124000\n",
      "(Iteration 6901 / 61200) loss: 2.305164\n",
      "(Iteration 7001 / 61200) loss: 2.305113\n",
      "(Iteration 7101 / 61200) loss: 2.305190\n",
      "(Iteration 7201 / 61200) loss: 2.305191\n",
      "(Iteration 7301 / 61200) loss: 2.305122\n",
      "(Iteration 7401 / 61200) loss: 2.305078\n",
      "(Iteration 7501 / 61200) loss: 2.305078\n",
      "(Iteration 7601 / 61200) loss: 2.305053\n",
      "(Epoch 10 / 80) train acc: 0.105000; val_acc: 0.120000\n",
      "(Iteration 7701 / 61200) loss: 2.304945\n",
      "(Iteration 7801 / 61200) loss: 2.305018\n",
      "(Iteration 7901 / 61200) loss: 2.304948\n",
      "(Iteration 8001 / 61200) loss: 2.304885\n",
      "(Iteration 8101 / 61200) loss: 2.305080\n",
      "(Iteration 8201 / 61200) loss: 2.304888\n",
      "(Iteration 8301 / 61200) loss: 2.304901\n",
      "(Iteration 8401 / 61200) loss: 2.304885\n",
      "(Epoch 11 / 80) train acc: 0.105000; val_acc: 0.119000\n",
      "(Iteration 8501 / 61200) loss: 2.304850\n",
      "(Iteration 8601 / 61200) loss: 2.304868\n",
      "(Iteration 8701 / 61200) loss: 2.304789\n",
      "(Iteration 8801 / 61200) loss: 2.304774\n",
      "(Iteration 8901 / 61200) loss: 2.304778\n",
      "(Iteration 9001 / 61200) loss: 2.304742\n",
      "(Iteration 9101 / 61200) loss: 2.304786\n",
      "(Epoch 12 / 80) train acc: 0.122000; val_acc: 0.098000\n",
      "(Iteration 9201 / 61200) loss: 2.304736\n",
      "(Iteration 9301 / 61200) loss: 2.304716\n",
      "(Iteration 9401 / 61200) loss: 2.304682\n",
      "(Iteration 9501 / 61200) loss: 2.304691\n",
      "(Iteration 9601 / 61200) loss: 2.304596\n",
      "(Iteration 9701 / 61200) loss: 2.304692\n",
      "(Iteration 9801 / 61200) loss: 2.304648\n",
      "(Iteration 9901 / 61200) loss: 2.304624\n",
      "(Epoch 13 / 80) train acc: 0.104000; val_acc: 0.099000\n",
      "(Iteration 10001 / 61200) loss: 2.304552\n",
      "(Iteration 10101 / 61200) loss: 2.304610\n",
      "(Iteration 10201 / 61200) loss: 2.304556\n",
      "(Iteration 10301 / 61200) loss: 2.304539\n",
      "(Iteration 10401 / 61200) loss: 2.304505\n",
      "(Iteration 10501 / 61200) loss: 2.304455\n",
      "(Iteration 10601 / 61200) loss: 2.304481\n",
      "(Iteration 10701 / 61200) loss: 2.304561\n",
      "(Epoch 14 / 80) train acc: 0.108000; val_acc: 0.095000\n",
      "(Iteration 10801 / 61200) loss: 2.304462\n",
      "(Iteration 10901 / 61200) loss: 2.304383\n",
      "(Iteration 11001 / 61200) loss: 2.304419\n",
      "(Iteration 11101 / 61200) loss: 2.304504\n",
      "(Iteration 11201 / 61200) loss: 2.304463\n",
      "(Iteration 11301 / 61200) loss: 2.304403\n",
      "(Iteration 11401 / 61200) loss: 2.304504\n",
      "(Epoch 15 / 80) train acc: 0.102000; val_acc: 0.110000\n",
      "(Iteration 11501 / 61200) loss: 2.304423\n",
      "(Iteration 11601 / 61200) loss: 2.304422\n",
      "(Iteration 11701 / 61200) loss: 2.304330\n",
      "(Iteration 11801 / 61200) loss: 2.304333\n",
      "(Iteration 11901 / 61200) loss: 2.304326\n",
      "(Iteration 12001 / 61200) loss: 2.304334\n",
      "(Iteration 12101 / 61200) loss: 2.304328\n",
      "(Iteration 12201 / 61200) loss: 2.304276\n",
      "(Epoch 16 / 80) train acc: 0.106000; val_acc: 0.102000\n",
      "(Iteration 12301 / 61200) loss: 2.304389\n",
      "(Iteration 12401 / 61200) loss: 2.304274\n",
      "(Iteration 12501 / 61200) loss: 2.304293\n",
      "(Iteration 12601 / 61200) loss: 2.304273\n",
      "(Iteration 12701 / 61200) loss: 2.304216\n",
      "(Iteration 12801 / 61200) loss: 2.304257\n",
      "(Iteration 12901 / 61200) loss: 2.304255\n",
      "(Iteration 13001 / 61200) loss: 2.304201\n",
      "(Epoch 17 / 80) train acc: 0.121000; val_acc: 0.103000\n",
      "(Iteration 13101 / 61200) loss: 2.304200\n",
      "(Iteration 13201 / 61200) loss: 2.304229\n",
      "(Iteration 13301 / 61200) loss: 2.304205\n",
      "(Iteration 13401 / 61200) loss: 2.304200\n",
      "(Iteration 13501 / 61200) loss: 2.304249\n",
      "(Iteration 13601 / 61200) loss: 2.304157\n",
      "(Iteration 13701 / 61200) loss: 2.304091\n",
      "(Epoch 18 / 80) train acc: 0.096000; val_acc: 0.098000\n",
      "(Iteration 13801 / 61200) loss: 2.304146\n",
      "(Iteration 13901 / 61200) loss: 2.304170\n",
      "(Iteration 14001 / 61200) loss: 2.304113\n",
      "(Iteration 14101 / 61200) loss: 2.304070\n",
      "(Iteration 14201 / 61200) loss: 2.304164\n",
      "(Iteration 14301 / 61200) loss: 2.304053\n",
      "(Iteration 14401 / 61200) loss: 2.304101\n",
      "(Iteration 14501 / 61200) loss: 2.304112\n",
      "(Epoch 19 / 80) train acc: 0.088000; val_acc: 0.106000\n",
      "(Iteration 14601 / 61200) loss: 2.304142\n",
      "(Iteration 14701 / 61200) loss: 2.304069\n",
      "(Iteration 14801 / 61200) loss: 2.304083\n",
      "(Iteration 14901 / 61200) loss: 2.304097\n",
      "(Iteration 15001 / 61200) loss: 2.303987\n",
      "(Iteration 15101 / 61200) loss: 2.304005\n",
      "(Iteration 15201 / 61200) loss: 2.304141\n",
      "(Epoch 20 / 80) train acc: 0.111000; val_acc: 0.107000\n",
      "(Iteration 15301 / 61200) loss: 2.304097\n",
      "(Iteration 15401 / 61200) loss: 2.304002\n",
      "(Iteration 15501 / 61200) loss: 2.304036\n",
      "(Iteration 15601 / 61200) loss: 2.304007\n",
      "(Iteration 15701 / 61200) loss: 2.304130\n",
      "(Iteration 15801 / 61200) loss: 2.303952\n",
      "(Iteration 15901 / 61200) loss: 2.303998\n",
      "(Iteration 16001 / 61200) loss: 2.304042\n",
      "(Epoch 21 / 80) train acc: 0.108000; val_acc: 0.107000\n",
      "(Iteration 16101 / 61200) loss: 2.304065\n",
      "(Iteration 16201 / 61200) loss: 2.303996\n",
      "(Iteration 16301 / 61200) loss: 2.303974\n",
      "(Iteration 16401 / 61200) loss: 2.303987\n",
      "(Iteration 16501 / 61200) loss: 2.303988\n",
      "(Iteration 16601 / 61200) loss: 2.303943\n",
      "(Iteration 16701 / 61200) loss: 2.304072\n",
      "(Iteration 16801 / 61200) loss: 2.303924\n",
      "(Epoch 22 / 80) train acc: 0.098000; val_acc: 0.107000\n",
      "(Iteration 16901 / 61200) loss: 2.303971\n",
      "(Iteration 17001 / 61200) loss: 2.303942\n",
      "(Iteration 17101 / 61200) loss: 2.303922\n",
      "(Iteration 17201 / 61200) loss: 2.303825\n",
      "(Iteration 17301 / 61200) loss: 2.303903\n",
      "(Iteration 17401 / 61200) loss: 2.303836\n",
      "(Iteration 17501 / 61200) loss: 2.303876\n",
      "(Epoch 23 / 80) train acc: 0.102000; val_acc: 0.107000\n",
      "(Iteration 17601 / 61200) loss: 2.303908\n",
      "(Iteration 17701 / 61200) loss: 2.303936\n",
      "(Iteration 17801 / 61200) loss: 2.303838\n",
      "(Iteration 17901 / 61200) loss: 2.303890\n",
      "(Iteration 18001 / 61200) loss: 2.304021\n",
      "(Iteration 18101 / 61200) loss: 2.303825\n",
      "(Iteration 18201 / 61200) loss: 2.303852\n",
      "(Iteration 18301 / 61200) loss: 2.303848\n",
      "(Epoch 24 / 80) train acc: 0.096000; val_acc: 0.107000\n",
      "(Iteration 18401 / 61200) loss: 2.303825\n",
      "(Iteration 18501 / 61200) loss: 2.303847\n",
      "(Iteration 18601 / 61200) loss: 2.303820\n",
      "(Iteration 18701 / 61200) loss: 2.303724\n",
      "(Iteration 18801 / 61200) loss: 2.303804\n",
      "(Iteration 18901 / 61200) loss: 2.303861\n",
      "(Iteration 19001 / 61200) loss: 2.303877\n",
      "(Iteration 19101 / 61200) loss: 2.303792\n",
      "(Epoch 25 / 80) train acc: 0.100000; val_acc: 0.107000\n",
      "(Iteration 19201 / 61200) loss: 2.303792\n",
      "(Iteration 19301 / 61200) loss: 2.303870\n",
      "(Iteration 19401 / 61200) loss: 2.303807\n",
      "(Iteration 19501 / 61200) loss: 2.303870\n",
      "(Iteration 19601 / 61200) loss: 2.303785\n",
      "(Iteration 19701 / 61200) loss: 2.303784\n",
      "(Iteration 19801 / 61200) loss: 2.303809\n",
      "(Epoch 26 / 80) train acc: 0.090000; val_acc: 0.107000\n",
      "(Iteration 19901 / 61200) loss: 2.303751\n",
      "(Iteration 20001 / 61200) loss: 2.303741\n",
      "(Iteration 20101 / 61200) loss: 2.303733\n",
      "(Iteration 20201 / 61200) loss: 2.303708\n",
      "(Iteration 20301 / 61200) loss: 2.303794\n",
      "(Iteration 20401 / 61200) loss: 2.303664\n",
      "(Iteration 20501 / 61200) loss: 2.303699\n",
      "(Iteration 20601 / 61200) loss: 2.303732\n",
      "(Epoch 27 / 80) train acc: 0.089000; val_acc: 0.107000\n",
      "(Iteration 20701 / 61200) loss: 2.303724\n",
      "(Iteration 20801 / 61200) loss: 2.303736\n",
      "(Iteration 20901 / 61200) loss: 2.303747\n",
      "(Iteration 21001 / 61200) loss: 2.303684\n",
      "(Iteration 21101 / 61200) loss: 2.303723\n",
      "(Iteration 21201 / 61200) loss: 2.303655\n",
      "(Iteration 21301 / 61200) loss: 2.303659\n",
      "(Iteration 21401 / 61200) loss: 2.303733\n",
      "(Epoch 28 / 80) train acc: 0.093000; val_acc: 0.107000\n",
      "(Iteration 21501 / 61200) loss: 2.303692\n",
      "(Iteration 21601 / 61200) loss: 2.303721\n",
      "(Iteration 21701 / 61200) loss: 2.303636\n",
      "(Iteration 21801 / 61200) loss: 2.303722\n",
      "(Iteration 21901 / 61200) loss: 2.303625\n",
      "(Iteration 22001 / 61200) loss: 2.303716\n",
      "(Iteration 22101 / 61200) loss: 2.303724\n",
      "(Epoch 29 / 80) train acc: 0.101000; val_acc: 0.107000\n",
      "(Iteration 22201 / 61200) loss: 2.303746\n",
      "(Iteration 22301 / 61200) loss: 2.303704\n",
      "(Iteration 22401 / 61200) loss: 2.303624\n",
      "(Iteration 22501 / 61200) loss: 2.303649\n",
      "(Iteration 22601 / 61200) loss: 2.303698\n",
      "(Iteration 22701 / 61200) loss: 2.303666\n",
      "(Iteration 22801 / 61200) loss: 2.303604\n",
      "(Iteration 22901 / 61200) loss: 2.303719\n",
      "(Epoch 30 / 80) train acc: 0.102000; val_acc: 0.107000\n",
      "(Iteration 23001 / 61200) loss: 2.303706\n",
      "(Iteration 23101 / 61200) loss: 2.303624\n",
      "(Iteration 23201 / 61200) loss: 2.303587\n",
      "(Iteration 23301 / 61200) loss: 2.303702\n",
      "(Iteration 23401 / 61200) loss: 2.303602\n",
      "(Iteration 23501 / 61200) loss: 2.303708\n",
      "(Iteration 23601 / 61200) loss: 2.303623\n",
      "(Iteration 23701 / 61200) loss: 2.303615\n",
      "(Epoch 31 / 80) train acc: 0.102000; val_acc: 0.107000\n",
      "(Iteration 23801 / 61200) loss: 2.303613\n",
      "(Iteration 23901 / 61200) loss: 2.303717\n",
      "(Iteration 24001 / 61200) loss: 2.303687\n",
      "(Iteration 24101 / 61200) loss: 2.303560\n",
      "(Iteration 24201 / 61200) loss: 2.303666\n",
      "(Iteration 24301 / 61200) loss: 2.303635\n",
      "(Iteration 24401 / 61200) loss: 2.303617\n",
      "(Epoch 32 / 80) train acc: 0.093000; val_acc: 0.107000\n",
      "(Iteration 24501 / 61200) loss: 2.303633\n",
      "(Iteration 24601 / 61200) loss: 2.303525\n",
      "(Iteration 24701 / 61200) loss: 2.303613\n",
      "(Iteration 24801 / 61200) loss: 2.303545\n",
      "(Iteration 24901 / 61200) loss: 2.303612\n",
      "(Iteration 25001 / 61200) loss: 2.303617\n",
      "(Iteration 25101 / 61200) loss: 2.303479\n",
      "(Iteration 25201 / 61200) loss: 2.303623\n",
      "(Epoch 33 / 80) train acc: 0.111000; val_acc: 0.107000\n",
      "(Iteration 25301 / 61200) loss: 2.303510\n",
      "(Iteration 25401 / 61200) loss: 2.303697\n",
      "(Iteration 25501 / 61200) loss: 2.303706\n",
      "(Iteration 25601 / 61200) loss: 2.303609\n",
      "(Iteration 25701 / 61200) loss: 2.303631\n",
      "(Iteration 25801 / 61200) loss: 2.303538\n",
      "(Iteration 25901 / 61200) loss: 2.303708\n",
      "(Iteration 26001 / 61200) loss: 2.303498\n",
      "(Epoch 34 / 80) train acc: 0.088000; val_acc: 0.107000\n",
      "(Iteration 26101 / 61200) loss: 2.303570\n",
      "(Iteration 26201 / 61200) loss: 2.303495\n",
      "(Iteration 26301 / 61200) loss: 2.303592\n",
      "(Iteration 26401 / 61200) loss: 2.303554\n",
      "(Iteration 26501 / 61200) loss: 2.303517\n",
      "(Iteration 26601 / 61200) loss: 2.303542\n",
      "(Iteration 26701 / 61200) loss: 2.303465\n",
      "(Epoch 35 / 80) train acc: 0.095000; val_acc: 0.107000\n",
      "(Iteration 26801 / 61200) loss: 2.303528\n",
      "(Iteration 26901 / 61200) loss: 2.303541\n",
      "(Iteration 27001 / 61200) loss: 2.303509\n",
      "(Iteration 27101 / 61200) loss: 2.303443\n",
      "(Iteration 27201 / 61200) loss: 2.303562\n",
      "(Iteration 27301 / 61200) loss: 2.303559\n",
      "(Iteration 27401 / 61200) loss: 2.303584\n",
      "(Iteration 27501 / 61200) loss: 2.303490\n",
      "(Epoch 36 / 80) train acc: 0.104000; val_acc: 0.107000\n",
      "(Iteration 27601 / 61200) loss: 2.303500\n",
      "(Iteration 27701 / 61200) loss: 2.303449\n",
      "(Iteration 27801 / 61200) loss: 2.303477\n",
      "(Iteration 27901 / 61200) loss: 2.303611\n",
      "(Iteration 28001 / 61200) loss: 2.303443\n",
      "(Iteration 28101 / 61200) loss: 2.303550\n",
      "(Iteration 28201 / 61200) loss: 2.303531\n",
      "(Iteration 28301 / 61200) loss: 2.303517\n",
      "(Epoch 37 / 80) train acc: 0.117000; val_acc: 0.107000\n",
      "(Iteration 28401 / 61200) loss: 2.303456\n",
      "(Iteration 28501 / 61200) loss: 2.303473\n",
      "(Iteration 28601 / 61200) loss: 2.303555\n",
      "(Iteration 28701 / 61200) loss: 2.303485\n",
      "(Iteration 28801 / 61200) loss: 2.303523\n",
      "(Iteration 28901 / 61200) loss: 2.303600\n",
      "(Iteration 29001 / 61200) loss: 2.303550\n",
      "(Epoch 38 / 80) train acc: 0.112000; val_acc: 0.107000\n",
      "(Iteration 29101 / 61200) loss: 2.303493\n",
      "(Iteration 29201 / 61200) loss: 2.303475\n",
      "(Iteration 29301 / 61200) loss: 2.303524\n",
      "(Iteration 29401 / 61200) loss: 2.303442\n",
      "(Iteration 29501 / 61200) loss: 2.303481\n",
      "(Iteration 29601 / 61200) loss: 2.303524\n",
      "(Iteration 29701 / 61200) loss: 2.303515\n",
      "(Iteration 29801 / 61200) loss: 2.303474\n",
      "(Epoch 39 / 80) train acc: 0.086000; val_acc: 0.107000\n",
      "(Iteration 29901 / 61200) loss: 2.303492\n",
      "(Iteration 30001 / 61200) loss: 2.303442\n",
      "(Iteration 30101 / 61200) loss: 2.303438\n",
      "(Iteration 30201 / 61200) loss: 2.303412\n",
      "(Iteration 30301 / 61200) loss: 2.303473\n",
      "(Iteration 30401 / 61200) loss: 2.303495\n",
      "(Iteration 30501 / 61200) loss: 2.303465\n",
      "(Epoch 40 / 80) train acc: 0.088000; val_acc: 0.107000\n",
      "(Iteration 30601 / 61200) loss: 2.303518\n",
      "(Iteration 30701 / 61200) loss: 2.303489\n",
      "(Iteration 30801 / 61200) loss: 2.303465\n",
      "(Iteration 30901 / 61200) loss: 2.303521\n",
      "(Iteration 31001 / 61200) loss: 2.303466\n",
      "(Iteration 31101 / 61200) loss: 2.303493\n",
      "(Iteration 31201 / 61200) loss: 2.303517\n",
      "(Iteration 31301 / 61200) loss: 2.303538\n",
      "(Epoch 41 / 80) train acc: 0.109000; val_acc: 0.107000\n",
      "(Iteration 31401 / 61200) loss: 2.303437\n",
      "(Iteration 31501 / 61200) loss: 2.303507\n",
      "(Iteration 31601 / 61200) loss: 2.303349\n",
      "(Iteration 31701 / 61200) loss: 2.303422\n",
      "(Iteration 31801 / 61200) loss: 2.303417\n",
      "(Iteration 31901 / 61200) loss: 2.303526\n",
      "(Iteration 32001 / 61200) loss: 2.303469\n",
      "(Iteration 32101 / 61200) loss: 2.303370\n",
      "(Epoch 42 / 80) train acc: 0.103000; val_acc: 0.107000\n",
      "(Iteration 32201 / 61200) loss: 2.303421\n",
      "(Iteration 32301 / 61200) loss: 2.303410\n",
      "(Iteration 32401 / 61200) loss: 2.303408\n",
      "(Iteration 32501 / 61200) loss: 2.303544\n",
      "(Iteration 32601 / 61200) loss: 2.303527\n",
      "(Iteration 32701 / 61200) loss: 2.303538\n",
      "(Iteration 32801 / 61200) loss: 2.303399\n",
      "(Epoch 43 / 80) train acc: 0.101000; val_acc: 0.107000\n",
      "(Iteration 32901 / 61200) loss: 2.303418\n",
      "(Iteration 33001 / 61200) loss: 2.303498\n",
      "(Iteration 33101 / 61200) loss: 2.303447\n",
      "(Iteration 33201 / 61200) loss: 2.303413\n",
      "(Iteration 33301 / 61200) loss: 2.303508\n",
      "(Iteration 33401 / 61200) loss: 2.303374\n",
      "(Iteration 33501 / 61200) loss: 2.303481\n",
      "(Iteration 33601 / 61200) loss: 2.303437\n",
      "(Epoch 44 / 80) train acc: 0.111000; val_acc: 0.107000\n",
      "(Iteration 33701 / 61200) loss: 2.303347\n",
      "(Iteration 33801 / 61200) loss: 2.303382\n",
      "(Iteration 33901 / 61200) loss: 2.303407\n",
      "(Iteration 34001 / 61200) loss: 2.303470\n",
      "(Iteration 34101 / 61200) loss: 2.303469\n",
      "(Iteration 34201 / 61200) loss: 2.303385\n",
      "(Iteration 34301 / 61200) loss: 2.303462\n",
      "(Iteration 34401 / 61200) loss: 2.303424\n",
      "(Epoch 45 / 80) train acc: 0.102000; val_acc: 0.107000\n",
      "(Iteration 34501 / 61200) loss: 2.303414\n",
      "(Iteration 34601 / 61200) loss: 2.303399\n",
      "(Iteration 34701 / 61200) loss: 2.303408\n",
      "(Iteration 34801 / 61200) loss: 2.303464\n",
      "(Iteration 34901 / 61200) loss: 2.303476\n",
      "(Iteration 35001 / 61200) loss: 2.303412\n",
      "(Iteration 35101 / 61200) loss: 2.303416\n",
      "(Epoch 46 / 80) train acc: 0.099000; val_acc: 0.107000\n",
      "(Iteration 35201 / 61200) loss: 2.303443\n",
      "(Iteration 35301 / 61200) loss: 2.303439\n",
      "(Iteration 35401 / 61200) loss: 2.303343\n",
      "(Iteration 35501 / 61200) loss: 2.303399\n",
      "(Iteration 35601 / 61200) loss: 2.303372\n",
      "(Iteration 35701 / 61200) loss: 2.303463\n",
      "(Iteration 35801 / 61200) loss: 2.303529\n",
      "(Iteration 35901 / 61200) loss: 2.303374\n",
      "(Epoch 47 / 80) train acc: 0.102000; val_acc: 0.107000\n",
      "(Iteration 36001 / 61200) loss: 2.303375\n",
      "(Iteration 36101 / 61200) loss: 2.303350\n",
      "(Iteration 36201 / 61200) loss: 2.303445\n",
      "(Iteration 36301 / 61200) loss: 2.303382\n",
      "(Iteration 36401 / 61200) loss: 2.303485\n",
      "(Iteration 36501 / 61200) loss: 2.303445\n",
      "(Iteration 36601 / 61200) loss: 2.303396\n",
      "(Iteration 36701 / 61200) loss: 2.303333\n",
      "(Epoch 48 / 80) train acc: 0.086000; val_acc: 0.107000\n",
      "(Iteration 36801 / 61200) loss: 2.303367\n",
      "(Iteration 36901 / 61200) loss: 2.303383\n",
      "(Iteration 37001 / 61200) loss: 2.303396\n",
      "(Iteration 37101 / 61200) loss: 2.303373\n",
      "(Iteration 37201 / 61200) loss: 2.303438\n",
      "(Iteration 37301 / 61200) loss: 2.303395\n",
      "(Iteration 37401 / 61200) loss: 2.303342\n",
      "(Epoch 49 / 80) train acc: 0.074000; val_acc: 0.107000\n",
      "(Iteration 37501 / 61200) loss: 2.303330\n",
      "(Iteration 37601 / 61200) loss: 2.303413\n",
      "(Iteration 37701 / 61200) loss: 2.303417\n",
      "(Iteration 37801 / 61200) loss: 2.303356\n",
      "(Iteration 37901 / 61200) loss: 2.303326\n",
      "(Iteration 38001 / 61200) loss: 2.303416\n",
      "(Iteration 38101 / 61200) loss: 2.303359\n",
      "(Iteration 38201 / 61200) loss: 2.303377\n",
      "(Epoch 50 / 80) train acc: 0.102000; val_acc: 0.107000\n",
      "(Iteration 38301 / 61200) loss: 2.303422\n",
      "(Iteration 38401 / 61200) loss: 2.303390\n",
      "(Iteration 38501 / 61200) loss: 2.303415\n",
      "(Iteration 38601 / 61200) loss: 2.303377\n",
      "(Iteration 38701 / 61200) loss: 2.303377\n",
      "(Iteration 38801 / 61200) loss: 2.303351\n",
      "(Iteration 38901 / 61200) loss: 2.303441\n",
      "(Iteration 39001 / 61200) loss: 2.303369\n",
      "(Epoch 51 / 80) train acc: 0.103000; val_acc: 0.107000\n",
      "(Iteration 39101 / 61200) loss: 2.303375\n",
      "(Iteration 39201 / 61200) loss: 2.303393\n",
      "(Iteration 39301 / 61200) loss: 2.303502\n",
      "(Iteration 39401 / 61200) loss: 2.303422\n",
      "(Iteration 39501 / 61200) loss: 2.303422\n",
      "(Iteration 39601 / 61200) loss: 2.303486\n",
      "(Iteration 39701 / 61200) loss: 2.303373\n",
      "(Epoch 52 / 80) train acc: 0.103000; val_acc: 0.107000\n",
      "(Iteration 39801 / 61200) loss: 2.303427\n",
      "(Iteration 39901 / 61200) loss: 2.303317\n",
      "(Iteration 40001 / 61200) loss: 2.303348\n",
      "(Iteration 40101 / 61200) loss: 2.303282\n",
      "(Iteration 40201 / 61200) loss: 2.303344\n",
      "(Iteration 40301 / 61200) loss: 2.303430\n",
      "(Iteration 40401 / 61200) loss: 2.303392\n",
      "(Iteration 40501 / 61200) loss: 2.303408\n",
      "(Epoch 53 / 80) train acc: 0.097000; val_acc: 0.107000\n",
      "(Iteration 40601 / 61200) loss: 2.303396\n",
      "(Iteration 40701 / 61200) loss: 2.303387\n",
      "(Iteration 40801 / 61200) loss: 2.303351\n",
      "(Iteration 40901 / 61200) loss: 2.303342\n",
      "(Iteration 41001 / 61200) loss: 2.303350\n",
      "(Iteration 41101 / 61200) loss: 2.303333\n",
      "(Iteration 41201 / 61200) loss: 2.303394\n",
      "(Iteration 41301 / 61200) loss: 2.303420\n",
      "(Epoch 54 / 80) train acc: 0.096000; val_acc: 0.107000\n",
      "(Iteration 41401 / 61200) loss: 2.303386\n",
      "(Iteration 41501 / 61200) loss: 2.303397\n",
      "(Iteration 41601 / 61200) loss: 2.303348\n",
      "(Iteration 41701 / 61200) loss: 2.303415\n",
      "(Iteration 41801 / 61200) loss: 2.303434\n",
      "(Iteration 41901 / 61200) loss: 2.303262\n",
      "(Iteration 42001 / 61200) loss: 2.303329\n",
      "(Epoch 55 / 80) train acc: 0.101000; val_acc: 0.107000\n",
      "(Iteration 42101 / 61200) loss: 2.303372\n",
      "(Iteration 42201 / 61200) loss: 2.303492\n",
      "(Iteration 42301 / 61200) loss: 2.303332\n",
      "(Iteration 42401 / 61200) loss: 2.303280\n",
      "(Iteration 42501 / 61200) loss: 2.303385\n",
      "(Iteration 42601 / 61200) loss: 2.303344\n",
      "(Iteration 42701 / 61200) loss: 2.303482\n",
      "(Iteration 42801 / 61200) loss: 2.303342\n",
      "(Epoch 56 / 80) train acc: 0.091000; val_acc: 0.107000\n",
      "(Iteration 42901 / 61200) loss: 2.303342\n",
      "(Iteration 43001 / 61200) loss: 2.303331\n",
      "(Iteration 43101 / 61200) loss: 2.303466\n",
      "(Iteration 43201 / 61200) loss: 2.303368\n",
      "(Iteration 43301 / 61200) loss: 2.303310\n",
      "(Iteration 43401 / 61200) loss: 2.303439\n",
      "(Iteration 43501 / 61200) loss: 2.303445\n",
      "(Iteration 43601 / 61200) loss: 2.303299\n",
      "(Epoch 57 / 80) train acc: 0.102000; val_acc: 0.107000\n",
      "(Iteration 43701 / 61200) loss: 2.303329\n",
      "(Iteration 43801 / 61200) loss: 2.303530\n",
      "(Iteration 43901 / 61200) loss: 2.303353\n",
      "(Iteration 44001 / 61200) loss: 2.303348\n",
      "(Iteration 44101 / 61200) loss: 2.303277\n",
      "(Iteration 44201 / 61200) loss: 2.303233\n",
      "(Iteration 44301 / 61200) loss: 2.303316\n",
      "(Epoch 58 / 80) train acc: 0.126000; val_acc: 0.107000\n",
      "(Iteration 44401 / 61200) loss: 2.303362\n",
      "(Iteration 44501 / 61200) loss: 2.303434\n",
      "(Iteration 44601 / 61200) loss: 2.303307\n",
      "(Iteration 44701 / 61200) loss: 2.303316\n",
      "(Iteration 44801 / 61200) loss: 2.303353\n",
      "(Iteration 44901 / 61200) loss: 2.303266\n",
      "(Iteration 45001 / 61200) loss: 2.303432\n",
      "(Iteration 45101 / 61200) loss: 2.303320\n",
      "(Epoch 59 / 80) train acc: 0.099000; val_acc: 0.107000\n",
      "(Iteration 45201 / 61200) loss: 2.303286\n",
      "(Iteration 45301 / 61200) loss: 2.303359\n",
      "(Iteration 45401 / 61200) loss: 2.303200\n",
      "(Iteration 45501 / 61200) loss: 2.303349\n",
      "(Iteration 45601 / 61200) loss: 2.303245\n",
      "(Iteration 45701 / 61200) loss: 2.303354\n",
      "(Iteration 45801 / 61200) loss: 2.303359\n",
      "(Epoch 60 / 80) train acc: 0.082000; val_acc: 0.107000\n",
      "(Iteration 45901 / 61200) loss: 2.303343\n",
      "(Iteration 46001 / 61200) loss: 2.303346\n",
      "(Iteration 46101 / 61200) loss: 2.303344\n",
      "(Iteration 46201 / 61200) loss: 2.303394\n",
      "(Iteration 46301 / 61200) loss: 2.303407\n",
      "(Iteration 46401 / 61200) loss: 2.303290\n",
      "(Iteration 46501 / 61200) loss: 2.303432\n",
      "(Iteration 46601 / 61200) loss: 2.303319\n",
      "(Epoch 61 / 80) train acc: 0.106000; val_acc: 0.107000\n",
      "(Iteration 46701 / 61200) loss: 2.303345\n",
      "(Iteration 46801 / 61200) loss: 2.303317\n",
      "(Iteration 46901 / 61200) loss: 2.303326\n",
      "(Iteration 47001 / 61200) loss: 2.303241\n",
      "(Iteration 47101 / 61200) loss: 2.303286\n",
      "(Iteration 47201 / 61200) loss: 2.303317\n",
      "(Iteration 47301 / 61200) loss: 2.303237\n",
      "(Iteration 47401 / 61200) loss: 2.303376\n",
      "(Epoch 62 / 80) train acc: 0.106000; val_acc: 0.107000\n",
      "(Iteration 47501 / 61200) loss: 2.303367\n",
      "(Iteration 47601 / 61200) loss: 2.303327\n",
      "(Iteration 47701 / 61200) loss: 2.303333\n",
      "(Iteration 47801 / 61200) loss: 2.303374\n",
      "(Iteration 47901 / 61200) loss: 2.303395\n",
      "(Iteration 48001 / 61200) loss: 2.303349\n",
      "(Iteration 48101 / 61200) loss: 2.303423\n",
      "(Epoch 63 / 80) train acc: 0.104000; val_acc: 0.107000\n",
      "(Iteration 48201 / 61200) loss: 2.303315\n",
      "(Iteration 48301 / 61200) loss: 2.303339\n",
      "(Iteration 48401 / 61200) loss: 2.303318\n",
      "(Iteration 48501 / 61200) loss: 2.303245\n",
      "(Iteration 48601 / 61200) loss: 2.303330\n",
      "(Iteration 48701 / 61200) loss: 2.303284\n",
      "(Iteration 48801 / 61200) loss: 2.303412\n",
      "(Iteration 48901 / 61200) loss: 2.303282\n",
      "(Epoch 64 / 80) train acc: 0.112000; val_acc: 0.107000\n",
      "(Iteration 49001 / 61200) loss: 2.303291\n",
      "(Iteration 49101 / 61200) loss: 2.303320\n",
      "(Iteration 49201 / 61200) loss: 2.303359\n",
      "(Iteration 49301 / 61200) loss: 2.303307\n",
      "(Iteration 49401 / 61200) loss: 2.303172\n",
      "(Iteration 49501 / 61200) loss: 2.303457\n",
      "(Iteration 49601 / 61200) loss: 2.303241\n",
      "(Iteration 49701 / 61200) loss: 2.303353\n",
      "(Epoch 65 / 80) train acc: 0.098000; val_acc: 0.107000\n",
      "(Iteration 49801 / 61200) loss: 2.303345\n",
      "(Iteration 49901 / 61200) loss: 2.303393\n",
      "(Iteration 50001 / 61200) loss: 2.303357\n",
      "(Iteration 50101 / 61200) loss: 2.303250\n",
      "(Iteration 50201 / 61200) loss: 2.303313\n",
      "(Iteration 50301 / 61200) loss: 2.303298\n",
      "(Iteration 50401 / 61200) loss: 2.303227\n",
      "(Epoch 66 / 80) train acc: 0.107000; val_acc: 0.107000\n",
      "(Iteration 50501 / 61200) loss: 2.303252\n",
      "(Iteration 50601 / 61200) loss: 2.303369\n",
      "(Iteration 50701 / 61200) loss: 2.303311\n",
      "(Iteration 50801 / 61200) loss: 2.303288\n",
      "(Iteration 50901 / 61200) loss: 2.303250\n",
      "(Iteration 51001 / 61200) loss: 2.303343\n",
      "(Iteration 51101 / 61200) loss: 2.303431\n",
      "(Iteration 51201 / 61200) loss: 2.303246\n",
      "(Epoch 67 / 80) train acc: 0.112000; val_acc: 0.107000\n",
      "(Iteration 51301 / 61200) loss: 2.303272\n",
      "(Iteration 51401 / 61200) loss: 2.303295\n",
      "(Iteration 51501 / 61200) loss: 2.303349\n",
      "(Iteration 51601 / 61200) loss: 2.303265\n",
      "(Iteration 51701 / 61200) loss: 2.303310\n",
      "(Iteration 51801 / 61200) loss: 2.303487\n",
      "(Iteration 51901 / 61200) loss: 2.303346\n",
      "(Iteration 52001 / 61200) loss: 2.303349\n",
      "(Epoch 68 / 80) train acc: 0.093000; val_acc: 0.107000\n",
      "(Iteration 52101 / 61200) loss: 2.303316\n",
      "(Iteration 52201 / 61200) loss: 2.303292\n",
      "(Iteration 52301 / 61200) loss: 2.303384\n",
      "(Iteration 52401 / 61200) loss: 2.303314\n",
      "(Iteration 52501 / 61200) loss: 2.303309\n",
      "(Iteration 52601 / 61200) loss: 2.303289\n",
      "(Iteration 52701 / 61200) loss: 2.303279\n",
      "(Epoch 69 / 80) train acc: 0.098000; val_acc: 0.107000\n",
      "(Iteration 52801 / 61200) loss: 2.303275\n",
      "(Iteration 52901 / 61200) loss: 2.303323\n",
      "(Iteration 53001 / 61200) loss: 2.303307\n",
      "(Iteration 53101 / 61200) loss: 2.303225\n",
      "(Iteration 53201 / 61200) loss: 2.303227\n",
      "(Iteration 53301 / 61200) loss: 2.303195\n",
      "(Iteration 53401 / 61200) loss: 2.303324\n",
      "(Iteration 53501 / 61200) loss: 2.303246\n",
      "(Epoch 70 / 80) train acc: 0.105000; val_acc: 0.107000\n",
      "(Iteration 53601 / 61200) loss: 2.303352\n",
      "(Iteration 53701 / 61200) loss: 2.303282\n",
      "(Iteration 53801 / 61200) loss: 2.303484\n",
      "(Iteration 53901 / 61200) loss: 2.303335\n",
      "(Iteration 54001 / 61200) loss: 2.303288\n",
      "(Iteration 54101 / 61200) loss: 2.303247\n",
      "(Iteration 54201 / 61200) loss: 2.303359\n",
      "(Iteration 54301 / 61200) loss: 2.303328\n",
      "(Epoch 71 / 80) train acc: 0.105000; val_acc: 0.107000\n",
      "(Iteration 54401 / 61200) loss: 2.303375\n",
      "(Iteration 54501 / 61200) loss: 2.303311\n",
      "(Iteration 54601 / 61200) loss: 2.303408\n",
      "(Iteration 54701 / 61200) loss: 2.303249\n",
      "(Iteration 54801 / 61200) loss: 2.303359\n",
      "(Iteration 54901 / 61200) loss: 2.303348\n",
      "(Iteration 55001 / 61200) loss: 2.303363\n",
      "(Epoch 72 / 80) train acc: 0.088000; val_acc: 0.107000\n",
      "(Iteration 55101 / 61200) loss: 2.303347\n",
      "(Iteration 55201 / 61200) loss: 2.303312\n",
      "(Iteration 55301 / 61200) loss: 2.303305\n",
      "(Iteration 55401 / 61200) loss: 2.303273\n",
      "(Iteration 55501 / 61200) loss: 2.303253\n",
      "(Iteration 55601 / 61200) loss: 2.303295\n",
      "(Iteration 55701 / 61200) loss: 2.303275\n",
      "(Iteration 55801 / 61200) loss: 2.303298\n",
      "(Epoch 73 / 80) train acc: 0.103000; val_acc: 0.107000\n",
      "(Iteration 55901 / 61200) loss: 2.303206\n",
      "(Iteration 56001 / 61200) loss: 2.303280\n",
      "(Iteration 56101 / 61200) loss: 2.303314\n",
      "(Iteration 56201 / 61200) loss: 2.303256\n",
      "(Iteration 56301 / 61200) loss: 2.303219\n",
      "(Iteration 56401 / 61200) loss: 2.303320\n",
      "(Iteration 56501 / 61200) loss: 2.303255\n",
      "(Iteration 56601 / 61200) loss: 2.303259\n",
      "(Epoch 74 / 80) train acc: 0.094000; val_acc: 0.107000\n",
      "(Iteration 56701 / 61200) loss: 2.303305\n",
      "(Iteration 56801 / 61200) loss: 2.303295\n",
      "(Iteration 56901 / 61200) loss: 2.303266\n",
      "(Iteration 57001 / 61200) loss: 2.303274\n",
      "(Iteration 57101 / 61200) loss: 2.303367\n",
      "(Iteration 57201 / 61200) loss: 2.303271\n",
      "(Iteration 57301 / 61200) loss: 2.303176\n",
      "(Epoch 75 / 80) train acc: 0.106000; val_acc: 0.107000\n",
      "(Iteration 57401 / 61200) loss: 2.303384\n",
      "(Iteration 57501 / 61200) loss: 2.303279\n",
      "(Iteration 57601 / 61200) loss: 2.303261\n",
      "(Iteration 57701 / 61200) loss: 2.303320\n",
      "(Iteration 57801 / 61200) loss: 2.303280\n",
      "(Iteration 57901 / 61200) loss: 2.303231\n",
      "(Iteration 58001 / 61200) loss: 2.303356\n",
      "(Iteration 58101 / 61200) loss: 2.303329\n",
      "(Epoch 76 / 80) train acc: 0.098000; val_acc: 0.107000\n",
      "(Iteration 58201 / 61200) loss: 2.303231\n",
      "(Iteration 58301 / 61200) loss: 2.303243\n",
      "(Iteration 58401 / 61200) loss: 2.303301\n",
      "(Iteration 58501 / 61200) loss: 2.303384\n",
      "(Iteration 58601 / 61200) loss: 2.303267\n",
      "(Iteration 58701 / 61200) loss: 2.303333\n",
      "(Iteration 58801 / 61200) loss: 2.303356\n",
      "(Iteration 58901 / 61200) loss: 2.303325\n",
      "(Epoch 77 / 80) train acc: 0.090000; val_acc: 0.107000\n",
      "(Iteration 59001 / 61200) loss: 2.303271\n",
      "(Iteration 59101 / 61200) loss: 2.303334\n",
      "(Iteration 59201 / 61200) loss: 2.303220\n",
      "(Iteration 59301 / 61200) loss: 2.303321\n",
      "(Iteration 59401 / 61200) loss: 2.303336\n",
      "(Iteration 59501 / 61200) loss: 2.303414\n",
      "(Iteration 59601 / 61200) loss: 2.303396\n",
      "(Epoch 78 / 80) train acc: 0.095000; val_acc: 0.107000\n",
      "(Iteration 59701 / 61200) loss: 2.303269\n",
      "(Iteration 59801 / 61200) loss: 2.303319\n",
      "(Iteration 59901 / 61200) loss: 2.303218\n",
      "(Iteration 60001 / 61200) loss: 2.303207\n",
      "(Iteration 60101 / 61200) loss: 2.303342\n",
      "(Iteration 60201 / 61200) loss: 2.303300\n",
      "(Iteration 60301 / 61200) loss: 2.303202\n",
      "(Iteration 60401 / 61200) loss: 2.303209\n",
      "(Epoch 79 / 80) train acc: 0.102000; val_acc: 0.107000\n",
      "(Iteration 60501 / 61200) loss: 2.303357\n",
      "(Iteration 60601 / 61200) loss: 2.303272\n",
      "(Iteration 60701 / 61200) loss: 2.303240\n",
      "(Iteration 60801 / 61200) loss: 2.303249\n",
      "(Iteration 60901 / 61200) loss: 2.303284\n",
      "(Iteration 61001 / 61200) loss: 2.303263\n",
      "(Iteration 61101 / 61200) loss: 2.303219\n",
      "(Epoch 80 / 80) train acc: 0.096000; val_acc: 0.107000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 80, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 30560) loss: 2.308315\n",
      "(Epoch 0 / 80) train acc: 0.091000; val_acc: 0.109000\n",
      "(Iteration 101 / 30560) loss: 2.308241\n",
      "(Iteration 201 / 30560) loss: 2.308158\n",
      "(Iteration 301 / 30560) loss: 2.308085\n",
      "(Epoch 1 / 80) train acc: 0.085000; val_acc: 0.077000\n",
      "(Iteration 401 / 30560) loss: 2.308010\n",
      "(Iteration 501 / 30560) loss: 2.307933\n",
      "(Iteration 601 / 30560) loss: 2.307869\n",
      "(Iteration 701 / 30560) loss: 2.307787\n",
      "(Epoch 2 / 80) train acc: 0.093000; val_acc: 0.083000\n",
      "(Iteration 801 / 30560) loss: 2.307734\n",
      "(Iteration 901 / 30560) loss: 2.307669\n",
      "(Iteration 1001 / 30560) loss: 2.307590\n",
      "(Iteration 1101 / 30560) loss: 2.307527\n",
      "(Epoch 3 / 80) train acc: 0.103000; val_acc: 0.073000\n",
      "(Iteration 1201 / 30560) loss: 2.307471\n",
      "(Iteration 1301 / 30560) loss: 2.307426\n",
      "(Iteration 1401 / 30560) loss: 2.307355\n",
      "(Iteration 1501 / 30560) loss: 2.307287\n",
      "(Epoch 4 / 80) train acc: 0.090000; val_acc: 0.086000\n",
      "(Iteration 1601 / 30560) loss: 2.307249\n",
      "(Iteration 1701 / 30560) loss: 2.307197\n",
      "(Iteration 1801 / 30560) loss: 2.307128\n",
      "(Iteration 1901 / 30560) loss: 2.307077\n",
      "(Epoch 5 / 80) train acc: 0.133000; val_acc: 0.111000\n",
      "(Iteration 2001 / 30560) loss: 2.307040\n",
      "(Iteration 2101 / 30560) loss: 2.306991\n",
      "(Iteration 2201 / 30560) loss: 2.306948\n",
      "(Epoch 6 / 80) train acc: 0.123000; val_acc: 0.095000\n",
      "(Iteration 2301 / 30560) loss: 2.306904\n",
      "(Iteration 2401 / 30560) loss: 2.306864\n",
      "(Iteration 2501 / 30560) loss: 2.306787\n",
      "(Iteration 2601 / 30560) loss: 2.306776\n",
      "(Epoch 7 / 80) train acc: 0.099000; val_acc: 0.091000\n",
      "(Iteration 2701 / 30560) loss: 2.306730\n",
      "(Iteration 2801 / 30560) loss: 2.306682\n",
      "(Iteration 2901 / 30560) loss: 2.306638\n",
      "(Iteration 3001 / 30560) loss: 2.306599\n",
      "(Epoch 8 / 80) train acc: 0.107000; val_acc: 0.081000\n",
      "(Iteration 3101 / 30560) loss: 2.306589\n",
      "(Iteration 3201 / 30560) loss: 2.306531\n",
      "(Iteration 3301 / 30560) loss: 2.306478\n",
      "(Iteration 3401 / 30560) loss: 2.306445\n",
      "(Epoch 9 / 80) train acc: 0.095000; val_acc: 0.089000\n",
      "(Iteration 3501 / 30560) loss: 2.306416\n",
      "(Iteration 3601 / 30560) loss: 2.306363\n",
      "(Iteration 3701 / 30560) loss: 2.306342\n",
      "(Iteration 3801 / 30560) loss: 2.306317\n",
      "(Epoch 10 / 80) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 3901 / 30560) loss: 2.306298\n",
      "(Iteration 4001 / 30560) loss: 2.306267\n",
      "(Iteration 4101 / 30560) loss: 2.306222\n",
      "(Iteration 4201 / 30560) loss: 2.306198\n",
      "(Epoch 11 / 80) train acc: 0.116000; val_acc: 0.095000\n",
      "(Iteration 4301 / 30560) loss: 2.306137\n",
      "(Iteration 4401 / 30560) loss: 2.306132\n",
      "(Iteration 4501 / 30560) loss: 2.306110\n",
      "(Epoch 12 / 80) train acc: 0.091000; val_acc: 0.083000\n",
      "(Iteration 4601 / 30560) loss: 2.306081\n",
      "(Iteration 4701 / 30560) loss: 2.306093\n",
      "(Iteration 4801 / 30560) loss: 2.306012\n",
      "(Iteration 4901 / 30560) loss: 2.306006\n",
      "(Epoch 13 / 80) train acc: 0.126000; val_acc: 0.100000\n",
      "(Iteration 5001 / 30560) loss: 2.306002\n",
      "(Iteration 5101 / 30560) loss: 2.305953\n",
      "(Iteration 5201 / 30560) loss: 2.305917\n",
      "(Iteration 5301 / 30560) loss: 2.305912\n",
      "(Epoch 14 / 80) train acc: 0.109000; val_acc: 0.101000\n",
      "(Iteration 5401 / 30560) loss: 2.305838\n",
      "(Iteration 5501 / 30560) loss: 2.305859\n",
      "(Iteration 5601 / 30560) loss: 2.305827\n",
      "(Iteration 5701 / 30560) loss: 2.305841\n",
      "(Epoch 15 / 80) train acc: 0.112000; val_acc: 0.085000\n",
      "(Iteration 5801 / 30560) loss: 2.305811\n",
      "(Iteration 5901 / 30560) loss: 2.305805\n",
      "(Iteration 6001 / 30560) loss: 2.305738\n",
      "(Iteration 6101 / 30560) loss: 2.305712\n",
      "(Epoch 16 / 80) train acc: 0.101000; val_acc: 0.083000\n",
      "(Iteration 6201 / 30560) loss: 2.305715\n",
      "(Iteration 6301 / 30560) loss: 2.305694\n",
      "(Iteration 6401 / 30560) loss: 2.305657\n",
      "(Epoch 17 / 80) train acc: 0.095000; val_acc: 0.080000\n",
      "(Iteration 6501 / 30560) loss: 2.305629\n",
      "(Iteration 6601 / 30560) loss: 2.305637\n",
      "(Iteration 6701 / 30560) loss: 2.305622\n",
      "(Iteration 6801 / 30560) loss: 2.305604\n",
      "(Epoch 18 / 80) train acc: 0.107000; val_acc: 0.082000\n",
      "(Iteration 6901 / 30560) loss: 2.305639\n",
      "(Iteration 7001 / 30560) loss: 2.305570\n",
      "(Iteration 7101 / 30560) loss: 2.305552\n",
      "(Iteration 7201 / 30560) loss: 2.305526\n",
      "(Epoch 19 / 80) train acc: 0.101000; val_acc: 0.081000\n",
      "(Iteration 7301 / 30560) loss: 2.305520\n",
      "(Iteration 7401 / 30560) loss: 2.305489\n",
      "(Iteration 7501 / 30560) loss: 2.305503\n",
      "(Iteration 7601 / 30560) loss: 2.305520\n",
      "(Epoch 20 / 80) train acc: 0.092000; val_acc: 0.080000\n",
      "(Iteration 7701 / 30560) loss: 2.305454\n",
      "(Iteration 7801 / 30560) loss: 2.305462\n",
      "(Iteration 7901 / 30560) loss: 2.305400\n",
      "(Iteration 8001 / 30560) loss: 2.305404\n",
      "(Epoch 21 / 80) train acc: 0.115000; val_acc: 0.083000\n",
      "(Iteration 8101 / 30560) loss: 2.305402\n",
      "(Iteration 8201 / 30560) loss: 2.305377\n",
      "(Iteration 8301 / 30560) loss: 2.305355\n",
      "(Iteration 8401 / 30560) loss: 2.305385\n",
      "(Epoch 22 / 80) train acc: 0.107000; val_acc: 0.083000\n",
      "(Iteration 8501 / 30560) loss: 2.305378\n",
      "(Iteration 8601 / 30560) loss: 2.305321\n",
      "(Iteration 8701 / 30560) loss: 2.305324\n",
      "(Epoch 23 / 80) train acc: 0.088000; val_acc: 0.081000\n",
      "(Iteration 8801 / 30560) loss: 2.305291\n",
      "(Iteration 8901 / 30560) loss: 2.305291\n",
      "(Iteration 9001 / 30560) loss: 2.305250\n",
      "(Iteration 9101 / 30560) loss: 2.305279\n",
      "(Epoch 24 / 80) train acc: 0.098000; val_acc: 0.080000\n",
      "(Iteration 9201 / 30560) loss: 2.305242\n",
      "(Iteration 9301 / 30560) loss: 2.305262\n",
      "(Iteration 9401 / 30560) loss: 2.305227\n",
      "(Iteration 9501 / 30560) loss: 2.305219\n",
      "(Epoch 25 / 80) train acc: 0.087000; val_acc: 0.080000\n",
      "(Iteration 9601 / 30560) loss: 2.305238\n",
      "(Iteration 9701 / 30560) loss: 2.305223\n",
      "(Iteration 9801 / 30560) loss: 2.305234\n",
      "(Iteration 9901 / 30560) loss: 2.305184\n",
      "(Epoch 26 / 80) train acc: 0.099000; val_acc: 0.080000\n",
      "(Iteration 10001 / 30560) loss: 2.305190\n",
      "(Iteration 10101 / 30560) loss: 2.305182\n",
      "(Iteration 10201 / 30560) loss: 2.305149\n",
      "(Iteration 10301 / 30560) loss: 2.305168\n",
      "(Epoch 27 / 80) train acc: 0.086000; val_acc: 0.081000\n",
      "(Iteration 10401 / 30560) loss: 2.305142\n",
      "(Iteration 10501 / 30560) loss: 2.305112\n",
      "(Iteration 10601 / 30560) loss: 2.305131\n",
      "(Epoch 28 / 80) train acc: 0.111000; val_acc: 0.082000\n",
      "(Iteration 10701 / 30560) loss: 2.305112\n",
      "(Iteration 10801 / 30560) loss: 2.305114\n",
      "(Iteration 10901 / 30560) loss: 2.305082\n",
      "(Iteration 11001 / 30560) loss: 2.305103\n",
      "(Epoch 29 / 80) train acc: 0.110000; val_acc: 0.083000\n",
      "(Iteration 11101 / 30560) loss: 2.305076\n",
      "(Iteration 11201 / 30560) loss: 2.305048\n",
      "(Iteration 11301 / 30560) loss: 2.305068\n",
      "(Iteration 11401 / 30560) loss: 2.305080\n",
      "(Epoch 30 / 80) train acc: 0.109000; val_acc: 0.084000\n",
      "(Iteration 11501 / 30560) loss: 2.305012\n",
      "(Iteration 11601 / 30560) loss: 2.305028\n",
      "(Iteration 11701 / 30560) loss: 2.305048\n",
      "(Iteration 11801 / 30560) loss: 2.305029\n",
      "(Epoch 31 / 80) train acc: 0.103000; val_acc: 0.083000\n",
      "(Iteration 11901 / 30560) loss: 2.305034\n",
      "(Iteration 12001 / 30560) loss: 2.305005\n",
      "(Iteration 12101 / 30560) loss: 2.305012\n",
      "(Iteration 12201 / 30560) loss: 2.305006\n",
      "(Epoch 32 / 80) train acc: 0.097000; val_acc: 0.083000\n",
      "(Iteration 12301 / 30560) loss: 2.304963\n",
      "(Iteration 12401 / 30560) loss: 2.304968\n",
      "(Iteration 12501 / 30560) loss: 2.304964\n",
      "(Iteration 12601 / 30560) loss: 2.304974\n",
      "(Epoch 33 / 80) train acc: 0.107000; val_acc: 0.084000\n",
      "(Iteration 12701 / 30560) loss: 2.304972\n",
      "(Iteration 12801 / 30560) loss: 2.304959\n",
      "(Iteration 12901 / 30560) loss: 2.304961\n",
      "(Epoch 34 / 80) train acc: 0.108000; val_acc: 0.084000\n",
      "(Iteration 13001 / 30560) loss: 2.304961\n",
      "(Iteration 13101 / 30560) loss: 2.304933\n",
      "(Iteration 13201 / 30560) loss: 2.304934\n",
      "(Iteration 13301 / 30560) loss: 2.304930\n",
      "(Epoch 35 / 80) train acc: 0.086000; val_acc: 0.085000\n",
      "(Iteration 13401 / 30560) loss: 2.304898\n",
      "(Iteration 13501 / 30560) loss: 2.304949\n",
      "(Iteration 13601 / 30560) loss: 2.304870\n",
      "(Iteration 13701 / 30560) loss: 2.304915\n",
      "(Epoch 36 / 80) train acc: 0.108000; val_acc: 0.084000\n",
      "(Iteration 13801 / 30560) loss: 2.304932\n",
      "(Iteration 13901 / 30560) loss: 2.304876\n",
      "(Iteration 14001 / 30560) loss: 2.304866\n",
      "(Iteration 14101 / 30560) loss: 2.304925\n",
      "(Epoch 37 / 80) train acc: 0.107000; val_acc: 0.084000\n",
      "(Iteration 14201 / 30560) loss: 2.304907\n",
      "(Iteration 14301 / 30560) loss: 2.304883\n",
      "(Iteration 14401 / 30560) loss: 2.304875\n",
      "(Iteration 14501 / 30560) loss: 2.304900\n",
      "(Epoch 38 / 80) train acc: 0.100000; val_acc: 0.084000\n",
      "(Iteration 14601 / 30560) loss: 2.304854\n",
      "(Iteration 14701 / 30560) loss: 2.304861\n",
      "(Iteration 14801 / 30560) loss: 2.304834\n",
      "(Epoch 39 / 80) train acc: 0.092000; val_acc: 0.084000\n",
      "(Iteration 14901 / 30560) loss: 2.304802\n",
      "(Iteration 15001 / 30560) loss: 2.304829\n",
      "(Iteration 15101 / 30560) loss: 2.304854\n",
      "(Iteration 15201 / 30560) loss: 2.304815\n",
      "(Epoch 40 / 80) train acc: 0.109000; val_acc: 0.084000\n",
      "(Iteration 15301 / 30560) loss: 2.304823\n",
      "(Iteration 15401 / 30560) loss: 2.304827\n",
      "(Iteration 15501 / 30560) loss: 2.304857\n",
      "(Iteration 15601 / 30560) loss: 2.304821\n",
      "(Epoch 41 / 80) train acc: 0.103000; val_acc: 0.083000\n",
      "(Iteration 15701 / 30560) loss: 2.304787\n",
      "(Iteration 15801 / 30560) loss: 2.304824\n",
      "(Iteration 15901 / 30560) loss: 2.304808\n",
      "(Iteration 16001 / 30560) loss: 2.304830\n",
      "(Epoch 42 / 80) train acc: 0.109000; val_acc: 0.084000\n",
      "(Iteration 16101 / 30560) loss: 2.304841\n",
      "(Iteration 16201 / 30560) loss: 2.304806\n",
      "(Iteration 16301 / 30560) loss: 2.304806\n",
      "(Iteration 16401 / 30560) loss: 2.304796\n",
      "(Epoch 43 / 80) train acc: 0.114000; val_acc: 0.084000\n",
      "(Iteration 16501 / 30560) loss: 2.304809\n",
      "(Iteration 16601 / 30560) loss: 2.304784\n",
      "(Iteration 16701 / 30560) loss: 2.304786\n",
      "(Iteration 16801 / 30560) loss: 2.304764\n",
      "(Epoch 44 / 80) train acc: 0.129000; val_acc: 0.085000\n",
      "(Iteration 16901 / 30560) loss: 2.304743\n",
      "(Iteration 17001 / 30560) loss: 2.304750\n",
      "(Iteration 17101 / 30560) loss: 2.304767\n",
      "(Epoch 45 / 80) train acc: 0.096000; val_acc: 0.084000\n",
      "(Iteration 17201 / 30560) loss: 2.304810\n",
      "(Iteration 17301 / 30560) loss: 2.304747\n",
      "(Iteration 17401 / 30560) loss: 2.304771\n",
      "(Iteration 17501 / 30560) loss: 2.304764\n",
      "(Epoch 46 / 80) train acc: 0.109000; val_acc: 0.084000\n",
      "(Iteration 17601 / 30560) loss: 2.304778\n",
      "(Iteration 17701 / 30560) loss: 2.304755\n",
      "(Iteration 17801 / 30560) loss: 2.304738\n",
      "(Iteration 17901 / 30560) loss: 2.304740\n",
      "(Epoch 47 / 80) train acc: 0.100000; val_acc: 0.083000\n",
      "(Iteration 18001 / 30560) loss: 2.304717\n",
      "(Iteration 18101 / 30560) loss: 2.304766\n",
      "(Iteration 18201 / 30560) loss: 2.304726\n",
      "(Iteration 18301 / 30560) loss: 2.304732\n",
      "(Epoch 48 / 80) train acc: 0.114000; val_acc: 0.083000\n",
      "(Iteration 18401 / 30560) loss: 2.304732\n",
      "(Iteration 18501 / 30560) loss: 2.304717\n",
      "(Iteration 18601 / 30560) loss: 2.304714\n",
      "(Iteration 18701 / 30560) loss: 2.304705\n",
      "(Epoch 49 / 80) train acc: 0.102000; val_acc: 0.082000\n",
      "(Iteration 18801 / 30560) loss: 2.304735\n",
      "(Iteration 18901 / 30560) loss: 2.304716\n",
      "(Iteration 19001 / 30560) loss: 2.304716\n",
      "(Epoch 50 / 80) train acc: 0.116000; val_acc: 0.082000\n",
      "(Iteration 19101 / 30560) loss: 2.304734\n",
      "(Iteration 19201 / 30560) loss: 2.304720\n",
      "(Iteration 19301 / 30560) loss: 2.304706\n",
      "(Iteration 19401 / 30560) loss: 2.304702\n",
      "(Epoch 51 / 80) train acc: 0.106000; val_acc: 0.083000\n",
      "(Iteration 19501 / 30560) loss: 2.304735\n",
      "(Iteration 19601 / 30560) loss: 2.304723\n",
      "(Iteration 19701 / 30560) loss: 2.304725\n",
      "(Iteration 19801 / 30560) loss: 2.304725\n",
      "(Epoch 52 / 80) train acc: 0.100000; val_acc: 0.083000\n",
      "(Iteration 19901 / 30560) loss: 2.304709\n",
      "(Iteration 20001 / 30560) loss: 2.304684\n",
      "(Iteration 20101 / 30560) loss: 2.304663\n",
      "(Iteration 20201 / 30560) loss: 2.304680\n",
      "(Epoch 53 / 80) train acc: 0.100000; val_acc: 0.083000\n",
      "(Iteration 20301 / 30560) loss: 2.304726\n",
      "(Iteration 20401 / 30560) loss: 2.304694\n",
      "(Iteration 20501 / 30560) loss: 2.304690\n",
      "(Iteration 20601 / 30560) loss: 2.304702\n",
      "(Epoch 54 / 80) train acc: 0.101000; val_acc: 0.083000\n",
      "(Iteration 20701 / 30560) loss: 2.304705\n",
      "(Iteration 20801 / 30560) loss: 2.304676\n",
      "(Iteration 20901 / 30560) loss: 2.304687\n",
      "(Iteration 21001 / 30560) loss: 2.304729\n",
      "(Epoch 55 / 80) train acc: 0.126000; val_acc: 0.083000\n",
      "(Iteration 21101 / 30560) loss: 2.304668\n",
      "(Iteration 21201 / 30560) loss: 2.304653\n",
      "(Iteration 21301 / 30560) loss: 2.304670\n",
      "(Epoch 56 / 80) train acc: 0.098000; val_acc: 0.083000\n",
      "(Iteration 21401 / 30560) loss: 2.304676\n",
      "(Iteration 21501 / 30560) loss: 2.304635\n",
      "(Iteration 21601 / 30560) loss: 2.304649\n",
      "(Iteration 21701 / 30560) loss: 2.304693\n",
      "(Epoch 57 / 80) train acc: 0.110000; val_acc: 0.083000\n",
      "(Iteration 21801 / 30560) loss: 2.304672\n",
      "(Iteration 21901 / 30560) loss: 2.304670\n",
      "(Iteration 22001 / 30560) loss: 2.304712\n",
      "(Iteration 22101 / 30560) loss: 2.304661\n",
      "(Epoch 58 / 80) train acc: 0.137000; val_acc: 0.083000\n",
      "(Iteration 22201 / 30560) loss: 2.304643\n",
      "(Iteration 22301 / 30560) loss: 2.304683\n",
      "(Iteration 22401 / 30560) loss: 2.304632\n",
      "(Iteration 22501 / 30560) loss: 2.304672\n",
      "(Epoch 59 / 80) train acc: 0.099000; val_acc: 0.083000\n",
      "(Iteration 22601 / 30560) loss: 2.304686\n",
      "(Iteration 22701 / 30560) loss: 2.304633\n",
      "(Iteration 22801 / 30560) loss: 2.304652\n",
      "(Iteration 22901 / 30560) loss: 2.304645\n",
      "(Epoch 60 / 80) train acc: 0.109000; val_acc: 0.084000\n",
      "(Iteration 23001 / 30560) loss: 2.304627\n",
      "(Iteration 23101 / 30560) loss: 2.304638\n",
      "(Iteration 23201 / 30560) loss: 2.304601\n",
      "(Iteration 23301 / 30560) loss: 2.304679\n",
      "(Epoch 61 / 80) train acc: 0.108000; val_acc: 0.085000\n",
      "(Iteration 23401 / 30560) loss: 2.304686\n",
      "(Iteration 23501 / 30560) loss: 2.304620\n",
      "(Iteration 23601 / 30560) loss: 2.304678\n",
      "(Epoch 62 / 80) train acc: 0.103000; val_acc: 0.084000\n",
      "(Iteration 23701 / 30560) loss: 2.304646\n",
      "(Iteration 23801 / 30560) loss: 2.304624\n",
      "(Iteration 23901 / 30560) loss: 2.304612\n",
      "(Iteration 24001 / 30560) loss: 2.304615\n",
      "(Epoch 63 / 80) train acc: 0.104000; val_acc: 0.085000\n",
      "(Iteration 24101 / 30560) loss: 2.304652\n",
      "(Iteration 24201 / 30560) loss: 2.304623\n",
      "(Iteration 24301 / 30560) loss: 2.304621\n",
      "(Iteration 24401 / 30560) loss: 2.304600\n",
      "(Epoch 64 / 80) train acc: 0.109000; val_acc: 0.085000\n",
      "(Iteration 24501 / 30560) loss: 2.304631\n",
      "(Iteration 24601 / 30560) loss: 2.304569\n",
      "(Iteration 24701 / 30560) loss: 2.304648\n",
      "(Iteration 24801 / 30560) loss: 2.304611\n",
      "(Epoch 65 / 80) train acc: 0.088000; val_acc: 0.083000\n",
      "(Iteration 24901 / 30560) loss: 2.304592\n",
      "(Iteration 25001 / 30560) loss: 2.304596\n",
      "(Iteration 25101 / 30560) loss: 2.304585\n",
      "(Iteration 25201 / 30560) loss: 2.304619\n",
      "(Epoch 66 / 80) train acc: 0.104000; val_acc: 0.083000\n",
      "(Iteration 25301 / 30560) loss: 2.304654\n",
      "(Iteration 25401 / 30560) loss: 2.304585\n",
      "(Iteration 25501 / 30560) loss: 2.304627\n",
      "(Epoch 67 / 80) train acc: 0.116000; val_acc: 0.085000\n",
      "(Iteration 25601 / 30560) loss: 2.304595\n",
      "(Iteration 25701 / 30560) loss: 2.304639\n",
      "(Iteration 25801 / 30560) loss: 2.304606\n",
      "(Iteration 25901 / 30560) loss: 2.304604\n",
      "(Epoch 68 / 80) train acc: 0.079000; val_acc: 0.085000\n",
      "(Iteration 26001 / 30560) loss: 2.304594\n",
      "(Iteration 26101 / 30560) loss: 2.304620\n",
      "(Iteration 26201 / 30560) loss: 2.304617\n",
      "(Iteration 26301 / 30560) loss: 2.304549\n",
      "(Epoch 69 / 80) train acc: 0.102000; val_acc: 0.085000\n",
      "(Iteration 26401 / 30560) loss: 2.304650\n",
      "(Iteration 26501 / 30560) loss: 2.304608\n",
      "(Iteration 26601 / 30560) loss: 2.304601\n",
      "(Iteration 26701 / 30560) loss: 2.304638\n",
      "(Epoch 70 / 80) train acc: 0.106000; val_acc: 0.085000\n",
      "(Iteration 26801 / 30560) loss: 2.304621\n",
      "(Iteration 26901 / 30560) loss: 2.304609\n",
      "(Iteration 27001 / 30560) loss: 2.304600\n",
      "(Iteration 27101 / 30560) loss: 2.304614\n",
      "(Epoch 71 / 80) train acc: 0.093000; val_acc: 0.085000\n",
      "(Iteration 27201 / 30560) loss: 2.304653\n",
      "(Iteration 27301 / 30560) loss: 2.304649\n",
      "(Iteration 27401 / 30560) loss: 2.304548\n",
      "(Iteration 27501 / 30560) loss: 2.304602\n",
      "(Epoch 72 / 80) train acc: 0.101000; val_acc: 0.085000\n",
      "(Iteration 27601 / 30560) loss: 2.304598\n",
      "(Iteration 27701 / 30560) loss: 2.304612\n",
      "(Iteration 27801 / 30560) loss: 2.304597\n",
      "(Epoch 73 / 80) train acc: 0.107000; val_acc: 0.085000\n",
      "(Iteration 27901 / 30560) loss: 2.304595\n",
      "(Iteration 28001 / 30560) loss: 2.304611\n",
      "(Iteration 28101 / 30560) loss: 2.304624\n",
      "(Iteration 28201 / 30560) loss: 2.304609\n",
      "(Epoch 74 / 80) train acc: 0.100000; val_acc: 0.085000\n",
      "(Iteration 28301 / 30560) loss: 2.304577\n",
      "(Iteration 28401 / 30560) loss: 2.304631\n",
      "(Iteration 28501 / 30560) loss: 2.304590\n",
      "(Iteration 28601 / 30560) loss: 2.304594\n",
      "(Epoch 75 / 80) train acc: 0.121000; val_acc: 0.085000\n",
      "(Iteration 28701 / 30560) loss: 2.304593\n",
      "(Iteration 28801 / 30560) loss: 2.304589\n",
      "(Iteration 28901 / 30560) loss: 2.304615\n",
      "(Iteration 29001 / 30560) loss: 2.304581\n",
      "(Epoch 76 / 80) train acc: 0.096000; val_acc: 0.085000\n",
      "(Iteration 29101 / 30560) loss: 2.304600\n",
      "(Iteration 29201 / 30560) loss: 2.304591\n",
      "(Iteration 29301 / 30560) loss: 2.304555\n",
      "(Iteration 29401 / 30560) loss: 2.304605\n",
      "(Epoch 77 / 80) train acc: 0.091000; val_acc: 0.085000\n",
      "(Iteration 29501 / 30560) loss: 2.304590\n",
      "(Iteration 29601 / 30560) loss: 2.304589\n",
      "(Iteration 29701 / 30560) loss: 2.304563\n",
      "(Epoch 78 / 80) train acc: 0.110000; val_acc: 0.084000\n",
      "(Iteration 29801 / 30560) loss: 2.304587\n",
      "(Iteration 29901 / 30560) loss: 2.304584\n",
      "(Iteration 30001 / 30560) loss: 2.304562\n",
      "(Iteration 30101 / 30560) loss: 2.304577\n",
      "(Epoch 79 / 80) train acc: 0.107000; val_acc: 0.084000\n",
      "(Iteration 30201 / 30560) loss: 2.304547\n",
      "(Iteration 30301 / 30560) loss: 2.304595\n",
      "(Iteration 30401 / 30560) loss: 2.304559\n",
      "(Iteration 30501 / 30560) loss: 2.304564\n",
      "(Epoch 80 / 80) train acc: 0.095000; val_acc: 0.084000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.306651\n",
      "(Epoch 0 / 100) train acc: 0.107000; val_acc: 0.093000\n",
      "(Iteration 101 / 76500) loss: 2.306609\n",
      "(Iteration 201 / 76500) loss: 2.306580\n",
      "(Iteration 301 / 76500) loss: 2.306533\n",
      "(Iteration 401 / 76500) loss: 2.306490\n",
      "(Iteration 501 / 76500) loss: 2.306480\n",
      "(Iteration 601 / 76500) loss: 2.306419\n",
      "(Iteration 701 / 76500) loss: 2.306380\n",
      "(Epoch 1 / 100) train acc: 0.093000; val_acc: 0.079000\n",
      "(Iteration 801 / 76500) loss: 2.306335\n",
      "(Iteration 901 / 76500) loss: 2.306286\n",
      "(Iteration 1001 / 76500) loss: 2.306278\n",
      "(Iteration 1101 / 76500) loss: 2.306247\n",
      "(Iteration 1201 / 76500) loss: 2.306195\n",
      "(Iteration 1301 / 76500) loss: 2.306162\n",
      "(Iteration 1401 / 76500) loss: 2.306119\n",
      "(Iteration 1501 / 76500) loss: 2.306116\n",
      "(Epoch 2 / 100) train acc: 0.110000; val_acc: 0.081000\n",
      "(Iteration 1601 / 76500) loss: 2.306101\n",
      "(Iteration 1701 / 76500) loss: 2.306043\n",
      "(Iteration 1801 / 76500) loss: 2.306041\n",
      "(Iteration 1901 / 76500) loss: 2.306006\n",
      "(Iteration 2001 / 76500) loss: 2.305924\n",
      "(Iteration 2101 / 76500) loss: 2.305989\n",
      "(Iteration 2201 / 76500) loss: 2.305902\n",
      "(Epoch 3 / 100) train acc: 0.086000; val_acc: 0.079000\n",
      "(Iteration 2301 / 76500) loss: 2.305941\n",
      "(Iteration 2401 / 76500) loss: 2.305878\n",
      "(Iteration 2501 / 76500) loss: 2.305834\n",
      "(Iteration 2601 / 76500) loss: 2.305762\n",
      "(Iteration 2701 / 76500) loss: 2.305807\n",
      "(Iteration 2801 / 76500) loss: 2.305766\n",
      "(Iteration 2901 / 76500) loss: 2.305806\n",
      "(Iteration 3001 / 76500) loss: 2.305692\n",
      "(Epoch 4 / 100) train acc: 0.110000; val_acc: 0.084000\n",
      "(Iteration 3101 / 76500) loss: 2.305699\n",
      "(Iteration 3201 / 76500) loss: 2.305701\n",
      "(Iteration 3301 / 76500) loss: 2.305676\n",
      "(Iteration 3401 / 76500) loss: 2.305623\n",
      "(Iteration 3501 / 76500) loss: 2.305588\n",
      "(Iteration 3601 / 76500) loss: 2.305617\n",
      "(Iteration 3701 / 76500) loss: 2.305556\n",
      "(Iteration 3801 / 76500) loss: 2.305623\n",
      "(Epoch 5 / 100) train acc: 0.111000; val_acc: 0.083000\n",
      "(Iteration 3901 / 76500) loss: 2.305504\n",
      "(Iteration 4001 / 76500) loss: 2.305547\n",
      "(Iteration 4101 / 76500) loss: 2.305541\n",
      "(Iteration 4201 / 76500) loss: 2.305512\n",
      "(Iteration 4301 / 76500) loss: 2.305434\n",
      "(Iteration 4401 / 76500) loss: 2.305455\n",
      "(Iteration 4501 / 76500) loss: 2.305467\n",
      "(Epoch 6 / 100) train acc: 0.118000; val_acc: 0.096000\n",
      "(Iteration 4601 / 76500) loss: 2.305430\n",
      "(Iteration 4701 / 76500) loss: 2.305442\n",
      "(Iteration 4801 / 76500) loss: 2.305447\n",
      "(Iteration 4901 / 76500) loss: 2.305346\n",
      "(Iteration 5001 / 76500) loss: 2.305342\n",
      "(Iteration 5101 / 76500) loss: 2.305354\n",
      "(Iteration 5201 / 76500) loss: 2.305365\n",
      "(Iteration 5301 / 76500) loss: 2.305359\n",
      "(Epoch 7 / 100) train acc: 0.122000; val_acc: 0.106000\n",
      "(Iteration 5401 / 76500) loss: 2.305281\n",
      "(Iteration 5501 / 76500) loss: 2.305281\n",
      "(Iteration 5601 / 76500) loss: 2.305262\n",
      "(Iteration 5701 / 76500) loss: 2.305286\n",
      "(Iteration 5801 / 76500) loss: 2.305219\n",
      "(Iteration 5901 / 76500) loss: 2.305223\n",
      "(Iteration 6001 / 76500) loss: 2.305222\n",
      "(Iteration 6101 / 76500) loss: 2.305176\n",
      "(Epoch 8 / 100) train acc: 0.110000; val_acc: 0.099000\n",
      "(Iteration 6201 / 76500) loss: 2.305205\n",
      "(Iteration 6301 / 76500) loss: 2.305164\n",
      "(Iteration 6401 / 76500) loss: 2.305142\n",
      "(Iteration 6501 / 76500) loss: 2.305158\n",
      "(Iteration 6601 / 76500) loss: 2.305166\n",
      "(Iteration 6701 / 76500) loss: 2.305183\n",
      "(Iteration 6801 / 76500) loss: 2.305124\n",
      "(Epoch 9 / 100) train acc: 0.098000; val_acc: 0.105000\n",
      "(Iteration 6901 / 76500) loss: 2.305123\n",
      "(Iteration 7001 / 76500) loss: 2.305083\n",
      "(Iteration 7101 / 76500) loss: 2.305042\n",
      "(Iteration 7201 / 76500) loss: 2.305116\n",
      "(Iteration 7301 / 76500) loss: 2.305153\n",
      "(Iteration 7401 / 76500) loss: 2.305083\n",
      "(Iteration 7501 / 76500) loss: 2.305111\n",
      "(Iteration 7601 / 76500) loss: 2.305073\n",
      "(Epoch 10 / 100) train acc: 0.092000; val_acc: 0.107000\n",
      "(Iteration 7701 / 76500) loss: 2.305005\n",
      "(Iteration 7801 / 76500) loss: 2.304996\n",
      "(Iteration 7901 / 76500) loss: 2.305078\n",
      "(Iteration 8001 / 76500) loss: 2.305018\n",
      "(Iteration 8101 / 76500) loss: 2.305022\n",
      "(Iteration 8201 / 76500) loss: 2.305024\n",
      "(Iteration 8301 / 76500) loss: 2.305012\n",
      "(Iteration 8401 / 76500) loss: 2.305017\n",
      "(Epoch 11 / 100) train acc: 0.098000; val_acc: 0.105000\n",
      "(Iteration 8501 / 76500) loss: 2.304940\n",
      "(Iteration 8601 / 76500) loss: 2.305003\n",
      "(Iteration 8701 / 76500) loss: 2.304967\n",
      "(Iteration 8801 / 76500) loss: 2.304906\n",
      "(Iteration 8901 / 76500) loss: 2.304963\n",
      "(Iteration 9001 / 76500) loss: 2.304890\n",
      "(Iteration 9101 / 76500) loss: 2.304915\n",
      "(Epoch 12 / 100) train acc: 0.097000; val_acc: 0.105000\n",
      "(Iteration 9201 / 76500) loss: 2.304966\n",
      "(Iteration 9301 / 76500) loss: 2.304935\n",
      "(Iteration 9401 / 76500) loss: 2.304940\n",
      "(Iteration 9501 / 76500) loss: 2.304888\n",
      "(Iteration 9601 / 76500) loss: 2.304891\n",
      "(Iteration 9701 / 76500) loss: 2.304927\n",
      "(Iteration 9801 / 76500) loss: 2.304857\n",
      "(Iteration 9901 / 76500) loss: 2.304938\n",
      "(Epoch 13 / 100) train acc: 0.088000; val_acc: 0.105000\n",
      "(Iteration 10001 / 76500) loss: 2.304862\n",
      "(Iteration 10101 / 76500) loss: 2.304847\n",
      "(Iteration 10201 / 76500) loss: 2.304845\n",
      "(Iteration 10301 / 76500) loss: 2.304864\n",
      "(Iteration 10401 / 76500) loss: 2.304843\n",
      "(Iteration 10501 / 76500) loss: 2.304840\n",
      "(Iteration 10601 / 76500) loss: 2.304856\n",
      "(Iteration 10701 / 76500) loss: 2.304812\n",
      "(Epoch 14 / 100) train acc: 0.114000; val_acc: 0.105000\n",
      "(Iteration 10801 / 76500) loss: 2.304805\n",
      "(Iteration 10901 / 76500) loss: 2.304808\n",
      "(Iteration 11001 / 76500) loss: 2.304856\n",
      "(Iteration 11101 / 76500) loss: 2.304842\n",
      "(Iteration 11201 / 76500) loss: 2.304827\n",
      "(Iteration 11301 / 76500) loss: 2.304803\n",
      "(Iteration 11401 / 76500) loss: 2.304786\n",
      "(Epoch 15 / 100) train acc: 0.103000; val_acc: 0.105000\n",
      "(Iteration 11501 / 76500) loss: 2.304821\n",
      "(Iteration 11601 / 76500) loss: 2.304806\n",
      "(Iteration 11701 / 76500) loss: 2.304824\n",
      "(Iteration 11801 / 76500) loss: 2.304774\n",
      "(Iteration 11901 / 76500) loss: 2.304759\n",
      "(Iteration 12001 / 76500) loss: 2.304770\n",
      "(Iteration 12101 / 76500) loss: 2.304806\n",
      "(Iteration 12201 / 76500) loss: 2.304772\n",
      "(Epoch 16 / 100) train acc: 0.109000; val_acc: 0.105000\n",
      "(Iteration 12301 / 76500) loss: 2.304862\n",
      "(Iteration 12401 / 76500) loss: 2.304694\n",
      "(Iteration 12501 / 76500) loss: 2.304682\n",
      "(Iteration 12601 / 76500) loss: 2.304706\n",
      "(Iteration 12701 / 76500) loss: 2.304745\n",
      "(Iteration 12801 / 76500) loss: 2.304726\n",
      "(Iteration 12901 / 76500) loss: 2.304772\n",
      "(Iteration 13001 / 76500) loss: 2.304732\n",
      "(Epoch 17 / 100) train acc: 0.105000; val_acc: 0.105000\n",
      "(Iteration 13101 / 76500) loss: 2.304715\n",
      "(Iteration 13201 / 76500) loss: 2.304675\n",
      "(Iteration 13301 / 76500) loss: 2.304686\n",
      "(Iteration 13401 / 76500) loss: 2.304634\n",
      "(Iteration 13501 / 76500) loss: 2.304692\n",
      "(Iteration 13601 / 76500) loss: 2.304733\n",
      "(Iteration 13701 / 76500) loss: 2.304678\n",
      "(Epoch 18 / 100) train acc: 0.103000; val_acc: 0.105000\n",
      "(Iteration 13801 / 76500) loss: 2.304683\n",
      "(Iteration 13901 / 76500) loss: 2.304730\n",
      "(Iteration 14001 / 76500) loss: 2.304655\n",
      "(Iteration 14101 / 76500) loss: 2.304670\n",
      "(Iteration 14201 / 76500) loss: 2.304737\n",
      "(Iteration 14301 / 76500) loss: 2.304616\n",
      "(Iteration 14401 / 76500) loss: 2.304656\n",
      "(Iteration 14501 / 76500) loss: 2.304664\n",
      "(Epoch 19 / 100) train acc: 0.094000; val_acc: 0.105000\n",
      "(Iteration 14601 / 76500) loss: 2.304708\n",
      "(Iteration 14701 / 76500) loss: 2.304645\n",
      "(Iteration 14801 / 76500) loss: 2.304742\n",
      "(Iteration 14901 / 76500) loss: 2.304655\n",
      "(Iteration 15001 / 76500) loss: 2.304666\n",
      "(Iteration 15101 / 76500) loss: 2.304702\n",
      "(Iteration 15201 / 76500) loss: 2.304679\n",
      "(Epoch 20 / 100) train acc: 0.097000; val_acc: 0.105000\n",
      "(Iteration 15301 / 76500) loss: 2.304628\n",
      "(Iteration 15401 / 76500) loss: 2.304705\n",
      "(Iteration 15501 / 76500) loss: 2.304712\n",
      "(Iteration 15601 / 76500) loss: 2.304653\n",
      "(Iteration 15701 / 76500) loss: 2.304675\n",
      "(Iteration 15801 / 76500) loss: 2.304545\n",
      "(Iteration 15901 / 76500) loss: 2.304623\n",
      "(Iteration 16001 / 76500) loss: 2.304687\n",
      "(Epoch 21 / 100) train acc: 0.097000; val_acc: 0.105000\n",
      "(Iteration 16101 / 76500) loss: 2.304609\n",
      "(Iteration 16201 / 76500) loss: 2.304669\n",
      "(Iteration 16301 / 76500) loss: 2.304678\n",
      "(Iteration 16401 / 76500) loss: 2.304700\n",
      "(Iteration 16501 / 76500) loss: 2.304566\n",
      "(Iteration 16601 / 76500) loss: 2.304605\n",
      "(Iteration 16701 / 76500) loss: 2.304577\n",
      "(Iteration 16801 / 76500) loss: 2.304589\n",
      "(Epoch 22 / 100) train acc: 0.105000; val_acc: 0.105000\n",
      "(Iteration 16901 / 76500) loss: 2.304582\n",
      "(Iteration 17001 / 76500) loss: 2.304629\n",
      "(Iteration 17101 / 76500) loss: 2.304717\n",
      "(Iteration 17201 / 76500) loss: 2.304541\n",
      "(Iteration 17301 / 76500) loss: 2.304601\n",
      "(Iteration 17401 / 76500) loss: 2.304687\n",
      "(Iteration 17501 / 76500) loss: 2.304586\n",
      "(Epoch 23 / 100) train acc: 0.104000; val_acc: 0.105000\n",
      "(Iteration 17601 / 76500) loss: 2.304685\n",
      "(Iteration 17701 / 76500) loss: 2.304548\n",
      "(Iteration 17801 / 76500) loss: 2.304667\n",
      "(Iteration 17901 / 76500) loss: 2.304600\n",
      "(Iteration 18001 / 76500) loss: 2.304619\n",
      "(Iteration 18101 / 76500) loss: 2.304577\n",
      "(Iteration 18201 / 76500) loss: 2.304602\n",
      "(Iteration 18301 / 76500) loss: 2.304607\n",
      "(Epoch 24 / 100) train acc: 0.100000; val_acc: 0.105000\n",
      "(Iteration 18401 / 76500) loss: 2.304604\n",
      "(Iteration 18501 / 76500) loss: 2.304579\n",
      "(Iteration 18601 / 76500) loss: 2.304613\n",
      "(Iteration 18701 / 76500) loss: 2.304558\n",
      "(Iteration 18801 / 76500) loss: 2.304565\n",
      "(Iteration 18901 / 76500) loss: 2.304536\n",
      "(Iteration 19001 / 76500) loss: 2.304596\n",
      "(Iteration 19101 / 76500) loss: 2.304581\n",
      "(Epoch 25 / 100) train acc: 0.106000; val_acc: 0.105000\n",
      "(Iteration 19201 / 76500) loss: 2.304622\n",
      "(Iteration 19301 / 76500) loss: 2.304599\n",
      "(Iteration 19401 / 76500) loss: 2.304516\n",
      "(Iteration 19501 / 76500) loss: 2.304510\n",
      "(Iteration 19601 / 76500) loss: 2.304535\n",
      "(Iteration 19701 / 76500) loss: 2.304607\n",
      "(Iteration 19801 / 76500) loss: 2.304602\n",
      "(Epoch 26 / 100) train acc: 0.094000; val_acc: 0.105000\n",
      "(Iteration 19901 / 76500) loss: 2.304562\n",
      "(Iteration 20001 / 76500) loss: 2.304569\n",
      "(Iteration 20101 / 76500) loss: 2.304516\n",
      "(Iteration 20201 / 76500) loss: 2.304526\n",
      "(Iteration 20301 / 76500) loss: 2.304653\n",
      "(Iteration 20401 / 76500) loss: 2.304560\n",
      "(Iteration 20501 / 76500) loss: 2.304589\n",
      "(Iteration 20601 / 76500) loss: 2.304548\n",
      "(Epoch 27 / 100) train acc: 0.103000; val_acc: 0.105000\n",
      "(Iteration 20701 / 76500) loss: 2.304602\n",
      "(Iteration 20801 / 76500) loss: 2.304541\n",
      "(Iteration 20901 / 76500) loss: 2.304536\n",
      "(Iteration 21001 / 76500) loss: 2.304498\n",
      "(Iteration 21101 / 76500) loss: 2.304604\n",
      "(Iteration 21201 / 76500) loss: 2.304576\n",
      "(Iteration 21301 / 76500) loss: 2.304611\n",
      "(Iteration 21401 / 76500) loss: 2.304560\n",
      "(Epoch 28 / 100) train acc: 0.090000; val_acc: 0.105000\n",
      "(Iteration 21501 / 76500) loss: 2.304553\n",
      "(Iteration 21601 / 76500) loss: 2.304523\n",
      "(Iteration 21701 / 76500) loss: 2.304500\n",
      "(Iteration 21801 / 76500) loss: 2.304558\n",
      "(Iteration 21901 / 76500) loss: 2.304560\n",
      "(Iteration 22001 / 76500) loss: 2.304533\n",
      "(Iteration 22101 / 76500) loss: 2.304556\n",
      "(Epoch 29 / 100) train acc: 0.108000; val_acc: 0.105000\n",
      "(Iteration 22201 / 76500) loss: 2.304585\n",
      "(Iteration 22301 / 76500) loss: 2.304623\n",
      "(Iteration 22401 / 76500) loss: 2.304512\n",
      "(Iteration 22501 / 76500) loss: 2.304499\n",
      "(Iteration 22601 / 76500) loss: 2.304569\n",
      "(Iteration 22701 / 76500) loss: 2.304526\n",
      "(Iteration 22801 / 76500) loss: 2.304548\n",
      "(Iteration 22901 / 76500) loss: 2.304573\n",
      "(Epoch 30 / 100) train acc: 0.106000; val_acc: 0.105000\n",
      "(Iteration 23001 / 76500) loss: 2.304501\n",
      "(Iteration 23101 / 76500) loss: 2.304567\n",
      "(Iteration 23201 / 76500) loss: 2.304517\n",
      "(Iteration 23301 / 76500) loss: 2.304617\n",
      "(Iteration 23401 / 76500) loss: 2.304560\n",
      "(Iteration 23501 / 76500) loss: 2.304505\n",
      "(Iteration 23601 / 76500) loss: 2.304485\n",
      "(Iteration 23701 / 76500) loss: 2.304537\n",
      "(Epoch 31 / 100) train acc: 0.103000; val_acc: 0.105000\n",
      "(Iteration 23801 / 76500) loss: 2.304484\n",
      "(Iteration 23901 / 76500) loss: 2.304458\n",
      "(Iteration 24001 / 76500) loss: 2.304545\n",
      "(Iteration 24101 / 76500) loss: 2.304533\n",
      "(Iteration 24201 / 76500) loss: 2.304573\n",
      "(Iteration 24301 / 76500) loss: 2.304557\n",
      "(Iteration 24401 / 76500) loss: 2.304503\n",
      "(Epoch 32 / 100) train acc: 0.115000; val_acc: 0.105000\n",
      "(Iteration 24501 / 76500) loss: 2.304574\n",
      "(Iteration 24601 / 76500) loss: 2.304531\n",
      "(Iteration 24701 / 76500) loss: 2.304481\n",
      "(Iteration 24801 / 76500) loss: 2.304539\n",
      "(Iteration 24901 / 76500) loss: 2.304532\n",
      "(Iteration 25001 / 76500) loss: 2.304529\n",
      "(Iteration 25101 / 76500) loss: 2.304612\n",
      "(Iteration 25201 / 76500) loss: 2.304554\n",
      "(Epoch 33 / 100) train acc: 0.106000; val_acc: 0.105000\n",
      "(Iteration 25301 / 76500) loss: 2.304536\n",
      "(Iteration 25401 / 76500) loss: 2.304482\n",
      "(Iteration 25501 / 76500) loss: 2.304469\n",
      "(Iteration 25601 / 76500) loss: 2.304589\n",
      "(Iteration 25701 / 76500) loss: 2.304534\n",
      "(Iteration 25801 / 76500) loss: 2.304491\n",
      "(Iteration 25901 / 76500) loss: 2.304469\n",
      "(Iteration 26001 / 76500) loss: 2.304551\n",
      "(Epoch 34 / 100) train acc: 0.102000; val_acc: 0.105000\n",
      "(Iteration 26101 / 76500) loss: 2.304388\n",
      "(Iteration 26201 / 76500) loss: 2.304494\n",
      "(Iteration 26301 / 76500) loss: 2.304475\n",
      "(Iteration 26401 / 76500) loss: 2.304537\n",
      "(Iteration 26501 / 76500) loss: 2.304552\n",
      "(Iteration 26601 / 76500) loss: 2.304511\n",
      "(Iteration 26701 / 76500) loss: 2.304475\n",
      "(Epoch 35 / 100) train acc: 0.106000; val_acc: 0.105000\n",
      "(Iteration 26801 / 76500) loss: 2.304504\n",
      "(Iteration 26901 / 76500) loss: 2.304530\n",
      "(Iteration 27001 / 76500) loss: 2.304487\n",
      "(Iteration 27101 / 76500) loss: 2.304482\n",
      "(Iteration 27201 / 76500) loss: 2.304484\n",
      "(Iteration 27301 / 76500) loss: 2.304537\n",
      "(Iteration 27401 / 76500) loss: 2.304458\n",
      "(Iteration 27501 / 76500) loss: 2.304461\n",
      "(Epoch 36 / 100) train acc: 0.087000; val_acc: 0.105000\n",
      "(Iteration 27601 / 76500) loss: 2.304505\n",
      "(Iteration 27701 / 76500) loss: 2.304521\n",
      "(Iteration 27801 / 76500) loss: 2.304615\n",
      "(Iteration 27901 / 76500) loss: 2.304561\n",
      "(Iteration 28001 / 76500) loss: 2.304523\n",
      "(Iteration 28101 / 76500) loss: 2.304460\n",
      "(Iteration 28201 / 76500) loss: 2.304537\n",
      "(Iteration 28301 / 76500) loss: 2.304497\n",
      "(Epoch 37 / 100) train acc: 0.107000; val_acc: 0.105000\n",
      "(Iteration 28401 / 76500) loss: 2.304437\n",
      "(Iteration 28501 / 76500) loss: 2.304544\n",
      "(Iteration 28601 / 76500) loss: 2.304507\n",
      "(Iteration 28701 / 76500) loss: 2.304514\n",
      "(Iteration 28801 / 76500) loss: 2.304465\n",
      "(Iteration 28901 / 76500) loss: 2.304466\n",
      "(Iteration 29001 / 76500) loss: 2.304544\n",
      "(Epoch 38 / 100) train acc: 0.099000; val_acc: 0.105000\n",
      "(Iteration 29101 / 76500) loss: 2.304555\n",
      "(Iteration 29201 / 76500) loss: 2.304464\n",
      "(Iteration 29301 / 76500) loss: 2.304482\n",
      "(Iteration 29401 / 76500) loss: 2.304477\n",
      "(Iteration 29501 / 76500) loss: 2.304465\n",
      "(Iteration 29601 / 76500) loss: 2.304509\n",
      "(Iteration 29701 / 76500) loss: 2.304523\n",
      "(Iteration 29801 / 76500) loss: 2.304546\n",
      "(Epoch 39 / 100) train acc: 0.102000; val_acc: 0.105000\n",
      "(Iteration 29901 / 76500) loss: 2.304438\n",
      "(Iteration 30001 / 76500) loss: 2.304459\n",
      "(Iteration 30101 / 76500) loss: 2.304495\n",
      "(Iteration 30201 / 76500) loss: 2.304547\n",
      "(Iteration 30301 / 76500) loss: 2.304505\n",
      "(Iteration 30401 / 76500) loss: 2.304467\n",
      "(Iteration 30501 / 76500) loss: 2.304471\n",
      "(Epoch 40 / 100) train acc: 0.087000; val_acc: 0.105000\n",
      "(Iteration 30601 / 76500) loss: 2.304515\n",
      "(Iteration 30701 / 76500) loss: 2.304535\n",
      "(Iteration 30801 / 76500) loss: 2.304487\n",
      "(Iteration 30901 / 76500) loss: 2.304543\n",
      "(Iteration 31001 / 76500) loss: 2.304473\n",
      "(Iteration 31101 / 76500) loss: 2.304461\n",
      "(Iteration 31201 / 76500) loss: 2.304518\n",
      "(Iteration 31301 / 76500) loss: 2.304473\n",
      "(Epoch 41 / 100) train acc: 0.113000; val_acc: 0.105000\n",
      "(Iteration 31401 / 76500) loss: 2.304501\n",
      "(Iteration 31501 / 76500) loss: 2.304439\n",
      "(Iteration 31601 / 76500) loss: 2.304537\n",
      "(Iteration 31701 / 76500) loss: 2.304537\n",
      "(Iteration 31801 / 76500) loss: 2.304510\n",
      "(Iteration 31901 / 76500) loss: 2.304532\n",
      "(Iteration 32001 / 76500) loss: 2.304476\n",
      "(Iteration 32101 / 76500) loss: 2.304488\n",
      "(Epoch 42 / 100) train acc: 0.098000; val_acc: 0.105000\n",
      "(Iteration 32201 / 76500) loss: 2.304491\n",
      "(Iteration 32301 / 76500) loss: 2.304530\n",
      "(Iteration 32401 / 76500) loss: 2.304578\n",
      "(Iteration 32501 / 76500) loss: 2.304470\n",
      "(Iteration 32601 / 76500) loss: 2.304471\n",
      "(Iteration 32701 / 76500) loss: 2.304513\n",
      "(Iteration 32801 / 76500) loss: 2.304531\n",
      "(Epoch 43 / 100) train acc: 0.099000; val_acc: 0.105000\n",
      "(Iteration 32901 / 76500) loss: 2.304418\n",
      "(Iteration 33001 / 76500) loss: 2.304574\n",
      "(Iteration 33101 / 76500) loss: 2.304576\n",
      "(Iteration 33201 / 76500) loss: 2.304462\n",
      "(Iteration 33301 / 76500) loss: 2.304509\n",
      "(Iteration 33401 / 76500) loss: 2.304472\n",
      "(Iteration 33501 / 76500) loss: 2.304505\n",
      "(Iteration 33601 / 76500) loss: 2.304474\n",
      "(Epoch 44 / 100) train acc: 0.089000; val_acc: 0.105000\n",
      "(Iteration 33701 / 76500) loss: 2.304577\n",
      "(Iteration 33801 / 76500) loss: 2.304571\n",
      "(Iteration 33901 / 76500) loss: 2.304524\n",
      "(Iteration 34001 / 76500) loss: 2.304508\n",
      "(Iteration 34101 / 76500) loss: 2.304404\n",
      "(Iteration 34201 / 76500) loss: 2.304458\n",
      "(Iteration 34301 / 76500) loss: 2.304485\n",
      "(Iteration 34401 / 76500) loss: 2.304450\n",
      "(Epoch 45 / 100) train acc: 0.100000; val_acc: 0.105000\n",
      "(Iteration 34501 / 76500) loss: 2.304427\n",
      "(Iteration 34601 / 76500) loss: 2.304380\n",
      "(Iteration 34701 / 76500) loss: 2.304494\n",
      "(Iteration 34801 / 76500) loss: 2.304471\n",
      "(Iteration 34901 / 76500) loss: 2.304439\n",
      "(Iteration 35001 / 76500) loss: 2.304461\n",
      "(Iteration 35101 / 76500) loss: 2.304510\n",
      "(Epoch 46 / 100) train acc: 0.099000; val_acc: 0.105000\n",
      "(Iteration 35201 / 76500) loss: 2.304529\n",
      "(Iteration 35301 / 76500) loss: 2.304547\n",
      "(Iteration 35401 / 76500) loss: 2.304467\n",
      "(Iteration 35501 / 76500) loss: 2.304536\n",
      "(Iteration 35601 / 76500) loss: 2.304549\n",
      "(Iteration 35701 / 76500) loss: 2.304505\n",
      "(Iteration 35801 / 76500) loss: 2.304489\n",
      "(Iteration 35901 / 76500) loss: 2.304497\n",
      "(Epoch 47 / 100) train acc: 0.102000; val_acc: 0.105000\n",
      "(Iteration 36001 / 76500) loss: 2.304485\n",
      "(Iteration 36101 / 76500) loss: 2.304487\n",
      "(Iteration 36201 / 76500) loss: 2.304489\n",
      "(Iteration 36301 / 76500) loss: 2.304475\n",
      "(Iteration 36401 / 76500) loss: 2.304452\n",
      "(Iteration 36501 / 76500) loss: 2.304462\n",
      "(Iteration 36601 / 76500) loss: 2.304453\n",
      "(Iteration 36701 / 76500) loss: 2.304455\n",
      "(Epoch 48 / 100) train acc: 0.098000; val_acc: 0.105000\n",
      "(Iteration 36801 / 76500) loss: 2.304506\n",
      "(Iteration 36901 / 76500) loss: 2.304541\n",
      "(Iteration 37001 / 76500) loss: 2.304482\n",
      "(Iteration 37101 / 76500) loss: 2.304502\n",
      "(Iteration 37201 / 76500) loss: 2.304463\n",
      "(Iteration 37301 / 76500) loss: 2.304444\n",
      "(Iteration 37401 / 76500) loss: 2.304428\n",
      "(Epoch 49 / 100) train acc: 0.118000; val_acc: 0.105000\n",
      "(Iteration 37501 / 76500) loss: 2.304476\n",
      "(Iteration 37601 / 76500) loss: 2.304442\n",
      "(Iteration 37701 / 76500) loss: 2.304440\n",
      "(Iteration 37801 / 76500) loss: 2.304476\n",
      "(Iteration 37901 / 76500) loss: 2.304499\n",
      "(Iteration 38001 / 76500) loss: 2.304424\n",
      "(Iteration 38101 / 76500) loss: 2.304457\n",
      "(Iteration 38201 / 76500) loss: 2.304468\n",
      "(Epoch 50 / 100) train acc: 0.099000; val_acc: 0.105000\n",
      "(Iteration 38301 / 76500) loss: 2.304510\n",
      "(Iteration 38401 / 76500) loss: 2.304473\n",
      "(Iteration 38501 / 76500) loss: 2.304487\n",
      "(Iteration 38601 / 76500) loss: 2.304478\n",
      "(Iteration 38701 / 76500) loss: 2.304548\n",
      "(Iteration 38801 / 76500) loss: 2.304515\n",
      "(Iteration 38901 / 76500) loss: 2.304470\n",
      "(Iteration 39001 / 76500) loss: 2.304482\n",
      "(Epoch 51 / 100) train acc: 0.095000; val_acc: 0.105000\n",
      "(Iteration 39101 / 76500) loss: 2.304461\n",
      "(Iteration 39201 / 76500) loss: 2.304489\n",
      "(Iteration 39301 / 76500) loss: 2.304486\n",
      "(Iteration 39401 / 76500) loss: 2.304557\n",
      "(Iteration 39501 / 76500) loss: 2.304551\n",
      "(Iteration 39601 / 76500) loss: 2.304491\n",
      "(Iteration 39701 / 76500) loss: 2.304359\n",
      "(Epoch 52 / 100) train acc: 0.096000; val_acc: 0.105000\n",
      "(Iteration 39801 / 76500) loss: 2.304579\n",
      "(Iteration 39901 / 76500) loss: 2.304484\n",
      "(Iteration 40001 / 76500) loss: 2.304451\n",
      "(Iteration 40101 / 76500) loss: 2.304411\n",
      "(Iteration 40201 / 76500) loss: 2.304454\n",
      "(Iteration 40301 / 76500) loss: 2.304517\n",
      "(Iteration 40401 / 76500) loss: 2.304552\n",
      "(Iteration 40501 / 76500) loss: 2.304467\n",
      "(Epoch 53 / 100) train acc: 0.102000; val_acc: 0.105000\n",
      "(Iteration 40601 / 76500) loss: 2.304478\n",
      "(Iteration 40701 / 76500) loss: 2.304494\n",
      "(Iteration 40801 / 76500) loss: 2.304546\n",
      "(Iteration 40901 / 76500) loss: 2.304502\n",
      "(Iteration 41001 / 76500) loss: 2.304423\n",
      "(Iteration 41101 / 76500) loss: 2.304447\n",
      "(Iteration 41201 / 76500) loss: 2.304538\n",
      "(Iteration 41301 / 76500) loss: 2.304432\n",
      "(Epoch 54 / 100) train acc: 0.100000; val_acc: 0.105000\n",
      "(Iteration 41401 / 76500) loss: 2.304538\n",
      "(Iteration 41501 / 76500) loss: 2.304447\n",
      "(Iteration 41601 / 76500) loss: 2.304525\n",
      "(Iteration 41701 / 76500) loss: 2.304493\n",
      "(Iteration 41801 / 76500) loss: 2.304515\n",
      "(Iteration 41901 / 76500) loss: 2.304507\n",
      "(Iteration 42001 / 76500) loss: 2.304509\n",
      "(Epoch 55 / 100) train acc: 0.097000; val_acc: 0.105000\n",
      "(Iteration 42101 / 76500) loss: 2.304481\n",
      "(Iteration 42201 / 76500) loss: 2.304496\n",
      "(Iteration 42301 / 76500) loss: 2.304529\n",
      "(Iteration 42401 / 76500) loss: 2.304515\n",
      "(Iteration 42501 / 76500) loss: 2.304489\n",
      "(Iteration 42601 / 76500) loss: 2.304468\n",
      "(Iteration 42701 / 76500) loss: 2.304470\n",
      "(Iteration 42801 / 76500) loss: 2.304510\n",
      "(Epoch 56 / 100) train acc: 0.125000; val_acc: 0.105000\n",
      "(Iteration 42901 / 76500) loss: 2.304542\n",
      "(Iteration 43001 / 76500) loss: 2.304464\n",
      "(Iteration 43101 / 76500) loss: 2.304416\n",
      "(Iteration 43201 / 76500) loss: 2.304448\n",
      "(Iteration 43301 / 76500) loss: 2.304403\n",
      "(Iteration 43401 / 76500) loss: 2.304475\n",
      "(Iteration 43501 / 76500) loss: 2.304511\n",
      "(Iteration 43601 / 76500) loss: 2.304487\n",
      "(Epoch 57 / 100) train acc: 0.101000; val_acc: 0.105000\n",
      "(Iteration 43701 / 76500) loss: 2.304464\n",
      "(Iteration 43801 / 76500) loss: 2.304514\n",
      "(Iteration 43901 / 76500) loss: 2.304475\n",
      "(Iteration 44001 / 76500) loss: 2.304480\n",
      "(Iteration 44101 / 76500) loss: 2.304471\n",
      "(Iteration 44201 / 76500) loss: 2.304496\n",
      "(Iteration 44301 / 76500) loss: 2.304507\n",
      "(Epoch 58 / 100) train acc: 0.093000; val_acc: 0.105000\n",
      "(Iteration 44401 / 76500) loss: 2.304474\n",
      "(Iteration 44501 / 76500) loss: 2.304412\n",
      "(Iteration 44601 / 76500) loss: 2.304541\n",
      "(Iteration 44701 / 76500) loss: 2.304479\n",
      "(Iteration 44801 / 76500) loss: 2.304464\n",
      "(Iteration 44901 / 76500) loss: 2.304479\n",
      "(Iteration 45001 / 76500) loss: 2.304457\n",
      "(Iteration 45101 / 76500) loss: 2.304462\n",
      "(Epoch 59 / 100) train acc: 0.101000; val_acc: 0.105000\n",
      "(Iteration 45201 / 76500) loss: 2.304470\n",
      "(Iteration 45301 / 76500) loss: 2.304499\n",
      "(Iteration 45401 / 76500) loss: 2.304511\n",
      "(Iteration 45501 / 76500) loss: 2.304444\n",
      "(Iteration 45601 / 76500) loss: 2.304522\n",
      "(Iteration 45701 / 76500) loss: 2.304443\n",
      "(Iteration 45801 / 76500) loss: 2.304456\n",
      "(Epoch 60 / 100) train acc: 0.098000; val_acc: 0.105000\n",
      "(Iteration 45901 / 76500) loss: 2.304464\n",
      "(Iteration 46001 / 76500) loss: 2.304443\n",
      "(Iteration 46101 / 76500) loss: 2.304523\n",
      "(Iteration 46201 / 76500) loss: 2.304523\n",
      "(Iteration 46301 / 76500) loss: 2.304498\n",
      "(Iteration 46401 / 76500) loss: 2.304462\n",
      "(Iteration 46501 / 76500) loss: 2.304580\n",
      "(Iteration 46601 / 76500) loss: 2.304486\n",
      "(Epoch 61 / 100) train acc: 0.099000; val_acc: 0.105000\n",
      "(Iteration 46701 / 76500) loss: 2.304500\n",
      "(Iteration 46801 / 76500) loss: 2.304455\n",
      "(Iteration 46901 / 76500) loss: 2.304508\n",
      "(Iteration 47001 / 76500) loss: 2.304500\n",
      "(Iteration 47101 / 76500) loss: 2.304538\n",
      "(Iteration 47201 / 76500) loss: 2.304488\n",
      "(Iteration 47301 / 76500) loss: 2.304453\n",
      "(Iteration 47401 / 76500) loss: 2.304482\n",
      "(Epoch 62 / 100) train acc: 0.087000; val_acc: 0.105000\n",
      "(Iteration 47501 / 76500) loss: 2.304445\n",
      "(Iteration 47601 / 76500) loss: 2.304440\n",
      "(Iteration 47701 / 76500) loss: 2.304519\n",
      "(Iteration 47801 / 76500) loss: 2.304477\n",
      "(Iteration 47901 / 76500) loss: 2.304386\n",
      "(Iteration 48001 / 76500) loss: 2.304475\n",
      "(Iteration 48101 / 76500) loss: 2.304519\n",
      "(Epoch 63 / 100) train acc: 0.105000; val_acc: 0.105000\n",
      "(Iteration 48201 / 76500) loss: 2.304462\n",
      "(Iteration 48301 / 76500) loss: 2.304460\n",
      "(Iteration 48401 / 76500) loss: 2.304490\n",
      "(Iteration 48501 / 76500) loss: 2.304510\n",
      "(Iteration 48601 / 76500) loss: 2.304470\n",
      "(Iteration 48701 / 76500) loss: 2.304411\n",
      "(Iteration 48801 / 76500) loss: 2.304474\n",
      "(Iteration 48901 / 76500) loss: 2.304506\n",
      "(Epoch 64 / 100) train acc: 0.094000; val_acc: 0.105000\n",
      "(Iteration 49001 / 76500) loss: 2.304390\n",
      "(Iteration 49101 / 76500) loss: 2.304510\n",
      "(Iteration 49201 / 76500) loss: 2.304449\n",
      "(Iteration 49301 / 76500) loss: 2.304454\n",
      "(Iteration 49401 / 76500) loss: 2.304455\n",
      "(Iteration 49501 / 76500) loss: 2.304466\n",
      "(Iteration 49601 / 76500) loss: 2.304442\n",
      "(Iteration 49701 / 76500) loss: 2.304441\n",
      "(Epoch 65 / 100) train acc: 0.100000; val_acc: 0.105000\n",
      "(Iteration 49801 / 76500) loss: 2.304497\n",
      "(Iteration 49901 / 76500) loss: 2.304466\n",
      "(Iteration 50001 / 76500) loss: 2.304496\n",
      "(Iteration 50101 / 76500) loss: 2.304456\n",
      "(Iteration 50201 / 76500) loss: 2.304509\n",
      "(Iteration 50301 / 76500) loss: 2.304458\n",
      "(Iteration 50401 / 76500) loss: 2.304401\n",
      "(Epoch 66 / 100) train acc: 0.104000; val_acc: 0.105000\n",
      "(Iteration 50501 / 76500) loss: 2.304536\n",
      "(Iteration 50601 / 76500) loss: 2.304494\n",
      "(Iteration 50701 / 76500) loss: 2.304501\n",
      "(Iteration 50801 / 76500) loss: 2.304435\n",
      "(Iteration 50901 / 76500) loss: 2.304431\n",
      "(Iteration 51001 / 76500) loss: 2.304476\n",
      "(Iteration 51101 / 76500) loss: 2.304455\n",
      "(Iteration 51201 / 76500) loss: 2.304486\n",
      "(Epoch 67 / 100) train acc: 0.111000; val_acc: 0.105000\n",
      "(Iteration 51301 / 76500) loss: 2.304389\n",
      "(Iteration 51401 / 76500) loss: 2.304454\n",
      "(Iteration 51501 / 76500) loss: 2.304472\n",
      "(Iteration 51601 / 76500) loss: 2.304453\n",
      "(Iteration 51701 / 76500) loss: 2.304512\n",
      "(Iteration 51801 / 76500) loss: 2.304460\n",
      "(Iteration 51901 / 76500) loss: 2.304453\n",
      "(Iteration 52001 / 76500) loss: 2.304477\n",
      "(Epoch 68 / 100) train acc: 0.089000; val_acc: 0.105000\n",
      "(Iteration 52101 / 76500) loss: 2.304397\n",
      "(Iteration 52201 / 76500) loss: 2.304435\n",
      "(Iteration 52301 / 76500) loss: 2.304462\n",
      "(Iteration 52401 / 76500) loss: 2.304502\n",
      "(Iteration 52501 / 76500) loss: 2.304513\n",
      "(Iteration 52601 / 76500) loss: 2.304451\n",
      "(Iteration 52701 / 76500) loss: 2.304485\n",
      "(Epoch 69 / 100) train acc: 0.084000; val_acc: 0.105000\n",
      "(Iteration 52801 / 76500) loss: 2.304466\n",
      "(Iteration 52901 / 76500) loss: 2.304501\n",
      "(Iteration 53001 / 76500) loss: 2.304472\n",
      "(Iteration 53101 / 76500) loss: 2.304535\n",
      "(Iteration 53201 / 76500) loss: 2.304439\n",
      "(Iteration 53301 / 76500) loss: 2.304485\n",
      "(Iteration 53401 / 76500) loss: 2.304429\n",
      "(Iteration 53501 / 76500) loss: 2.304523\n",
      "(Epoch 70 / 100) train acc: 0.109000; val_acc: 0.105000\n",
      "(Iteration 53601 / 76500) loss: 2.304465\n",
      "(Iteration 53701 / 76500) loss: 2.304465\n",
      "(Iteration 53801 / 76500) loss: 2.304426\n",
      "(Iteration 53901 / 76500) loss: 2.304561\n",
      "(Iteration 54001 / 76500) loss: 2.304499\n",
      "(Iteration 54101 / 76500) loss: 2.304495\n",
      "(Iteration 54201 / 76500) loss: 2.304435\n",
      "(Iteration 54301 / 76500) loss: 2.304453\n",
      "(Epoch 71 / 100) train acc: 0.090000; val_acc: 0.105000\n",
      "(Iteration 54401 / 76500) loss: 2.304477\n",
      "(Iteration 54501 / 76500) loss: 2.304517\n",
      "(Iteration 54601 / 76500) loss: 2.304446\n",
      "(Iteration 54701 / 76500) loss: 2.304428\n",
      "(Iteration 54801 / 76500) loss: 2.304502\n",
      "(Iteration 54901 / 76500) loss: 2.304472\n",
      "(Iteration 55001 / 76500) loss: 2.304450\n",
      "(Epoch 72 / 100) train acc: 0.110000; val_acc: 0.105000\n",
      "(Iteration 55101 / 76500) loss: 2.304411\n",
      "(Iteration 55201 / 76500) loss: 2.304434\n",
      "(Iteration 55301 / 76500) loss: 2.304506\n",
      "(Iteration 55401 / 76500) loss: 2.304456\n",
      "(Iteration 55501 / 76500) loss: 2.304499\n",
      "(Iteration 55601 / 76500) loss: 2.304491\n",
      "(Iteration 55701 / 76500) loss: 2.304379\n",
      "(Iteration 55801 / 76500) loss: 2.304505\n",
      "(Epoch 73 / 100) train acc: 0.093000; val_acc: 0.105000\n",
      "(Iteration 55901 / 76500) loss: 2.304481\n",
      "(Iteration 56001 / 76500) loss: 2.304523\n",
      "(Iteration 56101 / 76500) loss: 2.304478\n",
      "(Iteration 56201 / 76500) loss: 2.304489\n",
      "(Iteration 56301 / 76500) loss: 2.304496\n",
      "(Iteration 56401 / 76500) loss: 2.304413\n",
      "(Iteration 56501 / 76500) loss: 2.304479\n",
      "(Iteration 56601 / 76500) loss: 2.304484\n",
      "(Epoch 74 / 100) train acc: 0.094000; val_acc: 0.105000\n",
      "(Iteration 56701 / 76500) loss: 2.304455\n",
      "(Iteration 56801 / 76500) loss: 2.304497\n",
      "(Iteration 56901 / 76500) loss: 2.304508\n",
      "(Iteration 57001 / 76500) loss: 2.304425\n",
      "(Iteration 57101 / 76500) loss: 2.304472\n",
      "(Iteration 57201 / 76500) loss: 2.304528\n",
      "(Iteration 57301 / 76500) loss: 2.304510\n",
      "(Epoch 75 / 100) train acc: 0.107000; val_acc: 0.105000\n",
      "(Iteration 57401 / 76500) loss: 2.304469\n",
      "(Iteration 57501 / 76500) loss: 2.304460\n",
      "(Iteration 57601 / 76500) loss: 2.304437\n",
      "(Iteration 57701 / 76500) loss: 2.304483\n",
      "(Iteration 57801 / 76500) loss: 2.304433\n",
      "(Iteration 57901 / 76500) loss: 2.304541\n",
      "(Iteration 58001 / 76500) loss: 2.304513\n",
      "(Iteration 58101 / 76500) loss: 2.304478\n",
      "(Epoch 76 / 100) train acc: 0.110000; val_acc: 0.105000\n",
      "(Iteration 58201 / 76500) loss: 2.304421\n",
      "(Iteration 58301 / 76500) loss: 2.304493\n",
      "(Iteration 58401 / 76500) loss: 2.304401\n",
      "(Iteration 58501 / 76500) loss: 2.304431\n",
      "(Iteration 58601 / 76500) loss: 2.304481\n",
      "(Iteration 58701 / 76500) loss: 2.304464\n",
      "(Iteration 58801 / 76500) loss: 2.304456\n",
      "(Iteration 58901 / 76500) loss: 2.304452\n",
      "(Epoch 77 / 100) train acc: 0.081000; val_acc: 0.105000\n",
      "(Iteration 59001 / 76500) loss: 2.304492\n",
      "(Iteration 59101 / 76500) loss: 2.304523\n",
      "(Iteration 59201 / 76500) loss: 2.304443\n",
      "(Iteration 59301 / 76500) loss: 2.304454\n",
      "(Iteration 59401 / 76500) loss: 2.304447\n",
      "(Iteration 59501 / 76500) loss: 2.304505\n",
      "(Iteration 59601 / 76500) loss: 2.304484\n",
      "(Epoch 78 / 100) train acc: 0.093000; val_acc: 0.105000\n",
      "(Iteration 59701 / 76500) loss: 2.304504\n",
      "(Iteration 59801 / 76500) loss: 2.304411\n",
      "(Iteration 59901 / 76500) loss: 2.304498\n",
      "(Iteration 60001 / 76500) loss: 2.304502\n",
      "(Iteration 60101 / 76500) loss: 2.304486\n",
      "(Iteration 60201 / 76500) loss: 2.304411\n",
      "(Iteration 60301 / 76500) loss: 2.304474\n",
      "(Iteration 60401 / 76500) loss: 2.304537\n",
      "(Epoch 79 / 100) train acc: 0.094000; val_acc: 0.105000\n",
      "(Iteration 60501 / 76500) loss: 2.304452\n",
      "(Iteration 60601 / 76500) loss: 2.304479\n",
      "(Iteration 60701 / 76500) loss: 2.304526\n",
      "(Iteration 60801 / 76500) loss: 2.304435\n",
      "(Iteration 60901 / 76500) loss: 2.304457\n",
      "(Iteration 61001 / 76500) loss: 2.304496\n",
      "(Iteration 61101 / 76500) loss: 2.304438\n",
      "(Epoch 80 / 100) train acc: 0.088000; val_acc: 0.105000\n",
      "(Iteration 61201 / 76500) loss: 2.304445\n",
      "(Iteration 61301 / 76500) loss: 2.304522\n",
      "(Iteration 61401 / 76500) loss: 2.304441\n",
      "(Iteration 61501 / 76500) loss: 2.304562\n",
      "(Iteration 61601 / 76500) loss: 2.304414\n",
      "(Iteration 61701 / 76500) loss: 2.304523\n",
      "(Iteration 61801 / 76500) loss: 2.304430\n",
      "(Iteration 61901 / 76500) loss: 2.304430\n",
      "(Epoch 81 / 100) train acc: 0.100000; val_acc: 0.105000\n",
      "(Iteration 62001 / 76500) loss: 2.304395\n",
      "(Iteration 62101 / 76500) loss: 2.304511\n",
      "(Iteration 62201 / 76500) loss: 2.304434\n",
      "(Iteration 62301 / 76500) loss: 2.304530\n",
      "(Iteration 62401 / 76500) loss: 2.304437\n",
      "(Iteration 62501 / 76500) loss: 2.304469\n",
      "(Iteration 62601 / 76500) loss: 2.304410\n",
      "(Iteration 62701 / 76500) loss: 2.304503\n",
      "(Epoch 82 / 100) train acc: 0.108000; val_acc: 0.105000\n",
      "(Iteration 62801 / 76500) loss: 2.304488\n",
      "(Iteration 62901 / 76500) loss: 2.304473\n",
      "(Iteration 63001 / 76500) loss: 2.304478\n",
      "(Iteration 63101 / 76500) loss: 2.304424\n",
      "(Iteration 63201 / 76500) loss: 2.304461\n",
      "(Iteration 63301 / 76500) loss: 2.304531\n",
      "(Iteration 63401 / 76500) loss: 2.304474\n",
      "(Epoch 83 / 100) train acc: 0.106000; val_acc: 0.105000\n",
      "(Iteration 63501 / 76500) loss: 2.304510\n",
      "(Iteration 63601 / 76500) loss: 2.304463\n",
      "(Iteration 63701 / 76500) loss: 2.304437\n",
      "(Iteration 63801 / 76500) loss: 2.304447\n",
      "(Iteration 63901 / 76500) loss: 2.304477\n",
      "(Iteration 64001 / 76500) loss: 2.304477\n",
      "(Iteration 64101 / 76500) loss: 2.304506\n",
      "(Iteration 64201 / 76500) loss: 2.304494\n",
      "(Epoch 84 / 100) train acc: 0.098000; val_acc: 0.105000\n",
      "(Iteration 64301 / 76500) loss: 2.304528\n",
      "(Iteration 64401 / 76500) loss: 2.304476\n",
      "(Iteration 64501 / 76500) loss: 2.304421\n",
      "(Iteration 64601 / 76500) loss: 2.304510\n",
      "(Iteration 64701 / 76500) loss: 2.304451\n",
      "(Iteration 64801 / 76500) loss: 2.304471\n",
      "(Iteration 64901 / 76500) loss: 2.304453\n",
      "(Iteration 65001 / 76500) loss: 2.304483\n",
      "(Epoch 85 / 100) train acc: 0.111000; val_acc: 0.105000\n",
      "(Iteration 65101 / 76500) loss: 2.304453\n",
      "(Iteration 65201 / 76500) loss: 2.304458\n",
      "(Iteration 65301 / 76500) loss: 2.304464\n",
      "(Iteration 65401 / 76500) loss: 2.304456\n",
      "(Iteration 65501 / 76500) loss: 2.304430\n",
      "(Iteration 65601 / 76500) loss: 2.304450\n",
      "(Iteration 65701 / 76500) loss: 2.304483\n",
      "(Epoch 86 / 100) train acc: 0.089000; val_acc: 0.105000\n",
      "(Iteration 65801 / 76500) loss: 2.304455\n",
      "(Iteration 65901 / 76500) loss: 2.304517\n",
      "(Iteration 66001 / 76500) loss: 2.304509\n",
      "(Iteration 66101 / 76500) loss: 2.304450\n",
      "(Iteration 66201 / 76500) loss: 2.304524\n",
      "(Iteration 66301 / 76500) loss: 2.304514\n",
      "(Iteration 66401 / 76500) loss: 2.304462\n",
      "(Iteration 66501 / 76500) loss: 2.304450\n",
      "(Epoch 87 / 100) train acc: 0.093000; val_acc: 0.105000\n",
      "(Iteration 66601 / 76500) loss: 2.304474\n",
      "(Iteration 66701 / 76500) loss: 2.304473\n",
      "(Iteration 66801 / 76500) loss: 2.304479\n",
      "(Iteration 66901 / 76500) loss: 2.304470\n",
      "(Iteration 67001 / 76500) loss: 2.304434\n",
      "(Iteration 67101 / 76500) loss: 2.304514\n",
      "(Iteration 67201 / 76500) loss: 2.304417\n",
      "(Iteration 67301 / 76500) loss: 2.304515\n",
      "(Epoch 88 / 100) train acc: 0.094000; val_acc: 0.105000\n",
      "(Iteration 67401 / 76500) loss: 2.304452\n",
      "(Iteration 67501 / 76500) loss: 2.304504\n",
      "(Iteration 67601 / 76500) loss: 2.304460\n",
      "(Iteration 67701 / 76500) loss: 2.304568\n",
      "(Iteration 67801 / 76500) loss: 2.304473\n",
      "(Iteration 67901 / 76500) loss: 2.304424\n",
      "(Iteration 68001 / 76500) loss: 2.304475\n",
      "(Epoch 89 / 100) train acc: 0.080000; val_acc: 0.105000\n",
      "(Iteration 68101 / 76500) loss: 2.304368\n",
      "(Iteration 68201 / 76500) loss: 2.304519\n",
      "(Iteration 68301 / 76500) loss: 2.304410\n",
      "(Iteration 68401 / 76500) loss: 2.304448\n",
      "(Iteration 68501 / 76500) loss: 2.304496\n",
      "(Iteration 68601 / 76500) loss: 2.304514\n",
      "(Iteration 68701 / 76500) loss: 2.304383\n",
      "(Iteration 68801 / 76500) loss: 2.304470\n",
      "(Epoch 90 / 100) train acc: 0.109000; val_acc: 0.105000\n",
      "(Iteration 68901 / 76500) loss: 2.304460\n",
      "(Iteration 69001 / 76500) loss: 2.304411\n",
      "(Iteration 69101 / 76500) loss: 2.304433\n",
      "(Iteration 69201 / 76500) loss: 2.304430\n",
      "(Iteration 69301 / 76500) loss: 2.304472\n",
      "(Iteration 69401 / 76500) loss: 2.304486\n",
      "(Iteration 69501 / 76500) loss: 2.304461\n",
      "(Iteration 69601 / 76500) loss: 2.304469\n",
      "(Epoch 91 / 100) train acc: 0.122000; val_acc: 0.105000\n",
      "(Iteration 69701 / 76500) loss: 2.304509\n",
      "(Iteration 69801 / 76500) loss: 2.304503\n",
      "(Iteration 69901 / 76500) loss: 2.304509\n",
      "(Iteration 70001 / 76500) loss: 2.304481\n",
      "(Iteration 70101 / 76500) loss: 2.304438\n",
      "(Iteration 70201 / 76500) loss: 2.304512\n",
      "(Iteration 70301 / 76500) loss: 2.304482\n",
      "(Epoch 92 / 100) train acc: 0.081000; val_acc: 0.105000\n",
      "(Iteration 70401 / 76500) loss: 2.304442\n",
      "(Iteration 70501 / 76500) loss: 2.304491\n",
      "(Iteration 70601 / 76500) loss: 2.304470\n",
      "(Iteration 70701 / 76500) loss: 2.304493\n",
      "(Iteration 70801 / 76500) loss: 2.304466\n",
      "(Iteration 70901 / 76500) loss: 2.304484\n",
      "(Iteration 71001 / 76500) loss: 2.304501\n",
      "(Iteration 71101 / 76500) loss: 2.304426\n",
      "(Epoch 93 / 100) train acc: 0.085000; val_acc: 0.105000\n",
      "(Iteration 71201 / 76500) loss: 2.304460\n",
      "(Iteration 71301 / 76500) loss: 2.304410\n",
      "(Iteration 71401 / 76500) loss: 2.304441\n",
      "(Iteration 71501 / 76500) loss: 2.304524\n",
      "(Iteration 71601 / 76500) loss: 2.304433\n",
      "(Iteration 71701 / 76500) loss: 2.304445\n",
      "(Iteration 71801 / 76500) loss: 2.304525\n",
      "(Iteration 71901 / 76500) loss: 2.304510\n",
      "(Epoch 94 / 100) train acc: 0.096000; val_acc: 0.105000\n",
      "(Iteration 72001 / 76500) loss: 2.304458\n",
      "(Iteration 72101 / 76500) loss: 2.304417\n",
      "(Iteration 72201 / 76500) loss: 2.304508\n",
      "(Iteration 72301 / 76500) loss: 2.304471\n",
      "(Iteration 72401 / 76500) loss: 2.304451\n",
      "(Iteration 72501 / 76500) loss: 2.304494\n",
      "(Iteration 72601 / 76500) loss: 2.304449\n",
      "(Epoch 95 / 100) train acc: 0.096000; val_acc: 0.105000\n",
      "(Iteration 72701 / 76500) loss: 2.304456\n",
      "(Iteration 72801 / 76500) loss: 2.304471\n",
      "(Iteration 72901 / 76500) loss: 2.304463\n",
      "(Iteration 73001 / 76500) loss: 2.304532\n",
      "(Iteration 73101 / 76500) loss: 2.304481\n",
      "(Iteration 73201 / 76500) loss: 2.304392\n",
      "(Iteration 73301 / 76500) loss: 2.304474\n",
      "(Iteration 73401 / 76500) loss: 2.304467\n",
      "(Epoch 96 / 100) train acc: 0.113000; val_acc: 0.105000\n",
      "(Iteration 73501 / 76500) loss: 2.304445\n",
      "(Iteration 73601 / 76500) loss: 2.304477\n",
      "(Iteration 73701 / 76500) loss: 2.304500\n",
      "(Iteration 73801 / 76500) loss: 2.304500\n",
      "(Iteration 73901 / 76500) loss: 2.304509\n",
      "(Iteration 74001 / 76500) loss: 2.304468\n",
      "(Iteration 74101 / 76500) loss: 2.304541\n",
      "(Iteration 74201 / 76500) loss: 2.304486\n",
      "(Epoch 97 / 100) train acc: 0.092000; val_acc: 0.105000\n",
      "(Iteration 74301 / 76500) loss: 2.304427\n",
      "(Iteration 74401 / 76500) loss: 2.304451\n",
      "(Iteration 74501 / 76500) loss: 2.304426\n",
      "(Iteration 74601 / 76500) loss: 2.304514\n",
      "(Iteration 74701 / 76500) loss: 2.304496\n",
      "(Iteration 74801 / 76500) loss: 2.304462\n",
      "(Iteration 74901 / 76500) loss: 2.304450\n",
      "(Epoch 98 / 100) train acc: 0.092000; val_acc: 0.105000\n",
      "(Iteration 75001 / 76500) loss: 2.304503\n",
      "(Iteration 75101 / 76500) loss: 2.304567\n",
      "(Iteration 75201 / 76500) loss: 2.304456\n",
      "(Iteration 75301 / 76500) loss: 2.304475\n",
      "(Iteration 75401 / 76500) loss: 2.304436\n",
      "(Iteration 75501 / 76500) loss: 2.304437\n",
      "(Iteration 75601 / 76500) loss: 2.304458\n",
      "(Iteration 75701 / 76500) loss: 2.304442\n",
      "(Epoch 99 / 100) train acc: 0.109000; val_acc: 0.105000\n",
      "(Iteration 75801 / 76500) loss: 2.304398\n",
      "(Iteration 75901 / 76500) loss: 2.304404\n",
      "(Iteration 76001 / 76500) loss: 2.304466\n",
      "(Iteration 76101 / 76500) loss: 2.304519\n",
      "(Iteration 76201 / 76500) loss: 2.304502\n",
      "(Iteration 76301 / 76500) loss: 2.304452\n",
      "(Iteration 76401 / 76500) loss: 2.304432\n",
      "(Epoch 100 / 100) train acc: 0.104000; val_acc: 0.105000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.306679\n",
      "(Epoch 0 / 100) train acc: 0.123000; val_acc: 0.128000\n",
      "(Iteration 101 / 38200) loss: 2.306643\n",
      "(Iteration 201 / 38200) loss: 2.306595\n",
      "(Iteration 301 / 38200) loss: 2.306564\n",
      "(Epoch 1 / 100) train acc: 0.146000; val_acc: 0.116000\n",
      "(Iteration 401 / 38200) loss: 2.306524\n",
      "(Iteration 501 / 38200) loss: 2.306486\n",
      "(Iteration 601 / 38200) loss: 2.306445\n",
      "(Iteration 701 / 38200) loss: 2.306402\n",
      "(Epoch 2 / 100) train acc: 0.110000; val_acc: 0.112000\n",
      "(Iteration 801 / 38200) loss: 2.306366\n",
      "(Iteration 901 / 38200) loss: 2.306356\n",
      "(Iteration 1001 / 38200) loss: 2.306308\n",
      "(Iteration 1101 / 38200) loss: 2.306289\n",
      "(Epoch 3 / 100) train acc: 0.107000; val_acc: 0.097000\n",
      "(Iteration 1201 / 38200) loss: 2.306272\n",
      "(Iteration 1301 / 38200) loss: 2.306232\n",
      "(Iteration 1401 / 38200) loss: 2.306213\n",
      "(Iteration 1501 / 38200) loss: 2.306175\n",
      "(Epoch 4 / 100) train acc: 0.117000; val_acc: 0.106000\n",
      "(Iteration 1601 / 38200) loss: 2.306152\n",
      "(Iteration 1701 / 38200) loss: 2.306139\n",
      "(Iteration 1801 / 38200) loss: 2.306083\n",
      "(Iteration 1901 / 38200) loss: 2.306084\n",
      "(Epoch 5 / 100) train acc: 0.095000; val_acc: 0.103000\n",
      "(Iteration 2001 / 38200) loss: 2.306067\n",
      "(Iteration 2101 / 38200) loss: 2.306044\n",
      "(Iteration 2201 / 38200) loss: 2.306005\n",
      "(Epoch 6 / 100) train acc: 0.121000; val_acc: 0.095000\n",
      "(Iteration 2301 / 38200) loss: 2.306019\n",
      "(Iteration 2401 / 38200) loss: 2.305985\n",
      "(Iteration 2501 / 38200) loss: 2.305968\n",
      "(Iteration 2601 / 38200) loss: 2.305962\n",
      "(Epoch 7 / 100) train acc: 0.112000; val_acc: 0.096000\n",
      "(Iteration 2701 / 38200) loss: 2.305930\n",
      "(Iteration 2801 / 38200) loss: 2.305928\n",
      "(Iteration 2901 / 38200) loss: 2.305898\n",
      "(Iteration 3001 / 38200) loss: 2.305880\n",
      "(Epoch 8 / 100) train acc: 0.088000; val_acc: 0.090000\n",
      "(Iteration 3101 / 38200) loss: 2.305867\n",
      "(Iteration 3201 / 38200) loss: 2.305859\n",
      "(Iteration 3301 / 38200) loss: 2.305828\n",
      "(Iteration 3401 / 38200) loss: 2.305850\n",
      "(Epoch 9 / 100) train acc: 0.107000; val_acc: 0.093000\n",
      "(Iteration 3501 / 38200) loss: 2.305788\n",
      "(Iteration 3601 / 38200) loss: 2.305788\n",
      "(Iteration 3701 / 38200) loss: 2.305772\n",
      "(Iteration 3801 / 38200) loss: 2.305775\n",
      "(Epoch 10 / 100) train acc: 0.106000; val_acc: 0.095000\n",
      "(Iteration 3901 / 38200) loss: 2.305725\n",
      "(Iteration 4001 / 38200) loss: 2.305769\n",
      "(Iteration 4101 / 38200) loss: 2.305765\n",
      "(Iteration 4201 / 38200) loss: 2.305739\n",
      "(Epoch 11 / 100) train acc: 0.116000; val_acc: 0.090000\n",
      "(Iteration 4301 / 38200) loss: 2.305733\n",
      "(Iteration 4401 / 38200) loss: 2.305705\n",
      "(Iteration 4501 / 38200) loss: 2.305689\n",
      "(Epoch 12 / 100) train acc: 0.099000; val_acc: 0.093000\n",
      "(Iteration 4601 / 38200) loss: 2.305717\n",
      "(Iteration 4701 / 38200) loss: 2.305687\n",
      "(Iteration 4801 / 38200) loss: 2.305671\n",
      "(Iteration 4901 / 38200) loss: 2.305664\n",
      "(Epoch 13 / 100) train acc: 0.105000; val_acc: 0.092000\n",
      "(Iteration 5001 / 38200) loss: 2.305629\n",
      "(Iteration 5101 / 38200) loss: 2.305648\n",
      "(Iteration 5201 / 38200) loss: 2.305635\n",
      "(Iteration 5301 / 38200) loss: 2.305638\n",
      "(Epoch 14 / 100) train acc: 0.103000; val_acc: 0.091000\n",
      "(Iteration 5401 / 38200) loss: 2.305642\n",
      "(Iteration 5501 / 38200) loss: 2.305606\n",
      "(Iteration 5601 / 38200) loss: 2.305608\n",
      "(Iteration 5701 / 38200) loss: 2.305604\n",
      "(Epoch 15 / 100) train acc: 0.101000; val_acc: 0.090000\n",
      "(Iteration 5801 / 38200) loss: 2.305613\n",
      "(Iteration 5901 / 38200) loss: 2.305605\n",
      "(Iteration 6001 / 38200) loss: 2.305562\n",
      "(Iteration 6101 / 38200) loss: 2.305583\n",
      "(Epoch 16 / 100) train acc: 0.113000; val_acc: 0.093000\n",
      "(Iteration 6201 / 38200) loss: 2.305582\n",
      "(Iteration 6301 / 38200) loss: 2.305567\n",
      "(Iteration 6401 / 38200) loss: 2.305596\n",
      "(Epoch 17 / 100) train acc: 0.084000; val_acc: 0.090000\n",
      "(Iteration 6501 / 38200) loss: 2.305532\n",
      "(Iteration 6601 / 38200) loss: 2.305545\n",
      "(Iteration 6701 / 38200) loss: 2.305536\n",
      "(Iteration 6801 / 38200) loss: 2.305534\n",
      "(Epoch 18 / 100) train acc: 0.094000; val_acc: 0.093000\n",
      "(Iteration 6901 / 38200) loss: 2.305520\n",
      "(Iteration 7001 / 38200) loss: 2.305516\n",
      "(Iteration 7101 / 38200) loss: 2.305496\n",
      "(Iteration 7201 / 38200) loss: 2.305548\n",
      "(Epoch 19 / 100) train acc: 0.093000; val_acc: 0.091000\n",
      "(Iteration 7301 / 38200) loss: 2.305524\n",
      "(Iteration 7401 / 38200) loss: 2.305514\n",
      "(Iteration 7501 / 38200) loss: 2.305511\n",
      "(Iteration 7601 / 38200) loss: 2.305490\n",
      "(Epoch 20 / 100) train acc: 0.112000; val_acc: 0.093000\n",
      "(Iteration 7701 / 38200) loss: 2.305494\n",
      "(Iteration 7801 / 38200) loss: 2.305506\n",
      "(Iteration 7901 / 38200) loss: 2.305471\n",
      "(Iteration 8001 / 38200) loss: 2.305471\n",
      "(Epoch 21 / 100) train acc: 0.106000; val_acc: 0.091000\n",
      "(Iteration 8101 / 38200) loss: 2.305488\n",
      "(Iteration 8201 / 38200) loss: 2.305485\n",
      "(Iteration 8301 / 38200) loss: 2.305468\n",
      "(Iteration 8401 / 38200) loss: 2.305492\n",
      "(Epoch 22 / 100) train acc: 0.116000; val_acc: 0.091000\n",
      "(Iteration 8501 / 38200) loss: 2.305513\n",
      "(Iteration 8601 / 38200) loss: 2.305476\n",
      "(Iteration 8701 / 38200) loss: 2.305473\n",
      "(Epoch 23 / 100) train acc: 0.106000; val_acc: 0.093000\n",
      "(Iteration 8801 / 38200) loss: 2.305463\n",
      "(Iteration 8901 / 38200) loss: 2.305440\n",
      "(Iteration 9001 / 38200) loss: 2.305459\n",
      "(Iteration 9101 / 38200) loss: 2.305460\n",
      "(Epoch 24 / 100) train acc: 0.109000; val_acc: 0.095000\n",
      "(Iteration 9201 / 38200) loss: 2.305446\n",
      "(Iteration 9301 / 38200) loss: 2.305477\n",
      "(Iteration 9401 / 38200) loss: 2.305439\n",
      "(Iteration 9501 / 38200) loss: 2.305458\n",
      "(Epoch 25 / 100) train acc: 0.117000; val_acc: 0.097000\n",
      "(Iteration 9601 / 38200) loss: 2.305438\n",
      "(Iteration 9701 / 38200) loss: 2.305439\n",
      "(Iteration 9801 / 38200) loss: 2.305450\n",
      "(Iteration 9901 / 38200) loss: 2.305460\n",
      "(Epoch 26 / 100) train acc: 0.117000; val_acc: 0.096000\n",
      "(Iteration 10001 / 38200) loss: 2.305429\n",
      "(Iteration 10101 / 38200) loss: 2.305433\n",
      "(Iteration 10201 / 38200) loss: 2.305419\n",
      "(Iteration 10301 / 38200) loss: 2.305422\n",
      "(Epoch 27 / 100) train acc: 0.127000; val_acc: 0.101000\n",
      "(Iteration 10401 / 38200) loss: 2.305423\n",
      "(Iteration 10501 / 38200) loss: 2.305387\n",
      "(Iteration 10601 / 38200) loss: 2.305426\n",
      "(Epoch 28 / 100) train acc: 0.129000; val_acc: 0.098000\n",
      "(Iteration 10701 / 38200) loss: 2.305421\n",
      "(Iteration 10801 / 38200) loss: 2.305427\n",
      "(Iteration 10901 / 38200) loss: 2.305445\n",
      "(Iteration 11001 / 38200) loss: 2.305459\n",
      "(Epoch 29 / 100) train acc: 0.119000; val_acc: 0.099000\n",
      "(Iteration 11101 / 38200) loss: 2.305439\n",
      "(Iteration 11201 / 38200) loss: 2.305427\n",
      "(Iteration 11301 / 38200) loss: 2.305397\n",
      "(Iteration 11401 / 38200) loss: 2.305422\n",
      "(Epoch 30 / 100) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 11501 / 38200) loss: 2.305398\n",
      "(Iteration 11601 / 38200) loss: 2.305429\n",
      "(Iteration 11701 / 38200) loss: 2.305416\n",
      "(Iteration 11801 / 38200) loss: 2.305442\n",
      "(Epoch 31 / 100) train acc: 0.114000; val_acc: 0.099000\n",
      "(Iteration 11901 / 38200) loss: 2.305407\n",
      "(Iteration 12001 / 38200) loss: 2.305383\n",
      "(Iteration 12101 / 38200) loss: 2.305420\n",
      "(Iteration 12201 / 38200) loss: 2.305413\n",
      "(Epoch 32 / 100) train acc: 0.134000; val_acc: 0.098000\n",
      "(Iteration 12301 / 38200) loss: 2.305423\n",
      "(Iteration 12401 / 38200) loss: 2.305438\n",
      "(Iteration 12501 / 38200) loss: 2.305389\n",
      "(Iteration 12601 / 38200) loss: 2.305392\n",
      "(Epoch 33 / 100) train acc: 0.115000; val_acc: 0.094000\n",
      "(Iteration 12701 / 38200) loss: 2.305404\n",
      "(Iteration 12801 / 38200) loss: 2.305410\n",
      "(Iteration 12901 / 38200) loss: 2.305416\n",
      "(Epoch 34 / 100) train acc: 0.103000; val_acc: 0.094000\n",
      "(Iteration 13001 / 38200) loss: 2.305388\n",
      "(Iteration 13101 / 38200) loss: 2.305417\n",
      "(Iteration 13201 / 38200) loss: 2.305404\n",
      "(Iteration 13301 / 38200) loss: 2.305394\n",
      "(Epoch 35 / 100) train acc: 0.125000; val_acc: 0.095000\n",
      "(Iteration 13401 / 38200) loss: 2.305414\n",
      "(Iteration 13501 / 38200) loss: 2.305388\n",
      "(Iteration 13601 / 38200) loss: 2.305391\n",
      "(Iteration 13701 / 38200) loss: 2.305398\n",
      "(Epoch 36 / 100) train acc: 0.135000; val_acc: 0.095000\n",
      "(Iteration 13801 / 38200) loss: 2.305394\n",
      "(Iteration 13901 / 38200) loss: 2.305424\n",
      "(Iteration 14001 / 38200) loss: 2.305395\n",
      "(Iteration 14101 / 38200) loss: 2.305379\n",
      "(Epoch 37 / 100) train acc: 0.127000; val_acc: 0.095000\n",
      "(Iteration 14201 / 38200) loss: 2.305367\n",
      "(Iteration 14301 / 38200) loss: 2.305406\n",
      "(Iteration 14401 / 38200) loss: 2.305389\n",
      "(Iteration 14501 / 38200) loss: 2.305407\n",
      "(Epoch 38 / 100) train acc: 0.127000; val_acc: 0.095000\n",
      "(Iteration 14601 / 38200) loss: 2.305401\n",
      "(Iteration 14701 / 38200) loss: 2.305390\n",
      "(Iteration 14801 / 38200) loss: 2.305395\n",
      "(Epoch 39 / 100) train acc: 0.119000; val_acc: 0.095000\n",
      "(Iteration 14901 / 38200) loss: 2.305379\n",
      "(Iteration 15001 / 38200) loss: 2.305378\n",
      "(Iteration 15101 / 38200) loss: 2.305424\n",
      "(Iteration 15201 / 38200) loss: 2.305408\n",
      "(Epoch 40 / 100) train acc: 0.111000; val_acc: 0.093000\n",
      "(Iteration 15301 / 38200) loss: 2.305381\n",
      "(Iteration 15401 / 38200) loss: 2.305385\n",
      "(Iteration 15501 / 38200) loss: 2.305420\n",
      "(Iteration 15601 / 38200) loss: 2.305365\n",
      "(Epoch 41 / 100) train acc: 0.124000; val_acc: 0.095000\n",
      "(Iteration 15701 / 38200) loss: 2.305383\n",
      "(Iteration 15801 / 38200) loss: 2.305393\n",
      "(Iteration 15901 / 38200) loss: 2.305403\n",
      "(Iteration 16001 / 38200) loss: 2.305370\n",
      "(Epoch 42 / 100) train acc: 0.113000; val_acc: 0.096000\n",
      "(Iteration 16101 / 38200) loss: 2.305404\n",
      "(Iteration 16201 / 38200) loss: 2.305396\n",
      "(Iteration 16301 / 38200) loss: 2.305393\n",
      "(Iteration 16401 / 38200) loss: 2.305375\n",
      "(Epoch 43 / 100) train acc: 0.120000; val_acc: 0.096000\n",
      "(Iteration 16501 / 38200) loss: 2.305370\n",
      "(Iteration 16601 / 38200) loss: 2.305384\n",
      "(Iteration 16701 / 38200) loss: 2.305369\n",
      "(Iteration 16801 / 38200) loss: 2.305367\n",
      "(Epoch 44 / 100) train acc: 0.131000; val_acc: 0.098000\n",
      "(Iteration 16901 / 38200) loss: 2.305396\n",
      "(Iteration 17001 / 38200) loss: 2.305403\n",
      "(Iteration 17101 / 38200) loss: 2.305369\n",
      "(Epoch 45 / 100) train acc: 0.114000; val_acc: 0.096000\n",
      "(Iteration 17201 / 38200) loss: 2.305361\n",
      "(Iteration 17301 / 38200) loss: 2.305406\n",
      "(Iteration 17401 / 38200) loss: 2.305392\n",
      "(Iteration 17501 / 38200) loss: 2.305369\n",
      "(Epoch 46 / 100) train acc: 0.125000; val_acc: 0.096000\n",
      "(Iteration 17601 / 38200) loss: 2.305406\n",
      "(Iteration 17701 / 38200) loss: 2.305392\n",
      "(Iteration 17801 / 38200) loss: 2.305395\n",
      "(Iteration 17901 / 38200) loss: 2.305390\n",
      "(Epoch 47 / 100) train acc: 0.122000; val_acc: 0.098000\n",
      "(Iteration 18001 / 38200) loss: 2.305362\n",
      "(Iteration 18101 / 38200) loss: 2.305369\n",
      "(Iteration 18201 / 38200) loss: 2.305368\n",
      "(Iteration 18301 / 38200) loss: 2.305373\n",
      "(Epoch 48 / 100) train acc: 0.130000; val_acc: 0.098000\n",
      "(Iteration 18401 / 38200) loss: 2.305380\n",
      "(Iteration 18501 / 38200) loss: 2.305375\n",
      "(Iteration 18601 / 38200) loss: 2.305342\n",
      "(Iteration 18701 / 38200) loss: 2.305384\n",
      "(Epoch 49 / 100) train acc: 0.124000; val_acc: 0.098000\n",
      "(Iteration 18801 / 38200) loss: 2.305360\n",
      "(Iteration 18901 / 38200) loss: 2.305364\n",
      "(Iteration 19001 / 38200) loss: 2.305386\n",
      "(Epoch 50 / 100) train acc: 0.117000; val_acc: 0.098000\n",
      "(Iteration 19101 / 38200) loss: 2.305374\n",
      "(Iteration 19201 / 38200) loss: 2.305398\n",
      "(Iteration 19301 / 38200) loss: 2.305342\n",
      "(Iteration 19401 / 38200) loss: 2.305389\n",
      "(Epoch 51 / 100) train acc: 0.112000; val_acc: 0.099000\n",
      "(Iteration 19501 / 38200) loss: 2.305378\n",
      "(Iteration 19601 / 38200) loss: 2.305366\n",
      "(Iteration 19701 / 38200) loss: 2.305372\n",
      "(Iteration 19801 / 38200) loss: 2.305362\n",
      "(Epoch 52 / 100) train acc: 0.128000; val_acc: 0.099000\n",
      "(Iteration 19901 / 38200) loss: 2.305362\n",
      "(Iteration 20001 / 38200) loss: 2.305349\n",
      "(Iteration 20101 / 38200) loss: 2.305409\n",
      "(Iteration 20201 / 38200) loss: 2.305391\n",
      "(Epoch 53 / 100) train acc: 0.112000; val_acc: 0.099000\n",
      "(Iteration 20301 / 38200) loss: 2.305375\n",
      "(Iteration 20401 / 38200) loss: 2.305373\n",
      "(Iteration 20501 / 38200) loss: 2.305360\n",
      "(Iteration 20601 / 38200) loss: 2.305380\n",
      "(Epoch 54 / 100) train acc: 0.121000; val_acc: 0.098000\n",
      "(Iteration 20701 / 38200) loss: 2.305380\n",
      "(Iteration 20801 / 38200) loss: 2.305376\n",
      "(Iteration 20901 / 38200) loss: 2.305376\n",
      "(Iteration 21001 / 38200) loss: 2.305376\n",
      "(Epoch 55 / 100) train acc: 0.130000; val_acc: 0.098000\n",
      "(Iteration 21101 / 38200) loss: 2.305390\n",
      "(Iteration 21201 / 38200) loss: 2.305425\n",
      "(Iteration 21301 / 38200) loss: 2.305396\n",
      "(Epoch 56 / 100) train acc: 0.111000; val_acc: 0.098000\n",
      "(Iteration 21401 / 38200) loss: 2.305358\n",
      "(Iteration 21501 / 38200) loss: 2.305380\n",
      "(Iteration 21601 / 38200) loss: 2.305352\n",
      "(Iteration 21701 / 38200) loss: 2.305379\n",
      "(Epoch 57 / 100) train acc: 0.103000; val_acc: 0.098000\n",
      "(Iteration 21801 / 38200) loss: 2.305378\n",
      "(Iteration 21901 / 38200) loss: 2.305381\n",
      "(Iteration 22001 / 38200) loss: 2.305381\n",
      "(Iteration 22101 / 38200) loss: 2.305367\n",
      "(Epoch 58 / 100) train acc: 0.117000; val_acc: 0.098000\n",
      "(Iteration 22201 / 38200) loss: 2.305352\n",
      "(Iteration 22301 / 38200) loss: 2.305348\n",
      "(Iteration 22401 / 38200) loss: 2.305397\n",
      "(Iteration 22501 / 38200) loss: 2.305389\n",
      "(Epoch 59 / 100) train acc: 0.120000; val_acc: 0.099000\n",
      "(Iteration 22601 / 38200) loss: 2.305399\n",
      "(Iteration 22701 / 38200) loss: 2.305352\n",
      "(Iteration 22801 / 38200) loss: 2.305362\n",
      "(Iteration 22901 / 38200) loss: 2.305385\n",
      "(Epoch 60 / 100) train acc: 0.103000; val_acc: 0.098000\n",
      "(Iteration 23001 / 38200) loss: 2.305381\n",
      "(Iteration 23101 / 38200) loss: 2.305357\n",
      "(Iteration 23201 / 38200) loss: 2.305357\n",
      "(Iteration 23301 / 38200) loss: 2.305386\n",
      "(Epoch 61 / 100) train acc: 0.124000; val_acc: 0.098000\n",
      "(Iteration 23401 / 38200) loss: 2.305366\n",
      "(Iteration 23501 / 38200) loss: 2.305359\n",
      "(Iteration 23601 / 38200) loss: 2.305375\n",
      "(Epoch 62 / 100) train acc: 0.120000; val_acc: 0.098000\n",
      "(Iteration 23701 / 38200) loss: 2.305380\n",
      "(Iteration 23801 / 38200) loss: 2.305352\n",
      "(Iteration 23901 / 38200) loss: 2.305360\n",
      "(Iteration 24001 / 38200) loss: 2.305391\n",
      "(Epoch 63 / 100) train acc: 0.125000; val_acc: 0.099000\n",
      "(Iteration 24101 / 38200) loss: 2.305365\n",
      "(Iteration 24201 / 38200) loss: 2.305386\n",
      "(Iteration 24301 / 38200) loss: 2.305397\n",
      "(Iteration 24401 / 38200) loss: 2.305361\n",
      "(Epoch 64 / 100) train acc: 0.130000; val_acc: 0.098000\n",
      "(Iteration 24501 / 38200) loss: 2.305377\n",
      "(Iteration 24601 / 38200) loss: 2.305366\n",
      "(Iteration 24701 / 38200) loss: 2.305365\n",
      "(Iteration 24801 / 38200) loss: 2.305372\n",
      "(Epoch 65 / 100) train acc: 0.119000; val_acc: 0.098000\n",
      "(Iteration 24901 / 38200) loss: 2.305381\n",
      "(Iteration 25001 / 38200) loss: 2.305357\n",
      "(Iteration 25101 / 38200) loss: 2.305382\n",
      "(Iteration 25201 / 38200) loss: 2.305381\n",
      "(Epoch 66 / 100) train acc: 0.110000; val_acc: 0.098000\n",
      "(Iteration 25301 / 38200) loss: 2.305365\n",
      "(Iteration 25401 / 38200) loss: 2.305380\n",
      "(Iteration 25501 / 38200) loss: 2.305378\n",
      "(Epoch 67 / 100) train acc: 0.120000; val_acc: 0.098000\n",
      "(Iteration 25601 / 38200) loss: 2.305384\n",
      "(Iteration 25701 / 38200) loss: 2.305382\n",
      "(Iteration 25801 / 38200) loss: 2.305367\n",
      "(Iteration 25901 / 38200) loss: 2.305399\n",
      "(Epoch 68 / 100) train acc: 0.121000; val_acc: 0.099000\n",
      "(Iteration 26001 / 38200) loss: 2.305377\n",
      "(Iteration 26101 / 38200) loss: 2.305381\n",
      "(Iteration 26201 / 38200) loss: 2.305382\n",
      "(Iteration 26301 / 38200) loss: 2.305350\n",
      "(Epoch 69 / 100) train acc: 0.108000; val_acc: 0.099000\n",
      "(Iteration 26401 / 38200) loss: 2.305375\n",
      "(Iteration 26501 / 38200) loss: 2.305366\n",
      "(Iteration 26601 / 38200) loss: 2.305372\n",
      "(Iteration 26701 / 38200) loss: 2.305381\n",
      "(Epoch 70 / 100) train acc: 0.135000; val_acc: 0.099000\n",
      "(Iteration 26801 / 38200) loss: 2.305385\n",
      "(Iteration 26901 / 38200) loss: 2.305372\n",
      "(Iteration 27001 / 38200) loss: 2.305378\n",
      "(Iteration 27101 / 38200) loss: 2.305370\n",
      "(Epoch 71 / 100) train acc: 0.118000; val_acc: 0.099000\n",
      "(Iteration 27201 / 38200) loss: 2.305360\n",
      "(Iteration 27301 / 38200) loss: 2.305351\n",
      "(Iteration 27401 / 38200) loss: 2.305378\n",
      "(Iteration 27501 / 38200) loss: 2.305370\n",
      "(Epoch 72 / 100) train acc: 0.126000; val_acc: 0.099000\n",
      "(Iteration 27601 / 38200) loss: 2.305371\n",
      "(Iteration 27701 / 38200) loss: 2.305346\n",
      "(Iteration 27801 / 38200) loss: 2.305375\n",
      "(Epoch 73 / 100) train acc: 0.111000; val_acc: 0.099000\n",
      "(Iteration 27901 / 38200) loss: 2.305354\n",
      "(Iteration 28001 / 38200) loss: 2.305372\n",
      "(Iteration 28101 / 38200) loss: 2.305371\n",
      "(Iteration 28201 / 38200) loss: 2.305373\n",
      "(Epoch 74 / 100) train acc: 0.108000; val_acc: 0.099000\n",
      "(Iteration 28301 / 38200) loss: 2.305377\n",
      "(Iteration 28401 / 38200) loss: 2.305341\n",
      "(Iteration 28501 / 38200) loss: 2.305391\n",
      "(Iteration 28601 / 38200) loss: 2.305381\n",
      "(Epoch 75 / 100) train acc: 0.117000; val_acc: 0.099000\n",
      "(Iteration 28701 / 38200) loss: 2.305383\n",
      "(Iteration 28801 / 38200) loss: 2.305362\n",
      "(Iteration 28901 / 38200) loss: 2.305397\n",
      "(Iteration 29001 / 38200) loss: 2.305367\n",
      "(Epoch 76 / 100) train acc: 0.115000; val_acc: 0.099000\n",
      "(Iteration 29101 / 38200) loss: 2.305354\n",
      "(Iteration 29201 / 38200) loss: 2.305364\n",
      "(Iteration 29301 / 38200) loss: 2.305348\n",
      "(Iteration 29401 / 38200) loss: 2.305364\n",
      "(Epoch 77 / 100) train acc: 0.116000; val_acc: 0.099000\n",
      "(Iteration 29501 / 38200) loss: 2.305388\n",
      "(Iteration 29601 / 38200) loss: 2.305390\n",
      "(Iteration 29701 / 38200) loss: 2.305403\n",
      "(Epoch 78 / 100) train acc: 0.134000; val_acc: 0.099000\n",
      "(Iteration 29801 / 38200) loss: 2.305359\n",
      "(Iteration 29901 / 38200) loss: 2.305388\n",
      "(Iteration 30001 / 38200) loss: 2.305372\n",
      "(Iteration 30101 / 38200) loss: 2.305351\n",
      "(Epoch 79 / 100) train acc: 0.132000; val_acc: 0.099000\n",
      "(Iteration 30201 / 38200) loss: 2.305371\n",
      "(Iteration 30301 / 38200) loss: 2.305359\n",
      "(Iteration 30401 / 38200) loss: 2.305381\n",
      "(Iteration 30501 / 38200) loss: 2.305382\n",
      "(Epoch 80 / 100) train acc: 0.116000; val_acc: 0.099000\n",
      "(Iteration 30601 / 38200) loss: 2.305386\n",
      "(Iteration 30701 / 38200) loss: 2.305366\n",
      "(Iteration 30801 / 38200) loss: 2.305363\n",
      "(Iteration 30901 / 38200) loss: 2.305367\n",
      "(Epoch 81 / 100) train acc: 0.118000; val_acc: 0.099000\n",
      "(Iteration 31001 / 38200) loss: 2.305369\n",
      "(Iteration 31101 / 38200) loss: 2.305386\n",
      "(Iteration 31201 / 38200) loss: 2.305408\n",
      "(Iteration 31301 / 38200) loss: 2.305400\n",
      "(Epoch 82 / 100) train acc: 0.107000; val_acc: 0.099000\n",
      "(Iteration 31401 / 38200) loss: 2.305355\n",
      "(Iteration 31501 / 38200) loss: 2.305368\n",
      "(Iteration 31601 / 38200) loss: 2.305370\n",
      "(Iteration 31701 / 38200) loss: 2.305379\n",
      "(Epoch 83 / 100) train acc: 0.137000; val_acc: 0.099000\n",
      "(Iteration 31801 / 38200) loss: 2.305369\n",
      "(Iteration 31901 / 38200) loss: 2.305387\n",
      "(Iteration 32001 / 38200) loss: 2.305396\n",
      "(Epoch 84 / 100) train acc: 0.143000; val_acc: 0.099000\n",
      "(Iteration 32101 / 38200) loss: 2.305379\n",
      "(Iteration 32201 / 38200) loss: 2.305362\n",
      "(Iteration 32301 / 38200) loss: 2.305389\n",
      "(Iteration 32401 / 38200) loss: 2.305380\n",
      "(Epoch 85 / 100) train acc: 0.120000; val_acc: 0.099000\n",
      "(Iteration 32501 / 38200) loss: 2.305371\n",
      "(Iteration 32601 / 38200) loss: 2.305375\n",
      "(Iteration 32701 / 38200) loss: 2.305381\n",
      "(Iteration 32801 / 38200) loss: 2.305376\n",
      "(Epoch 86 / 100) train acc: 0.120000; val_acc: 0.099000\n",
      "(Iteration 32901 / 38200) loss: 2.305380\n",
      "(Iteration 33001 / 38200) loss: 2.305369\n",
      "(Iteration 33101 / 38200) loss: 2.305400\n",
      "(Iteration 33201 / 38200) loss: 2.305370\n",
      "(Epoch 87 / 100) train acc: 0.099000; val_acc: 0.099000\n",
      "(Iteration 33301 / 38200) loss: 2.305369\n",
      "(Iteration 33401 / 38200) loss: 2.305357\n",
      "(Iteration 33501 / 38200) loss: 2.305389\n",
      "(Iteration 33601 / 38200) loss: 2.305402\n",
      "(Epoch 88 / 100) train acc: 0.107000; val_acc: 0.099000\n",
      "(Iteration 33701 / 38200) loss: 2.305393\n",
      "(Iteration 33801 / 38200) loss: 2.305370\n",
      "(Iteration 33901 / 38200) loss: 2.305356\n",
      "(Epoch 89 / 100) train acc: 0.100000; val_acc: 0.099000\n",
      "(Iteration 34001 / 38200) loss: 2.305377\n",
      "(Iteration 34101 / 38200) loss: 2.305380\n",
      "(Iteration 34201 / 38200) loss: 2.305401\n",
      "(Iteration 34301 / 38200) loss: 2.305368\n",
      "(Epoch 90 / 100) train acc: 0.114000; val_acc: 0.099000\n",
      "(Iteration 34401 / 38200) loss: 2.305354\n",
      "(Iteration 34501 / 38200) loss: 2.305373\n",
      "(Iteration 34601 / 38200) loss: 2.305380\n",
      "(Iteration 34701 / 38200) loss: 2.305361\n",
      "(Epoch 91 / 100) train acc: 0.100000; val_acc: 0.099000\n",
      "(Iteration 34801 / 38200) loss: 2.305363\n",
      "(Iteration 34901 / 38200) loss: 2.305378\n",
      "(Iteration 35001 / 38200) loss: 2.305389\n",
      "(Iteration 35101 / 38200) loss: 2.305361\n",
      "(Epoch 92 / 100) train acc: 0.128000; val_acc: 0.099000\n",
      "(Iteration 35201 / 38200) loss: 2.305369\n",
      "(Iteration 35301 / 38200) loss: 2.305377\n",
      "(Iteration 35401 / 38200) loss: 2.305359\n",
      "(Iteration 35501 / 38200) loss: 2.305390\n",
      "(Epoch 93 / 100) train acc: 0.120000; val_acc: 0.099000\n",
      "(Iteration 35601 / 38200) loss: 2.305362\n",
      "(Iteration 35701 / 38200) loss: 2.305399\n",
      "(Iteration 35801 / 38200) loss: 2.305385\n",
      "(Iteration 35901 / 38200) loss: 2.305361\n",
      "(Epoch 94 / 100) train acc: 0.123000; val_acc: 0.099000\n",
      "(Iteration 36001 / 38200) loss: 2.305362\n",
      "(Iteration 36101 / 38200) loss: 2.305375\n",
      "(Iteration 36201 / 38200) loss: 2.305384\n",
      "(Epoch 95 / 100) train acc: 0.101000; val_acc: 0.099000\n",
      "(Iteration 36301 / 38200) loss: 2.305363\n",
      "(Iteration 36401 / 38200) loss: 2.305405\n",
      "(Iteration 36501 / 38200) loss: 2.305359\n",
      "(Iteration 36601 / 38200) loss: 2.305369\n",
      "(Epoch 96 / 100) train acc: 0.121000; val_acc: 0.099000\n",
      "(Iteration 36701 / 38200) loss: 2.305378\n",
      "(Iteration 36801 / 38200) loss: 2.305357\n",
      "(Iteration 36901 / 38200) loss: 2.305365\n",
      "(Iteration 37001 / 38200) loss: 2.305347\n",
      "(Epoch 97 / 100) train acc: 0.104000; val_acc: 0.099000\n",
      "(Iteration 37101 / 38200) loss: 2.305351\n",
      "(Iteration 37201 / 38200) loss: 2.305378\n",
      "(Iteration 37301 / 38200) loss: 2.305378\n",
      "(Iteration 37401 / 38200) loss: 2.305353\n",
      "(Epoch 98 / 100) train acc: 0.108000; val_acc: 0.099000\n",
      "(Iteration 37501 / 38200) loss: 2.305386\n",
      "(Iteration 37601 / 38200) loss: 2.305364\n",
      "(Iteration 37701 / 38200) loss: 2.305371\n",
      "(Iteration 37801 / 38200) loss: 2.305368\n",
      "(Epoch 99 / 100) train acc: 0.132000; val_acc: 0.099000\n",
      "(Iteration 37901 / 38200) loss: 2.305362\n",
      "(Iteration 38001 / 38200) loss: 2.305353\n",
      "(Iteration 38101 / 38200) loss: 2.305381\n",
      "(Epoch 100 / 100) train acc: 0.120000; val_acc: 0.099000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.306706\n",
      "(Epoch 0 / 100) train acc: 0.129000; val_acc: 0.111000\n",
      "(Iteration 101 / 76500) loss: 2.306676\n",
      "(Iteration 201 / 76500) loss: 2.306642\n",
      "(Iteration 301 / 76500) loss: 2.306597\n",
      "(Iteration 401 / 76500) loss: 2.306554\n",
      "(Iteration 501 / 76500) loss: 2.306505\n",
      "(Iteration 601 / 76500) loss: 2.306457\n",
      "(Iteration 701 / 76500) loss: 2.306424\n",
      "(Epoch 1 / 100) train acc: 0.103000; val_acc: 0.101000\n",
      "(Iteration 801 / 76500) loss: 2.306391\n",
      "(Iteration 901 / 76500) loss: 2.306334\n",
      "(Iteration 1001 / 76500) loss: 2.306318\n",
      "(Iteration 1101 / 76500) loss: 2.306271\n",
      "(Iteration 1201 / 76500) loss: 2.306255\n",
      "(Iteration 1301 / 76500) loss: 2.306225\n",
      "(Iteration 1401 / 76500) loss: 2.306170\n",
      "(Iteration 1501 / 76500) loss: 2.306147\n",
      "(Epoch 2 / 100) train acc: 0.113000; val_acc: 0.080000\n",
      "(Iteration 1601 / 76500) loss: 2.306107\n",
      "(Iteration 1701 / 76500) loss: 2.306094\n",
      "(Iteration 1801 / 76500) loss: 2.306037\n",
      "(Iteration 1901 / 76500) loss: 2.306053\n",
      "(Iteration 2001 / 76500) loss: 2.306021\n",
      "(Iteration 2101 / 76500) loss: 2.306005\n",
      "(Iteration 2201 / 76500) loss: 2.305968\n",
      "(Epoch 3 / 100) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 2301 / 76500) loss: 2.305917\n",
      "(Iteration 2401 / 76500) loss: 2.305909\n",
      "(Iteration 2501 / 76500) loss: 2.305821\n",
      "(Iteration 2601 / 76500) loss: 2.305856\n",
      "(Iteration 2701 / 76500) loss: 2.305808\n",
      "(Iteration 2801 / 76500) loss: 2.305717\n",
      "(Iteration 2901 / 76500) loss: 2.305686\n",
      "(Iteration 3001 / 76500) loss: 2.305774\n",
      "(Epoch 4 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 3101 / 76500) loss: 2.305625\n",
      "(Iteration 3201 / 76500) loss: 2.305666\n",
      "(Iteration 3301 / 76500) loss: 2.305623\n",
      "(Iteration 3401 / 76500) loss: 2.305615\n",
      "(Iteration 3501 / 76500) loss: 2.305513\n",
      "(Iteration 3601 / 76500) loss: 2.305558\n",
      "(Iteration 3701 / 76500) loss: 2.305487\n",
      "(Iteration 3801 / 76500) loss: 2.305547\n",
      "(Epoch 5 / 100) train acc: 0.117000; val_acc: 0.078000\n",
      "(Iteration 3901 / 76500) loss: 2.305480\n",
      "(Iteration 4001 / 76500) loss: 2.305466\n",
      "(Iteration 4101 / 76500) loss: 2.305475\n",
      "(Iteration 4201 / 76500) loss: 2.305437\n",
      "(Iteration 4301 / 76500) loss: 2.305424\n",
      "(Iteration 4401 / 76500) loss: 2.305352\n",
      "(Iteration 4501 / 76500) loss: 2.305373\n",
      "(Epoch 6 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 4601 / 76500) loss: 2.305359\n",
      "(Iteration 4701 / 76500) loss: 2.305324\n",
      "(Iteration 4801 / 76500) loss: 2.305208\n",
      "(Iteration 4901 / 76500) loss: 2.305261\n",
      "(Iteration 5001 / 76500) loss: 2.305203\n",
      "(Iteration 5101 / 76500) loss: 2.305279\n",
      "(Iteration 5201 / 76500) loss: 2.305232\n",
      "(Iteration 5301 / 76500) loss: 2.305169\n",
      "(Epoch 7 / 100) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 5401 / 76500) loss: 2.305217\n",
      "(Iteration 5501 / 76500) loss: 2.305211\n",
      "(Iteration 5601 / 76500) loss: 2.305109\n",
      "(Iteration 5701 / 76500) loss: 2.305154\n",
      "(Iteration 5801 / 76500) loss: 2.305067\n",
      "(Iteration 5901 / 76500) loss: 2.305187\n",
      "(Iteration 6001 / 76500) loss: 2.305108\n",
      "(Iteration 6101 / 76500) loss: 2.305032\n",
      "(Epoch 8 / 100) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 6201 / 76500) loss: 2.305025\n",
      "(Iteration 6301 / 76500) loss: 2.305111\n",
      "(Iteration 6401 / 76500) loss: 2.304933\n",
      "(Iteration 6501 / 76500) loss: 2.304958\n",
      "(Iteration 6601 / 76500) loss: 2.304938\n",
      "(Iteration 6701 / 76500) loss: 2.305026\n",
      "(Iteration 6801 / 76500) loss: 2.304943\n",
      "(Epoch 9 / 100) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 6901 / 76500) loss: 2.304948\n",
      "(Iteration 7001 / 76500) loss: 2.304877\n",
      "(Iteration 7101 / 76500) loss: 2.304869\n",
      "(Iteration 7201 / 76500) loss: 2.304749\n",
      "(Iteration 7301 / 76500) loss: 2.304857\n",
      "(Iteration 7401 / 76500) loss: 2.304775\n",
      "(Iteration 7501 / 76500) loss: 2.304836\n",
      "(Iteration 7601 / 76500) loss: 2.304968\n",
      "(Epoch 10 / 100) train acc: 0.085000; val_acc: 0.079000\n",
      "(Iteration 7701 / 76500) loss: 2.304831\n",
      "(Iteration 7801 / 76500) loss: 2.304759\n",
      "(Iteration 7901 / 76500) loss: 2.304937\n",
      "(Iteration 8001 / 76500) loss: 2.304828\n",
      "(Iteration 8101 / 76500) loss: 2.304712\n",
      "(Iteration 8201 / 76500) loss: 2.304721\n",
      "(Iteration 8301 / 76500) loss: 2.304710\n",
      "(Iteration 8401 / 76500) loss: 2.304857\n",
      "(Epoch 11 / 100) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 8501 / 76500) loss: 2.304736\n",
      "(Iteration 8601 / 76500) loss: 2.304691\n",
      "(Iteration 8701 / 76500) loss: 2.304725\n",
      "(Iteration 8801 / 76500) loss: 2.304639\n",
      "(Iteration 8901 / 76500) loss: 2.304635\n",
      "(Iteration 9001 / 76500) loss: 2.304573\n",
      "(Iteration 9101 / 76500) loss: 2.304642\n",
      "(Epoch 12 / 100) train acc: 0.083000; val_acc: 0.079000\n",
      "(Iteration 9201 / 76500) loss: 2.304591\n",
      "(Iteration 9301 / 76500) loss: 2.304576\n",
      "(Iteration 9401 / 76500) loss: 2.304599\n",
      "(Iteration 9501 / 76500) loss: 2.304468\n",
      "(Iteration 9601 / 76500) loss: 2.304612\n",
      "(Iteration 9701 / 76500) loss: 2.304548\n",
      "(Iteration 9801 / 76500) loss: 2.304553\n",
      "(Iteration 9901 / 76500) loss: 2.304612\n",
      "(Epoch 13 / 100) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 10001 / 76500) loss: 2.304588\n",
      "(Iteration 10101 / 76500) loss: 2.304493\n",
      "(Iteration 10201 / 76500) loss: 2.304498\n",
      "(Iteration 10301 / 76500) loss: 2.304508\n",
      "(Iteration 10401 / 76500) loss: 2.304513\n",
      "(Iteration 10501 / 76500) loss: 2.304521\n",
      "(Iteration 10601 / 76500) loss: 2.304430\n",
      "(Iteration 10701 / 76500) loss: 2.304381\n",
      "(Epoch 14 / 100) train acc: 0.121000; val_acc: 0.079000\n",
      "(Iteration 10801 / 76500) loss: 2.304434\n",
      "(Iteration 10901 / 76500) loss: 2.304513\n",
      "(Iteration 11001 / 76500) loss: 2.304394\n",
      "(Iteration 11101 / 76500) loss: 2.304366\n",
      "(Iteration 11201 / 76500) loss: 2.304360\n",
      "(Iteration 11301 / 76500) loss: 2.304277\n",
      "(Iteration 11401 / 76500) loss: 2.304458\n",
      "(Epoch 15 / 100) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 11501 / 76500) loss: 2.304376\n",
      "(Iteration 11601 / 76500) loss: 2.304385\n",
      "(Iteration 11701 / 76500) loss: 2.304565\n",
      "(Iteration 11801 / 76500) loss: 2.304270\n",
      "(Iteration 11901 / 76500) loss: 2.304298\n",
      "(Iteration 12001 / 76500) loss: 2.304362\n",
      "(Iteration 12101 / 76500) loss: 2.304325\n",
      "(Iteration 12201 / 76500) loss: 2.304357\n",
      "(Epoch 16 / 100) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 12301 / 76500) loss: 2.304311\n",
      "(Iteration 12401 / 76500) loss: 2.304378\n",
      "(Iteration 12501 / 76500) loss: 2.304331\n",
      "(Iteration 12601 / 76500) loss: 2.304171\n",
      "(Iteration 12701 / 76500) loss: 2.304340\n",
      "(Iteration 12801 / 76500) loss: 2.304254\n",
      "(Iteration 12901 / 76500) loss: 2.304280\n",
      "(Iteration 13001 / 76500) loss: 2.304227\n",
      "(Epoch 17 / 100) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 13101 / 76500) loss: 2.304224\n",
      "(Iteration 13201 / 76500) loss: 2.304272\n",
      "(Iteration 13301 / 76500) loss: 2.304235\n",
      "(Iteration 13401 / 76500) loss: 2.304215\n",
      "(Iteration 13501 / 76500) loss: 2.304346\n",
      "(Iteration 13601 / 76500) loss: 2.304189\n",
      "(Iteration 13701 / 76500) loss: 2.304175\n",
      "(Epoch 18 / 100) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 13801 / 76500) loss: 2.304152\n",
      "(Iteration 13901 / 76500) loss: 2.304283\n",
      "(Iteration 14001 / 76500) loss: 2.304177\n",
      "(Iteration 14101 / 76500) loss: 2.304187\n",
      "(Iteration 14201 / 76500) loss: 2.304196\n",
      "(Iteration 14301 / 76500) loss: 2.304198\n",
      "(Iteration 14401 / 76500) loss: 2.304293\n",
      "(Iteration 14501 / 76500) loss: 2.304164\n",
      "(Epoch 19 / 100) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 14601 / 76500) loss: 2.304123\n",
      "(Iteration 14701 / 76500) loss: 2.304005\n",
      "(Iteration 14801 / 76500) loss: 2.304124\n",
      "(Iteration 14901 / 76500) loss: 2.304276\n",
      "(Iteration 15001 / 76500) loss: 2.304132\n",
      "(Iteration 15101 / 76500) loss: 2.304287\n",
      "(Iteration 15201 / 76500) loss: 2.304102\n",
      "(Epoch 20 / 100) train acc: 0.087000; val_acc: 0.079000\n",
      "(Iteration 15301 / 76500) loss: 2.304052\n",
      "(Iteration 15401 / 76500) loss: 2.304114\n",
      "(Iteration 15501 / 76500) loss: 2.304174\n",
      "(Iteration 15601 / 76500) loss: 2.304077\n",
      "(Iteration 15701 / 76500) loss: 2.304150\n",
      "(Iteration 15801 / 76500) loss: 2.304043\n",
      "(Iteration 15901 / 76500) loss: 2.304111\n",
      "(Iteration 16001 / 76500) loss: 2.304082\n",
      "(Epoch 21 / 100) train acc: 0.091000; val_acc: 0.079000\n",
      "(Iteration 16101 / 76500) loss: 2.304022\n",
      "(Iteration 16201 / 76500) loss: 2.304123\n",
      "(Iteration 16301 / 76500) loss: 2.304008\n",
      "(Iteration 16401 / 76500) loss: 2.304028\n",
      "(Iteration 16501 / 76500) loss: 2.303959\n",
      "(Iteration 16601 / 76500) loss: 2.304153\n",
      "(Iteration 16701 / 76500) loss: 2.304073\n",
      "(Iteration 16801 / 76500) loss: 2.303961\n",
      "(Epoch 22 / 100) train acc: 0.115000; val_acc: 0.079000\n",
      "(Iteration 16901 / 76500) loss: 2.304072\n",
      "(Iteration 17001 / 76500) loss: 2.304233\n",
      "(Iteration 17101 / 76500) loss: 2.303915\n",
      "(Iteration 17201 / 76500) loss: 2.304065\n",
      "(Iteration 17301 / 76500) loss: 2.303949\n",
      "(Iteration 17401 / 76500) loss: 2.304123\n",
      "(Iteration 17501 / 76500) loss: 2.304022\n",
      "(Epoch 23 / 100) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 17601 / 76500) loss: 2.304058\n",
      "(Iteration 17701 / 76500) loss: 2.303939\n",
      "(Iteration 17801 / 76500) loss: 2.303912\n",
      "(Iteration 17901 / 76500) loss: 2.303994\n",
      "(Iteration 18001 / 76500) loss: 2.304050\n",
      "(Iteration 18101 / 76500) loss: 2.303909\n",
      "(Iteration 18201 / 76500) loss: 2.303781\n",
      "(Iteration 18301 / 76500) loss: 2.303858\n",
      "(Epoch 24 / 100) train acc: 0.085000; val_acc: 0.079000\n",
      "(Iteration 18401 / 76500) loss: 2.304031\n",
      "(Iteration 18501 / 76500) loss: 2.303949\n",
      "(Iteration 18601 / 76500) loss: 2.303897\n",
      "(Iteration 18701 / 76500) loss: 2.303914\n",
      "(Iteration 18801 / 76500) loss: 2.303949\n",
      "(Iteration 18901 / 76500) loss: 2.304072\n",
      "(Iteration 19001 / 76500) loss: 2.304017\n",
      "(Iteration 19101 / 76500) loss: 2.303996\n",
      "(Epoch 25 / 100) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 19201 / 76500) loss: 2.303959\n",
      "(Iteration 19301 / 76500) loss: 2.303966\n",
      "(Iteration 19401 / 76500) loss: 2.303873\n",
      "(Iteration 19501 / 76500) loss: 2.303953\n",
      "(Iteration 19601 / 76500) loss: 2.303898\n",
      "(Iteration 19701 / 76500) loss: 2.303998\n",
      "(Iteration 19801 / 76500) loss: 2.303728\n",
      "(Epoch 26 / 100) train acc: 0.120000; val_acc: 0.079000\n",
      "(Iteration 19901 / 76500) loss: 2.303847\n",
      "(Iteration 20001 / 76500) loss: 2.304034\n",
      "(Iteration 20101 / 76500) loss: 2.303870\n",
      "(Iteration 20201 / 76500) loss: 2.303832\n",
      "(Iteration 20301 / 76500) loss: 2.303877\n",
      "(Iteration 20401 / 76500) loss: 2.303903\n",
      "(Iteration 20501 / 76500) loss: 2.303869\n",
      "(Iteration 20601 / 76500) loss: 2.303870\n",
      "(Epoch 27 / 100) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 20701 / 76500) loss: 2.303893\n",
      "(Iteration 20801 / 76500) loss: 2.303858\n",
      "(Iteration 20901 / 76500) loss: 2.303772\n",
      "(Iteration 21001 / 76500) loss: 2.303937\n",
      "(Iteration 21101 / 76500) loss: 2.303903\n",
      "(Iteration 21201 / 76500) loss: 2.303932\n",
      "(Iteration 21301 / 76500) loss: 2.304050\n",
      "(Iteration 21401 / 76500) loss: 2.303835\n",
      "(Epoch 28 / 100) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 21501 / 76500) loss: 2.303865\n",
      "(Iteration 21601 / 76500) loss: 2.303824\n",
      "(Iteration 21701 / 76500) loss: 2.303825\n",
      "(Iteration 21801 / 76500) loss: 2.303788\n",
      "(Iteration 21901 / 76500) loss: 2.303784\n",
      "(Iteration 22001 / 76500) loss: 2.303840\n",
      "(Iteration 22101 / 76500) loss: 2.303808\n",
      "(Epoch 29 / 100) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 22201 / 76500) loss: 2.303726\n",
      "(Iteration 22301 / 76500) loss: 2.303800\n",
      "(Iteration 22401 / 76500) loss: 2.303780\n",
      "(Iteration 22501 / 76500) loss: 2.303745\n",
      "(Iteration 22601 / 76500) loss: 2.303800\n",
      "(Iteration 22701 / 76500) loss: 2.303903\n",
      "(Iteration 22801 / 76500) loss: 2.303861\n",
      "(Iteration 22901 / 76500) loss: 2.303861\n",
      "(Epoch 30 / 100) train acc: 0.120000; val_acc: 0.079000\n",
      "(Iteration 23001 / 76500) loss: 2.303871\n",
      "(Iteration 23101 / 76500) loss: 2.303911\n",
      "(Iteration 23201 / 76500) loss: 2.303746\n",
      "(Iteration 23301 / 76500) loss: 2.303751\n",
      "(Iteration 23401 / 76500) loss: 2.303838\n",
      "(Iteration 23501 / 76500) loss: 2.303846\n",
      "(Iteration 23601 / 76500) loss: 2.303708\n",
      "(Iteration 23701 / 76500) loss: 2.303631\n",
      "(Epoch 31 / 100) train acc: 0.120000; val_acc: 0.079000\n",
      "(Iteration 23801 / 76500) loss: 2.303751\n",
      "(Iteration 23901 / 76500) loss: 2.303811\n",
      "(Iteration 24001 / 76500) loss: 2.303726\n",
      "(Iteration 24101 / 76500) loss: 2.303750\n",
      "(Iteration 24201 / 76500) loss: 2.303906\n",
      "(Iteration 24301 / 76500) loss: 2.303828\n",
      "(Iteration 24401 / 76500) loss: 2.303588\n",
      "(Epoch 32 / 100) train acc: 0.112000; val_acc: 0.079000\n",
      "(Iteration 24501 / 76500) loss: 2.303698\n",
      "(Iteration 24601 / 76500) loss: 2.303658\n",
      "(Iteration 24701 / 76500) loss: 2.303912\n",
      "(Iteration 24801 / 76500) loss: 2.303714\n",
      "(Iteration 24901 / 76500) loss: 2.303731\n",
      "(Iteration 25001 / 76500) loss: 2.303861\n",
      "(Iteration 25101 / 76500) loss: 2.303763\n",
      "(Iteration 25201 / 76500) loss: 2.303897\n",
      "(Epoch 33 / 100) train acc: 0.090000; val_acc: 0.079000\n",
      "(Iteration 25301 / 76500) loss: 2.303799\n",
      "(Iteration 25401 / 76500) loss: 2.303870\n",
      "(Iteration 25501 / 76500) loss: 2.303818\n",
      "(Iteration 25601 / 76500) loss: 2.303866\n",
      "(Iteration 25701 / 76500) loss: 2.303773\n",
      "(Iteration 25801 / 76500) loss: 2.303739\n",
      "(Iteration 25901 / 76500) loss: 2.303564\n",
      "(Iteration 26001 / 76500) loss: 2.303565\n",
      "(Epoch 34 / 100) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 26101 / 76500) loss: 2.303825\n",
      "(Iteration 26201 / 76500) loss: 2.303690\n",
      "(Iteration 26301 / 76500) loss: 2.303808\n",
      "(Iteration 26401 / 76500) loss: 2.303868\n",
      "(Iteration 26501 / 76500) loss: 2.303730\n",
      "(Iteration 26601 / 76500) loss: 2.303688\n",
      "(Iteration 26701 / 76500) loss: 2.303634\n",
      "(Epoch 35 / 100) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 26801 / 76500) loss: 2.303745\n",
      "(Iteration 26901 / 76500) loss: 2.303677\n",
      "(Iteration 27001 / 76500) loss: 2.303740\n",
      "(Iteration 27101 / 76500) loss: 2.303814\n",
      "(Iteration 27201 / 76500) loss: 2.303875\n",
      "(Iteration 27301 / 76500) loss: 2.303736\n",
      "(Iteration 27401 / 76500) loss: 2.303845\n",
      "(Iteration 27501 / 76500) loss: 2.303812\n",
      "(Epoch 36 / 100) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 27601 / 76500) loss: 2.303787\n",
      "(Iteration 27701 / 76500) loss: 2.303710\n",
      "(Iteration 27801 / 76500) loss: 2.303735\n",
      "(Iteration 27901 / 76500) loss: 2.303826\n",
      "(Iteration 28001 / 76500) loss: 2.303618\n",
      "(Iteration 28101 / 76500) loss: 2.303690\n",
      "(Iteration 28201 / 76500) loss: 2.303823\n",
      "(Iteration 28301 / 76500) loss: 2.303626\n",
      "(Epoch 37 / 100) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 28401 / 76500) loss: 2.303703\n",
      "(Iteration 28501 / 76500) loss: 2.303694\n",
      "(Iteration 28601 / 76500) loss: 2.303714\n",
      "(Iteration 28701 / 76500) loss: 2.303700\n",
      "(Iteration 28801 / 76500) loss: 2.303717\n",
      "(Iteration 28901 / 76500) loss: 2.303747\n",
      "(Iteration 29001 / 76500) loss: 2.303654\n",
      "(Epoch 38 / 100) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 29101 / 76500) loss: 2.303557\n",
      "(Iteration 29201 / 76500) loss: 2.303687\n",
      "(Iteration 29301 / 76500) loss: 2.303756\n",
      "(Iteration 29401 / 76500) loss: 2.303686\n",
      "(Iteration 29501 / 76500) loss: 2.303697\n",
      "(Iteration 29601 / 76500) loss: 2.303808\n",
      "(Iteration 29701 / 76500) loss: 2.303677\n",
      "(Iteration 29801 / 76500) loss: 2.303662\n",
      "(Epoch 39 / 100) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 29901 / 76500) loss: 2.303661\n",
      "(Iteration 30001 / 76500) loss: 2.303765\n",
      "(Iteration 30101 / 76500) loss: 2.303721\n",
      "(Iteration 30201 / 76500) loss: 2.303795\n",
      "(Iteration 30301 / 76500) loss: 2.303581\n",
      "(Iteration 30401 / 76500) loss: 2.303711\n",
      "(Iteration 30501 / 76500) loss: 2.303774\n",
      "(Epoch 40 / 100) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 30601 / 76500) loss: 2.303616\n",
      "(Iteration 30701 / 76500) loss: 2.303689\n",
      "(Iteration 30801 / 76500) loss: 2.303653\n",
      "(Iteration 30901 / 76500) loss: 2.303526\n",
      "(Iteration 31001 / 76500) loss: 2.303620\n",
      "(Iteration 31101 / 76500) loss: 2.303529\n",
      "(Iteration 31201 / 76500) loss: 2.303632\n",
      "(Iteration 31301 / 76500) loss: 2.303599\n",
      "(Epoch 41 / 100) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 31401 / 76500) loss: 2.303687\n",
      "(Iteration 31501 / 76500) loss: 2.303760\n",
      "(Iteration 31601 / 76500) loss: 2.303728\n",
      "(Iteration 31701 / 76500) loss: 2.303735\n",
      "(Iteration 31801 / 76500) loss: 2.303559\n",
      "(Iteration 31901 / 76500) loss: 2.303702\n",
      "(Iteration 32001 / 76500) loss: 2.303573\n",
      "(Iteration 32101 / 76500) loss: 2.303628\n",
      "(Epoch 42 / 100) train acc: 0.109000; val_acc: 0.079000\n",
      "(Iteration 32201 / 76500) loss: 2.303756\n",
      "(Iteration 32301 / 76500) loss: 2.303633\n",
      "(Iteration 32401 / 76500) loss: 2.303721\n",
      "(Iteration 32501 / 76500) loss: 2.303681\n",
      "(Iteration 32601 / 76500) loss: 2.303717\n",
      "(Iteration 32701 / 76500) loss: 2.303631\n",
      "(Iteration 32801 / 76500) loss: 2.303601\n",
      "(Epoch 43 / 100) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 32901 / 76500) loss: 2.303530\n",
      "(Iteration 33001 / 76500) loss: 2.303590\n",
      "(Iteration 33101 / 76500) loss: 2.303661\n",
      "(Iteration 33201 / 76500) loss: 2.303515\n",
      "(Iteration 33301 / 76500) loss: 2.303724\n",
      "(Iteration 33401 / 76500) loss: 2.303482\n",
      "(Iteration 33501 / 76500) loss: 2.303863\n",
      "(Iteration 33601 / 76500) loss: 2.303505\n",
      "(Epoch 44 / 100) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 33701 / 76500) loss: 2.303537\n",
      "(Iteration 33801 / 76500) loss: 2.303665\n",
      "(Iteration 33901 / 76500) loss: 2.303728\n",
      "(Iteration 34001 / 76500) loss: 2.303665\n",
      "(Iteration 34101 / 76500) loss: 2.303439\n",
      "(Iteration 34201 / 76500) loss: 2.303576\n",
      "(Iteration 34301 / 76500) loss: 2.303709\n",
      "(Iteration 34401 / 76500) loss: 2.303562\n",
      "(Epoch 45 / 100) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 34501 / 76500) loss: 2.303637\n",
      "(Iteration 34601 / 76500) loss: 2.303606\n",
      "(Iteration 34701 / 76500) loss: 2.303553\n",
      "(Iteration 34801 / 76500) loss: 2.303614\n",
      "(Iteration 34901 / 76500) loss: 2.303618\n",
      "(Iteration 35001 / 76500) loss: 2.303615\n",
      "(Iteration 35101 / 76500) loss: 2.303622\n",
      "(Epoch 46 / 100) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 35201 / 76500) loss: 2.303547\n",
      "(Iteration 35301 / 76500) loss: 2.303567\n",
      "(Iteration 35401 / 76500) loss: 2.303559\n",
      "(Iteration 35501 / 76500) loss: 2.303574\n",
      "(Iteration 35601 / 76500) loss: 2.303623\n",
      "(Iteration 35701 / 76500) loss: 2.303677\n",
      "(Iteration 35801 / 76500) loss: 2.303705\n",
      "(Iteration 35901 / 76500) loss: 2.303688\n",
      "(Epoch 47 / 100) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 36001 / 76500) loss: 2.303658\n",
      "(Iteration 36101 / 76500) loss: 2.303393\n",
      "(Iteration 36201 / 76500) loss: 2.303745\n",
      "(Iteration 36301 / 76500) loss: 2.303433\n",
      "(Iteration 36401 / 76500) loss: 2.303661\n",
      "(Iteration 36501 / 76500) loss: 2.303603\n",
      "(Iteration 36601 / 76500) loss: 2.303607\n",
      "(Iteration 36701 / 76500) loss: 2.303585\n",
      "(Epoch 48 / 100) train acc: 0.118000; val_acc: 0.079000\n",
      "(Iteration 36801 / 76500) loss: 2.303505\n",
      "(Iteration 36901 / 76500) loss: 2.303735\n",
      "(Iteration 37001 / 76500) loss: 2.303539\n",
      "(Iteration 37101 / 76500) loss: 2.303511\n",
      "(Iteration 37201 / 76500) loss: 2.303651\n",
      "(Iteration 37301 / 76500) loss: 2.303757\n",
      "(Iteration 37401 / 76500) loss: 2.303607\n",
      "(Epoch 49 / 100) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 37501 / 76500) loss: 2.303730\n",
      "(Iteration 37601 / 76500) loss: 2.303683\n",
      "(Iteration 37701 / 76500) loss: 2.303680\n",
      "(Iteration 37801 / 76500) loss: 2.303625\n",
      "(Iteration 37901 / 76500) loss: 2.303701\n",
      "(Iteration 38001 / 76500) loss: 2.303583\n",
      "(Iteration 38101 / 76500) loss: 2.303516\n",
      "(Iteration 38201 / 76500) loss: 2.303614\n",
      "(Epoch 50 / 100) train acc: 0.117000; val_acc: 0.079000\n",
      "(Iteration 38301 / 76500) loss: 2.303531\n",
      "(Iteration 38401 / 76500) loss: 2.303777\n",
      "(Iteration 38501 / 76500) loss: 2.303658\n",
      "(Iteration 38601 / 76500) loss: 2.303549\n",
      "(Iteration 38701 / 76500) loss: 2.303539\n",
      "(Iteration 38801 / 76500) loss: 2.303627\n",
      "(Iteration 38901 / 76500) loss: 2.303576\n",
      "(Iteration 39001 / 76500) loss: 2.303515\n",
      "(Epoch 51 / 100) train acc: 0.094000; val_acc: 0.079000\n",
      "(Iteration 39101 / 76500) loss: 2.303635\n",
      "(Iteration 39201 / 76500) loss: 2.303420\n",
      "(Iteration 39301 / 76500) loss: 2.303785\n",
      "(Iteration 39401 / 76500) loss: 2.303630\n",
      "(Iteration 39501 / 76500) loss: 2.303510\n",
      "(Iteration 39601 / 76500) loss: 2.303607\n",
      "(Iteration 39701 / 76500) loss: 2.303594\n",
      "(Epoch 52 / 100) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 39801 / 76500) loss: 2.303582\n",
      "(Iteration 39901 / 76500) loss: 2.303443\n",
      "(Iteration 40001 / 76500) loss: 2.303760\n",
      "(Iteration 40101 / 76500) loss: 2.303622\n",
      "(Iteration 40201 / 76500) loss: 2.303692\n",
      "(Iteration 40301 / 76500) loss: 2.303621\n",
      "(Iteration 40401 / 76500) loss: 2.303534\n",
      "(Iteration 40501 / 76500) loss: 2.303695\n",
      "(Epoch 53 / 100) train acc: 0.110000; val_acc: 0.079000\n",
      "(Iteration 40601 / 76500) loss: 2.303524\n",
      "(Iteration 40701 / 76500) loss: 2.303586\n",
      "(Iteration 40801 / 76500) loss: 2.303621\n",
      "(Iteration 40901 / 76500) loss: 2.303643\n",
      "(Iteration 41001 / 76500) loss: 2.303557\n",
      "(Iteration 41101 / 76500) loss: 2.303552\n",
      "(Iteration 41201 / 76500) loss: 2.303624\n",
      "(Iteration 41301 / 76500) loss: 2.303410\n",
      "(Epoch 54 / 100) train acc: 0.091000; val_acc: 0.079000\n",
      "(Iteration 41401 / 76500) loss: 2.303608\n",
      "(Iteration 41501 / 76500) loss: 2.303501\n",
      "(Iteration 41601 / 76500) loss: 2.303763\n",
      "(Iteration 41701 / 76500) loss: 2.303716\n",
      "(Iteration 41801 / 76500) loss: 2.303468\n",
      "(Iteration 41901 / 76500) loss: 2.303548\n",
      "(Iteration 42001 / 76500) loss: 2.303598\n",
      "(Epoch 55 / 100) train acc: 0.091000; val_acc: 0.079000\n",
      "(Iteration 42101 / 76500) loss: 2.303518\n",
      "(Iteration 42201 / 76500) loss: 2.303588\n",
      "(Iteration 42301 / 76500) loss: 2.303519\n",
      "(Iteration 42401 / 76500) loss: 2.303481\n",
      "(Iteration 42501 / 76500) loss: 2.303593\n",
      "(Iteration 42601 / 76500) loss: 2.303576\n",
      "(Iteration 42701 / 76500) loss: 2.303590\n",
      "(Iteration 42801 / 76500) loss: 2.303481\n",
      "(Epoch 56 / 100) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 42901 / 76500) loss: 2.303556\n",
      "(Iteration 43001 / 76500) loss: 2.303624\n",
      "(Iteration 43101 / 76500) loss: 2.303623\n",
      "(Iteration 43201 / 76500) loss: 2.303550\n",
      "(Iteration 43301 / 76500) loss: 2.303522\n",
      "(Iteration 43401 / 76500) loss: 2.303541\n",
      "(Iteration 43501 / 76500) loss: 2.303595\n",
      "(Iteration 43601 / 76500) loss: 2.303442\n",
      "(Epoch 57 / 100) train acc: 0.087000; val_acc: 0.079000\n",
      "(Iteration 43701 / 76500) loss: 2.303546\n",
      "(Iteration 43801 / 76500) loss: 2.303410\n",
      "(Iteration 43901 / 76500) loss: 2.303624\n",
      "(Iteration 44001 / 76500) loss: 2.303526\n",
      "(Iteration 44101 / 76500) loss: 2.303804\n",
      "(Iteration 44201 / 76500) loss: 2.303488\n",
      "(Iteration 44301 / 76500) loss: 2.303582\n",
      "(Epoch 58 / 100) train acc: 0.091000; val_acc: 0.079000\n",
      "(Iteration 44401 / 76500) loss: 2.303618\n",
      "(Iteration 44501 / 76500) loss: 2.303517\n",
      "(Iteration 44601 / 76500) loss: 2.303437\n",
      "(Iteration 44701 / 76500) loss: 2.303663\n",
      "(Iteration 44801 / 76500) loss: 2.303454\n",
      "(Iteration 44901 / 76500) loss: 2.303552\n",
      "(Iteration 45001 / 76500) loss: 2.303538\n",
      "(Iteration 45101 / 76500) loss: 2.303632\n",
      "(Epoch 59 / 100) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 45201 / 76500) loss: 2.303533\n",
      "(Iteration 45301 / 76500) loss: 2.303549\n",
      "(Iteration 45401 / 76500) loss: 2.303459\n",
      "(Iteration 45501 / 76500) loss: 2.303428\n",
      "(Iteration 45601 / 76500) loss: 2.303487\n",
      "(Iteration 45701 / 76500) loss: 2.303617\n",
      "(Iteration 45801 / 76500) loss: 2.303622\n",
      "(Epoch 60 / 100) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 45901 / 76500) loss: 2.303689\n",
      "(Iteration 46001 / 76500) loss: 2.303525\n",
      "(Iteration 46101 / 76500) loss: 2.303484\n",
      "(Iteration 46201 / 76500) loss: 2.303741\n",
      "(Iteration 46301 / 76500) loss: 2.303535\n",
      "(Iteration 46401 / 76500) loss: 2.303573\n",
      "(Iteration 46501 / 76500) loss: 2.303628\n",
      "(Iteration 46601 / 76500) loss: 2.303593\n",
      "(Epoch 61 / 100) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 46701 / 76500) loss: 2.303544\n",
      "(Iteration 46801 / 76500) loss: 2.303465\n",
      "(Iteration 46901 / 76500) loss: 2.303519\n",
      "(Iteration 47001 / 76500) loss: 2.303461\n",
      "(Iteration 47101 / 76500) loss: 2.303631\n",
      "(Iteration 47201 / 76500) loss: 2.303608\n",
      "(Iteration 47301 / 76500) loss: 2.303626\n",
      "(Iteration 47401 / 76500) loss: 2.303544\n",
      "(Epoch 62 / 100) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 47501 / 76500) loss: 2.303441\n",
      "(Iteration 47601 / 76500) loss: 2.303437\n",
      "(Iteration 47701 / 76500) loss: 2.303602\n",
      "(Iteration 47801 / 76500) loss: 2.303525\n",
      "(Iteration 47901 / 76500) loss: 2.303554\n",
      "(Iteration 48001 / 76500) loss: 2.303492\n",
      "(Iteration 48101 / 76500) loss: 2.303599\n",
      "(Epoch 63 / 100) train acc: 0.090000; val_acc: 0.079000\n",
      "(Iteration 48201 / 76500) loss: 2.303543\n",
      "(Iteration 48301 / 76500) loss: 2.303542\n",
      "(Iteration 48401 / 76500) loss: 2.303558\n",
      "(Iteration 48501 / 76500) loss: 2.303581\n",
      "(Iteration 48601 / 76500) loss: 2.303715\n",
      "(Iteration 48701 / 76500) loss: 2.303482\n",
      "(Iteration 48801 / 76500) loss: 2.303463\n",
      "(Iteration 48901 / 76500) loss: 2.303483\n",
      "(Epoch 64 / 100) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 49001 / 76500) loss: 2.303552\n",
      "(Iteration 49101 / 76500) loss: 2.303482\n",
      "(Iteration 49201 / 76500) loss: 2.303576\n",
      "(Iteration 49301 / 76500) loss: 2.303480\n",
      "(Iteration 49401 / 76500) loss: 2.303476\n",
      "(Iteration 49501 / 76500) loss: 2.303636\n",
      "(Iteration 49601 / 76500) loss: 2.303619\n",
      "(Iteration 49701 / 76500) loss: 2.303470\n",
      "(Epoch 65 / 100) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 49801 / 76500) loss: 2.303500\n",
      "(Iteration 49901 / 76500) loss: 2.303547\n",
      "(Iteration 50001 / 76500) loss: 2.303616\n",
      "(Iteration 50101 / 76500) loss: 2.303605\n",
      "(Iteration 50201 / 76500) loss: 2.303560\n",
      "(Iteration 50301 / 76500) loss: 2.303579\n",
      "(Iteration 50401 / 76500) loss: 2.303543\n",
      "(Epoch 66 / 100) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 50501 / 76500) loss: 2.303468\n",
      "(Iteration 50601 / 76500) loss: 2.303448\n",
      "(Iteration 50701 / 76500) loss: 2.303735\n",
      "(Iteration 50801 / 76500) loss: 2.303674\n",
      "(Iteration 50901 / 76500) loss: 2.303599\n",
      "(Iteration 51001 / 76500) loss: 2.303510\n",
      "(Iteration 51101 / 76500) loss: 2.303591\n",
      "(Iteration 51201 / 76500) loss: 2.303487\n",
      "(Epoch 67 / 100) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 51301 / 76500) loss: 2.303471\n",
      "(Iteration 51401 / 76500) loss: 2.303565\n",
      "(Iteration 51501 / 76500) loss: 2.303433\n",
      "(Iteration 51601 / 76500) loss: 2.303433\n",
      "(Iteration 51701 / 76500) loss: 2.303506\n",
      "(Iteration 51801 / 76500) loss: 2.303570\n",
      "(Iteration 51901 / 76500) loss: 2.303440\n",
      "(Iteration 52001 / 76500) loss: 2.303437\n",
      "(Epoch 68 / 100) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 52101 / 76500) loss: 2.303418\n",
      "(Iteration 52201 / 76500) loss: 2.303560\n",
      "(Iteration 52301 / 76500) loss: 2.303304\n",
      "(Iteration 52401 / 76500) loss: 2.303537\n",
      "(Iteration 52501 / 76500) loss: 2.303630\n",
      "(Iteration 52601 / 76500) loss: 2.303594\n",
      "(Iteration 52701 / 76500) loss: 2.303592\n",
      "(Epoch 69 / 100) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 52801 / 76500) loss: 2.303610\n",
      "(Iteration 52901 / 76500) loss: 2.303392\n",
      "(Iteration 53001 / 76500) loss: 2.303627\n",
      "(Iteration 53101 / 76500) loss: 2.303570\n",
      "(Iteration 53201 / 76500) loss: 2.303659\n",
      "(Iteration 53301 / 76500) loss: 2.303499\n",
      "(Iteration 53401 / 76500) loss: 2.303700\n",
      "(Iteration 53501 / 76500) loss: 2.303512\n",
      "(Epoch 70 / 100) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 53601 / 76500) loss: 2.303627\n",
      "(Iteration 53701 / 76500) loss: 2.303464\n",
      "(Iteration 53801 / 76500) loss: 2.303375\n",
      "(Iteration 53901 / 76500) loss: 2.303519\n",
      "(Iteration 54001 / 76500) loss: 2.303380\n",
      "(Iteration 54101 / 76500) loss: 2.303405\n",
      "(Iteration 54201 / 76500) loss: 2.303483\n",
      "(Iteration 54301 / 76500) loss: 2.303276\n",
      "(Epoch 71 / 100) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 54401 / 76500) loss: 2.303355\n",
      "(Iteration 54501 / 76500) loss: 2.303563\n",
      "(Iteration 54601 / 76500) loss: 2.303599\n",
      "(Iteration 54701 / 76500) loss: 2.303595\n",
      "(Iteration 54801 / 76500) loss: 2.303573\n",
      "(Iteration 54901 / 76500) loss: 2.303580\n",
      "(Iteration 55001 / 76500) loss: 2.303492\n",
      "(Epoch 72 / 100) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 55101 / 76500) loss: 2.303498\n",
      "(Iteration 55201 / 76500) loss: 2.303692\n",
      "(Iteration 55301 / 76500) loss: 2.303387\n",
      "(Iteration 55401 / 76500) loss: 2.303427\n",
      "(Iteration 55501 / 76500) loss: 2.303680\n",
      "(Iteration 55601 / 76500) loss: 2.303467\n",
      "(Iteration 55701 / 76500) loss: 2.303597\n",
      "(Iteration 55801 / 76500) loss: 2.303584\n",
      "(Epoch 73 / 100) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 55901 / 76500) loss: 2.303464\n",
      "(Iteration 56001 / 76500) loss: 2.303458\n",
      "(Iteration 56101 / 76500) loss: 2.303617\n",
      "(Iteration 56201 / 76500) loss: 2.303528\n",
      "(Iteration 56301 / 76500) loss: 2.303514\n",
      "(Iteration 56401 / 76500) loss: 2.303476\n",
      "(Iteration 56501 / 76500) loss: 2.303545\n",
      "(Iteration 56601 / 76500) loss: 2.303460\n",
      "(Epoch 74 / 100) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 56701 / 76500) loss: 2.303742\n",
      "(Iteration 56801 / 76500) loss: 2.303504\n",
      "(Iteration 56901 / 76500) loss: 2.303539\n",
      "(Iteration 57001 / 76500) loss: 2.303556\n",
      "(Iteration 57101 / 76500) loss: 2.303588\n",
      "(Iteration 57201 / 76500) loss: 2.303505\n",
      "(Iteration 57301 / 76500) loss: 2.303570\n",
      "(Epoch 75 / 100) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 57401 / 76500) loss: 2.303375\n",
      "(Iteration 57501 / 76500) loss: 2.303513\n",
      "(Iteration 57601 / 76500) loss: 2.303384\n",
      "(Iteration 57701 / 76500) loss: 2.303595\n",
      "(Iteration 57801 / 76500) loss: 2.303699\n",
      "(Iteration 57901 / 76500) loss: 2.303624\n",
      "(Iteration 58001 / 76500) loss: 2.303655\n",
      "(Iteration 58101 / 76500) loss: 2.303434\n",
      "(Epoch 76 / 100) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 58201 / 76500) loss: 2.303578\n",
      "(Iteration 58301 / 76500) loss: 2.303415\n",
      "(Iteration 58401 / 76500) loss: 2.303511\n",
      "(Iteration 58501 / 76500) loss: 2.303256\n",
      "(Iteration 58601 / 76500) loss: 2.303407\n",
      "(Iteration 58701 / 76500) loss: 2.303378\n",
      "(Iteration 58801 / 76500) loss: 2.303336\n",
      "(Iteration 58901 / 76500) loss: 2.303513\n",
      "(Epoch 77 / 100) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 59001 / 76500) loss: 2.303350\n",
      "(Iteration 59101 / 76500) loss: 2.303417\n",
      "(Iteration 59201 / 76500) loss: 2.303499\n",
      "(Iteration 59301 / 76500) loss: 2.303530\n",
      "(Iteration 59401 / 76500) loss: 2.303475\n",
      "(Iteration 59501 / 76500) loss: 2.303520\n",
      "(Iteration 59601 / 76500) loss: 2.303505\n",
      "(Epoch 78 / 100) train acc: 0.121000; val_acc: 0.079000\n",
      "(Iteration 59701 / 76500) loss: 2.303445\n",
      "(Iteration 59801 / 76500) loss: 2.303569\n",
      "(Iteration 59901 / 76500) loss: 2.303319\n",
      "(Iteration 60001 / 76500) loss: 2.303622\n",
      "(Iteration 60101 / 76500) loss: 2.303415\n",
      "(Iteration 60201 / 76500) loss: 2.303472\n",
      "(Iteration 60301 / 76500) loss: 2.303577\n",
      "(Iteration 60401 / 76500) loss: 2.303589\n",
      "(Epoch 79 / 100) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 60501 / 76500) loss: 2.303548\n",
      "(Iteration 60601 / 76500) loss: 2.303410\n",
      "(Iteration 60701 / 76500) loss: 2.303466\n",
      "(Iteration 60801 / 76500) loss: 2.303689\n",
      "(Iteration 60901 / 76500) loss: 2.303548\n",
      "(Iteration 61001 / 76500) loss: 2.303477\n",
      "(Iteration 61101 / 76500) loss: 2.303449\n",
      "(Epoch 80 / 100) train acc: 0.089000; val_acc: 0.079000\n",
      "(Iteration 61201 / 76500) loss: 2.303501\n",
      "(Iteration 61301 / 76500) loss: 2.303464\n",
      "(Iteration 61401 / 76500) loss: 2.303500\n",
      "(Iteration 61501 / 76500) loss: 2.303528\n",
      "(Iteration 61601 / 76500) loss: 2.303616\n",
      "(Iteration 61701 / 76500) loss: 2.303492\n",
      "(Iteration 61801 / 76500) loss: 2.303378\n",
      "(Iteration 61901 / 76500) loss: 2.303519\n",
      "(Epoch 81 / 100) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 62001 / 76500) loss: 2.303453\n",
      "(Iteration 62101 / 76500) loss: 2.303628\n",
      "(Iteration 62201 / 76500) loss: 2.303312\n",
      "(Iteration 62301 / 76500) loss: 2.303330\n",
      "(Iteration 62401 / 76500) loss: 2.303417\n",
      "(Iteration 62501 / 76500) loss: 2.303556\n",
      "(Iteration 62601 / 76500) loss: 2.303554\n",
      "(Iteration 62701 / 76500) loss: 2.303507\n",
      "(Epoch 82 / 100) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 62801 / 76500) loss: 2.303350\n",
      "(Iteration 62901 / 76500) loss: 2.303684\n",
      "(Iteration 63001 / 76500) loss: 2.303477\n",
      "(Iteration 63101 / 76500) loss: 2.303472\n",
      "(Iteration 63201 / 76500) loss: 2.303521\n",
      "(Iteration 63301 / 76500) loss: 2.303583\n",
      "(Iteration 63401 / 76500) loss: 2.303606\n",
      "(Epoch 83 / 100) train acc: 0.109000; val_acc: 0.079000\n",
      "(Iteration 63501 / 76500) loss: 2.303595\n",
      "(Iteration 63601 / 76500) loss: 2.303526\n",
      "(Iteration 63701 / 76500) loss: 2.303464\n",
      "(Iteration 63801 / 76500) loss: 2.303522\n",
      "(Iteration 63901 / 76500) loss: 2.303616\n",
      "(Iteration 64001 / 76500) loss: 2.303429\n",
      "(Iteration 64101 / 76500) loss: 2.303564\n",
      "(Iteration 64201 / 76500) loss: 2.303578\n",
      "(Epoch 84 / 100) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 64301 / 76500) loss: 2.303534\n",
      "(Iteration 64401 / 76500) loss: 2.303426\n",
      "(Iteration 64501 / 76500) loss: 2.303456\n",
      "(Iteration 64601 / 76500) loss: 2.303648\n",
      "(Iteration 64701 / 76500) loss: 2.303465\n",
      "(Iteration 64801 / 76500) loss: 2.303532\n",
      "(Iteration 64901 / 76500) loss: 2.303620\n",
      "(Iteration 65001 / 76500) loss: 2.303486\n",
      "(Epoch 85 / 100) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 65101 / 76500) loss: 2.303498\n",
      "(Iteration 65201 / 76500) loss: 2.303199\n",
      "(Iteration 65301 / 76500) loss: 2.303304\n",
      "(Iteration 65401 / 76500) loss: 2.303599\n",
      "(Iteration 65501 / 76500) loss: 2.303584\n",
      "(Iteration 65601 / 76500) loss: 2.303251\n",
      "(Iteration 65701 / 76500) loss: 2.303475\n",
      "(Epoch 86 / 100) train acc: 0.089000; val_acc: 0.079000\n",
      "(Iteration 65801 / 76500) loss: 2.303555\n",
      "(Iteration 65901 / 76500) loss: 2.303431\n",
      "(Iteration 66001 / 76500) loss: 2.303575\n",
      "(Iteration 66101 / 76500) loss: 2.303579\n",
      "(Iteration 66201 / 76500) loss: 2.303438\n",
      "(Iteration 66301 / 76500) loss: 2.303657\n",
      "(Iteration 66401 / 76500) loss: 2.303383\n",
      "(Iteration 66501 / 76500) loss: 2.303406\n",
      "(Epoch 87 / 100) train acc: 0.100000; val_acc: 0.079000\n",
      "(Iteration 66601 / 76500) loss: 2.303410\n",
      "(Iteration 66701 / 76500) loss: 2.303564\n",
      "(Iteration 66801 / 76500) loss: 2.303485\n",
      "(Iteration 66901 / 76500) loss: 2.303532\n",
      "(Iteration 67001 / 76500) loss: 2.303513\n",
      "(Iteration 67101 / 76500) loss: 2.303396\n",
      "(Iteration 67201 / 76500) loss: 2.303527\n",
      "(Iteration 67301 / 76500) loss: 2.303355\n",
      "(Epoch 88 / 100) train acc: 0.093000; val_acc: 0.079000\n",
      "(Iteration 67401 / 76500) loss: 2.303363\n",
      "(Iteration 67501 / 76500) loss: 2.303351\n",
      "(Iteration 67601 / 76500) loss: 2.303556\n",
      "(Iteration 67701 / 76500) loss: 2.303436\n",
      "(Iteration 67801 / 76500) loss: 2.303435\n",
      "(Iteration 67901 / 76500) loss: 2.303574\n",
      "(Iteration 68001 / 76500) loss: 2.303452\n",
      "(Epoch 89 / 100) train acc: 0.096000; val_acc: 0.079000\n",
      "(Iteration 68101 / 76500) loss: 2.303463\n",
      "(Iteration 68201 / 76500) loss: 2.303599\n",
      "(Iteration 68301 / 76500) loss: 2.303363\n",
      "(Iteration 68401 / 76500) loss: 2.303658\n",
      "(Iteration 68501 / 76500) loss: 2.303552\n",
      "(Iteration 68601 / 76500) loss: 2.303412\n",
      "(Iteration 68701 / 76500) loss: 2.303570\n",
      "(Iteration 68801 / 76500) loss: 2.303497\n",
      "(Epoch 90 / 100) train acc: 0.095000; val_acc: 0.079000\n",
      "(Iteration 68901 / 76500) loss: 2.303589\n",
      "(Iteration 69001 / 76500) loss: 2.303574\n",
      "(Iteration 69101 / 76500) loss: 2.303458\n",
      "(Iteration 69201 / 76500) loss: 2.303553\n",
      "(Iteration 69301 / 76500) loss: 2.303408\n",
      "(Iteration 69401 / 76500) loss: 2.303473\n",
      "(Iteration 69501 / 76500) loss: 2.303343\n",
      "(Iteration 69601 / 76500) loss: 2.303464\n",
      "(Epoch 91 / 100) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 69701 / 76500) loss: 2.303348\n",
      "(Iteration 69801 / 76500) loss: 2.303534\n",
      "(Iteration 69901 / 76500) loss: 2.303574\n",
      "(Iteration 70001 / 76500) loss: 2.303489\n",
      "(Iteration 70101 / 76500) loss: 2.303717\n",
      "(Iteration 70201 / 76500) loss: 2.303594\n",
      "(Iteration 70301 / 76500) loss: 2.303406\n",
      "(Epoch 92 / 100) train acc: 0.101000; val_acc: 0.079000\n",
      "(Iteration 70401 / 76500) loss: 2.303435\n",
      "(Iteration 70501 / 76500) loss: 2.303540\n",
      "(Iteration 70601 / 76500) loss: 2.303413\n",
      "(Iteration 70701 / 76500) loss: 2.303462\n",
      "(Iteration 70801 / 76500) loss: 2.303498\n",
      "(Iteration 70901 / 76500) loss: 2.303461\n",
      "(Iteration 71001 / 76500) loss: 2.303578\n",
      "(Iteration 71101 / 76500) loss: 2.303415\n",
      "(Epoch 93 / 100) train acc: 0.082000; val_acc: 0.079000\n",
      "(Iteration 71201 / 76500) loss: 2.303473\n",
      "(Iteration 71301 / 76500) loss: 2.303530\n",
      "(Iteration 71401 / 76500) loss: 2.303325\n",
      "(Iteration 71501 / 76500) loss: 2.303478\n",
      "(Iteration 71601 / 76500) loss: 2.303405\n",
      "(Iteration 71701 / 76500) loss: 2.303455\n",
      "(Iteration 71801 / 76500) loss: 2.303443\n",
      "(Iteration 71901 / 76500) loss: 2.303634\n",
      "(Epoch 94 / 100) train acc: 0.111000; val_acc: 0.079000\n",
      "(Iteration 72001 / 76500) loss: 2.303387\n",
      "(Iteration 72101 / 76500) loss: 2.303602\n",
      "(Iteration 72201 / 76500) loss: 2.303373\n",
      "(Iteration 72301 / 76500) loss: 2.303409\n",
      "(Iteration 72401 / 76500) loss: 2.303402\n",
      "(Iteration 72501 / 76500) loss: 2.303617\n",
      "(Iteration 72601 / 76500) loss: 2.303382\n",
      "(Epoch 95 / 100) train acc: 0.103000; val_acc: 0.079000\n",
      "(Iteration 72701 / 76500) loss: 2.303527\n",
      "(Iteration 72801 / 76500) loss: 2.303371\n",
      "(Iteration 72901 / 76500) loss: 2.303426\n",
      "(Iteration 73001 / 76500) loss: 2.303437\n",
      "(Iteration 73101 / 76500) loss: 2.303606\n",
      "(Iteration 73201 / 76500) loss: 2.303518\n",
      "(Iteration 73301 / 76500) loss: 2.303371\n",
      "(Iteration 73401 / 76500) loss: 2.303512\n",
      "(Epoch 96 / 100) train acc: 0.097000; val_acc: 0.079000\n",
      "(Iteration 73501 / 76500) loss: 2.303431\n",
      "(Iteration 73601 / 76500) loss: 2.303765\n",
      "(Iteration 73701 / 76500) loss: 2.303521\n",
      "(Iteration 73801 / 76500) loss: 2.303642\n",
      "(Iteration 73901 / 76500) loss: 2.303432\n",
      "(Iteration 74001 / 76500) loss: 2.303493\n",
      "(Iteration 74101 / 76500) loss: 2.303564\n",
      "(Iteration 74201 / 76500) loss: 2.303499\n",
      "(Epoch 97 / 100) train acc: 0.107000; val_acc: 0.079000\n",
      "(Iteration 74301 / 76500) loss: 2.303631\n",
      "(Iteration 74401 / 76500) loss: 2.303505\n",
      "(Iteration 74501 / 76500) loss: 2.303527\n",
      "(Iteration 74601 / 76500) loss: 2.303288\n",
      "(Iteration 74701 / 76500) loss: 2.303481\n",
      "(Iteration 74801 / 76500) loss: 2.303521\n",
      "(Iteration 74901 / 76500) loss: 2.303636\n",
      "(Epoch 98 / 100) train acc: 0.079000; val_acc: 0.079000\n",
      "(Iteration 75001 / 76500) loss: 2.303550\n",
      "(Iteration 75101 / 76500) loss: 2.303498\n",
      "(Iteration 75201 / 76500) loss: 2.303582\n",
      "(Iteration 75301 / 76500) loss: 2.303564\n",
      "(Iteration 75401 / 76500) loss: 2.303543\n",
      "(Iteration 75501 / 76500) loss: 2.303410\n",
      "(Iteration 75601 / 76500) loss: 2.303372\n",
      "(Iteration 75701 / 76500) loss: 2.303700\n",
      "(Epoch 99 / 100) train acc: 0.108000; val_acc: 0.079000\n",
      "(Iteration 75801 / 76500) loss: 2.303449\n",
      "(Iteration 75901 / 76500) loss: 2.303486\n",
      "(Iteration 76001 / 76500) loss: 2.303498\n",
      "(Iteration 76101 / 76500) loss: 2.303476\n",
      "(Iteration 76201 / 76500) loss: 2.303498\n",
      "(Iteration 76301 / 76500) loss: 2.303513\n",
      "(Iteration 76401 / 76500) loss: 2.303374\n",
      "(Epoch 100 / 100) train acc: 0.112000; val_acc: 0.079000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.306735\n",
      "(Epoch 0 / 100) train acc: 0.078000; val_acc: 0.093000\n",
      "(Iteration 101 / 38200) loss: 2.306701\n",
      "(Iteration 201 / 38200) loss: 2.306681\n",
      "(Iteration 301 / 38200) loss: 2.306613\n",
      "(Epoch 1 / 100) train acc: 0.094000; val_acc: 0.088000\n",
      "(Iteration 401 / 38200) loss: 2.306562\n",
      "(Iteration 501 / 38200) loss: 2.306538\n",
      "(Iteration 601 / 38200) loss: 2.306508\n",
      "(Iteration 701 / 38200) loss: 2.306465\n",
      "(Epoch 2 / 100) train acc: 0.108000; val_acc: 0.086000\n",
      "(Iteration 801 / 38200) loss: 2.306421\n",
      "(Iteration 901 / 38200) loss: 2.306387\n",
      "(Iteration 1001 / 38200) loss: 2.306382\n",
      "(Iteration 1101 / 38200) loss: 2.306332\n",
      "(Epoch 3 / 100) train acc: 0.084000; val_acc: 0.089000\n",
      "(Iteration 1201 / 38200) loss: 2.306314\n",
      "(Iteration 1301 / 38200) loss: 2.306257\n",
      "(Iteration 1401 / 38200) loss: 2.306214\n",
      "(Iteration 1501 / 38200) loss: 2.306175\n",
      "(Epoch 4 / 100) train acc: 0.107000; val_acc: 0.089000\n",
      "(Iteration 1601 / 38200) loss: 2.306124\n",
      "(Iteration 1701 / 38200) loss: 2.306111\n",
      "(Iteration 1801 / 38200) loss: 2.306122\n",
      "(Iteration 1901 / 38200) loss: 2.306060\n",
      "(Epoch 5 / 100) train acc: 0.089000; val_acc: 0.089000\n",
      "(Iteration 2001 / 38200) loss: 2.306052\n",
      "(Iteration 2101 / 38200) loss: 2.306034\n",
      "(Iteration 2201 / 38200) loss: 2.305992\n",
      "(Epoch 6 / 100) train acc: 0.092000; val_acc: 0.087000\n",
      "(Iteration 2301 / 38200) loss: 2.305931\n",
      "(Iteration 2401 / 38200) loss: 2.305952\n",
      "(Iteration 2501 / 38200) loss: 2.305955\n",
      "(Iteration 2601 / 38200) loss: 2.305874\n",
      "(Epoch 7 / 100) train acc: 0.111000; val_acc: 0.085000\n",
      "(Iteration 2701 / 38200) loss: 2.305860\n",
      "(Iteration 2801 / 38200) loss: 2.305866\n",
      "(Iteration 2901 / 38200) loss: 2.305850\n",
      "(Iteration 3001 / 38200) loss: 2.305827\n",
      "(Epoch 8 / 100) train acc: 0.114000; val_acc: 0.084000\n",
      "(Iteration 3101 / 38200) loss: 2.305777\n",
      "(Iteration 3201 / 38200) loss: 2.305781\n",
      "(Iteration 3301 / 38200) loss: 2.305721\n",
      "(Iteration 3401 / 38200) loss: 2.305708\n",
      "(Epoch 9 / 100) train acc: 0.085000; val_acc: 0.083000\n",
      "(Iteration 3501 / 38200) loss: 2.305697\n",
      "(Iteration 3601 / 38200) loss: 2.305656\n",
      "(Iteration 3701 / 38200) loss: 2.305652\n",
      "(Iteration 3801 / 38200) loss: 2.305625\n",
      "(Epoch 10 / 100) train acc: 0.099000; val_acc: 0.086000\n",
      "(Iteration 3901 / 38200) loss: 2.305621\n",
      "(Iteration 4001 / 38200) loss: 2.305602\n",
      "(Iteration 4101 / 38200) loss: 2.305568\n",
      "(Iteration 4201 / 38200) loss: 2.305554\n",
      "(Epoch 11 / 100) train acc: 0.100000; val_acc: 0.085000\n",
      "(Iteration 4301 / 38200) loss: 2.305543\n",
      "(Iteration 4401 / 38200) loss: 2.305534\n",
      "(Iteration 4501 / 38200) loss: 2.305513\n",
      "(Epoch 12 / 100) train acc: 0.093000; val_acc: 0.083000\n",
      "(Iteration 4601 / 38200) loss: 2.305504\n",
      "(Iteration 4701 / 38200) loss: 2.305464\n",
      "(Iteration 4801 / 38200) loss: 2.305476\n",
      "(Iteration 4901 / 38200) loss: 2.305465\n",
      "(Epoch 13 / 100) train acc: 0.120000; val_acc: 0.083000\n",
      "(Iteration 5001 / 38200) loss: 2.305436\n",
      "(Iteration 5101 / 38200) loss: 2.305418\n",
      "(Iteration 5201 / 38200) loss: 2.305423\n",
      "(Iteration 5301 / 38200) loss: 2.305413\n",
      "(Epoch 14 / 100) train acc: 0.093000; val_acc: 0.091000\n",
      "(Iteration 5401 / 38200) loss: 2.305377\n",
      "(Iteration 5501 / 38200) loss: 2.305395\n",
      "(Iteration 5601 / 38200) loss: 2.305371\n",
      "(Iteration 5701 / 38200) loss: 2.305357\n",
      "(Epoch 15 / 100) train acc: 0.111000; val_acc: 0.086000\n",
      "(Iteration 5801 / 38200) loss: 2.305288\n",
      "(Iteration 5901 / 38200) loss: 2.305307\n",
      "(Iteration 6001 / 38200) loss: 2.305268\n",
      "(Iteration 6101 / 38200) loss: 2.305276\n",
      "(Epoch 16 / 100) train acc: 0.097000; val_acc: 0.084000\n",
      "(Iteration 6201 / 38200) loss: 2.305277\n",
      "(Iteration 6301 / 38200) loss: 2.305248\n",
      "(Iteration 6401 / 38200) loss: 2.305284\n",
      "(Epoch 17 / 100) train acc: 0.111000; val_acc: 0.086000\n",
      "(Iteration 6501 / 38200) loss: 2.305265\n",
      "(Iteration 6601 / 38200) loss: 2.305189\n",
      "(Iteration 6701 / 38200) loss: 2.305184\n",
      "(Iteration 6801 / 38200) loss: 2.305224\n",
      "(Epoch 18 / 100) train acc: 0.103000; val_acc: 0.085000\n",
      "(Iteration 6901 / 38200) loss: 2.305175\n",
      "(Iteration 7001 / 38200) loss: 2.305161\n",
      "(Iteration 7101 / 38200) loss: 2.305177\n",
      "(Iteration 7201 / 38200) loss: 2.305136\n",
      "(Epoch 19 / 100) train acc: 0.106000; val_acc: 0.083000\n",
      "(Iteration 7301 / 38200) loss: 2.305185\n",
      "(Iteration 7401 / 38200) loss: 2.305134\n",
      "(Iteration 7501 / 38200) loss: 2.305134\n",
      "(Iteration 7601 / 38200) loss: 2.305128\n",
      "(Epoch 20 / 100) train acc: 0.096000; val_acc: 0.084000\n",
      "(Iteration 7701 / 38200) loss: 2.305123\n",
      "(Iteration 7801 / 38200) loss: 2.305112\n",
      "(Iteration 7901 / 38200) loss: 2.305099\n",
      "(Iteration 8001 / 38200) loss: 2.305093\n",
      "(Epoch 21 / 100) train acc: 0.093000; val_acc: 0.083000\n",
      "(Iteration 8101 / 38200) loss: 2.305101\n",
      "(Iteration 8201 / 38200) loss: 2.305042\n",
      "(Iteration 8301 / 38200) loss: 2.305101\n",
      "(Iteration 8401 / 38200) loss: 2.305074\n",
      "(Epoch 22 / 100) train acc: 0.094000; val_acc: 0.085000\n",
      "(Iteration 8501 / 38200) loss: 2.305049\n",
      "(Iteration 8601 / 38200) loss: 2.305008\n",
      "(Iteration 8701 / 38200) loss: 2.305015\n",
      "(Epoch 23 / 100) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 8801 / 38200) loss: 2.305007\n",
      "(Iteration 8901 / 38200) loss: 2.305005\n",
      "(Iteration 9001 / 38200) loss: 2.305008\n",
      "(Iteration 9101 / 38200) loss: 2.305006\n",
      "(Epoch 24 / 100) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 9201 / 38200) loss: 2.305000\n",
      "(Iteration 9301 / 38200) loss: 2.304985\n",
      "(Iteration 9401 / 38200) loss: 2.304992\n",
      "(Iteration 9501 / 38200) loss: 2.304944\n",
      "(Epoch 25 / 100) train acc: 0.110000; val_acc: 0.087000\n",
      "(Iteration 9601 / 38200) loss: 2.304985\n",
      "(Iteration 9701 / 38200) loss: 2.304929\n",
      "(Iteration 9801 / 38200) loss: 2.304949\n",
      "(Iteration 9901 / 38200) loss: 2.304926\n",
      "(Epoch 26 / 100) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 10001 / 38200) loss: 2.304930\n",
      "(Iteration 10101 / 38200) loss: 2.304963\n",
      "(Iteration 10201 / 38200) loss: 2.304878\n",
      "(Iteration 10301 / 38200) loss: 2.304909\n",
      "(Epoch 27 / 100) train acc: 0.091000; val_acc: 0.087000\n",
      "(Iteration 10401 / 38200) loss: 2.304934\n",
      "(Iteration 10501 / 38200) loss: 2.304926\n",
      "(Iteration 10601 / 38200) loss: 2.304882\n",
      "(Epoch 28 / 100) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 10701 / 38200) loss: 2.304891\n",
      "(Iteration 10801 / 38200) loss: 2.304879\n",
      "(Iteration 10901 / 38200) loss: 2.304877\n",
      "(Iteration 11001 / 38200) loss: 2.304870\n",
      "(Epoch 29 / 100) train acc: 0.109000; val_acc: 0.087000\n",
      "(Iteration 11101 / 38200) loss: 2.304891\n",
      "(Iteration 11201 / 38200) loss: 2.304882\n",
      "(Iteration 11301 / 38200) loss: 2.304830\n",
      "(Iteration 11401 / 38200) loss: 2.304860\n",
      "(Epoch 30 / 100) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 11501 / 38200) loss: 2.304862\n",
      "(Iteration 11601 / 38200) loss: 2.304852\n",
      "(Iteration 11701 / 38200) loss: 2.304879\n",
      "(Iteration 11801 / 38200) loss: 2.304853\n",
      "(Epoch 31 / 100) train acc: 0.098000; val_acc: 0.084000\n",
      "(Iteration 11901 / 38200) loss: 2.304833\n",
      "(Iteration 12001 / 38200) loss: 2.304829\n",
      "(Iteration 12101 / 38200) loss: 2.304845\n",
      "(Iteration 12201 / 38200) loss: 2.304820\n",
      "(Epoch 32 / 100) train acc: 0.105000; val_acc: 0.084000\n",
      "(Iteration 12301 / 38200) loss: 2.304814\n",
      "(Iteration 12401 / 38200) loss: 2.304799\n",
      "(Iteration 12501 / 38200) loss: 2.304816\n",
      "(Iteration 12601 / 38200) loss: 2.304788\n",
      "(Epoch 33 / 100) train acc: 0.105000; val_acc: 0.083000\n",
      "(Iteration 12701 / 38200) loss: 2.304829\n",
      "(Iteration 12801 / 38200) loss: 2.304764\n",
      "(Iteration 12901 / 38200) loss: 2.304787\n",
      "(Epoch 34 / 100) train acc: 0.107000; val_acc: 0.084000\n",
      "(Iteration 13001 / 38200) loss: 2.304816\n",
      "(Iteration 13101 / 38200) loss: 2.304784\n",
      "(Iteration 13201 / 38200) loss: 2.304788\n",
      "(Iteration 13301 / 38200) loss: 2.304764\n",
      "(Epoch 35 / 100) train acc: 0.109000; val_acc: 0.083000\n",
      "(Iteration 13401 / 38200) loss: 2.304750\n",
      "(Iteration 13501 / 38200) loss: 2.304766\n",
      "(Iteration 13601 / 38200) loss: 2.304774\n",
      "(Iteration 13701 / 38200) loss: 2.304737\n",
      "(Epoch 36 / 100) train acc: 0.089000; val_acc: 0.084000\n",
      "(Iteration 13801 / 38200) loss: 2.304713\n",
      "(Iteration 13901 / 38200) loss: 2.304755\n",
      "(Iteration 14001 / 38200) loss: 2.304777\n",
      "(Iteration 14101 / 38200) loss: 2.304791\n",
      "(Epoch 37 / 100) train acc: 0.099000; val_acc: 0.084000\n",
      "(Iteration 14201 / 38200) loss: 2.304729\n",
      "(Iteration 14301 / 38200) loss: 2.304723\n",
      "(Iteration 14401 / 38200) loss: 2.304742\n",
      "(Iteration 14501 / 38200) loss: 2.304744\n",
      "(Epoch 38 / 100) train acc: 0.100000; val_acc: 0.084000\n",
      "(Iteration 14601 / 38200) loss: 2.304755\n",
      "(Iteration 14701 / 38200) loss: 2.304691\n",
      "(Iteration 14801 / 38200) loss: 2.304715\n",
      "(Epoch 39 / 100) train acc: 0.083000; val_acc: 0.084000\n",
      "(Iteration 14901 / 38200) loss: 2.304694\n",
      "(Iteration 15001 / 38200) loss: 2.304724\n",
      "(Iteration 15101 / 38200) loss: 2.304720\n",
      "(Iteration 15201 / 38200) loss: 2.304703\n",
      "(Epoch 40 / 100) train acc: 0.103000; val_acc: 0.084000\n",
      "(Iteration 15301 / 38200) loss: 2.304734\n",
      "(Iteration 15401 / 38200) loss: 2.304688\n",
      "(Iteration 15501 / 38200) loss: 2.304689\n",
      "(Iteration 15601 / 38200) loss: 2.304681\n",
      "(Epoch 41 / 100) train acc: 0.107000; val_acc: 0.084000\n",
      "(Iteration 15701 / 38200) loss: 2.304718\n",
      "(Iteration 15801 / 38200) loss: 2.304678\n",
      "(Iteration 15901 / 38200) loss: 2.304672\n",
      "(Iteration 16001 / 38200) loss: 2.304681\n",
      "(Epoch 42 / 100) train acc: 0.108000; val_acc: 0.084000\n",
      "(Iteration 16101 / 38200) loss: 2.304721\n",
      "(Iteration 16201 / 38200) loss: 2.304686\n",
      "(Iteration 16301 / 38200) loss: 2.304701\n",
      "(Iteration 16401 / 38200) loss: 2.304697\n",
      "(Epoch 43 / 100) train acc: 0.103000; val_acc: 0.084000\n",
      "(Iteration 16501 / 38200) loss: 2.304655\n",
      "(Iteration 16601 / 38200) loss: 2.304679\n",
      "(Iteration 16701 / 38200) loss: 2.304629\n",
      "(Iteration 16801 / 38200) loss: 2.304689\n",
      "(Epoch 44 / 100) train acc: 0.105000; val_acc: 0.084000\n",
      "(Iteration 16901 / 38200) loss: 2.304662\n",
      "(Iteration 17001 / 38200) loss: 2.304670\n",
      "(Iteration 17101 / 38200) loss: 2.304679\n",
      "(Epoch 45 / 100) train acc: 0.104000; val_acc: 0.084000\n",
      "(Iteration 17201 / 38200) loss: 2.304640\n",
      "(Iteration 17301 / 38200) loss: 2.304645\n",
      "(Iteration 17401 / 38200) loss: 2.304670\n",
      "(Iteration 17501 / 38200) loss: 2.304663\n",
      "(Epoch 46 / 100) train acc: 0.101000; val_acc: 0.086000\n",
      "(Iteration 17601 / 38200) loss: 2.304683\n",
      "(Iteration 17701 / 38200) loss: 2.304655\n",
      "(Iteration 17801 / 38200) loss: 2.304655\n",
      "(Iteration 17901 / 38200) loss: 2.304663\n",
      "(Epoch 47 / 100) train acc: 0.089000; val_acc: 0.086000\n",
      "(Iteration 18001 / 38200) loss: 2.304651\n",
      "(Iteration 18101 / 38200) loss: 2.304657\n",
      "(Iteration 18201 / 38200) loss: 2.304640\n",
      "(Iteration 18301 / 38200) loss: 2.304637\n",
      "(Epoch 48 / 100) train acc: 0.096000; val_acc: 0.084000\n",
      "(Iteration 18401 / 38200) loss: 2.304639\n",
      "(Iteration 18501 / 38200) loss: 2.304655\n",
      "(Iteration 18601 / 38200) loss: 2.304633\n",
      "(Iteration 18701 / 38200) loss: 2.304694\n",
      "(Epoch 49 / 100) train acc: 0.100000; val_acc: 0.086000\n",
      "(Iteration 18801 / 38200) loss: 2.304633\n",
      "(Iteration 18901 / 38200) loss: 2.304651\n",
      "(Iteration 19001 / 38200) loss: 2.304628\n",
      "(Epoch 50 / 100) train acc: 0.093000; val_acc: 0.086000\n",
      "(Iteration 19101 / 38200) loss: 2.304616\n",
      "(Iteration 19201 / 38200) loss: 2.304610\n",
      "(Iteration 19301 / 38200) loss: 2.304643\n",
      "(Iteration 19401 / 38200) loss: 2.304645\n",
      "(Epoch 51 / 100) train acc: 0.094000; val_acc: 0.086000\n",
      "(Iteration 19501 / 38200) loss: 2.304603\n",
      "(Iteration 19601 / 38200) loss: 2.304640\n",
      "(Iteration 19701 / 38200) loss: 2.304618\n",
      "(Iteration 19801 / 38200) loss: 2.304589\n",
      "(Epoch 52 / 100) train acc: 0.104000; val_acc: 0.086000\n",
      "(Iteration 19901 / 38200) loss: 2.304646\n",
      "(Iteration 20001 / 38200) loss: 2.304589\n",
      "(Iteration 20101 / 38200) loss: 2.304619\n",
      "(Iteration 20201 / 38200) loss: 2.304598\n",
      "(Epoch 53 / 100) train acc: 0.116000; val_acc: 0.086000\n",
      "(Iteration 20301 / 38200) loss: 2.304616\n",
      "(Iteration 20401 / 38200) loss: 2.304644\n",
      "(Iteration 20501 / 38200) loss: 2.304599\n",
      "(Iteration 20601 / 38200) loss: 2.304620\n",
      "(Epoch 54 / 100) train acc: 0.126000; val_acc: 0.086000\n",
      "(Iteration 20701 / 38200) loss: 2.304611\n",
      "(Iteration 20801 / 38200) loss: 2.304636\n",
      "(Iteration 20901 / 38200) loss: 2.304606\n",
      "(Iteration 21001 / 38200) loss: 2.304558\n",
      "(Epoch 55 / 100) train acc: 0.089000; val_acc: 0.086000\n",
      "(Iteration 21101 / 38200) loss: 2.304594\n",
      "(Iteration 21201 / 38200) loss: 2.304594\n",
      "(Iteration 21301 / 38200) loss: 2.304601\n",
      "(Epoch 56 / 100) train acc: 0.111000; val_acc: 0.086000\n",
      "(Iteration 21401 / 38200) loss: 2.304592\n",
      "(Iteration 21501 / 38200) loss: 2.304592\n",
      "(Iteration 21601 / 38200) loss: 2.304597\n",
      "(Iteration 21701 / 38200) loss: 2.304585\n",
      "(Epoch 57 / 100) train acc: 0.101000; val_acc: 0.086000\n",
      "(Iteration 21801 / 38200) loss: 2.304616\n",
      "(Iteration 21901 / 38200) loss: 2.304582\n",
      "(Iteration 22001 / 38200) loss: 2.304581\n",
      "(Iteration 22101 / 38200) loss: 2.304579\n",
      "(Epoch 58 / 100) train acc: 0.101000; val_acc: 0.086000\n",
      "(Iteration 22201 / 38200) loss: 2.304568\n",
      "(Iteration 22301 / 38200) loss: 2.304570\n",
      "(Iteration 22401 / 38200) loss: 2.304569\n",
      "(Iteration 22501 / 38200) loss: 2.304590\n",
      "(Epoch 59 / 100) train acc: 0.091000; val_acc: 0.086000\n",
      "(Iteration 22601 / 38200) loss: 2.304546\n",
      "(Iteration 22701 / 38200) loss: 2.304610\n",
      "(Iteration 22801 / 38200) loss: 2.304580\n",
      "(Iteration 22901 / 38200) loss: 2.304590\n",
      "(Epoch 60 / 100) train acc: 0.092000; val_acc: 0.086000\n",
      "(Iteration 23001 / 38200) loss: 2.304551\n",
      "(Iteration 23101 / 38200) loss: 2.304605\n",
      "(Iteration 23201 / 38200) loss: 2.304546\n",
      "(Iteration 23301 / 38200) loss: 2.304566\n",
      "(Epoch 61 / 100) train acc: 0.097000; val_acc: 0.086000\n",
      "(Iteration 23401 / 38200) loss: 2.304569\n",
      "(Iteration 23501 / 38200) loss: 2.304571\n",
      "(Iteration 23601 / 38200) loss: 2.304570\n",
      "(Epoch 62 / 100) train acc: 0.101000; val_acc: 0.086000\n",
      "(Iteration 23701 / 38200) loss: 2.304587\n",
      "(Iteration 23801 / 38200) loss: 2.304568\n",
      "(Iteration 23901 / 38200) loss: 2.304584\n",
      "(Iteration 24001 / 38200) loss: 2.304550\n",
      "(Epoch 63 / 100) train acc: 0.100000; val_acc: 0.086000\n",
      "(Iteration 24101 / 38200) loss: 2.304580\n",
      "(Iteration 24201 / 38200) loss: 2.304581\n",
      "(Iteration 24301 / 38200) loss: 2.304556\n",
      "(Iteration 24401 / 38200) loss: 2.304581\n",
      "(Epoch 64 / 100) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 24501 / 38200) loss: 2.304552\n",
      "(Iteration 24601 / 38200) loss: 2.304608\n",
      "(Iteration 24701 / 38200) loss: 2.304552\n",
      "(Iteration 24801 / 38200) loss: 2.304551\n",
      "(Epoch 65 / 100) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 24901 / 38200) loss: 2.304574\n",
      "(Iteration 25001 / 38200) loss: 2.304567\n",
      "(Iteration 25101 / 38200) loss: 2.304554\n",
      "(Iteration 25201 / 38200) loss: 2.304606\n",
      "(Epoch 66 / 100) train acc: 0.120000; val_acc: 0.087000\n",
      "(Iteration 25301 / 38200) loss: 2.304556\n",
      "(Iteration 25401 / 38200) loss: 2.304558\n",
      "(Iteration 25501 / 38200) loss: 2.304578\n",
      "(Epoch 67 / 100) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 25601 / 38200) loss: 2.304558\n",
      "(Iteration 25701 / 38200) loss: 2.304593\n",
      "(Iteration 25801 / 38200) loss: 2.304529\n",
      "(Iteration 25901 / 38200) loss: 2.304558\n",
      "(Epoch 68 / 100) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 26001 / 38200) loss: 2.304543\n",
      "(Iteration 26101 / 38200) loss: 2.304499\n",
      "(Iteration 26201 / 38200) loss: 2.304545\n",
      "(Iteration 26301 / 38200) loss: 2.304588\n",
      "(Epoch 69 / 100) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 26401 / 38200) loss: 2.304554\n",
      "(Iteration 26501 / 38200) loss: 2.304608\n",
      "(Iteration 26601 / 38200) loss: 2.304562\n",
      "(Iteration 26701 / 38200) loss: 2.304559\n",
      "(Epoch 70 / 100) train acc: 0.090000; val_acc: 0.087000\n",
      "(Iteration 26801 / 38200) loss: 2.304540\n",
      "(Iteration 26901 / 38200) loss: 2.304552\n",
      "(Iteration 27001 / 38200) loss: 2.304525\n",
      "(Iteration 27101 / 38200) loss: 2.304531\n",
      "(Epoch 71 / 100) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 27201 / 38200) loss: 2.304564\n",
      "(Iteration 27301 / 38200) loss: 2.304538\n",
      "(Iteration 27401 / 38200) loss: 2.304543\n",
      "(Iteration 27501 / 38200) loss: 2.304506\n",
      "(Epoch 72 / 100) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 27601 / 38200) loss: 2.304551\n",
      "(Iteration 27701 / 38200) loss: 2.304526\n",
      "(Iteration 27801 / 38200) loss: 2.304568\n",
      "(Epoch 73 / 100) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 27901 / 38200) loss: 2.304564\n",
      "(Iteration 28001 / 38200) loss: 2.304565\n",
      "(Iteration 28101 / 38200) loss: 2.304537\n",
      "(Iteration 28201 / 38200) loss: 2.304560\n",
      "(Epoch 74 / 100) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 28301 / 38200) loss: 2.304548\n",
      "(Iteration 28401 / 38200) loss: 2.304549\n",
      "(Iteration 28501 / 38200) loss: 2.304543\n",
      "(Iteration 28601 / 38200) loss: 2.304502\n",
      "(Epoch 75 / 100) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 28701 / 38200) loss: 2.304592\n",
      "(Iteration 28801 / 38200) loss: 2.304508\n",
      "(Iteration 28901 / 38200) loss: 2.304534\n",
      "(Iteration 29001 / 38200) loss: 2.304535\n",
      "(Epoch 76 / 100) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 29101 / 38200) loss: 2.304526\n",
      "(Iteration 29201 / 38200) loss: 2.304512\n",
      "(Iteration 29301 / 38200) loss: 2.304532\n",
      "(Iteration 29401 / 38200) loss: 2.304528\n",
      "(Epoch 77 / 100) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 29501 / 38200) loss: 2.304569\n",
      "(Iteration 29601 / 38200) loss: 2.304507\n",
      "(Iteration 29701 / 38200) loss: 2.304571\n",
      "(Epoch 78 / 100) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 29801 / 38200) loss: 2.304547\n",
      "(Iteration 29901 / 38200) loss: 2.304516\n",
      "(Iteration 30001 / 38200) loss: 2.304533\n",
      "(Iteration 30101 / 38200) loss: 2.304513\n",
      "(Epoch 79 / 100) train acc: 0.111000; val_acc: 0.087000\n",
      "(Iteration 30201 / 38200) loss: 2.304510\n",
      "(Iteration 30301 / 38200) loss: 2.304552\n",
      "(Iteration 30401 / 38200) loss: 2.304522\n",
      "(Iteration 30501 / 38200) loss: 2.304548\n",
      "(Epoch 80 / 100) train acc: 0.110000; val_acc: 0.087000\n",
      "(Iteration 30601 / 38200) loss: 2.304525\n",
      "(Iteration 30701 / 38200) loss: 2.304570\n",
      "(Iteration 30801 / 38200) loss: 2.304546\n",
      "(Iteration 30901 / 38200) loss: 2.304532\n",
      "(Epoch 81 / 100) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 31001 / 38200) loss: 2.304522\n",
      "(Iteration 31101 / 38200) loss: 2.304555\n",
      "(Iteration 31201 / 38200) loss: 2.304518\n",
      "(Iteration 31301 / 38200) loss: 2.304556\n",
      "(Epoch 82 / 100) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 31401 / 38200) loss: 2.304513\n",
      "(Iteration 31501 / 38200) loss: 2.304506\n",
      "(Iteration 31601 / 38200) loss: 2.304510\n",
      "(Iteration 31701 / 38200) loss: 2.304527\n",
      "(Epoch 83 / 100) train acc: 0.116000; val_acc: 0.087000\n",
      "(Iteration 31801 / 38200) loss: 2.304558\n",
      "(Iteration 31901 / 38200) loss: 2.304548\n",
      "(Iteration 32001 / 38200) loss: 2.304528\n",
      "(Epoch 84 / 100) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 32101 / 38200) loss: 2.304517\n",
      "(Iteration 32201 / 38200) loss: 2.304523\n",
      "(Iteration 32301 / 38200) loss: 2.304501\n",
      "(Iteration 32401 / 38200) loss: 2.304521\n",
      "(Epoch 85 / 100) train acc: 0.111000; val_acc: 0.087000\n",
      "(Iteration 32501 / 38200) loss: 2.304534\n",
      "(Iteration 32601 / 38200) loss: 2.304557\n",
      "(Iteration 32701 / 38200) loss: 2.304545\n",
      "(Iteration 32801 / 38200) loss: 2.304543\n",
      "(Epoch 86 / 100) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 32901 / 38200) loss: 2.304526\n",
      "(Iteration 33001 / 38200) loss: 2.304468\n",
      "(Iteration 33101 / 38200) loss: 2.304549\n",
      "(Iteration 33201 / 38200) loss: 2.304513\n",
      "(Epoch 87 / 100) train acc: 0.090000; val_acc: 0.087000\n",
      "(Iteration 33301 / 38200) loss: 2.304539\n",
      "(Iteration 33401 / 38200) loss: 2.304526\n",
      "(Iteration 33501 / 38200) loss: 2.304542\n",
      "(Iteration 33601 / 38200) loss: 2.304522\n",
      "(Epoch 88 / 100) train acc: 0.109000; val_acc: 0.087000\n",
      "(Iteration 33701 / 38200) loss: 2.304542\n",
      "(Iteration 33801 / 38200) loss: 2.304540\n",
      "(Iteration 33901 / 38200) loss: 2.304517\n",
      "(Epoch 89 / 100) train acc: 0.116000; val_acc: 0.087000\n",
      "(Iteration 34001 / 38200) loss: 2.304487\n",
      "(Iteration 34101 / 38200) loss: 2.304518\n",
      "(Iteration 34201 / 38200) loss: 2.304521\n",
      "(Iteration 34301 / 38200) loss: 2.304563\n",
      "(Epoch 90 / 100) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 34401 / 38200) loss: 2.304538\n",
      "(Iteration 34501 / 38200) loss: 2.304553\n",
      "(Iteration 34601 / 38200) loss: 2.304506\n",
      "(Iteration 34701 / 38200) loss: 2.304526\n",
      "(Epoch 91 / 100) train acc: 0.118000; val_acc: 0.087000\n",
      "(Iteration 34801 / 38200) loss: 2.304511\n",
      "(Iteration 34901 / 38200) loss: 2.304544\n",
      "(Iteration 35001 / 38200) loss: 2.304525\n",
      "(Iteration 35101 / 38200) loss: 2.304532\n",
      "(Epoch 92 / 100) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 35201 / 38200) loss: 2.304509\n",
      "(Iteration 35301 / 38200) loss: 2.304541\n",
      "(Iteration 35401 / 38200) loss: 2.304481\n",
      "(Iteration 35501 / 38200) loss: 2.304529\n",
      "(Epoch 93 / 100) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 35601 / 38200) loss: 2.304499\n",
      "(Iteration 35701 / 38200) loss: 2.304534\n",
      "(Iteration 35801 / 38200) loss: 2.304495\n",
      "(Iteration 35901 / 38200) loss: 2.304558\n",
      "(Epoch 94 / 100) train acc: 0.115000; val_acc: 0.087000\n",
      "(Iteration 36001 / 38200) loss: 2.304513\n",
      "(Iteration 36101 / 38200) loss: 2.304535\n",
      "(Iteration 36201 / 38200) loss: 2.304543\n",
      "(Epoch 95 / 100) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 36301 / 38200) loss: 2.304513\n",
      "(Iteration 36401 / 38200) loss: 2.304513\n",
      "(Iteration 36501 / 38200) loss: 2.304483\n",
      "(Iteration 36601 / 38200) loss: 2.304494\n",
      "(Epoch 96 / 100) train acc: 0.087000; val_acc: 0.087000\n",
      "(Iteration 36701 / 38200) loss: 2.304515\n",
      "(Iteration 36801 / 38200) loss: 2.304555\n",
      "(Iteration 36901 / 38200) loss: 2.304495\n",
      "(Iteration 37001 / 38200) loss: 2.304518\n",
      "(Epoch 97 / 100) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 37101 / 38200) loss: 2.304545\n",
      "(Iteration 37201 / 38200) loss: 2.304495\n",
      "(Iteration 37301 / 38200) loss: 2.304502\n",
      "(Iteration 37401 / 38200) loss: 2.304492\n",
      "(Epoch 98 / 100) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 37501 / 38200) loss: 2.304521\n",
      "(Iteration 37601 / 38200) loss: 2.304509\n",
      "(Iteration 37701 / 38200) loss: 2.304520\n",
      "(Iteration 37801 / 38200) loss: 2.304522\n",
      "(Epoch 99 / 100) train acc: 0.121000; val_acc: 0.087000\n",
      "(Iteration 37901 / 38200) loss: 2.304472\n",
      "(Iteration 38001 / 38200) loss: 2.304527\n",
      "(Iteration 38101 / 38200) loss: 2.304536\n",
      "(Epoch 100 / 100) train acc: 0.100000; val_acc: 0.087000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.308330\n",
      "(Epoch 0 / 100) train acc: 0.094000; val_acc: 0.104000\n",
      "(Iteration 101 / 76500) loss: 2.308249\n",
      "(Iteration 201 / 76500) loss: 2.308163\n",
      "(Iteration 301 / 76500) loss: 2.308071\n",
      "(Iteration 401 / 76500) loss: 2.308019\n",
      "(Iteration 501 / 76500) loss: 2.307936\n",
      "(Iteration 601 / 76500) loss: 2.307850\n",
      "(Iteration 701 / 76500) loss: 2.307780\n",
      "(Epoch 1 / 100) train acc: 0.107000; val_acc: 0.108000\n",
      "(Iteration 801 / 76500) loss: 2.307723\n",
      "(Iteration 901 / 76500) loss: 2.307644\n",
      "(Iteration 1001 / 76500) loss: 2.307602\n",
      "(Iteration 1101 / 76500) loss: 2.307546\n",
      "(Iteration 1201 / 76500) loss: 2.307478\n",
      "(Iteration 1301 / 76500) loss: 2.307395\n",
      "(Iteration 1401 / 76500) loss: 2.307353\n",
      "(Iteration 1501 / 76500) loss: 2.307292\n",
      "(Epoch 2 / 100) train acc: 0.089000; val_acc: 0.113000\n",
      "(Iteration 1601 / 76500) loss: 2.307217\n",
      "(Iteration 1701 / 76500) loss: 2.307208\n",
      "(Iteration 1801 / 76500) loss: 2.307125\n",
      "(Iteration 1901 / 76500) loss: 2.307086\n",
      "(Iteration 2001 / 76500) loss: 2.307019\n",
      "(Iteration 2101 / 76500) loss: 2.306962\n",
      "(Iteration 2201 / 76500) loss: 2.306930\n",
      "(Epoch 3 / 100) train acc: 0.105000; val_acc: 0.103000\n",
      "(Iteration 2301 / 76500) loss: 2.306880\n",
      "(Iteration 2401 / 76500) loss: 2.306822\n",
      "(Iteration 2501 / 76500) loss: 2.306787\n",
      "(Iteration 2601 / 76500) loss: 2.306759\n",
      "(Iteration 2701 / 76500) loss: 2.306710\n",
      "(Iteration 2801 / 76500) loss: 2.306670\n",
      "(Iteration 2901 / 76500) loss: 2.306610\n",
      "(Iteration 3001 / 76500) loss: 2.306538\n",
      "(Epoch 4 / 100) train acc: 0.097000; val_acc: 0.092000\n",
      "(Iteration 3101 / 76500) loss: 2.306526\n",
      "(Iteration 3201 / 76500) loss: 2.306490\n",
      "(Iteration 3301 / 76500) loss: 2.306457\n",
      "(Iteration 3401 / 76500) loss: 2.306442\n",
      "(Iteration 3501 / 76500) loss: 2.306384\n",
      "(Iteration 3601 / 76500) loss: 2.306386\n",
      "(Iteration 3701 / 76500) loss: 2.306310\n",
      "(Iteration 3801 / 76500) loss: 2.306266\n",
      "(Epoch 5 / 100) train acc: 0.118000; val_acc: 0.112000\n",
      "(Iteration 3901 / 76500) loss: 2.306250\n",
      "(Iteration 4001 / 76500) loss: 2.306212\n",
      "(Iteration 4101 / 76500) loss: 2.306200\n",
      "(Iteration 4201 / 76500) loss: 2.306195\n",
      "(Iteration 4301 / 76500) loss: 2.306159\n",
      "(Iteration 4401 / 76500) loss: 2.306148\n",
      "(Iteration 4501 / 76500) loss: 2.306089\n",
      "(Epoch 6 / 100) train acc: 0.115000; val_acc: 0.117000\n",
      "(Iteration 4601 / 76500) loss: 2.306038\n",
      "(Iteration 4701 / 76500) loss: 2.306030\n",
      "(Iteration 4801 / 76500) loss: 2.306019\n",
      "(Iteration 4901 / 76500) loss: 2.305980\n",
      "(Iteration 5001 / 76500) loss: 2.305942\n",
      "(Iteration 5101 / 76500) loss: 2.305927\n",
      "(Iteration 5201 / 76500) loss: 2.305929\n",
      "(Iteration 5301 / 76500) loss: 2.305896\n",
      "(Epoch 7 / 100) train acc: 0.155000; val_acc: 0.120000\n",
      "(Iteration 5401 / 76500) loss: 2.305867\n",
      "(Iteration 5501 / 76500) loss: 2.305838\n",
      "(Iteration 5601 / 76500) loss: 2.305803\n",
      "(Iteration 5701 / 76500) loss: 2.305783\n",
      "(Iteration 5801 / 76500) loss: 2.305800\n",
      "(Iteration 5901 / 76500) loss: 2.305738\n",
      "(Iteration 6001 / 76500) loss: 2.305700\n",
      "(Iteration 6101 / 76500) loss: 2.305698\n",
      "(Epoch 8 / 100) train acc: 0.114000; val_acc: 0.100000\n",
      "(Iteration 6201 / 76500) loss: 2.305678\n",
      "(Iteration 6301 / 76500) loss: 2.305697\n",
      "(Iteration 6401 / 76500) loss: 2.305639\n",
      "(Iteration 6501 / 76500) loss: 2.305606\n",
      "(Iteration 6601 / 76500) loss: 2.305620\n",
      "(Iteration 6701 / 76500) loss: 2.305594\n",
      "(Iteration 6801 / 76500) loss: 2.305577\n",
      "(Epoch 9 / 100) train acc: 0.128000; val_acc: 0.100000\n",
      "(Iteration 6901 / 76500) loss: 2.305569\n",
      "(Iteration 7001 / 76500) loss: 2.305528\n",
      "(Iteration 7101 / 76500) loss: 2.305488\n",
      "(Iteration 7201 / 76500) loss: 2.305509\n",
      "(Iteration 7301 / 76500) loss: 2.305471\n",
      "(Iteration 7401 / 76500) loss: 2.305482\n",
      "(Iteration 7501 / 76500) loss: 2.305513\n",
      "(Iteration 7601 / 76500) loss: 2.305414\n",
      "(Epoch 10 / 100) train acc: 0.091000; val_acc: 0.088000\n",
      "(Iteration 7701 / 76500) loss: 2.305449\n",
      "(Iteration 7801 / 76500) loss: 2.305458\n",
      "(Iteration 7901 / 76500) loss: 2.305412\n",
      "(Iteration 8001 / 76500) loss: 2.305353\n",
      "(Iteration 8101 / 76500) loss: 2.305409\n",
      "(Iteration 8201 / 76500) loss: 2.305345\n",
      "(Iteration 8301 / 76500) loss: 2.305297\n",
      "(Iteration 8401 / 76500) loss: 2.305378\n",
      "(Epoch 11 / 100) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 8501 / 76500) loss: 2.305263\n",
      "(Iteration 8601 / 76500) loss: 2.305299\n",
      "(Iteration 8701 / 76500) loss: 2.305299\n",
      "(Iteration 8801 / 76500) loss: 2.305313\n",
      "(Iteration 8901 / 76500) loss: 2.305227\n",
      "(Iteration 9001 / 76500) loss: 2.305252\n",
      "(Iteration 9101 / 76500) loss: 2.305235\n",
      "(Epoch 12 / 100) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 9201 / 76500) loss: 2.305279\n",
      "(Iteration 9301 / 76500) loss: 2.305227\n",
      "(Iteration 9401 / 76500) loss: 2.305143\n",
      "(Iteration 9501 / 76500) loss: 2.305191\n",
      "(Iteration 9601 / 76500) loss: 2.305212\n",
      "(Iteration 9701 / 76500) loss: 2.305159\n",
      "(Iteration 9801 / 76500) loss: 2.305174\n",
      "(Iteration 9901 / 76500) loss: 2.305158\n",
      "(Epoch 13 / 100) train acc: 0.117000; val_acc: 0.087000\n",
      "(Iteration 10001 / 76500) loss: 2.305148\n",
      "(Iteration 10101 / 76500) loss: 2.305138\n",
      "(Iteration 10201 / 76500) loss: 2.305163\n",
      "(Iteration 10301 / 76500) loss: 2.305081\n",
      "(Iteration 10401 / 76500) loss: 2.305181\n",
      "(Iteration 10501 / 76500) loss: 2.305131\n",
      "(Iteration 10601 / 76500) loss: 2.305066\n",
      "(Iteration 10701 / 76500) loss: 2.305080\n",
      "(Epoch 14 / 100) train acc: 0.084000; val_acc: 0.087000\n",
      "(Iteration 10801 / 76500) loss: 2.305161\n",
      "(Iteration 10901 / 76500) loss: 2.305122\n",
      "(Iteration 11001 / 76500) loss: 2.305078\n",
      "(Iteration 11101 / 76500) loss: 2.305078\n",
      "(Iteration 11201 / 76500) loss: 2.305068\n",
      "(Iteration 11301 / 76500) loss: 2.305058\n",
      "(Iteration 11401 / 76500) loss: 2.305021\n",
      "(Epoch 15 / 100) train acc: 0.111000; val_acc: 0.087000\n",
      "(Iteration 11501 / 76500) loss: 2.305045\n",
      "(Iteration 11601 / 76500) loss: 2.305008\n",
      "(Iteration 11701 / 76500) loss: 2.305006\n",
      "(Iteration 11801 / 76500) loss: 2.304974\n",
      "(Iteration 11901 / 76500) loss: 2.304984\n",
      "(Iteration 12001 / 76500) loss: 2.305019\n",
      "(Iteration 12101 / 76500) loss: 2.304954\n",
      "(Iteration 12201 / 76500) loss: 2.304978\n",
      "(Epoch 16 / 100) train acc: 0.110000; val_acc: 0.087000\n",
      "(Iteration 12301 / 76500) loss: 2.304942\n",
      "(Iteration 12401 / 76500) loss: 2.304957\n",
      "(Iteration 12501 / 76500) loss: 2.304987\n",
      "(Iteration 12601 / 76500) loss: 2.304914\n",
      "(Iteration 12701 / 76500) loss: 2.304976\n",
      "(Iteration 12801 / 76500) loss: 2.304927\n",
      "(Iteration 12901 / 76500) loss: 2.304945\n",
      "(Iteration 13001 / 76500) loss: 2.304933\n",
      "(Epoch 17 / 100) train acc: 0.107000; val_acc: 0.087000\n",
      "(Iteration 13101 / 76500) loss: 2.304905\n",
      "(Iteration 13201 / 76500) loss: 2.304889\n",
      "(Iteration 13301 / 76500) loss: 2.304912\n",
      "(Iteration 13401 / 76500) loss: 2.304937\n",
      "(Iteration 13501 / 76500) loss: 2.304911\n",
      "(Iteration 13601 / 76500) loss: 2.304924\n",
      "(Iteration 13701 / 76500) loss: 2.304938\n",
      "(Epoch 18 / 100) train acc: 0.116000; val_acc: 0.087000\n",
      "(Iteration 13801 / 76500) loss: 2.304870\n",
      "(Iteration 13901 / 76500) loss: 2.304871\n",
      "(Iteration 14001 / 76500) loss: 2.304901\n",
      "(Iteration 14101 / 76500) loss: 2.304841\n",
      "(Iteration 14201 / 76500) loss: 2.304862\n",
      "(Iteration 14301 / 76500) loss: 2.304814\n",
      "(Iteration 14401 / 76500) loss: 2.304848\n",
      "(Iteration 14501 / 76500) loss: 2.304880\n",
      "(Epoch 19 / 100) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 14601 / 76500) loss: 2.304811\n",
      "(Iteration 14701 / 76500) loss: 2.304844\n",
      "(Iteration 14801 / 76500) loss: 2.304825\n",
      "(Iteration 14901 / 76500) loss: 2.304876\n",
      "(Iteration 15001 / 76500) loss: 2.304791\n",
      "(Iteration 15101 / 76500) loss: 2.304847\n",
      "(Iteration 15201 / 76500) loss: 2.304791\n",
      "(Epoch 20 / 100) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 15301 / 76500) loss: 2.304863\n",
      "(Iteration 15401 / 76500) loss: 2.304836\n",
      "(Iteration 15501 / 76500) loss: 2.304815\n",
      "(Iteration 15601 / 76500) loss: 2.304822\n",
      "(Iteration 15701 / 76500) loss: 2.304792\n",
      "(Iteration 15801 / 76500) loss: 2.304799\n",
      "(Iteration 15901 / 76500) loss: 2.304776\n",
      "(Iteration 16001 / 76500) loss: 2.304732\n",
      "(Epoch 21 / 100) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 16101 / 76500) loss: 2.304832\n",
      "(Iteration 16201 / 76500) loss: 2.304786\n",
      "(Iteration 16301 / 76500) loss: 2.304827\n",
      "(Iteration 16401 / 76500) loss: 2.304817\n",
      "(Iteration 16501 / 76500) loss: 2.304748\n",
      "(Iteration 16601 / 76500) loss: 2.304725\n",
      "(Iteration 16701 / 76500) loss: 2.304753\n",
      "(Iteration 16801 / 76500) loss: 2.304752\n",
      "(Epoch 22 / 100) train acc: 0.116000; val_acc: 0.087000\n",
      "(Iteration 16901 / 76500) loss: 2.304769\n",
      "(Iteration 17001 / 76500) loss: 2.304777\n",
      "(Iteration 17101 / 76500) loss: 2.304792\n",
      "(Iteration 17201 / 76500) loss: 2.304739\n",
      "(Iteration 17301 / 76500) loss: 2.304740\n",
      "(Iteration 17401 / 76500) loss: 2.304779\n",
      "(Iteration 17501 / 76500) loss: 2.304765\n",
      "(Epoch 23 / 100) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 17601 / 76500) loss: 2.304769\n",
      "(Iteration 17701 / 76500) loss: 2.304707\n",
      "(Iteration 17801 / 76500) loss: 2.304744\n",
      "(Iteration 17901 / 76500) loss: 2.304718\n",
      "(Iteration 18001 / 76500) loss: 2.304772\n",
      "(Iteration 18101 / 76500) loss: 2.304804\n",
      "(Iteration 18201 / 76500) loss: 2.304746\n",
      "(Iteration 18301 / 76500) loss: 2.304760\n",
      "(Epoch 24 / 100) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 18401 / 76500) loss: 2.304702\n",
      "(Iteration 18501 / 76500) loss: 2.304721\n",
      "(Iteration 18601 / 76500) loss: 2.304731\n",
      "(Iteration 18701 / 76500) loss: 2.304714\n",
      "(Iteration 18801 / 76500) loss: 2.304732\n",
      "(Iteration 18901 / 76500) loss: 2.304696\n",
      "(Iteration 19001 / 76500) loss: 2.304688\n",
      "(Iteration 19101 / 76500) loss: 2.304670\n",
      "(Epoch 25 / 100) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 19201 / 76500) loss: 2.304718\n",
      "(Iteration 19301 / 76500) loss: 2.304625\n",
      "(Iteration 19401 / 76500) loss: 2.304698\n",
      "(Iteration 19501 / 76500) loss: 2.304703\n",
      "(Iteration 19601 / 76500) loss: 2.304737\n",
      "(Iteration 19701 / 76500) loss: 2.304641\n",
      "(Iteration 19801 / 76500) loss: 2.304715\n",
      "(Epoch 26 / 100) train acc: 0.085000; val_acc: 0.087000\n",
      "(Iteration 19901 / 76500) loss: 2.304637\n",
      "(Iteration 20001 / 76500) loss: 2.304708\n",
      "(Iteration 20101 / 76500) loss: 2.304691\n",
      "(Iteration 20201 / 76500) loss: 2.304653\n",
      "(Iteration 20301 / 76500) loss: 2.304699\n",
      "(Iteration 20401 / 76500) loss: 2.304656\n",
      "(Iteration 20501 / 76500) loss: 2.304725\n",
      "(Iteration 20601 / 76500) loss: 2.304725\n",
      "(Epoch 27 / 100) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 20701 / 76500) loss: 2.304716\n",
      "(Iteration 20801 / 76500) loss: 2.304682\n",
      "(Iteration 20901 / 76500) loss: 2.304598\n",
      "(Iteration 21001 / 76500) loss: 2.304692\n",
      "(Iteration 21101 / 76500) loss: 2.304724\n",
      "(Iteration 21201 / 76500) loss: 2.304645\n",
      "(Iteration 21301 / 76500) loss: 2.304628\n",
      "(Iteration 21401 / 76500) loss: 2.304650\n",
      "(Epoch 28 / 100) train acc: 0.114000; val_acc: 0.087000\n",
      "(Iteration 21501 / 76500) loss: 2.304678\n",
      "(Iteration 21601 / 76500) loss: 2.304637\n",
      "(Iteration 21701 / 76500) loss: 2.304621\n",
      "(Iteration 21801 / 76500) loss: 2.304731\n",
      "(Iteration 21901 / 76500) loss: 2.304634\n",
      "(Iteration 22001 / 76500) loss: 2.304646\n",
      "(Iteration 22101 / 76500) loss: 2.304669\n",
      "(Epoch 29 / 100) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 22201 / 76500) loss: 2.304611\n",
      "(Iteration 22301 / 76500) loss: 2.304664\n",
      "(Iteration 22401 / 76500) loss: 2.304612\n",
      "(Iteration 22501 / 76500) loss: 2.304651\n",
      "(Iteration 22601 / 76500) loss: 2.304590\n",
      "(Iteration 22701 / 76500) loss: 2.304637\n",
      "(Iteration 22801 / 76500) loss: 2.304626\n",
      "(Iteration 22901 / 76500) loss: 2.304650\n",
      "(Epoch 30 / 100) train acc: 0.084000; val_acc: 0.087000\n",
      "(Iteration 23001 / 76500) loss: 2.304648\n",
      "(Iteration 23101 / 76500) loss: 2.304663\n",
      "(Iteration 23201 / 76500) loss: 2.304680\n",
      "(Iteration 23301 / 76500) loss: 2.304612\n",
      "(Iteration 23401 / 76500) loss: 2.304602\n",
      "(Iteration 23501 / 76500) loss: 2.304655\n",
      "(Iteration 23601 / 76500) loss: 2.304596\n",
      "(Iteration 23701 / 76500) loss: 2.304572\n",
      "(Epoch 31 / 100) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 23801 / 76500) loss: 2.304633\n",
      "(Iteration 23901 / 76500) loss: 2.304624\n",
      "(Iteration 24001 / 76500) loss: 2.304663\n",
      "(Iteration 24101 / 76500) loss: 2.304593\n",
      "(Iteration 24201 / 76500) loss: 2.304583\n",
      "(Iteration 24301 / 76500) loss: 2.304631\n",
      "(Iteration 24401 / 76500) loss: 2.304585\n",
      "(Epoch 32 / 100) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 24501 / 76500) loss: 2.304663\n",
      "(Iteration 24601 / 76500) loss: 2.304612\n",
      "(Iteration 24701 / 76500) loss: 2.304635\n",
      "(Iteration 24801 / 76500) loss: 2.304617\n",
      "(Iteration 24901 / 76500) loss: 2.304556\n",
      "(Iteration 25001 / 76500) loss: 2.304674\n",
      "(Iteration 25101 / 76500) loss: 2.304606\n",
      "(Iteration 25201 / 76500) loss: 2.304579\n",
      "(Epoch 33 / 100) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 25301 / 76500) loss: 2.304676\n",
      "(Iteration 25401 / 76500) loss: 2.304597\n",
      "(Iteration 25501 / 76500) loss: 2.304601\n",
      "(Iteration 25601 / 76500) loss: 2.304649\n",
      "(Iteration 25701 / 76500) loss: 2.304644\n",
      "(Iteration 25801 / 76500) loss: 2.304627\n",
      "(Iteration 25901 / 76500) loss: 2.304656\n",
      "(Iteration 26001 / 76500) loss: 2.304558\n",
      "(Epoch 34 / 100) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 26101 / 76500) loss: 2.304623\n",
      "(Iteration 26201 / 76500) loss: 2.304588\n",
      "(Iteration 26301 / 76500) loss: 2.304656\n",
      "(Iteration 26401 / 76500) loss: 2.304612\n",
      "(Iteration 26501 / 76500) loss: 2.304612\n",
      "(Iteration 26601 / 76500) loss: 2.304578\n",
      "(Iteration 26701 / 76500) loss: 2.304608\n",
      "(Epoch 35 / 100) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 26801 / 76500) loss: 2.304592\n",
      "(Iteration 26901 / 76500) loss: 2.304590\n",
      "(Iteration 27001 / 76500) loss: 2.304712\n",
      "(Iteration 27101 / 76500) loss: 2.304633\n",
      "(Iteration 27201 / 76500) loss: 2.304647\n",
      "(Iteration 27301 / 76500) loss: 2.304605\n",
      "(Iteration 27401 / 76500) loss: 2.304609\n",
      "(Iteration 27501 / 76500) loss: 2.304632\n",
      "(Epoch 36 / 100) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 27601 / 76500) loss: 2.304548\n",
      "(Iteration 27701 / 76500) loss: 2.304564\n",
      "(Iteration 27801 / 76500) loss: 2.304558\n",
      "(Iteration 27901 / 76500) loss: 2.304620\n",
      "(Iteration 28001 / 76500) loss: 2.304587\n",
      "(Iteration 28101 / 76500) loss: 2.304586\n",
      "(Iteration 28201 / 76500) loss: 2.304625\n",
      "(Iteration 28301 / 76500) loss: 2.304576\n",
      "(Epoch 37 / 100) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 28401 / 76500) loss: 2.304567\n",
      "(Iteration 28501 / 76500) loss: 2.304570\n",
      "(Iteration 28601 / 76500) loss: 2.304615\n",
      "(Iteration 28701 / 76500) loss: 2.304594\n",
      "(Iteration 28801 / 76500) loss: 2.304553\n",
      "(Iteration 28901 / 76500) loss: 2.304538\n",
      "(Iteration 29001 / 76500) loss: 2.304602\n",
      "(Epoch 38 / 100) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 29101 / 76500) loss: 2.304600\n",
      "(Iteration 29201 / 76500) loss: 2.304536\n",
      "(Iteration 29301 / 76500) loss: 2.304610\n",
      "(Iteration 29401 / 76500) loss: 2.304546\n",
      "(Iteration 29501 / 76500) loss: 2.304671\n",
      "(Iteration 29601 / 76500) loss: 2.304596\n",
      "(Iteration 29701 / 76500) loss: 2.304616\n",
      "(Iteration 29801 / 76500) loss: 2.304673\n",
      "(Epoch 39 / 100) train acc: 0.110000; val_acc: 0.087000\n",
      "(Iteration 29901 / 76500) loss: 2.304579\n",
      "(Iteration 30001 / 76500) loss: 2.304605\n",
      "(Iteration 30101 / 76500) loss: 2.304615\n",
      "(Iteration 30201 / 76500) loss: 2.304498\n",
      "(Iteration 30301 / 76500) loss: 2.304633\n",
      "(Iteration 30401 / 76500) loss: 2.304678\n",
      "(Iteration 30501 / 76500) loss: 2.304541\n",
      "(Epoch 40 / 100) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 30601 / 76500) loss: 2.304526\n",
      "(Iteration 30701 / 76500) loss: 2.304559\n",
      "(Iteration 30801 / 76500) loss: 2.304607\n",
      "(Iteration 30901 / 76500) loss: 2.304573\n",
      "(Iteration 31001 / 76500) loss: 2.304567\n",
      "(Iteration 31101 / 76500) loss: 2.304560\n",
      "(Iteration 31201 / 76500) loss: 2.304574\n",
      "(Iteration 31301 / 76500) loss: 2.304573\n",
      "(Epoch 41 / 100) train acc: 0.118000; val_acc: 0.087000\n",
      "(Iteration 31401 / 76500) loss: 2.304593\n",
      "(Iteration 31501 / 76500) loss: 2.304568\n",
      "(Iteration 31601 / 76500) loss: 2.304604\n",
      "(Iteration 31701 / 76500) loss: 2.304558\n",
      "(Iteration 31801 / 76500) loss: 2.304560\n",
      "(Iteration 31901 / 76500) loss: 2.304569\n",
      "(Iteration 32001 / 76500) loss: 2.304625\n",
      "(Iteration 32101 / 76500) loss: 2.304565\n",
      "(Epoch 42 / 100) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 32201 / 76500) loss: 2.304580\n",
      "(Iteration 32301 / 76500) loss: 2.304592\n",
      "(Iteration 32401 / 76500) loss: 2.304519\n",
      "(Iteration 32501 / 76500) loss: 2.304569\n",
      "(Iteration 32601 / 76500) loss: 2.304534\n",
      "(Iteration 32701 / 76500) loss: 2.304567\n",
      "(Iteration 32801 / 76500) loss: 2.304629\n",
      "(Epoch 43 / 100) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 32901 / 76500) loss: 2.304603\n",
      "(Iteration 33001 / 76500) loss: 2.304558\n",
      "(Iteration 33101 / 76500) loss: 2.304606\n",
      "(Iteration 33201 / 76500) loss: 2.304589\n",
      "(Iteration 33301 / 76500) loss: 2.304544\n",
      "(Iteration 33401 / 76500) loss: 2.304620\n",
      "(Iteration 33501 / 76500) loss: 2.304545\n",
      "(Iteration 33601 / 76500) loss: 2.304498\n",
      "(Epoch 44 / 100) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 33701 / 76500) loss: 2.304599\n",
      "(Iteration 33801 / 76500) loss: 2.304524\n",
      "(Iteration 33901 / 76500) loss: 2.304525\n",
      "(Iteration 34001 / 76500) loss: 2.304517\n",
      "(Iteration 34101 / 76500) loss: 2.304611\n",
      "(Iteration 34201 / 76500) loss: 2.304517\n",
      "(Iteration 34301 / 76500) loss: 2.304535\n",
      "(Iteration 34401 / 76500) loss: 2.304546\n",
      "(Epoch 45 / 100) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 34501 / 76500) loss: 2.304509\n",
      "(Iteration 34601 / 76500) loss: 2.304483\n",
      "(Iteration 34701 / 76500) loss: 2.304617\n",
      "(Iteration 34801 / 76500) loss: 2.304626\n",
      "(Iteration 34901 / 76500) loss: 2.304586\n",
      "(Iteration 35001 / 76500) loss: 2.304555\n",
      "(Iteration 35101 / 76500) loss: 2.304581\n",
      "(Epoch 46 / 100) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 35201 / 76500) loss: 2.304553\n",
      "(Iteration 35301 / 76500) loss: 2.304555\n",
      "(Iteration 35401 / 76500) loss: 2.304534\n",
      "(Iteration 35501 / 76500) loss: 2.304568\n",
      "(Iteration 35601 / 76500) loss: 2.304548\n",
      "(Iteration 35701 / 76500) loss: 2.304565\n",
      "(Iteration 35801 / 76500) loss: 2.304543\n",
      "(Iteration 35901 / 76500) loss: 2.304563\n",
      "(Epoch 47 / 100) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 36001 / 76500) loss: 2.304595\n",
      "(Iteration 36101 / 76500) loss: 2.304575\n",
      "(Iteration 36201 / 76500) loss: 2.304550\n",
      "(Iteration 36301 / 76500) loss: 2.304568\n",
      "(Iteration 36401 / 76500) loss: 2.304623\n",
      "(Iteration 36501 / 76500) loss: 2.304581\n",
      "(Iteration 36601 / 76500) loss: 2.304554\n",
      "(Iteration 36701 / 76500) loss: 2.304571\n",
      "(Epoch 48 / 100) train acc: 0.120000; val_acc: 0.087000\n",
      "(Iteration 36801 / 76500) loss: 2.304604\n",
      "(Iteration 36901 / 76500) loss: 2.304565\n",
      "(Iteration 37001 / 76500) loss: 2.304534\n",
      "(Iteration 37101 / 76500) loss: 2.304617\n",
      "(Iteration 37201 / 76500) loss: 2.304582\n",
      "(Iteration 37301 / 76500) loss: 2.304508\n",
      "(Iteration 37401 / 76500) loss: 2.304567\n",
      "(Epoch 49 / 100) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 37501 / 76500) loss: 2.304491\n",
      "(Iteration 37601 / 76500) loss: 2.304560\n",
      "(Iteration 37701 / 76500) loss: 2.304581\n",
      "(Iteration 37801 / 76500) loss: 2.304490\n",
      "(Iteration 37901 / 76500) loss: 2.304530\n",
      "(Iteration 38001 / 76500) loss: 2.304522\n",
      "(Iteration 38101 / 76500) loss: 2.304566\n",
      "(Iteration 38201 / 76500) loss: 2.304552\n",
      "(Epoch 50 / 100) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 38301 / 76500) loss: 2.304582\n",
      "(Iteration 38401 / 76500) loss: 2.304511\n",
      "(Iteration 38501 / 76500) loss: 2.304579\n",
      "(Iteration 38601 / 76500) loss: 2.304519\n",
      "(Iteration 38701 / 76500) loss: 2.304530\n",
      "(Iteration 38801 / 76500) loss: 2.304554\n",
      "(Iteration 38901 / 76500) loss: 2.304571\n",
      "(Iteration 39001 / 76500) loss: 2.304596\n",
      "(Epoch 51 / 100) train acc: 0.117000; val_acc: 0.087000\n",
      "(Iteration 39101 / 76500) loss: 2.304557\n",
      "(Iteration 39201 / 76500) loss: 2.304609\n",
      "(Iteration 39301 / 76500) loss: 2.304567\n",
      "(Iteration 39401 / 76500) loss: 2.304527\n",
      "(Iteration 39501 / 76500) loss: 2.304604\n",
      "(Iteration 39601 / 76500) loss: 2.304585\n",
      "(Iteration 39701 / 76500) loss: 2.304596\n",
      "(Epoch 52 / 100) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 39801 / 76500) loss: 2.304534\n",
      "(Iteration 39901 / 76500) loss: 2.304568\n",
      "(Iteration 40001 / 76500) loss: 2.304523\n",
      "(Iteration 40101 / 76500) loss: 2.304464\n",
      "(Iteration 40201 / 76500) loss: 2.304607\n",
      "(Iteration 40301 / 76500) loss: 2.304522\n",
      "(Iteration 40401 / 76500) loss: 2.304551\n",
      "(Iteration 40501 / 76500) loss: 2.304574\n",
      "(Epoch 53 / 100) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 40601 / 76500) loss: 2.304577\n",
      "(Iteration 40701 / 76500) loss: 2.304598\n",
      "(Iteration 40801 / 76500) loss: 2.304589\n",
      "(Iteration 40901 / 76500) loss: 2.304537\n",
      "(Iteration 41001 / 76500) loss: 2.304572\n",
      "(Iteration 41101 / 76500) loss: 2.304591\n",
      "(Iteration 41201 / 76500) loss: 2.304558\n",
      "(Iteration 41301 / 76500) loss: 2.304470\n",
      "(Epoch 54 / 100) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 41401 / 76500) loss: 2.304515\n",
      "(Iteration 41501 / 76500) loss: 2.304544\n",
      "(Iteration 41601 / 76500) loss: 2.304526\n",
      "(Iteration 41701 / 76500) loss: 2.304566\n",
      "(Iteration 41801 / 76500) loss: 2.304500\n",
      "(Iteration 41901 / 76500) loss: 2.304613\n",
      "(Iteration 42001 / 76500) loss: 2.304552\n",
      "(Epoch 55 / 100) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 42101 / 76500) loss: 2.304552\n",
      "(Iteration 42201 / 76500) loss: 2.304591\n",
      "(Iteration 42301 / 76500) loss: 2.304492\n",
      "(Iteration 42401 / 76500) loss: 2.304493\n",
      "(Iteration 42501 / 76500) loss: 2.304606\n",
      "(Iteration 42601 / 76500) loss: 2.304563\n",
      "(Iteration 42701 / 76500) loss: 2.304560\n",
      "(Iteration 42801 / 76500) loss: 2.304558\n",
      "(Epoch 56 / 100) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 42901 / 76500) loss: 2.304479\n",
      "(Iteration 43001 / 76500) loss: 2.304532\n",
      "(Iteration 43101 / 76500) loss: 2.304572\n",
      "(Iteration 43201 / 76500) loss: 2.304518\n",
      "(Iteration 43301 / 76500) loss: 2.304532\n",
      "(Iteration 43401 / 76500) loss: 2.304495\n",
      "(Iteration 43501 / 76500) loss: 2.304582\n",
      "(Iteration 43601 / 76500) loss: 2.304557\n",
      "(Epoch 57 / 100) train acc: 0.085000; val_acc: 0.087000\n",
      "(Iteration 43701 / 76500) loss: 2.304542\n",
      "(Iteration 43801 / 76500) loss: 2.304521\n",
      "(Iteration 43901 / 76500) loss: 2.304557\n",
      "(Iteration 44001 / 76500) loss: 2.304589\n",
      "(Iteration 44101 / 76500) loss: 2.304485\n",
      "(Iteration 44201 / 76500) loss: 2.304585\n",
      "(Iteration 44301 / 76500) loss: 2.304587\n",
      "(Epoch 58 / 100) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 44401 / 76500) loss: 2.304551\n",
      "(Iteration 44501 / 76500) loss: 2.304513\n",
      "(Iteration 44601 / 76500) loss: 2.304569\n",
      "(Iteration 44701 / 76500) loss: 2.304603\n",
      "(Iteration 44801 / 76500) loss: 2.304597\n",
      "(Iteration 44901 / 76500) loss: 2.304558\n",
      "(Iteration 45001 / 76500) loss: 2.304560\n",
      "(Iteration 45101 / 76500) loss: 2.304565\n",
      "(Epoch 59 / 100) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 45201 / 76500) loss: 2.304528\n",
      "(Iteration 45301 / 76500) loss: 2.304545\n",
      "(Iteration 45401 / 76500) loss: 2.304553\n",
      "(Iteration 45501 / 76500) loss: 2.304515\n",
      "(Iteration 45601 / 76500) loss: 2.304562\n",
      "(Iteration 45701 / 76500) loss: 2.304582\n",
      "(Iteration 45801 / 76500) loss: 2.304567\n",
      "(Epoch 60 / 100) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 45901 / 76500) loss: 2.304544\n",
      "(Iteration 46001 / 76500) loss: 2.304546\n",
      "(Iteration 46101 / 76500) loss: 2.304548\n",
      "(Iteration 46201 / 76500) loss: 2.304574\n",
      "(Iteration 46301 / 76500) loss: 2.304549\n",
      "(Iteration 46401 / 76500) loss: 2.304601\n",
      "(Iteration 46501 / 76500) loss: 2.304563\n",
      "(Iteration 46601 / 76500) loss: 2.304563\n",
      "(Epoch 61 / 100) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 46701 / 76500) loss: 2.304492\n",
      "(Iteration 46801 / 76500) loss: 2.304588\n",
      "(Iteration 46901 / 76500) loss: 2.304512\n",
      "(Iteration 47001 / 76500) loss: 2.304610\n",
      "(Iteration 47101 / 76500) loss: 2.304551\n",
      "(Iteration 47201 / 76500) loss: 2.304522\n",
      "(Iteration 47301 / 76500) loss: 2.304570\n",
      "(Iteration 47401 / 76500) loss: 2.304537\n",
      "(Epoch 62 / 100) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 47501 / 76500) loss: 2.304477\n",
      "(Iteration 47601 / 76500) loss: 2.304591\n",
      "(Iteration 47701 / 76500) loss: 2.304586\n",
      "(Iteration 47801 / 76500) loss: 2.304576\n",
      "(Iteration 47901 / 76500) loss: 2.304571\n",
      "(Iteration 48001 / 76500) loss: 2.304566\n",
      "(Iteration 48101 / 76500) loss: 2.304517\n",
      "(Epoch 63 / 100) train acc: 0.077000; val_acc: 0.087000\n",
      "(Iteration 48201 / 76500) loss: 2.304593\n",
      "(Iteration 48301 / 76500) loss: 2.304595\n",
      "(Iteration 48401 / 76500) loss: 2.304490\n",
      "(Iteration 48501 / 76500) loss: 2.304582\n",
      "(Iteration 48601 / 76500) loss: 2.304568\n",
      "(Iteration 48701 / 76500) loss: 2.304537\n",
      "(Iteration 48801 / 76500) loss: 2.304578\n",
      "(Iteration 48901 / 76500) loss: 2.304542\n",
      "(Epoch 64 / 100) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 49001 / 76500) loss: 2.304529\n",
      "(Iteration 49101 / 76500) loss: 2.304577\n",
      "(Iteration 49201 / 76500) loss: 2.304566\n",
      "(Iteration 49301 / 76500) loss: 2.304594\n",
      "(Iteration 49401 / 76500) loss: 2.304588\n",
      "(Iteration 49501 / 76500) loss: 2.304613\n",
      "(Iteration 49601 / 76500) loss: 2.304541\n",
      "(Iteration 49701 / 76500) loss: 2.304608\n",
      "(Epoch 65 / 100) train acc: 0.116000; val_acc: 0.087000\n",
      "(Iteration 49801 / 76500) loss: 2.304563\n",
      "(Iteration 49901 / 76500) loss: 2.304564\n",
      "(Iteration 50001 / 76500) loss: 2.304547\n",
      "(Iteration 50101 / 76500) loss: 2.304496\n",
      "(Iteration 50201 / 76500) loss: 2.304548\n",
      "(Iteration 50301 / 76500) loss: 2.304579\n",
      "(Iteration 50401 / 76500) loss: 2.304555\n",
      "(Epoch 66 / 100) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 50501 / 76500) loss: 2.304561\n",
      "(Iteration 50601 / 76500) loss: 2.304556\n",
      "(Iteration 50701 / 76500) loss: 2.304553\n",
      "(Iteration 50801 / 76500) loss: 2.304557\n",
      "(Iteration 50901 / 76500) loss: 2.304569\n",
      "(Iteration 51001 / 76500) loss: 2.304547\n",
      "(Iteration 51101 / 76500) loss: 2.304565\n",
      "(Iteration 51201 / 76500) loss: 2.304525\n",
      "(Epoch 67 / 100) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 51301 / 76500) loss: 2.304560\n",
      "(Iteration 51401 / 76500) loss: 2.304550\n",
      "(Iteration 51501 / 76500) loss: 2.304555\n",
      "(Iteration 51601 / 76500) loss: 2.304595\n",
      "(Iteration 51701 / 76500) loss: 2.304566\n",
      "(Iteration 51801 / 76500) loss: 2.304595\n",
      "(Iteration 51901 / 76500) loss: 2.304565\n",
      "(Iteration 52001 / 76500) loss: 2.304554\n",
      "(Epoch 68 / 100) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 52101 / 76500) loss: 2.304613\n",
      "(Iteration 52201 / 76500) loss: 2.304537\n",
      "(Iteration 52301 / 76500) loss: 2.304524\n",
      "(Iteration 52401 / 76500) loss: 2.304518\n",
      "(Iteration 52501 / 76500) loss: 2.304568\n",
      "(Iteration 52601 / 76500) loss: 2.304513\n",
      "(Iteration 52701 / 76500) loss: 2.304577\n",
      "(Epoch 69 / 100) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 52801 / 76500) loss: 2.304554\n",
      "(Iteration 52901 / 76500) loss: 2.304592\n",
      "(Iteration 53001 / 76500) loss: 2.304554\n",
      "(Iteration 53101 / 76500) loss: 2.304538\n",
      "(Iteration 53201 / 76500) loss: 2.304536\n",
      "(Iteration 53301 / 76500) loss: 2.304569\n",
      "(Iteration 53401 / 76500) loss: 2.304580\n",
      "(Iteration 53501 / 76500) loss: 2.304502\n",
      "(Epoch 70 / 100) train acc: 0.116000; val_acc: 0.087000\n",
      "(Iteration 53601 / 76500) loss: 2.304557\n",
      "(Iteration 53701 / 76500) loss: 2.304588\n",
      "(Iteration 53801 / 76500) loss: 2.304523\n",
      "(Iteration 53901 / 76500) loss: 2.304552\n",
      "(Iteration 54001 / 76500) loss: 2.304581\n",
      "(Iteration 54101 / 76500) loss: 2.304560\n",
      "(Iteration 54201 / 76500) loss: 2.304525\n",
      "(Iteration 54301 / 76500) loss: 2.304550\n",
      "(Epoch 71 / 100) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 54401 / 76500) loss: 2.304585\n",
      "(Iteration 54501 / 76500) loss: 2.304542\n",
      "(Iteration 54601 / 76500) loss: 2.304515\n",
      "(Iteration 54701 / 76500) loss: 2.304536\n",
      "(Iteration 54801 / 76500) loss: 2.304570\n",
      "(Iteration 54901 / 76500) loss: 2.304519\n",
      "(Iteration 55001 / 76500) loss: 2.304561\n",
      "(Epoch 72 / 100) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 55101 / 76500) loss: 2.304564\n",
      "(Iteration 55201 / 76500) loss: 2.304598\n",
      "(Iteration 55301 / 76500) loss: 2.304537\n",
      "(Iteration 55401 / 76500) loss: 2.304485\n",
      "(Iteration 55501 / 76500) loss: 2.304573\n",
      "(Iteration 55601 / 76500) loss: 2.304563\n",
      "(Iteration 55701 / 76500) loss: 2.304550\n",
      "(Iteration 55801 / 76500) loss: 2.304546\n",
      "(Epoch 73 / 100) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 55901 / 76500) loss: 2.304549\n",
      "(Iteration 56001 / 76500) loss: 2.304520\n",
      "(Iteration 56101 / 76500) loss: 2.304576\n",
      "(Iteration 56201 / 76500) loss: 2.304549\n",
      "(Iteration 56301 / 76500) loss: 2.304531\n",
      "(Iteration 56401 / 76500) loss: 2.304528\n",
      "(Iteration 56501 / 76500) loss: 2.304515\n",
      "(Iteration 56601 / 76500) loss: 2.304570\n",
      "(Epoch 74 / 100) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 56701 / 76500) loss: 2.304585\n",
      "(Iteration 56801 / 76500) loss: 2.304608\n",
      "(Iteration 56901 / 76500) loss: 2.304530\n",
      "(Iteration 57001 / 76500) loss: 2.304569\n",
      "(Iteration 57101 / 76500) loss: 2.304555\n",
      "(Iteration 57201 / 76500) loss: 2.304532\n",
      "(Iteration 57301 / 76500) loss: 2.304562\n",
      "(Epoch 75 / 100) train acc: 0.086000; val_acc: 0.087000\n",
      "(Iteration 57401 / 76500) loss: 2.304606\n",
      "(Iteration 57501 / 76500) loss: 2.304628\n",
      "(Iteration 57601 / 76500) loss: 2.304520\n",
      "(Iteration 57701 / 76500) loss: 2.304568\n",
      "(Iteration 57801 / 76500) loss: 2.304553\n",
      "(Iteration 57901 / 76500) loss: 2.304578\n",
      "(Iteration 58001 / 76500) loss: 2.304561\n",
      "(Iteration 58101 / 76500) loss: 2.304584\n",
      "(Epoch 76 / 100) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 58201 / 76500) loss: 2.304594\n",
      "(Iteration 58301 / 76500) loss: 2.304540\n",
      "(Iteration 58401 / 76500) loss: 2.304511\n",
      "(Iteration 58501 / 76500) loss: 2.304590\n",
      "(Iteration 58601 / 76500) loss: 2.304548\n",
      "(Iteration 58701 / 76500) loss: 2.304569\n",
      "(Iteration 58801 / 76500) loss: 2.304532\n",
      "(Iteration 58901 / 76500) loss: 2.304534\n",
      "(Epoch 77 / 100) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 59001 / 76500) loss: 2.304497\n",
      "(Iteration 59101 / 76500) loss: 2.304562\n",
      "(Iteration 59201 / 76500) loss: 2.304538\n",
      "(Iteration 59301 / 76500) loss: 2.304486\n",
      "(Iteration 59401 / 76500) loss: 2.304512\n",
      "(Iteration 59501 / 76500) loss: 2.304534\n",
      "(Iteration 59601 / 76500) loss: 2.304488\n",
      "(Epoch 78 / 100) train acc: 0.088000; val_acc: 0.087000\n",
      "(Iteration 59701 / 76500) loss: 2.304561\n",
      "(Iteration 59801 / 76500) loss: 2.304540\n",
      "(Iteration 59901 / 76500) loss: 2.304543\n",
      "(Iteration 60001 / 76500) loss: 2.304586\n",
      "(Iteration 60101 / 76500) loss: 2.304580\n",
      "(Iteration 60201 / 76500) loss: 2.304605\n",
      "(Iteration 60301 / 76500) loss: 2.304536\n",
      "(Iteration 60401 / 76500) loss: 2.304549\n",
      "(Epoch 79 / 100) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 60501 / 76500) loss: 2.304562\n",
      "(Iteration 60601 / 76500) loss: 2.304568\n",
      "(Iteration 60701 / 76500) loss: 2.304594\n",
      "(Iteration 60801 / 76500) loss: 2.304581\n",
      "(Iteration 60901 / 76500) loss: 2.304526\n",
      "(Iteration 61001 / 76500) loss: 2.304568\n",
      "(Iteration 61101 / 76500) loss: 2.304578\n",
      "(Epoch 80 / 100) train acc: 0.092000; val_acc: 0.087000\n",
      "(Iteration 61201 / 76500) loss: 2.304528\n",
      "(Iteration 61301 / 76500) loss: 2.304561\n",
      "(Iteration 61401 / 76500) loss: 2.304535\n",
      "(Iteration 61501 / 76500) loss: 2.304507\n",
      "(Iteration 61601 / 76500) loss: 2.304582\n",
      "(Iteration 61701 / 76500) loss: 2.304538\n",
      "(Iteration 61801 / 76500) loss: 2.304525\n",
      "(Iteration 61901 / 76500) loss: 2.304530\n",
      "(Epoch 81 / 100) train acc: 0.086000; val_acc: 0.087000\n",
      "(Iteration 62001 / 76500) loss: 2.304542\n",
      "(Iteration 62101 / 76500) loss: 2.304572\n",
      "(Iteration 62201 / 76500) loss: 2.304532\n",
      "(Iteration 62301 / 76500) loss: 2.304569\n",
      "(Iteration 62401 / 76500) loss: 2.304584\n",
      "(Iteration 62501 / 76500) loss: 2.304479\n",
      "(Iteration 62601 / 76500) loss: 2.304573\n",
      "(Iteration 62701 / 76500) loss: 2.304527\n",
      "(Epoch 82 / 100) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 62801 / 76500) loss: 2.304574\n",
      "(Iteration 62901 / 76500) loss: 2.304495\n",
      "(Iteration 63001 / 76500) loss: 2.304587\n",
      "(Iteration 63101 / 76500) loss: 2.304570\n",
      "(Iteration 63201 / 76500) loss: 2.304531\n",
      "(Iteration 63301 / 76500) loss: 2.304511\n",
      "(Iteration 63401 / 76500) loss: 2.304562\n",
      "(Epoch 83 / 100) train acc: 0.116000; val_acc: 0.087000\n",
      "(Iteration 63501 / 76500) loss: 2.304510\n",
      "(Iteration 63601 / 76500) loss: 2.304578\n",
      "(Iteration 63701 / 76500) loss: 2.304544\n",
      "(Iteration 63801 / 76500) loss: 2.304490\n",
      "(Iteration 63901 / 76500) loss: 2.304518\n",
      "(Iteration 64001 / 76500) loss: 2.304607\n",
      "(Iteration 64101 / 76500) loss: 2.304508\n",
      "(Iteration 64201 / 76500) loss: 2.304542\n",
      "(Epoch 84 / 100) train acc: 0.088000; val_acc: 0.087000\n",
      "(Iteration 64301 / 76500) loss: 2.304507\n",
      "(Iteration 64401 / 76500) loss: 2.304582\n",
      "(Iteration 64501 / 76500) loss: 2.304520\n",
      "(Iteration 64601 / 76500) loss: 2.304575\n",
      "(Iteration 64701 / 76500) loss: 2.304531\n",
      "(Iteration 64801 / 76500) loss: 2.304581\n",
      "(Iteration 64901 / 76500) loss: 2.304548\n",
      "(Iteration 65001 / 76500) loss: 2.304555\n",
      "(Epoch 85 / 100) train acc: 0.109000; val_acc: 0.087000\n",
      "(Iteration 65101 / 76500) loss: 2.304526\n",
      "(Iteration 65201 / 76500) loss: 2.304589\n",
      "(Iteration 65301 / 76500) loss: 2.304558\n",
      "(Iteration 65401 / 76500) loss: 2.304537\n",
      "(Iteration 65501 / 76500) loss: 2.304557\n",
      "(Iteration 65601 / 76500) loss: 2.304535\n",
      "(Iteration 65701 / 76500) loss: 2.304558\n",
      "(Epoch 86 / 100) train acc: 0.115000; val_acc: 0.087000\n",
      "(Iteration 65801 / 76500) loss: 2.304561\n",
      "(Iteration 65901 / 76500) loss: 2.304557\n",
      "(Iteration 66001 / 76500) loss: 2.304570\n",
      "(Iteration 66101 / 76500) loss: 2.304541\n",
      "(Iteration 66201 / 76500) loss: 2.304565\n",
      "(Iteration 66301 / 76500) loss: 2.304508\n",
      "(Iteration 66401 / 76500) loss: 2.304506\n",
      "(Iteration 66501 / 76500) loss: 2.304558\n",
      "(Epoch 87 / 100) train acc: 0.117000; val_acc: 0.087000\n",
      "(Iteration 66601 / 76500) loss: 2.304594\n",
      "(Iteration 66701 / 76500) loss: 2.304546\n",
      "(Iteration 66801 / 76500) loss: 2.304538\n",
      "(Iteration 66901 / 76500) loss: 2.304635\n",
      "(Iteration 67001 / 76500) loss: 2.304544\n",
      "(Iteration 67101 / 76500) loss: 2.304539\n",
      "(Iteration 67201 / 76500) loss: 2.304526\n",
      "(Iteration 67301 / 76500) loss: 2.304549\n",
      "(Epoch 88 / 100) train acc: 0.089000; val_acc: 0.087000\n",
      "(Iteration 67401 / 76500) loss: 2.304508\n",
      "(Iteration 67501 / 76500) loss: 2.304559\n",
      "(Iteration 67601 / 76500) loss: 2.304554\n",
      "(Iteration 67701 / 76500) loss: 2.304527\n",
      "(Iteration 67801 / 76500) loss: 2.304541\n",
      "(Iteration 67901 / 76500) loss: 2.304542\n",
      "(Iteration 68001 / 76500) loss: 2.304524\n",
      "(Epoch 89 / 100) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 68101 / 76500) loss: 2.304562\n",
      "(Iteration 68201 / 76500) loss: 2.304538\n",
      "(Iteration 68301 / 76500) loss: 2.304591\n",
      "(Iteration 68401 / 76500) loss: 2.304559\n",
      "(Iteration 68501 / 76500) loss: 2.304568\n",
      "(Iteration 68601 / 76500) loss: 2.304613\n",
      "(Iteration 68701 / 76500) loss: 2.304562\n",
      "(Iteration 68801 / 76500) loss: 2.304550\n",
      "(Epoch 90 / 100) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 68901 / 76500) loss: 2.304480\n",
      "(Iteration 69001 / 76500) loss: 2.304623\n",
      "(Iteration 69101 / 76500) loss: 2.304517\n",
      "(Iteration 69201 / 76500) loss: 2.304548\n",
      "(Iteration 69301 / 76500) loss: 2.304554\n",
      "(Iteration 69401 / 76500) loss: 2.304479\n",
      "(Iteration 69501 / 76500) loss: 2.304534\n",
      "(Iteration 69601 / 76500) loss: 2.304566\n",
      "(Epoch 91 / 100) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 69701 / 76500) loss: 2.304565\n",
      "(Iteration 69801 / 76500) loss: 2.304523\n",
      "(Iteration 69901 / 76500) loss: 2.304597\n",
      "(Iteration 70001 / 76500) loss: 2.304483\n",
      "(Iteration 70101 / 76500) loss: 2.304541\n",
      "(Iteration 70201 / 76500) loss: 2.304586\n",
      "(Iteration 70301 / 76500) loss: 2.304560\n",
      "(Epoch 92 / 100) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 70401 / 76500) loss: 2.304461\n",
      "(Iteration 70501 / 76500) loss: 2.304568\n",
      "(Iteration 70601 / 76500) loss: 2.304582\n",
      "(Iteration 70701 / 76500) loss: 2.304567\n",
      "(Iteration 70801 / 76500) loss: 2.304548\n",
      "(Iteration 70901 / 76500) loss: 2.304562\n",
      "(Iteration 71001 / 76500) loss: 2.304553\n",
      "(Iteration 71101 / 76500) loss: 2.304567\n",
      "(Epoch 93 / 100) train acc: 0.091000; val_acc: 0.087000\n",
      "(Iteration 71201 / 76500) loss: 2.304567\n",
      "(Iteration 71301 / 76500) loss: 2.304636\n",
      "(Iteration 71401 / 76500) loss: 2.304557\n",
      "(Iteration 71501 / 76500) loss: 2.304619\n",
      "(Iteration 71601 / 76500) loss: 2.304475\n",
      "(Iteration 71701 / 76500) loss: 2.304494\n",
      "(Iteration 71801 / 76500) loss: 2.304550\n",
      "(Iteration 71901 / 76500) loss: 2.304536\n",
      "(Epoch 94 / 100) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 72001 / 76500) loss: 2.304549\n",
      "(Iteration 72101 / 76500) loss: 2.304562\n",
      "(Iteration 72201 / 76500) loss: 2.304553\n",
      "(Iteration 72301 / 76500) loss: 2.304510\n",
      "(Iteration 72401 / 76500) loss: 2.304568\n",
      "(Iteration 72501 / 76500) loss: 2.304583\n",
      "(Iteration 72601 / 76500) loss: 2.304568\n",
      "(Epoch 95 / 100) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 72701 / 76500) loss: 2.304491\n",
      "(Iteration 72801 / 76500) loss: 2.304538\n",
      "(Iteration 72901 / 76500) loss: 2.304528\n",
      "(Iteration 73001 / 76500) loss: 2.304547\n",
      "(Iteration 73101 / 76500) loss: 2.304562\n",
      "(Iteration 73201 / 76500) loss: 2.304529\n",
      "(Iteration 73301 / 76500) loss: 2.304557\n",
      "(Iteration 73401 / 76500) loss: 2.304573\n",
      "(Epoch 96 / 100) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 73501 / 76500) loss: 2.304583\n",
      "(Iteration 73601 / 76500) loss: 2.304507\n",
      "(Iteration 73701 / 76500) loss: 2.304582\n",
      "(Iteration 73801 / 76500) loss: 2.304500\n",
      "(Iteration 73901 / 76500) loss: 2.304601\n",
      "(Iteration 74001 / 76500) loss: 2.304525\n",
      "(Iteration 74101 / 76500) loss: 2.304514\n",
      "(Iteration 74201 / 76500) loss: 2.304517\n",
      "(Epoch 97 / 100) train acc: 0.112000; val_acc: 0.087000\n",
      "(Iteration 74301 / 76500) loss: 2.304551\n",
      "(Iteration 74401 / 76500) loss: 2.304560\n",
      "(Iteration 74501 / 76500) loss: 2.304593\n",
      "(Iteration 74601 / 76500) loss: 2.304544\n",
      "(Iteration 74701 / 76500) loss: 2.304562\n",
      "(Iteration 74801 / 76500) loss: 2.304583\n",
      "(Iteration 74901 / 76500) loss: 2.304600\n",
      "(Epoch 98 / 100) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 75001 / 76500) loss: 2.304554\n",
      "(Iteration 75101 / 76500) loss: 2.304510\n",
      "(Iteration 75201 / 76500) loss: 2.304554\n",
      "(Iteration 75301 / 76500) loss: 2.304531\n",
      "(Iteration 75401 / 76500) loss: 2.304529\n",
      "(Iteration 75501 / 76500) loss: 2.304541\n",
      "(Iteration 75601 / 76500) loss: 2.304561\n",
      "(Iteration 75701 / 76500) loss: 2.304554\n",
      "(Epoch 99 / 100) train acc: 0.114000; val_acc: 0.087000\n",
      "(Iteration 75801 / 76500) loss: 2.304515\n",
      "(Iteration 75901 / 76500) loss: 2.304542\n",
      "(Iteration 76001 / 76500) loss: 2.304549\n",
      "(Iteration 76101 / 76500) loss: 2.304549\n",
      "(Iteration 76201 / 76500) loss: 2.304561\n",
      "(Iteration 76301 / 76500) loss: 2.304480\n",
      "(Iteration 76401 / 76500) loss: 2.304500\n",
      "(Epoch 100 / 100) train acc: 0.100000; val_acc: 0.087000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.9, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.308443\n",
      "(Epoch 0 / 100) train acc: 0.082000; val_acc: 0.090000\n",
      "(Iteration 101 / 38200) loss: 2.308352\n",
      "(Iteration 201 / 38200) loss: 2.308281\n",
      "(Iteration 301 / 38200) loss: 2.308195\n",
      "(Epoch 1 / 100) train acc: 0.119000; val_acc: 0.098000\n",
      "(Iteration 401 / 38200) loss: 2.308109\n",
      "(Iteration 501 / 38200) loss: 2.308051\n",
      "(Iteration 601 / 38200) loss: 2.307988\n",
      "(Iteration 701 / 38200) loss: 2.307912\n",
      "(Epoch 2 / 100) train acc: 0.125000; val_acc: 0.107000\n",
      "(Iteration 801 / 38200) loss: 2.307847\n",
      "(Iteration 901 / 38200) loss: 2.307794\n",
      "(Iteration 1001 / 38200) loss: 2.307726\n",
      "(Iteration 1101 / 38200) loss: 2.307658\n",
      "(Epoch 3 / 100) train acc: 0.131000; val_acc: 0.123000\n",
      "(Iteration 1201 / 38200) loss: 2.307606\n",
      "(Iteration 1301 / 38200) loss: 2.307565\n",
      "(Iteration 1401 / 38200) loss: 2.307526\n",
      "(Iteration 1501 / 38200) loss: 2.307456\n",
      "(Epoch 4 / 100) train acc: 0.131000; val_acc: 0.130000\n",
      "(Iteration 1601 / 38200) loss: 2.307419\n",
      "(Iteration 1701 / 38200) loss: 2.307400\n",
      "(Iteration 1801 / 38200) loss: 2.307333\n",
      "(Iteration 1901 / 38200) loss: 2.307280\n",
      "(Epoch 5 / 100) train acc: 0.148000; val_acc: 0.126000\n",
      "(Iteration 2001 / 38200) loss: 2.307244\n",
      "(Iteration 2101 / 38200) loss: 2.307208\n",
      "(Iteration 2201 / 38200) loss: 2.307180\n",
      "(Epoch 6 / 100) train acc: 0.129000; val_acc: 0.118000\n",
      "(Iteration 2301 / 38200) loss: 2.307134\n",
      "(Iteration 2401 / 38200) loss: 2.307086\n",
      "(Iteration 2501 / 38200) loss: 2.307065\n",
      "(Iteration 2601 / 38200) loss: 2.307038\n",
      "(Epoch 7 / 100) train acc: 0.118000; val_acc: 0.117000\n",
      "(Iteration 2701 / 38200) loss: 2.307005\n",
      "(Iteration 2801 / 38200) loss: 2.306991\n",
      "(Iteration 2901 / 38200) loss: 2.306941\n",
      "(Iteration 3001 / 38200) loss: 2.306914\n",
      "(Epoch 8 / 100) train acc: 0.100000; val_acc: 0.098000\n",
      "(Iteration 3101 / 38200) loss: 2.306900\n",
      "(Iteration 3201 / 38200) loss: 2.306884\n",
      "(Iteration 3301 / 38200) loss: 2.306846\n",
      "(Iteration 3401 / 38200) loss: 2.306805\n",
      "(Epoch 9 / 100) train acc: 0.108000; val_acc: 0.104000\n",
      "(Iteration 3501 / 38200) loss: 2.306787\n",
      "(Iteration 3601 / 38200) loss: 2.306760\n",
      "(Iteration 3701 / 38200) loss: 2.306725\n",
      "(Iteration 3801 / 38200) loss: 2.306715\n",
      "(Epoch 10 / 100) train acc: 0.124000; val_acc: 0.094000\n",
      "(Iteration 3901 / 38200) loss: 2.306685\n",
      "(Iteration 4001 / 38200) loss: 2.306668\n",
      "(Iteration 4101 / 38200) loss: 2.306665\n",
      "(Iteration 4201 / 38200) loss: 2.306616\n",
      "(Epoch 11 / 100) train acc: 0.133000; val_acc: 0.095000\n",
      "(Iteration 4301 / 38200) loss: 2.306591\n",
      "(Iteration 4401 / 38200) loss: 2.306607\n",
      "(Iteration 4501 / 38200) loss: 2.306555\n",
      "(Epoch 12 / 100) train acc: 0.120000; val_acc: 0.086000\n",
      "(Iteration 4601 / 38200) loss: 2.306557\n",
      "(Iteration 4701 / 38200) loss: 2.306551\n",
      "(Iteration 4801 / 38200) loss: 2.306523\n",
      "(Iteration 4901 / 38200) loss: 2.306528\n",
      "(Epoch 13 / 100) train acc: 0.099000; val_acc: 0.077000\n",
      "(Iteration 5001 / 38200) loss: 2.306505\n",
      "(Iteration 5101 / 38200) loss: 2.306482\n",
      "(Iteration 5201 / 38200) loss: 2.306472\n",
      "(Iteration 5301 / 38200) loss: 2.306476\n",
      "(Epoch 14 / 100) train acc: 0.121000; val_acc: 0.089000\n",
      "(Iteration 5401 / 38200) loss: 2.306447\n",
      "(Iteration 5501 / 38200) loss: 2.306436\n",
      "(Iteration 5601 / 38200) loss: 2.306429\n",
      "(Iteration 5701 / 38200) loss: 2.306383\n",
      "(Epoch 15 / 100) train acc: 0.091000; val_acc: 0.096000\n",
      "(Iteration 5801 / 38200) loss: 2.306374\n",
      "(Iteration 5901 / 38200) loss: 2.306382\n",
      "(Iteration 6001 / 38200) loss: 2.306362\n",
      "(Iteration 6101 / 38200) loss: 2.306370\n",
      "(Epoch 16 / 100) train acc: 0.124000; val_acc: 0.097000\n",
      "(Iteration 6201 / 38200) loss: 2.306364\n",
      "(Iteration 6301 / 38200) loss: 2.306330\n",
      "(Iteration 6401 / 38200) loss: 2.306367\n",
      "(Epoch 17 / 100) train acc: 0.111000; val_acc: 0.098000\n",
      "(Iteration 6501 / 38200) loss: 2.306327\n",
      "(Iteration 6601 / 38200) loss: 2.306318\n",
      "(Iteration 6701 / 38200) loss: 2.306328\n",
      "(Iteration 6801 / 38200) loss: 2.306321\n",
      "(Epoch 18 / 100) train acc: 0.117000; val_acc: 0.097000\n",
      "(Iteration 6901 / 38200) loss: 2.306281\n",
      "(Iteration 7001 / 38200) loss: 2.306287\n",
      "(Iteration 7101 / 38200) loss: 2.306246\n",
      "(Iteration 7201 / 38200) loss: 2.306254\n",
      "(Epoch 19 / 100) train acc: 0.115000; val_acc: 0.082000\n",
      "(Iteration 7301 / 38200) loss: 2.306262\n",
      "(Iteration 7401 / 38200) loss: 2.306258\n",
      "(Iteration 7501 / 38200) loss: 2.306254\n",
      "(Iteration 7601 / 38200) loss: 2.306231\n",
      "(Epoch 20 / 100) train acc: 0.112000; val_acc: 0.082000\n",
      "(Iteration 7701 / 38200) loss: 2.306243\n",
      "(Iteration 7801 / 38200) loss: 2.306224\n",
      "(Iteration 7901 / 38200) loss: 2.306219\n",
      "(Iteration 8001 / 38200) loss: 2.306232\n",
      "(Epoch 21 / 100) train acc: 0.106000; val_acc: 0.082000\n",
      "(Iteration 8101 / 38200) loss: 2.306210\n",
      "(Iteration 8201 / 38200) loss: 2.306195\n",
      "(Iteration 8301 / 38200) loss: 2.306206\n",
      "(Iteration 8401 / 38200) loss: 2.306206\n",
      "(Epoch 22 / 100) train acc: 0.125000; val_acc: 0.082000\n",
      "(Iteration 8501 / 38200) loss: 2.306198\n",
      "(Iteration 8601 / 38200) loss: 2.306185\n",
      "(Iteration 8701 / 38200) loss: 2.306185\n",
      "(Epoch 23 / 100) train acc: 0.099000; val_acc: 0.081000\n",
      "(Iteration 8801 / 38200) loss: 2.306143\n",
      "(Iteration 8901 / 38200) loss: 2.306172\n",
      "(Iteration 9001 / 38200) loss: 2.306191\n",
      "(Iteration 9101 / 38200) loss: 2.306158\n",
      "(Epoch 24 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 9201 / 38200) loss: 2.306174\n",
      "(Iteration 9301 / 38200) loss: 2.306157\n",
      "(Iteration 9401 / 38200) loss: 2.306141\n",
      "(Iteration 9501 / 38200) loss: 2.306144\n",
      "(Epoch 25 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 9601 / 38200) loss: 2.306151\n",
      "(Iteration 9701 / 38200) loss: 2.306123\n",
      "(Iteration 9801 / 38200) loss: 2.306131\n",
      "(Iteration 9901 / 38200) loss: 2.306147\n",
      "(Epoch 26 / 100) train acc: 0.112000; val_acc: 0.079000\n",
      "(Iteration 10001 / 38200) loss: 2.306125\n",
      "(Iteration 10101 / 38200) loss: 2.306155\n",
      "(Iteration 10201 / 38200) loss: 2.306113\n",
      "(Iteration 10301 / 38200) loss: 2.306101\n",
      "(Epoch 27 / 100) train acc: 0.127000; val_acc: 0.079000\n",
      "(Iteration 10401 / 38200) loss: 2.306096\n",
      "(Iteration 10501 / 38200) loss: 2.306104\n",
      "(Iteration 10601 / 38200) loss: 2.306098\n",
      "(Epoch 28 / 100) train acc: 0.115000; val_acc: 0.079000\n",
      "(Iteration 10701 / 38200) loss: 2.306087\n",
      "(Iteration 10801 / 38200) loss: 2.306100\n",
      "(Iteration 10901 / 38200) loss: 2.306113\n",
      "(Iteration 11001 / 38200) loss: 2.306093\n",
      "(Epoch 29 / 100) train acc: 0.124000; val_acc: 0.079000\n",
      "(Iteration 11101 / 38200) loss: 2.306101\n",
      "(Iteration 11201 / 38200) loss: 2.306094\n",
      "(Iteration 11301 / 38200) loss: 2.306106\n",
      "(Iteration 11401 / 38200) loss: 2.306079\n",
      "(Epoch 30 / 100) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 11501 / 38200) loss: 2.306094\n",
      "(Iteration 11601 / 38200) loss: 2.306064\n",
      "(Iteration 11701 / 38200) loss: 2.306083\n",
      "(Iteration 11801 / 38200) loss: 2.306068\n",
      "(Epoch 31 / 100) train acc: 0.086000; val_acc: 0.080000\n",
      "(Iteration 11901 / 38200) loss: 2.306098\n",
      "(Iteration 12001 / 38200) loss: 2.306066\n",
      "(Iteration 12101 / 38200) loss: 2.306097\n",
      "(Iteration 12201 / 38200) loss: 2.306077\n",
      "(Epoch 32 / 100) train acc: 0.113000; val_acc: 0.079000\n",
      "(Iteration 12301 / 38200) loss: 2.306075\n",
      "(Iteration 12401 / 38200) loss: 2.306055\n",
      "(Iteration 12501 / 38200) loss: 2.306062\n",
      "(Iteration 12601 / 38200) loss: 2.306074\n",
      "(Epoch 33 / 100) train acc: 0.098000; val_acc: 0.079000\n",
      "(Iteration 12701 / 38200) loss: 2.306061\n",
      "(Iteration 12801 / 38200) loss: 2.306058\n",
      "(Iteration 12901 / 38200) loss: 2.306042\n",
      "(Epoch 34 / 100) train acc: 0.102000; val_acc: 0.079000\n",
      "(Iteration 13001 / 38200) loss: 2.306027\n",
      "(Iteration 13101 / 38200) loss: 2.306047\n",
      "(Iteration 13201 / 38200) loss: 2.306052\n",
      "(Iteration 13301 / 38200) loss: 2.306040\n",
      "(Epoch 35 / 100) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 13401 / 38200) loss: 2.306069\n",
      "(Iteration 13501 / 38200) loss: 2.306076\n",
      "(Iteration 13601 / 38200) loss: 2.306052\n",
      "(Iteration 13701 / 38200) loss: 2.306058\n",
      "(Epoch 36 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 13801 / 38200) loss: 2.306056\n",
      "(Iteration 13901 / 38200) loss: 2.306037\n",
      "(Iteration 14001 / 38200) loss: 2.306035\n",
      "(Iteration 14101 / 38200) loss: 2.306062\n",
      "(Epoch 37 / 100) train acc: 0.135000; val_acc: 0.078000\n",
      "(Iteration 14201 / 38200) loss: 2.306025\n",
      "(Iteration 14301 / 38200) loss: 2.306035\n",
      "(Iteration 14401 / 38200) loss: 2.306030\n",
      "(Iteration 14501 / 38200) loss: 2.306026\n",
      "(Epoch 38 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 14601 / 38200) loss: 2.306046\n",
      "(Iteration 14701 / 38200) loss: 2.306049\n",
      "(Iteration 14801 / 38200) loss: 2.306053\n",
      "(Epoch 39 / 100) train acc: 0.124000; val_acc: 0.078000\n",
      "(Iteration 14901 / 38200) loss: 2.306042\n",
      "(Iteration 15001 / 38200) loss: 2.306038\n",
      "(Iteration 15101 / 38200) loss: 2.306040\n",
      "(Iteration 15201 / 38200) loss: 2.306037\n",
      "(Epoch 40 / 100) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 15301 / 38200) loss: 2.306020\n",
      "(Iteration 15401 / 38200) loss: 2.306029\n",
      "(Iteration 15501 / 38200) loss: 2.306050\n",
      "(Iteration 15601 / 38200) loss: 2.306019\n",
      "(Epoch 41 / 100) train acc: 0.118000; val_acc: 0.079000\n",
      "(Iteration 15701 / 38200) loss: 2.306035\n",
      "(Iteration 15801 / 38200) loss: 2.306029\n",
      "(Iteration 15901 / 38200) loss: 2.306019\n",
      "(Iteration 16001 / 38200) loss: 2.306022\n",
      "(Epoch 42 / 100) train acc: 0.106000; val_acc: 0.079000\n",
      "(Iteration 16101 / 38200) loss: 2.306031\n",
      "(Iteration 16201 / 38200) loss: 2.306024\n",
      "(Iteration 16301 / 38200) loss: 2.306035\n",
      "(Iteration 16401 / 38200) loss: 2.306027\n",
      "(Epoch 43 / 100) train acc: 0.104000; val_acc: 0.079000\n",
      "(Iteration 16501 / 38200) loss: 2.305999\n",
      "(Iteration 16601 / 38200) loss: 2.306024\n",
      "(Iteration 16701 / 38200) loss: 2.306008\n",
      "(Iteration 16801 / 38200) loss: 2.306022\n",
      "(Epoch 44 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 16901 / 38200) loss: 2.306048\n",
      "(Iteration 17001 / 38200) loss: 2.306009\n",
      "(Iteration 17101 / 38200) loss: 2.306027\n",
      "(Epoch 45 / 100) train acc: 0.119000; val_acc: 0.078000\n",
      "(Iteration 17201 / 38200) loss: 2.306040\n",
      "(Iteration 17301 / 38200) loss: 2.306046\n",
      "(Iteration 17401 / 38200) loss: 2.306015\n",
      "(Iteration 17501 / 38200) loss: 2.306025\n",
      "(Epoch 46 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 17601 / 38200) loss: 2.306021\n",
      "(Iteration 17701 / 38200) loss: 2.306031\n",
      "(Iteration 17801 / 38200) loss: 2.306018\n",
      "(Iteration 17901 / 38200) loss: 2.306011\n",
      "(Epoch 47 / 100) train acc: 0.112000; val_acc: 0.078000\n",
      "(Iteration 18001 / 38200) loss: 2.306025\n",
      "(Iteration 18101 / 38200) loss: 2.306010\n",
      "(Iteration 18201 / 38200) loss: 2.306009\n",
      "(Iteration 18301 / 38200) loss: 2.306021\n",
      "(Epoch 48 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 18401 / 38200) loss: 2.306007\n",
      "(Iteration 18501 / 38200) loss: 2.306012\n",
      "(Iteration 18601 / 38200) loss: 2.306018\n",
      "(Iteration 18701 / 38200) loss: 2.306057\n",
      "(Epoch 49 / 100) train acc: 0.123000; val_acc: 0.078000\n",
      "(Iteration 18801 / 38200) loss: 2.306001\n",
      "(Iteration 18901 / 38200) loss: 2.306025\n",
      "(Iteration 19001 / 38200) loss: 2.306018\n",
      "(Epoch 50 / 100) train acc: 0.114000; val_acc: 0.078000\n",
      "(Iteration 19101 / 38200) loss: 2.306008\n",
      "(Iteration 19201 / 38200) loss: 2.305999\n",
      "(Iteration 19301 / 38200) loss: 2.306000\n",
      "(Iteration 19401 / 38200) loss: 2.306004\n",
      "(Epoch 51 / 100) train acc: 0.114000; val_acc: 0.078000\n",
      "(Iteration 19501 / 38200) loss: 2.306019\n",
      "(Iteration 19601 / 38200) loss: 2.306005\n",
      "(Iteration 19701 / 38200) loss: 2.306017\n",
      "(Iteration 19801 / 38200) loss: 2.306003\n",
      "(Epoch 52 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 19901 / 38200) loss: 2.306009\n",
      "(Iteration 20001 / 38200) loss: 2.306008\n",
      "(Iteration 20101 / 38200) loss: 2.306007\n",
      "(Iteration 20201 / 38200) loss: 2.306042\n",
      "(Epoch 53 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 20301 / 38200) loss: 2.306009\n",
      "(Iteration 20401 / 38200) loss: 2.306023\n",
      "(Iteration 20501 / 38200) loss: 2.306009\n",
      "(Iteration 20601 / 38200) loss: 2.306010\n",
      "(Epoch 54 / 100) train acc: 0.112000; val_acc: 0.078000\n",
      "(Iteration 20701 / 38200) loss: 2.306013\n",
      "(Iteration 20801 / 38200) loss: 2.306031\n",
      "(Iteration 20901 / 38200) loss: 2.306035\n",
      "(Iteration 21001 / 38200) loss: 2.306004\n",
      "(Epoch 55 / 100) train acc: 0.117000; val_acc: 0.078000\n",
      "(Iteration 21101 / 38200) loss: 2.306022\n",
      "(Iteration 21201 / 38200) loss: 2.306018\n",
      "(Iteration 21301 / 38200) loss: 2.306000\n",
      "(Epoch 56 / 100) train acc: 0.121000; val_acc: 0.078000\n",
      "(Iteration 21401 / 38200) loss: 2.306013\n",
      "(Iteration 21501 / 38200) loss: 2.306005\n",
      "(Iteration 21601 / 38200) loss: 2.306020\n",
      "(Iteration 21701 / 38200) loss: 2.306024\n",
      "(Epoch 57 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 21801 / 38200) loss: 2.306016\n",
      "(Iteration 21901 / 38200) loss: 2.306011\n",
      "(Iteration 22001 / 38200) loss: 2.306013\n",
      "(Iteration 22101 / 38200) loss: 2.306016\n",
      "(Epoch 58 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 22201 / 38200) loss: 2.306026\n",
      "(Iteration 22301 / 38200) loss: 2.306006\n",
      "(Iteration 22401 / 38200) loss: 2.306009\n",
      "(Iteration 22501 / 38200) loss: 2.306015\n",
      "(Epoch 59 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 22601 / 38200) loss: 2.306000\n",
      "(Iteration 22701 / 38200) loss: 2.306020\n",
      "(Iteration 22801 / 38200) loss: 2.305997\n",
      "(Iteration 22901 / 38200) loss: 2.306013\n",
      "(Epoch 60 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 23001 / 38200) loss: 2.305981\n",
      "(Iteration 23101 / 38200) loss: 2.306005\n",
      "(Iteration 23201 / 38200) loss: 2.306006\n",
      "(Iteration 23301 / 38200) loss: 2.306008\n",
      "(Epoch 61 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 23401 / 38200) loss: 2.306016\n",
      "(Iteration 23501 / 38200) loss: 2.306009\n",
      "(Iteration 23601 / 38200) loss: 2.306025\n",
      "(Epoch 62 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 23701 / 38200) loss: 2.306035\n",
      "(Iteration 23801 / 38200) loss: 2.306013\n",
      "(Iteration 23901 / 38200) loss: 2.306015\n",
      "(Iteration 24001 / 38200) loss: 2.306013\n",
      "(Epoch 63 / 100) train acc: 0.118000; val_acc: 0.078000\n",
      "(Iteration 24101 / 38200) loss: 2.306024\n",
      "(Iteration 24201 / 38200) loss: 2.306019\n",
      "(Iteration 24301 / 38200) loss: 2.306021\n",
      "(Iteration 24401 / 38200) loss: 2.306003\n",
      "(Epoch 64 / 100) train acc: 0.134000; val_acc: 0.078000\n",
      "(Iteration 24501 / 38200) loss: 2.306009\n",
      "(Iteration 24601 / 38200) loss: 2.306015\n",
      "(Iteration 24701 / 38200) loss: 2.305998\n",
      "(Iteration 24801 / 38200) loss: 2.306017\n",
      "(Epoch 65 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 24901 / 38200) loss: 2.306004\n",
      "(Iteration 25001 / 38200) loss: 2.306018\n",
      "(Iteration 25101 / 38200) loss: 2.306007\n",
      "(Iteration 25201 / 38200) loss: 2.305986\n",
      "(Epoch 66 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 25301 / 38200) loss: 2.306018\n",
      "(Iteration 25401 / 38200) loss: 2.306016\n",
      "(Iteration 25501 / 38200) loss: 2.306005\n",
      "(Epoch 67 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 25601 / 38200) loss: 2.306019\n",
      "(Iteration 25701 / 38200) loss: 2.306036\n",
      "(Iteration 25801 / 38200) loss: 2.305998\n",
      "(Iteration 25901 / 38200) loss: 2.306017\n",
      "(Epoch 68 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 26001 / 38200) loss: 2.306021\n",
      "(Iteration 26101 / 38200) loss: 2.306006\n",
      "(Iteration 26201 / 38200) loss: 2.306000\n",
      "(Iteration 26301 / 38200) loss: 2.306005\n",
      "(Epoch 69 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 26401 / 38200) loss: 2.306019\n",
      "(Iteration 26501 / 38200) loss: 2.306022\n",
      "(Iteration 26601 / 38200) loss: 2.306006\n",
      "(Iteration 26701 / 38200) loss: 2.306026\n",
      "(Epoch 70 / 100) train acc: 0.115000; val_acc: 0.078000\n",
      "(Iteration 26801 / 38200) loss: 2.306004\n",
      "(Iteration 26901 / 38200) loss: 2.305987\n",
      "(Iteration 27001 / 38200) loss: 2.305997\n",
      "(Iteration 27101 / 38200) loss: 2.305991\n",
      "(Epoch 71 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 27201 / 38200) loss: 2.306005\n",
      "(Iteration 27301 / 38200) loss: 2.305993\n",
      "(Iteration 27401 / 38200) loss: 2.305995\n",
      "(Iteration 27501 / 38200) loss: 2.306002\n",
      "(Epoch 72 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 27601 / 38200) loss: 2.305996\n",
      "(Iteration 27701 / 38200) loss: 2.306012\n",
      "(Iteration 27801 / 38200) loss: 2.306005\n",
      "(Epoch 73 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 27901 / 38200) loss: 2.305999\n",
      "(Iteration 28001 / 38200) loss: 2.306001\n",
      "(Iteration 28101 / 38200) loss: 2.305996\n",
      "(Iteration 28201 / 38200) loss: 2.306002\n",
      "(Epoch 74 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 28301 / 38200) loss: 2.306007\n",
      "(Iteration 28401 / 38200) loss: 2.305988\n",
      "(Iteration 28501 / 38200) loss: 2.306011\n",
      "(Iteration 28601 / 38200) loss: 2.306004\n",
      "(Epoch 75 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 28701 / 38200) loss: 2.306010\n",
      "(Iteration 28801 / 38200) loss: 2.306005\n",
      "(Iteration 28901 / 38200) loss: 2.306010\n",
      "(Iteration 29001 / 38200) loss: 2.306002\n",
      "(Epoch 76 / 100) train acc: 0.123000; val_acc: 0.078000\n",
      "(Iteration 29101 / 38200) loss: 2.306007\n",
      "(Iteration 29201 / 38200) loss: 2.306006\n",
      "(Iteration 29301 / 38200) loss: 2.306022\n",
      "(Iteration 29401 / 38200) loss: 2.306021\n",
      "(Epoch 77 / 100) train acc: 0.119000; val_acc: 0.078000\n",
      "(Iteration 29501 / 38200) loss: 2.306012\n",
      "(Iteration 29601 / 38200) loss: 2.306011\n",
      "(Iteration 29701 / 38200) loss: 2.306015\n",
      "(Epoch 78 / 100) train acc: 0.126000; val_acc: 0.078000\n",
      "(Iteration 29801 / 38200) loss: 2.306007\n",
      "(Iteration 29901 / 38200) loss: 2.306002\n",
      "(Iteration 30001 / 38200) loss: 2.306004\n",
      "(Iteration 30101 / 38200) loss: 2.306016\n",
      "(Epoch 79 / 100) train acc: 0.111000; val_acc: 0.078000\n",
      "(Iteration 30201 / 38200) loss: 2.306027\n",
      "(Iteration 30301 / 38200) loss: 2.306002\n",
      "(Iteration 30401 / 38200) loss: 2.306000\n",
      "(Iteration 30501 / 38200) loss: 2.306033\n",
      "(Epoch 80 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 30601 / 38200) loss: 2.306003\n",
      "(Iteration 30701 / 38200) loss: 2.305984\n",
      "(Iteration 30801 / 38200) loss: 2.306016\n",
      "(Iteration 30901 / 38200) loss: 2.306034\n",
      "(Epoch 81 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 31001 / 38200) loss: 2.306007\n",
      "(Iteration 31101 / 38200) loss: 2.305991\n",
      "(Iteration 31201 / 38200) loss: 2.305981\n",
      "(Iteration 31301 / 38200) loss: 2.305997\n",
      "(Epoch 82 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 31401 / 38200) loss: 2.306024\n",
      "(Iteration 31501 / 38200) loss: 2.306017\n",
      "(Iteration 31601 / 38200) loss: 2.305985\n",
      "(Iteration 31701 / 38200) loss: 2.305992\n",
      "(Epoch 83 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 31801 / 38200) loss: 2.306005\n",
      "(Iteration 31901 / 38200) loss: 2.306010\n",
      "(Iteration 32001 / 38200) loss: 2.305987\n",
      "(Epoch 84 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 32101 / 38200) loss: 2.306005\n",
      "(Iteration 32201 / 38200) loss: 2.306006\n",
      "(Iteration 32301 / 38200) loss: 2.305985\n",
      "(Iteration 32401 / 38200) loss: 2.305983\n",
      "(Epoch 85 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 32501 / 38200) loss: 2.306011\n",
      "(Iteration 32601 / 38200) loss: 2.305997\n",
      "(Iteration 32701 / 38200) loss: 2.305998\n",
      "(Iteration 32801 / 38200) loss: 2.306009\n",
      "(Epoch 86 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 32901 / 38200) loss: 2.306009\n",
      "(Iteration 33001 / 38200) loss: 2.306002\n",
      "(Iteration 33101 / 38200) loss: 2.306003\n",
      "(Iteration 33201 / 38200) loss: 2.306000\n",
      "(Epoch 87 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 33301 / 38200) loss: 2.306001\n",
      "(Iteration 33401 / 38200) loss: 2.306003\n",
      "(Iteration 33501 / 38200) loss: 2.306000\n",
      "(Iteration 33601 / 38200) loss: 2.305998\n",
      "(Epoch 88 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 33701 / 38200) loss: 2.306023\n",
      "(Iteration 33801 / 38200) loss: 2.306008\n",
      "(Iteration 33901 / 38200) loss: 2.306013\n",
      "(Epoch 89 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 34001 / 38200) loss: 2.305998\n",
      "(Iteration 34101 / 38200) loss: 2.305987\n",
      "(Iteration 34201 / 38200) loss: 2.306021\n",
      "(Iteration 34301 / 38200) loss: 2.305975\n",
      "(Epoch 90 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 34401 / 38200) loss: 2.305999\n",
      "(Iteration 34501 / 38200) loss: 2.305997\n",
      "(Iteration 34601 / 38200) loss: 2.305992\n",
      "(Iteration 34701 / 38200) loss: 2.305997\n",
      "(Epoch 91 / 100) train acc: 0.131000; val_acc: 0.078000\n",
      "(Iteration 34801 / 38200) loss: 2.306007\n",
      "(Iteration 34901 / 38200) loss: 2.306006\n",
      "(Iteration 35001 / 38200) loss: 2.305994\n",
      "(Iteration 35101 / 38200) loss: 2.306010\n",
      "(Epoch 92 / 100) train acc: 0.118000; val_acc: 0.078000\n",
      "(Iteration 35201 / 38200) loss: 2.306020\n",
      "(Iteration 35301 / 38200) loss: 2.305987\n",
      "(Iteration 35401 / 38200) loss: 2.306015\n",
      "(Iteration 35501 / 38200) loss: 2.306025\n",
      "(Epoch 93 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 35601 / 38200) loss: 2.306006\n",
      "(Iteration 35701 / 38200) loss: 2.306003\n",
      "(Iteration 35801 / 38200) loss: 2.306019\n",
      "(Iteration 35901 / 38200) loss: 2.306015\n",
      "(Epoch 94 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 36001 / 38200) loss: 2.306009\n",
      "(Iteration 36101 / 38200) loss: 2.306012\n",
      "(Iteration 36201 / 38200) loss: 2.306003\n",
      "(Epoch 95 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 36301 / 38200) loss: 2.306007\n",
      "(Iteration 36401 / 38200) loss: 2.305995\n",
      "(Iteration 36501 / 38200) loss: 2.306000\n",
      "(Iteration 36601 / 38200) loss: 2.306024\n",
      "(Epoch 96 / 100) train acc: 0.125000; val_acc: 0.078000\n",
      "(Iteration 36701 / 38200) loss: 2.306000\n",
      "(Iteration 36801 / 38200) loss: 2.306013\n",
      "(Iteration 36901 / 38200) loss: 2.306028\n",
      "(Iteration 37001 / 38200) loss: 2.306012\n",
      "(Epoch 97 / 100) train acc: 0.111000; val_acc: 0.078000\n",
      "(Iteration 37101 / 38200) loss: 2.306014\n",
      "(Iteration 37201 / 38200) loss: 2.305989\n",
      "(Iteration 37301 / 38200) loss: 2.306006\n",
      "(Iteration 37401 / 38200) loss: 2.306003\n",
      "(Epoch 98 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 37501 / 38200) loss: 2.305997\n",
      "(Iteration 37601 / 38200) loss: 2.305999\n",
      "(Iteration 37701 / 38200) loss: 2.306023\n",
      "(Iteration 37801 / 38200) loss: 2.306012\n",
      "(Epoch 99 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 37901 / 38200) loss: 2.305998\n",
      "(Iteration 38001 / 38200) loss: 2.306000\n",
      "(Iteration 38101 / 38200) loss: 2.305997\n",
      "(Epoch 100 / 100) train acc: 0.121000; val_acc: 0.078000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 64}\n",
      "(Iteration 1 / 76500) loss: 2.308303\n",
      "(Epoch 0 / 100) train acc: 0.090000; val_acc: 0.095000\n",
      "(Iteration 101 / 76500) loss: 2.308227\n",
      "(Iteration 201 / 76500) loss: 2.308160\n",
      "(Iteration 301 / 76500) loss: 2.308061\n",
      "(Iteration 401 / 76500) loss: 2.307997\n",
      "(Iteration 501 / 76500) loss: 2.307915\n",
      "(Iteration 601 / 76500) loss: 2.307833\n",
      "(Iteration 701 / 76500) loss: 2.307768\n",
      "(Epoch 1 / 100) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 801 / 76500) loss: 2.307676\n",
      "(Iteration 901 / 76500) loss: 2.307635\n",
      "(Iteration 1001 / 76500) loss: 2.307556\n",
      "(Iteration 1101 / 76500) loss: 2.307493\n",
      "(Iteration 1201 / 76500) loss: 2.307416\n",
      "(Iteration 1301 / 76500) loss: 2.307419\n",
      "(Iteration 1401 / 76500) loss: 2.307340\n",
      "(Iteration 1501 / 76500) loss: 2.307262\n",
      "(Epoch 2 / 100) train acc: 0.092000; val_acc: 0.079000\n",
      "(Iteration 1601 / 76500) loss: 2.307230\n",
      "(Iteration 1701 / 76500) loss: 2.307098\n",
      "(Iteration 1801 / 76500) loss: 2.307096\n",
      "(Iteration 1901 / 76500) loss: 2.307023\n",
      "(Iteration 2001 / 76500) loss: 2.306951\n",
      "(Iteration 2101 / 76500) loss: 2.306931\n",
      "(Iteration 2201 / 76500) loss: 2.306855\n",
      "(Epoch 3 / 100) train acc: 0.080000; val_acc: 0.079000\n",
      "(Iteration 2301 / 76500) loss: 2.306751\n",
      "(Iteration 2401 / 76500) loss: 2.306786\n",
      "(Iteration 2501 / 76500) loss: 2.306677\n",
      "(Iteration 2601 / 76500) loss: 2.306719\n",
      "(Iteration 2701 / 76500) loss: 2.306588\n",
      "(Iteration 2801 / 76500) loss: 2.306481\n",
      "(Iteration 2901 / 76500) loss: 2.306518\n",
      "(Iteration 3001 / 76500) loss: 2.306438\n",
      "(Epoch 4 / 100) train acc: 0.133000; val_acc: 0.103000\n",
      "(Iteration 3101 / 76500) loss: 2.306357\n",
      "(Iteration 3201 / 76500) loss: 2.306360\n",
      "(Iteration 3301 / 76500) loss: 2.306300\n",
      "(Iteration 3401 / 76500) loss: 2.306235\n",
      "(Iteration 3501 / 76500) loss: 2.306291\n",
      "(Iteration 3601 / 76500) loss: 2.306199\n",
      "(Iteration 3701 / 76500) loss: 2.306149\n",
      "(Iteration 3801 / 76500) loss: 2.306120\n",
      "(Epoch 5 / 100) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 3901 / 76500) loss: 2.306081\n",
      "(Iteration 4001 / 76500) loss: 2.306047\n",
      "(Iteration 4101 / 76500) loss: 2.305984\n",
      "(Iteration 4201 / 76500) loss: 2.305947\n",
      "(Iteration 4301 / 76500) loss: 2.305929\n",
      "(Iteration 4401 / 76500) loss: 2.305916\n",
      "(Iteration 4501 / 76500) loss: 2.305835\n",
      "(Epoch 6 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 4601 / 76500) loss: 2.305770\n",
      "(Iteration 4701 / 76500) loss: 2.305734\n",
      "(Iteration 4801 / 76500) loss: 2.305797\n",
      "(Iteration 4901 / 76500) loss: 2.305761\n",
      "(Iteration 5001 / 76500) loss: 2.305745\n",
      "(Iteration 5101 / 76500) loss: 2.305705\n",
      "(Iteration 5201 / 76500) loss: 2.305588\n",
      "(Iteration 5301 / 76500) loss: 2.305534\n",
      "(Epoch 7 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 5401 / 76500) loss: 2.305547\n",
      "(Iteration 5501 / 76500) loss: 2.305534\n",
      "(Iteration 5601 / 76500) loss: 2.305486\n",
      "(Iteration 5701 / 76500) loss: 2.305479\n",
      "(Iteration 5801 / 76500) loss: 2.305385\n",
      "(Iteration 5901 / 76500) loss: 2.305427\n",
      "(Iteration 6001 / 76500) loss: 2.305397\n",
      "(Iteration 6101 / 76500) loss: 2.305362\n",
      "(Epoch 8 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 6201 / 76500) loss: 2.305329\n",
      "(Iteration 6301 / 76500) loss: 2.305307\n",
      "(Iteration 6401 / 76500) loss: 2.305232\n",
      "(Iteration 6501 / 76500) loss: 2.305264\n",
      "(Iteration 6601 / 76500) loss: 2.305244\n",
      "(Iteration 6701 / 76500) loss: 2.305250\n",
      "(Iteration 6801 / 76500) loss: 2.305150\n",
      "(Epoch 9 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 6901 / 76500) loss: 2.305166\n",
      "(Iteration 7001 / 76500) loss: 2.305083\n",
      "(Iteration 7101 / 76500) loss: 2.305128\n",
      "(Iteration 7201 / 76500) loss: 2.305031\n",
      "(Iteration 7301 / 76500) loss: 2.305124\n",
      "(Iteration 7401 / 76500) loss: 2.305068\n",
      "(Iteration 7501 / 76500) loss: 2.304988\n",
      "(Iteration 7601 / 76500) loss: 2.304962\n",
      "(Epoch 10 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 7701 / 76500) loss: 2.304960\n",
      "(Iteration 7801 / 76500) loss: 2.304928\n",
      "(Iteration 7901 / 76500) loss: 2.305023\n",
      "(Iteration 8001 / 76500) loss: 2.304955\n",
      "(Iteration 8101 / 76500) loss: 2.304895\n",
      "(Iteration 8201 / 76500) loss: 2.304908\n",
      "(Iteration 8301 / 76500) loss: 2.304790\n",
      "(Iteration 8401 / 76500) loss: 2.304814\n",
      "(Epoch 11 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 8501 / 76500) loss: 2.304860\n",
      "(Iteration 8601 / 76500) loss: 2.304786\n",
      "(Iteration 8701 / 76500) loss: 2.304805\n",
      "(Iteration 8801 / 76500) loss: 2.304794\n",
      "(Iteration 8901 / 76500) loss: 2.304801\n",
      "(Iteration 9001 / 76500) loss: 2.304778\n",
      "(Iteration 9101 / 76500) loss: 2.304738\n",
      "(Epoch 12 / 100) train acc: 0.109000; val_acc: 0.080000\n",
      "(Iteration 9201 / 76500) loss: 2.304739\n",
      "(Iteration 9301 / 76500) loss: 2.304709\n",
      "(Iteration 9401 / 76500) loss: 2.304755\n",
      "(Iteration 9501 / 76500) loss: 2.304643\n",
      "(Iteration 9601 / 76500) loss: 2.304716\n",
      "(Iteration 9701 / 76500) loss: 2.304580\n",
      "(Iteration 9801 / 76500) loss: 2.304645\n",
      "(Iteration 9901 / 76500) loss: 2.304559\n",
      "(Epoch 13 / 100) train acc: 0.105000; val_acc: 0.079000\n",
      "(Iteration 10001 / 76500) loss: 2.304573\n",
      "(Iteration 10101 / 76500) loss: 2.304635\n",
      "(Iteration 10201 / 76500) loss: 2.304551\n",
      "(Iteration 10301 / 76500) loss: 2.304578\n",
      "(Iteration 10401 / 76500) loss: 2.304393\n",
      "(Iteration 10501 / 76500) loss: 2.304535\n",
      "(Iteration 10601 / 76500) loss: 2.304579\n",
      "(Iteration 10701 / 76500) loss: 2.304525\n",
      "(Epoch 14 / 100) train acc: 0.099000; val_acc: 0.079000\n",
      "(Iteration 10801 / 76500) loss: 2.304459\n",
      "(Iteration 10901 / 76500) loss: 2.304526\n",
      "(Iteration 11001 / 76500) loss: 2.304391\n",
      "(Iteration 11101 / 76500) loss: 2.304411\n",
      "(Iteration 11201 / 76500) loss: 2.304440\n",
      "(Iteration 11301 / 76500) loss: 2.304437\n",
      "(Iteration 11401 / 76500) loss: 2.304373\n",
      "(Epoch 15 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 11501 / 76500) loss: 2.304411\n",
      "(Iteration 11601 / 76500) loss: 2.304360\n",
      "(Iteration 11701 / 76500) loss: 2.304359\n",
      "(Iteration 11801 / 76500) loss: 2.304333\n",
      "(Iteration 11901 / 76500) loss: 2.304410\n",
      "(Iteration 12001 / 76500) loss: 2.304434\n",
      "(Iteration 12101 / 76500) loss: 2.304337\n",
      "(Iteration 12201 / 76500) loss: 2.304232\n",
      "(Epoch 16 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 12301 / 76500) loss: 2.304304\n",
      "(Iteration 12401 / 76500) loss: 2.304280\n",
      "(Iteration 12501 / 76500) loss: 2.304288\n",
      "(Iteration 12601 / 76500) loss: 2.304361\n",
      "(Iteration 12701 / 76500) loss: 2.304344\n",
      "(Iteration 12801 / 76500) loss: 2.304245\n",
      "(Iteration 12901 / 76500) loss: 2.304173\n",
      "(Iteration 13001 / 76500) loss: 2.304328\n",
      "(Epoch 17 / 100) train acc: 0.083000; val_acc: 0.078000\n",
      "(Iteration 13101 / 76500) loss: 2.304195\n",
      "(Iteration 13201 / 76500) loss: 2.304227\n",
      "(Iteration 13301 / 76500) loss: 2.304153\n",
      "(Iteration 13401 / 76500) loss: 2.304257\n",
      "(Iteration 13501 / 76500) loss: 2.304205\n",
      "(Iteration 13601 / 76500) loss: 2.304169\n",
      "(Iteration 13701 / 76500) loss: 2.304147\n",
      "(Epoch 18 / 100) train acc: 0.120000; val_acc: 0.079000\n",
      "(Iteration 13801 / 76500) loss: 2.304087\n",
      "(Iteration 13901 / 76500) loss: 2.304176\n",
      "(Iteration 14001 / 76500) loss: 2.304169\n",
      "(Iteration 14101 / 76500) loss: 2.304123\n",
      "(Iteration 14201 / 76500) loss: 2.304065\n",
      "(Iteration 14301 / 76500) loss: 2.304073\n",
      "(Iteration 14401 / 76500) loss: 2.304080\n",
      "(Iteration 14501 / 76500) loss: 2.304048\n",
      "(Epoch 19 / 100) train acc: 0.110000; val_acc: 0.079000\n",
      "(Iteration 14601 / 76500) loss: 2.304104\n",
      "(Iteration 14701 / 76500) loss: 2.304041\n",
      "(Iteration 14801 / 76500) loss: 2.304101\n",
      "(Iteration 14901 / 76500) loss: 2.304074\n",
      "(Iteration 15001 / 76500) loss: 2.303971\n",
      "(Iteration 15101 / 76500) loss: 2.304048\n",
      "(Iteration 15201 / 76500) loss: 2.303911\n",
      "(Epoch 20 / 100) train acc: 0.114000; val_acc: 0.078000\n",
      "(Iteration 15301 / 76500) loss: 2.304014\n",
      "(Iteration 15401 / 76500) loss: 2.304018\n",
      "(Iteration 15501 / 76500) loss: 2.303919\n",
      "(Iteration 15601 / 76500) loss: 2.304060\n",
      "(Iteration 15701 / 76500) loss: 2.304063\n",
      "(Iteration 15801 / 76500) loss: 2.304033\n",
      "(Iteration 15901 / 76500) loss: 2.304004\n",
      "(Iteration 16001 / 76500) loss: 2.304035\n",
      "(Epoch 21 / 100) train acc: 0.086000; val_acc: 0.078000\n",
      "(Iteration 16101 / 76500) loss: 2.304025\n",
      "(Iteration 16201 / 76500) loss: 2.303966\n",
      "(Iteration 16301 / 76500) loss: 2.303982\n",
      "(Iteration 16401 / 76500) loss: 2.303918\n",
      "(Iteration 16501 / 76500) loss: 2.303931\n",
      "(Iteration 16601 / 76500) loss: 2.303929\n",
      "(Iteration 16701 / 76500) loss: 2.303913\n",
      "(Iteration 16801 / 76500) loss: 2.303985\n",
      "(Epoch 22 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 16901 / 76500) loss: 2.303946\n",
      "(Iteration 17001 / 76500) loss: 2.303842\n",
      "(Iteration 17101 / 76500) loss: 2.303939\n",
      "(Iteration 17201 / 76500) loss: 2.303873\n",
      "(Iteration 17301 / 76500) loss: 2.303911\n",
      "(Iteration 17401 / 76500) loss: 2.303844\n",
      "(Iteration 17501 / 76500) loss: 2.303941\n",
      "(Epoch 23 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 17601 / 76500) loss: 2.303924\n",
      "(Iteration 17701 / 76500) loss: 2.303909\n",
      "(Iteration 17801 / 76500) loss: 2.303907\n",
      "(Iteration 17901 / 76500) loss: 2.303895\n",
      "(Iteration 18001 / 76500) loss: 2.303829\n",
      "(Iteration 18101 / 76500) loss: 2.303815\n",
      "(Iteration 18201 / 76500) loss: 2.303880\n",
      "(Iteration 18301 / 76500) loss: 2.303838\n",
      "(Epoch 24 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 18401 / 76500) loss: 2.303887\n",
      "(Iteration 18501 / 76500) loss: 2.303831\n",
      "(Iteration 18601 / 76500) loss: 2.303897\n",
      "(Iteration 18701 / 76500) loss: 2.303811\n",
      "(Iteration 18801 / 76500) loss: 2.303789\n",
      "(Iteration 18901 / 76500) loss: 2.303805\n",
      "(Iteration 19001 / 76500) loss: 2.303781\n",
      "(Iteration 19101 / 76500) loss: 2.303751\n",
      "(Epoch 25 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 19201 / 76500) loss: 2.303768\n",
      "(Iteration 19301 / 76500) loss: 2.303849\n",
      "(Iteration 19401 / 76500) loss: 2.303868\n",
      "(Iteration 19501 / 76500) loss: 2.303788\n",
      "(Iteration 19601 / 76500) loss: 2.303748\n",
      "(Iteration 19701 / 76500) loss: 2.303699\n",
      "(Iteration 19801 / 76500) loss: 2.303789\n",
      "(Epoch 26 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 19901 / 76500) loss: 2.303810\n",
      "(Iteration 20001 / 76500) loss: 2.303703\n",
      "(Iteration 20101 / 76500) loss: 2.303774\n",
      "(Iteration 20201 / 76500) loss: 2.303769\n",
      "(Iteration 20301 / 76500) loss: 2.303810\n",
      "(Iteration 20401 / 76500) loss: 2.303708\n",
      "(Iteration 20501 / 76500) loss: 2.303744\n",
      "(Iteration 20601 / 76500) loss: 2.303720\n",
      "(Epoch 27 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 20701 / 76500) loss: 2.303783\n",
      "(Iteration 20801 / 76500) loss: 2.303743\n",
      "(Iteration 20901 / 76500) loss: 2.303684\n",
      "(Iteration 21001 / 76500) loss: 2.303685\n",
      "(Iteration 21101 / 76500) loss: 2.303694\n",
      "(Iteration 21201 / 76500) loss: 2.303697\n",
      "(Iteration 21301 / 76500) loss: 2.303720\n",
      "(Iteration 21401 / 76500) loss: 2.303655\n",
      "(Epoch 28 / 100) train acc: 0.083000; val_acc: 0.078000\n",
      "(Iteration 21501 / 76500) loss: 2.303719\n",
      "(Iteration 21601 / 76500) loss: 2.303657\n",
      "(Iteration 21701 / 76500) loss: 2.303758\n",
      "(Iteration 21801 / 76500) loss: 2.303709\n",
      "(Iteration 21901 / 76500) loss: 2.303695\n",
      "(Iteration 22001 / 76500) loss: 2.303753\n",
      "(Iteration 22101 / 76500) loss: 2.303775\n",
      "(Epoch 29 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 22201 / 76500) loss: 2.303626\n",
      "(Iteration 22301 / 76500) loss: 2.303659\n",
      "(Iteration 22401 / 76500) loss: 2.303656\n",
      "(Iteration 22501 / 76500) loss: 2.303656\n",
      "(Iteration 22601 / 76500) loss: 2.303738\n",
      "(Iteration 22701 / 76500) loss: 2.303613\n",
      "(Iteration 22801 / 76500) loss: 2.303688\n",
      "(Iteration 22901 / 76500) loss: 2.303683\n",
      "(Epoch 30 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 23001 / 76500) loss: 2.303683\n",
      "(Iteration 23101 / 76500) loss: 2.303650\n",
      "(Iteration 23201 / 76500) loss: 2.303661\n",
      "(Iteration 23301 / 76500) loss: 2.303543\n",
      "(Iteration 23401 / 76500) loss: 2.303592\n",
      "(Iteration 23501 / 76500) loss: 2.303574\n",
      "(Iteration 23601 / 76500) loss: 2.303571\n",
      "(Iteration 23701 / 76500) loss: 2.303568\n",
      "(Epoch 31 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 23801 / 76500) loss: 2.303558\n",
      "(Iteration 23901 / 76500) loss: 2.303780\n",
      "(Iteration 24001 / 76500) loss: 2.303578\n",
      "(Iteration 24101 / 76500) loss: 2.303574\n",
      "(Iteration 24201 / 76500) loss: 2.303526\n",
      "(Iteration 24301 / 76500) loss: 2.303619\n",
      "(Iteration 24401 / 76500) loss: 2.303566\n",
      "(Epoch 32 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 24501 / 76500) loss: 2.303648\n",
      "(Iteration 24601 / 76500) loss: 2.303581\n",
      "(Iteration 24701 / 76500) loss: 2.303603\n",
      "(Iteration 24801 / 76500) loss: 2.303594\n",
      "(Iteration 24901 / 76500) loss: 2.303558\n",
      "(Iteration 25001 / 76500) loss: 2.303595\n",
      "(Iteration 25101 / 76500) loss: 2.303633\n",
      "(Iteration 25201 / 76500) loss: 2.303641\n",
      "(Epoch 33 / 100) train acc: 0.111000; val_acc: 0.078000\n",
      "(Iteration 25301 / 76500) loss: 2.303553\n",
      "(Iteration 25401 / 76500) loss: 2.303545\n",
      "(Iteration 25501 / 76500) loss: 2.303550\n",
      "(Iteration 25601 / 76500) loss: 2.303491\n",
      "(Iteration 25701 / 76500) loss: 2.303499\n",
      "(Iteration 25801 / 76500) loss: 2.303575\n",
      "(Iteration 25901 / 76500) loss: 2.303649\n",
      "(Iteration 26001 / 76500) loss: 2.303552\n",
      "(Epoch 34 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 26101 / 76500) loss: 2.303461\n",
      "(Iteration 26201 / 76500) loss: 2.303516\n",
      "(Iteration 26301 / 76500) loss: 2.303587\n",
      "(Iteration 26401 / 76500) loss: 2.303567\n",
      "(Iteration 26501 / 76500) loss: 2.303560\n",
      "(Iteration 26601 / 76500) loss: 2.303508\n",
      "(Iteration 26701 / 76500) loss: 2.303508\n",
      "(Epoch 35 / 100) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 26801 / 76500) loss: 2.303496\n",
      "(Iteration 26901 / 76500) loss: 2.303560\n",
      "(Iteration 27001 / 76500) loss: 2.303469\n",
      "(Iteration 27101 / 76500) loss: 2.303567\n",
      "(Iteration 27201 / 76500) loss: 2.303529\n",
      "(Iteration 27301 / 76500) loss: 2.303508\n",
      "(Iteration 27401 / 76500) loss: 2.303519\n",
      "(Iteration 27501 / 76500) loss: 2.303505\n",
      "(Epoch 36 / 100) train acc: 0.114000; val_acc: 0.078000\n",
      "(Iteration 27601 / 76500) loss: 2.303470\n",
      "(Iteration 27701 / 76500) loss: 2.303564\n",
      "(Iteration 27801 / 76500) loss: 2.303539\n",
      "(Iteration 27901 / 76500) loss: 2.303491\n",
      "(Iteration 28001 / 76500) loss: 2.303486\n",
      "(Iteration 28101 / 76500) loss: 2.303587\n",
      "(Iteration 28201 / 76500) loss: 2.303536\n",
      "(Iteration 28301 / 76500) loss: 2.303579\n",
      "(Epoch 37 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 28401 / 76500) loss: 2.303432\n",
      "(Iteration 28501 / 76500) loss: 2.303479\n",
      "(Iteration 28601 / 76500) loss: 2.303421\n",
      "(Iteration 28701 / 76500) loss: 2.303533\n",
      "(Iteration 28801 / 76500) loss: 2.303557\n",
      "(Iteration 28901 / 76500) loss: 2.303455\n",
      "(Iteration 29001 / 76500) loss: 2.303496\n",
      "(Epoch 38 / 100) train acc: 0.114000; val_acc: 0.078000\n",
      "(Iteration 29101 / 76500) loss: 2.303483\n",
      "(Iteration 29201 / 76500) loss: 2.303469\n",
      "(Iteration 29301 / 76500) loss: 2.303468\n",
      "(Iteration 29401 / 76500) loss: 2.303487\n",
      "(Iteration 29501 / 76500) loss: 2.303492\n",
      "(Iteration 29601 / 76500) loss: 2.303498\n",
      "(Iteration 29701 / 76500) loss: 2.303497\n",
      "(Iteration 29801 / 76500) loss: 2.303513\n",
      "(Epoch 39 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 29901 / 76500) loss: 2.303489\n",
      "(Iteration 30001 / 76500) loss: 2.303525\n",
      "(Iteration 30101 / 76500) loss: 2.303476\n",
      "(Iteration 30201 / 76500) loss: 2.303381\n",
      "(Iteration 30301 / 76500) loss: 2.303383\n",
      "(Iteration 30401 / 76500) loss: 2.303489\n",
      "(Iteration 30501 / 76500) loss: 2.303516\n",
      "(Epoch 40 / 100) train acc: 0.083000; val_acc: 0.078000\n",
      "(Iteration 30601 / 76500) loss: 2.303456\n",
      "(Iteration 30701 / 76500) loss: 2.303469\n",
      "(Iteration 30801 / 76500) loss: 2.303534\n",
      "(Iteration 30901 / 76500) loss: 2.303405\n",
      "(Iteration 31001 / 76500) loss: 2.303419\n",
      "(Iteration 31101 / 76500) loss: 2.303437\n",
      "(Iteration 31201 / 76500) loss: 2.303473\n",
      "(Iteration 31301 / 76500) loss: 2.303420\n",
      "(Epoch 41 / 100) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 31401 / 76500) loss: 2.303398\n",
      "(Iteration 31501 / 76500) loss: 2.303453\n",
      "(Iteration 31601 / 76500) loss: 2.303371\n",
      "(Iteration 31701 / 76500) loss: 2.303442\n",
      "(Iteration 31801 / 76500) loss: 2.303345\n",
      "(Iteration 31901 / 76500) loss: 2.303421\n",
      "(Iteration 32001 / 76500) loss: 2.303510\n",
      "(Iteration 32101 / 76500) loss: 2.303446\n",
      "(Epoch 42 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 32201 / 76500) loss: 2.303383\n",
      "(Iteration 32301 / 76500) loss: 2.303489\n",
      "(Iteration 32401 / 76500) loss: 2.303490\n",
      "(Iteration 32501 / 76500) loss: 2.303545\n",
      "(Iteration 32601 / 76500) loss: 2.303367\n",
      "(Iteration 32701 / 76500) loss: 2.303452\n",
      "(Iteration 32801 / 76500) loss: 2.303371\n",
      "(Epoch 43 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 32901 / 76500) loss: 2.303350\n",
      "(Iteration 33001 / 76500) loss: 2.303431\n",
      "(Iteration 33101 / 76500) loss: 2.303454\n",
      "(Iteration 33201 / 76500) loss: 2.303433\n",
      "(Iteration 33301 / 76500) loss: 2.303363\n",
      "(Iteration 33401 / 76500) loss: 2.303484\n",
      "(Iteration 33501 / 76500) loss: 2.303475\n",
      "(Iteration 33601 / 76500) loss: 2.303357\n",
      "(Epoch 44 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 33701 / 76500) loss: 2.303380\n",
      "(Iteration 33801 / 76500) loss: 2.303439\n",
      "(Iteration 33901 / 76500) loss: 2.303493\n",
      "(Iteration 34001 / 76500) loss: 2.303451\n",
      "(Iteration 34101 / 76500) loss: 2.303459\n",
      "(Iteration 34201 / 76500) loss: 2.303432\n",
      "(Iteration 34301 / 76500) loss: 2.303396\n",
      "(Iteration 34401 / 76500) loss: 2.303430\n",
      "(Epoch 45 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 34501 / 76500) loss: 2.303415\n",
      "(Iteration 34601 / 76500) loss: 2.303444\n",
      "(Iteration 34701 / 76500) loss: 2.303381\n",
      "(Iteration 34801 / 76500) loss: 2.303448\n",
      "(Iteration 34901 / 76500) loss: 2.303386\n",
      "(Iteration 35001 / 76500) loss: 2.303421\n",
      "(Iteration 35101 / 76500) loss: 2.303464\n",
      "(Epoch 46 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 35201 / 76500) loss: 2.303393\n",
      "(Iteration 35301 / 76500) loss: 2.303458\n",
      "(Iteration 35401 / 76500) loss: 2.303407\n",
      "(Iteration 35501 / 76500) loss: 2.303384\n",
      "(Iteration 35601 / 76500) loss: 2.303307\n",
      "(Iteration 35701 / 76500) loss: 2.303317\n",
      "(Iteration 35801 / 76500) loss: 2.303356\n",
      "(Iteration 35901 / 76500) loss: 2.303378\n",
      "(Epoch 47 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 36001 / 76500) loss: 2.303432\n",
      "(Iteration 36101 / 76500) loss: 2.303354\n",
      "(Iteration 36201 / 76500) loss: 2.303453\n",
      "(Iteration 36301 / 76500) loss: 2.303250\n",
      "(Iteration 36401 / 76500) loss: 2.303414\n",
      "(Iteration 36501 / 76500) loss: 2.303345\n",
      "(Iteration 36601 / 76500) loss: 2.303293\n",
      "(Iteration 36701 / 76500) loss: 2.303540\n",
      "(Epoch 48 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 36801 / 76500) loss: 2.303354\n",
      "(Iteration 36901 / 76500) loss: 2.303332\n",
      "(Iteration 37001 / 76500) loss: 2.303291\n",
      "(Iteration 37101 / 76500) loss: 2.303406\n",
      "(Iteration 37201 / 76500) loss: 2.303485\n",
      "(Iteration 37301 / 76500) loss: 2.303408\n",
      "(Iteration 37401 / 76500) loss: 2.303427\n",
      "(Epoch 49 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 37501 / 76500) loss: 2.303352\n",
      "(Iteration 37601 / 76500) loss: 2.303274\n",
      "(Iteration 37701 / 76500) loss: 2.303440\n",
      "(Iteration 37801 / 76500) loss: 2.303521\n",
      "(Iteration 37901 / 76500) loss: 2.303509\n",
      "(Iteration 38001 / 76500) loss: 2.303392\n",
      "(Iteration 38101 / 76500) loss: 2.303309\n",
      "(Iteration 38201 / 76500) loss: 2.303446\n",
      "(Epoch 50 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 38301 / 76500) loss: 2.303376\n",
      "(Iteration 38401 / 76500) loss: 2.303194\n",
      "(Iteration 38501 / 76500) loss: 2.303295\n",
      "(Iteration 38601 / 76500) loss: 2.303371\n",
      "(Iteration 38701 / 76500) loss: 2.303386\n",
      "(Iteration 38801 / 76500) loss: 2.303306\n",
      "(Iteration 38901 / 76500) loss: 2.303404\n",
      "(Iteration 39001 / 76500) loss: 2.303375\n",
      "(Epoch 51 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 39101 / 76500) loss: 2.303324\n",
      "(Iteration 39201 / 76500) loss: 2.303374\n",
      "(Iteration 39301 / 76500) loss: 2.303459\n",
      "(Iteration 39401 / 76500) loss: 2.303414\n",
      "(Iteration 39501 / 76500) loss: 2.303382\n",
      "(Iteration 39601 / 76500) loss: 2.303344\n",
      "(Iteration 39701 / 76500) loss: 2.303296\n",
      "(Epoch 52 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 39801 / 76500) loss: 2.303245\n",
      "(Iteration 39901 / 76500) loss: 2.303434\n",
      "(Iteration 40001 / 76500) loss: 2.303266\n",
      "(Iteration 40101 / 76500) loss: 2.303322\n",
      "(Iteration 40201 / 76500) loss: 2.303283\n",
      "(Iteration 40301 / 76500) loss: 2.303304\n",
      "(Iteration 40401 / 76500) loss: 2.303358\n",
      "(Iteration 40501 / 76500) loss: 2.303317\n",
      "(Epoch 53 / 100) train acc: 0.121000; val_acc: 0.078000\n",
      "(Iteration 40601 / 76500) loss: 2.303480\n",
      "(Iteration 40701 / 76500) loss: 2.303325\n",
      "(Iteration 40801 / 76500) loss: 2.303250\n",
      "(Iteration 40901 / 76500) loss: 2.303264\n",
      "(Iteration 41001 / 76500) loss: 2.303365\n",
      "(Iteration 41101 / 76500) loss: 2.303317\n",
      "(Iteration 41201 / 76500) loss: 2.303387\n",
      "(Iteration 41301 / 76500) loss: 2.303324\n",
      "(Epoch 54 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 41401 / 76500) loss: 2.303443\n",
      "(Iteration 41501 / 76500) loss: 2.303255\n",
      "(Iteration 41601 / 76500) loss: 2.303304\n",
      "(Iteration 41701 / 76500) loss: 2.303387\n",
      "(Iteration 41801 / 76500) loss: 2.303418\n",
      "(Iteration 41901 / 76500) loss: 2.303354\n",
      "(Iteration 42001 / 76500) loss: 2.303398\n",
      "(Epoch 55 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 42101 / 76500) loss: 2.303507\n",
      "(Iteration 42201 / 76500) loss: 2.303290\n",
      "(Iteration 42301 / 76500) loss: 2.303286\n",
      "(Iteration 42401 / 76500) loss: 2.303328\n",
      "(Iteration 42501 / 76500) loss: 2.303225\n",
      "(Iteration 42601 / 76500) loss: 2.303465\n",
      "(Iteration 42701 / 76500) loss: 2.303463\n",
      "(Iteration 42801 / 76500) loss: 2.303217\n",
      "(Epoch 56 / 100) train acc: 0.112000; val_acc: 0.078000\n",
      "(Iteration 42901 / 76500) loss: 2.303330\n",
      "(Iteration 43001 / 76500) loss: 2.303338\n",
      "(Iteration 43101 / 76500) loss: 2.303343\n",
      "(Iteration 43201 / 76500) loss: 2.303266\n",
      "(Iteration 43301 / 76500) loss: 2.303446\n",
      "(Iteration 43401 / 76500) loss: 2.303334\n",
      "(Iteration 43501 / 76500) loss: 2.303466\n",
      "(Iteration 43601 / 76500) loss: 2.303316\n",
      "(Epoch 57 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 43701 / 76500) loss: 2.303306\n",
      "(Iteration 43801 / 76500) loss: 2.303384\n",
      "(Iteration 43901 / 76500) loss: 2.303439\n",
      "(Iteration 44001 / 76500) loss: 2.303295\n",
      "(Iteration 44101 / 76500) loss: 2.303353\n",
      "(Iteration 44201 / 76500) loss: 2.303285\n",
      "(Iteration 44301 / 76500) loss: 2.303309\n",
      "(Epoch 58 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 44401 / 76500) loss: 2.303280\n",
      "(Iteration 44501 / 76500) loss: 2.303296\n",
      "(Iteration 44601 / 76500) loss: 2.303265\n",
      "(Iteration 44701 / 76500) loss: 2.303254\n",
      "(Iteration 44801 / 76500) loss: 2.303354\n",
      "(Iteration 44901 / 76500) loss: 2.303326\n",
      "(Iteration 45001 / 76500) loss: 2.303405\n",
      "(Iteration 45101 / 76500) loss: 2.303326\n",
      "(Epoch 59 / 100) train acc: 0.117000; val_acc: 0.078000\n",
      "(Iteration 45201 / 76500) loss: 2.303338\n",
      "(Iteration 45301 / 76500) loss: 2.303391\n",
      "(Iteration 45401 / 76500) loss: 2.303387\n",
      "(Iteration 45501 / 76500) loss: 2.303217\n",
      "(Iteration 45601 / 76500) loss: 2.303295\n",
      "(Iteration 45701 / 76500) loss: 2.303295\n",
      "(Iteration 45801 / 76500) loss: 2.303353\n",
      "(Epoch 60 / 100) train acc: 0.084000; val_acc: 0.078000\n",
      "(Iteration 45901 / 76500) loss: 2.303325\n",
      "(Iteration 46001 / 76500) loss: 2.303284\n",
      "(Iteration 46101 / 76500) loss: 2.303286\n",
      "(Iteration 46201 / 76500) loss: 2.303227\n",
      "(Iteration 46301 / 76500) loss: 2.303267\n",
      "(Iteration 46401 / 76500) loss: 2.303250\n",
      "(Iteration 46501 / 76500) loss: 2.303376\n",
      "(Iteration 46601 / 76500) loss: 2.303315\n",
      "(Epoch 61 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 46701 / 76500) loss: 2.303232\n",
      "(Iteration 46801 / 76500) loss: 2.303332\n",
      "(Iteration 46901 / 76500) loss: 2.303235\n",
      "(Iteration 47001 / 76500) loss: 2.303303\n",
      "(Iteration 47101 / 76500) loss: 2.303325\n",
      "(Iteration 47201 / 76500) loss: 2.303384\n",
      "(Iteration 47301 / 76500) loss: 2.303393\n",
      "(Iteration 47401 / 76500) loss: 2.303416\n",
      "(Epoch 62 / 100) train acc: 0.116000; val_acc: 0.078000\n",
      "(Iteration 47501 / 76500) loss: 2.303343\n",
      "(Iteration 47601 / 76500) loss: 2.303270\n",
      "(Iteration 47701 / 76500) loss: 2.303340\n",
      "(Iteration 47801 / 76500) loss: 2.303289\n",
      "(Iteration 47901 / 76500) loss: 2.303248\n",
      "(Iteration 48001 / 76500) loss: 2.303289\n",
      "(Iteration 48101 / 76500) loss: 2.303411\n",
      "(Epoch 63 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 48201 / 76500) loss: 2.303349\n",
      "(Iteration 48301 / 76500) loss: 2.303359\n",
      "(Iteration 48401 / 76500) loss: 2.303330\n",
      "(Iteration 48501 / 76500) loss: 2.303306\n",
      "(Iteration 48601 / 76500) loss: 2.303251\n",
      "(Iteration 48701 / 76500) loss: 2.303304\n",
      "(Iteration 48801 / 76500) loss: 2.303298\n",
      "(Iteration 48901 / 76500) loss: 2.303342\n",
      "(Epoch 64 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 49001 / 76500) loss: 2.303278\n",
      "(Iteration 49101 / 76500) loss: 2.303259\n",
      "(Iteration 49201 / 76500) loss: 2.303403\n",
      "(Iteration 49301 / 76500) loss: 2.303413\n",
      "(Iteration 49401 / 76500) loss: 2.303327\n",
      "(Iteration 49501 / 76500) loss: 2.303403\n",
      "(Iteration 49601 / 76500) loss: 2.303231\n",
      "(Iteration 49701 / 76500) loss: 2.303354\n",
      "(Epoch 65 / 100) train acc: 0.114000; val_acc: 0.078000\n",
      "(Iteration 49801 / 76500) loss: 2.303222\n",
      "(Iteration 49901 / 76500) loss: 2.303343\n",
      "(Iteration 50001 / 76500) loss: 2.303244\n",
      "(Iteration 50101 / 76500) loss: 2.303334\n",
      "(Iteration 50201 / 76500) loss: 2.303223\n",
      "(Iteration 50301 / 76500) loss: 2.303241\n",
      "(Iteration 50401 / 76500) loss: 2.303290\n",
      "(Epoch 66 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 50501 / 76500) loss: 2.303255\n",
      "(Iteration 50601 / 76500) loss: 2.303404\n",
      "(Iteration 50701 / 76500) loss: 2.303351\n",
      "(Iteration 50801 / 76500) loss: 2.303187\n",
      "(Iteration 50901 / 76500) loss: 2.303257\n",
      "(Iteration 51001 / 76500) loss: 2.303356\n",
      "(Iteration 51101 / 76500) loss: 2.303213\n",
      "(Iteration 51201 / 76500) loss: 2.303339\n",
      "(Epoch 67 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 51301 / 76500) loss: 2.303425\n",
      "(Iteration 51401 / 76500) loss: 2.303373\n",
      "(Iteration 51501 / 76500) loss: 2.303339\n",
      "(Iteration 51601 / 76500) loss: 2.303239\n",
      "(Iteration 51701 / 76500) loss: 2.303301\n",
      "(Iteration 51801 / 76500) loss: 2.303252\n",
      "(Iteration 51901 / 76500) loss: 2.303186\n",
      "(Iteration 52001 / 76500) loss: 2.303207\n",
      "(Epoch 68 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 52101 / 76500) loss: 2.303300\n",
      "(Iteration 52201 / 76500) loss: 2.303326\n",
      "(Iteration 52301 / 76500) loss: 2.303276\n",
      "(Iteration 52401 / 76500) loss: 2.303372\n",
      "(Iteration 52501 / 76500) loss: 2.303355\n",
      "(Iteration 52601 / 76500) loss: 2.303321\n",
      "(Iteration 52701 / 76500) loss: 2.303386\n",
      "(Epoch 69 / 100) train acc: 0.105000; val_acc: 0.078000\n",
      "(Iteration 52801 / 76500) loss: 2.303387\n",
      "(Iteration 52901 / 76500) loss: 2.303211\n",
      "(Iteration 53001 / 76500) loss: 2.303351\n",
      "(Iteration 53101 / 76500) loss: 2.303294\n",
      "(Iteration 53201 / 76500) loss: 2.303270\n",
      "(Iteration 53301 / 76500) loss: 2.303361\n",
      "(Iteration 53401 / 76500) loss: 2.303305\n",
      "(Iteration 53501 / 76500) loss: 2.303371\n",
      "(Epoch 70 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 53601 / 76500) loss: 2.303389\n",
      "(Iteration 53701 / 76500) loss: 2.303239\n",
      "(Iteration 53801 / 76500) loss: 2.303318\n",
      "(Iteration 53901 / 76500) loss: 2.303265\n",
      "(Iteration 54001 / 76500) loss: 2.303267\n",
      "(Iteration 54101 / 76500) loss: 2.303162\n",
      "(Iteration 54201 / 76500) loss: 2.303294\n",
      "(Iteration 54301 / 76500) loss: 2.303302\n",
      "(Epoch 71 / 100) train acc: 0.090000; val_acc: 0.078000\n",
      "(Iteration 54401 / 76500) loss: 2.303311\n",
      "(Iteration 54501 / 76500) loss: 2.303320\n",
      "(Iteration 54601 / 76500) loss: 2.303247\n",
      "(Iteration 54701 / 76500) loss: 2.303378\n",
      "(Iteration 54801 / 76500) loss: 2.303237\n",
      "(Iteration 54901 / 76500) loss: 2.303215\n",
      "(Iteration 55001 / 76500) loss: 2.303297\n",
      "(Epoch 72 / 100) train acc: 0.111000; val_acc: 0.078000\n",
      "(Iteration 55101 / 76500) loss: 2.303219\n",
      "(Iteration 55201 / 76500) loss: 2.303326\n",
      "(Iteration 55301 / 76500) loss: 2.303394\n",
      "(Iteration 55401 / 76500) loss: 2.303188\n",
      "(Iteration 55501 / 76500) loss: 2.303396\n",
      "(Iteration 55601 / 76500) loss: 2.303286\n",
      "(Iteration 55701 / 76500) loss: 2.303311\n",
      "(Iteration 55801 / 76500) loss: 2.303344\n",
      "(Epoch 73 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 55901 / 76500) loss: 2.303278\n",
      "(Iteration 56001 / 76500) loss: 2.303347\n",
      "(Iteration 56101 / 76500) loss: 2.303313\n",
      "(Iteration 56201 / 76500) loss: 2.303267\n",
      "(Iteration 56301 / 76500) loss: 2.303208\n",
      "(Iteration 56401 / 76500) loss: 2.303279\n",
      "(Iteration 56501 / 76500) loss: 2.303274\n",
      "(Iteration 56601 / 76500) loss: 2.303282\n",
      "(Epoch 74 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 56701 / 76500) loss: 2.303211\n",
      "(Iteration 56801 / 76500) loss: 2.303327\n",
      "(Iteration 56901 / 76500) loss: 2.303305\n",
      "(Iteration 57001 / 76500) loss: 2.303371\n",
      "(Iteration 57101 / 76500) loss: 2.303345\n",
      "(Iteration 57201 / 76500) loss: 2.303349\n",
      "(Iteration 57301 / 76500) loss: 2.303252\n",
      "(Epoch 75 / 100) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 57401 / 76500) loss: 2.303313\n",
      "(Iteration 57501 / 76500) loss: 2.303327\n",
      "(Iteration 57601 / 76500) loss: 2.303199\n",
      "(Iteration 57701 / 76500) loss: 2.303297\n",
      "(Iteration 57801 / 76500) loss: 2.303213\n",
      "(Iteration 57901 / 76500) loss: 2.303351\n",
      "(Iteration 58001 / 76500) loss: 2.303318\n",
      "(Iteration 58101 / 76500) loss: 2.303221\n",
      "(Epoch 76 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 58201 / 76500) loss: 2.303335\n",
      "(Iteration 58301 / 76500) loss: 2.303281\n",
      "(Iteration 58401 / 76500) loss: 2.303380\n",
      "(Iteration 58501 / 76500) loss: 2.303313\n",
      "(Iteration 58601 / 76500) loss: 2.303253\n",
      "(Iteration 58701 / 76500) loss: 2.303407\n",
      "(Iteration 58801 / 76500) loss: 2.303255\n",
      "(Iteration 58901 / 76500) loss: 2.303323\n",
      "(Epoch 77 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 59001 / 76500) loss: 2.303272\n",
      "(Iteration 59101 / 76500) loss: 2.303216\n",
      "(Iteration 59201 / 76500) loss: 2.303347\n",
      "(Iteration 59301 / 76500) loss: 2.303246\n",
      "(Iteration 59401 / 76500) loss: 2.303173\n",
      "(Iteration 59501 / 76500) loss: 2.303228\n",
      "(Iteration 59601 / 76500) loss: 2.303234\n",
      "(Epoch 78 / 100) train acc: 0.079000; val_acc: 0.078000\n",
      "(Iteration 59701 / 76500) loss: 2.303228\n",
      "(Iteration 59801 / 76500) loss: 2.303413\n",
      "(Iteration 59901 / 76500) loss: 2.303325\n",
      "(Iteration 60001 / 76500) loss: 2.303341\n",
      "(Iteration 60101 / 76500) loss: 2.303270\n",
      "(Iteration 60201 / 76500) loss: 2.303291\n",
      "(Iteration 60301 / 76500) loss: 2.303298\n",
      "(Iteration 60401 / 76500) loss: 2.303196\n",
      "(Epoch 79 / 100) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 60501 / 76500) loss: 2.303240\n",
      "(Iteration 60601 / 76500) loss: 2.303306\n",
      "(Iteration 60701 / 76500) loss: 2.303294\n",
      "(Iteration 60801 / 76500) loss: 2.303181\n",
      "(Iteration 60901 / 76500) loss: 2.303249\n",
      "(Iteration 61001 / 76500) loss: 2.303236\n",
      "(Iteration 61101 / 76500) loss: 2.303225\n",
      "(Epoch 80 / 100) train acc: 0.118000; val_acc: 0.078000\n",
      "(Iteration 61201 / 76500) loss: 2.303360\n",
      "(Iteration 61301 / 76500) loss: 2.303273\n",
      "(Iteration 61401 / 76500) loss: 2.303356\n",
      "(Iteration 61501 / 76500) loss: 2.303245\n",
      "(Iteration 61601 / 76500) loss: 2.303346\n",
      "(Iteration 61701 / 76500) loss: 2.303258\n",
      "(Iteration 61801 / 76500) loss: 2.303268\n",
      "(Iteration 61901 / 76500) loss: 2.303325\n",
      "(Epoch 81 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 62001 / 76500) loss: 2.303285\n",
      "(Iteration 62101 / 76500) loss: 2.303196\n",
      "(Iteration 62201 / 76500) loss: 2.303262\n",
      "(Iteration 62301 / 76500) loss: 2.303253\n",
      "(Iteration 62401 / 76500) loss: 2.303289\n",
      "(Iteration 62501 / 76500) loss: 2.303333\n",
      "(Iteration 62601 / 76500) loss: 2.303257\n",
      "(Iteration 62701 / 76500) loss: 2.303160\n",
      "(Epoch 82 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 62801 / 76500) loss: 2.303274\n",
      "(Iteration 62901 / 76500) loss: 2.303288\n",
      "(Iteration 63001 / 76500) loss: 2.303253\n",
      "(Iteration 63101 / 76500) loss: 2.303333\n",
      "(Iteration 63201 / 76500) loss: 2.303290\n",
      "(Iteration 63301 / 76500) loss: 2.303211\n",
      "(Iteration 63401 / 76500) loss: 2.303231\n",
      "(Epoch 83 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 63501 / 76500) loss: 2.303149\n",
      "(Iteration 63601 / 76500) loss: 2.303349\n",
      "(Iteration 63701 / 76500) loss: 2.303240\n",
      "(Iteration 63801 / 76500) loss: 2.303331\n",
      "(Iteration 63901 / 76500) loss: 2.303215\n",
      "(Iteration 64001 / 76500) loss: 2.303254\n",
      "(Iteration 64101 / 76500) loss: 2.303279\n",
      "(Iteration 64201 / 76500) loss: 2.303232\n",
      "(Epoch 84 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 64301 / 76500) loss: 2.303340\n",
      "(Iteration 64401 / 76500) loss: 2.303306\n",
      "(Iteration 64501 / 76500) loss: 2.303376\n",
      "(Iteration 64601 / 76500) loss: 2.303345\n",
      "(Iteration 64701 / 76500) loss: 2.303351\n",
      "(Iteration 64801 / 76500) loss: 2.303299\n",
      "(Iteration 64901 / 76500) loss: 2.303264\n",
      "(Iteration 65001 / 76500) loss: 2.303255\n",
      "(Epoch 85 / 100) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 65101 / 76500) loss: 2.303264\n",
      "(Iteration 65201 / 76500) loss: 2.303377\n",
      "(Iteration 65301 / 76500) loss: 2.303319\n",
      "(Iteration 65401 / 76500) loss: 2.303164\n",
      "(Iteration 65501 / 76500) loss: 2.303252\n",
      "(Iteration 65601 / 76500) loss: 2.303330\n",
      "(Iteration 65701 / 76500) loss: 2.303341\n",
      "(Epoch 86 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 65801 / 76500) loss: 2.303250\n",
      "(Iteration 65901 / 76500) loss: 2.303400\n",
      "(Iteration 66001 / 76500) loss: 2.303266\n",
      "(Iteration 66101 / 76500) loss: 2.303267\n",
      "(Iteration 66201 / 76500) loss: 2.303239\n",
      "(Iteration 66301 / 76500) loss: 2.303312\n",
      "(Iteration 66401 / 76500) loss: 2.303253\n",
      "(Iteration 66501 / 76500) loss: 2.303292\n",
      "(Epoch 87 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 66601 / 76500) loss: 2.303189\n",
      "(Iteration 66701 / 76500) loss: 2.303227\n",
      "(Iteration 66801 / 76500) loss: 2.303249\n",
      "(Iteration 66901 / 76500) loss: 2.303233\n",
      "(Iteration 67001 / 76500) loss: 2.303304\n",
      "(Iteration 67101 / 76500) loss: 2.303238\n",
      "(Iteration 67201 / 76500) loss: 2.303238\n",
      "(Iteration 67301 / 76500) loss: 2.303345\n",
      "(Epoch 88 / 100) train acc: 0.113000; val_acc: 0.078000\n",
      "(Iteration 67401 / 76500) loss: 2.303276\n",
      "(Iteration 67501 / 76500) loss: 2.303353\n",
      "(Iteration 67601 / 76500) loss: 2.303218\n",
      "(Iteration 67701 / 76500) loss: 2.303279\n",
      "(Iteration 67801 / 76500) loss: 2.303239\n",
      "(Iteration 67901 / 76500) loss: 2.303245\n",
      "(Iteration 68001 / 76500) loss: 2.303207\n",
      "(Epoch 89 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 68101 / 76500) loss: 2.303325\n",
      "(Iteration 68201 / 76500) loss: 2.303260\n",
      "(Iteration 68301 / 76500) loss: 2.303328\n",
      "(Iteration 68401 / 76500) loss: 2.303391\n",
      "(Iteration 68501 / 76500) loss: 2.303336\n",
      "(Iteration 68601 / 76500) loss: 2.303380\n",
      "(Iteration 68701 / 76500) loss: 2.303251\n",
      "(Iteration 68801 / 76500) loss: 2.303284\n",
      "(Epoch 90 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 68901 / 76500) loss: 2.303254\n",
      "(Iteration 69001 / 76500) loss: 2.303367\n",
      "(Iteration 69101 / 76500) loss: 2.303255\n",
      "(Iteration 69201 / 76500) loss: 2.303242\n",
      "(Iteration 69301 / 76500) loss: 2.303229\n",
      "(Iteration 69401 / 76500) loss: 2.303276\n",
      "(Iteration 69501 / 76500) loss: 2.303226\n",
      "(Iteration 69601 / 76500) loss: 2.303294\n",
      "(Epoch 91 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 69701 / 76500) loss: 2.303271\n",
      "(Iteration 69801 / 76500) loss: 2.303372\n",
      "(Iteration 69901 / 76500) loss: 2.303204\n",
      "(Iteration 70001 / 76500) loss: 2.303214\n",
      "(Iteration 70101 / 76500) loss: 2.303263\n",
      "(Iteration 70201 / 76500) loss: 2.303325\n",
      "(Iteration 70301 / 76500) loss: 2.303213\n",
      "(Epoch 92 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 70401 / 76500) loss: 2.303213\n",
      "(Iteration 70501 / 76500) loss: 2.303203\n",
      "(Iteration 70601 / 76500) loss: 2.303234\n",
      "(Iteration 70701 / 76500) loss: 2.303235\n",
      "(Iteration 70801 / 76500) loss: 2.303301\n",
      "(Iteration 70901 / 76500) loss: 2.303155\n",
      "(Iteration 71001 / 76500) loss: 2.303161\n",
      "(Iteration 71101 / 76500) loss: 2.303200\n",
      "(Epoch 93 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 71201 / 76500) loss: 2.303206\n",
      "(Iteration 71301 / 76500) loss: 2.303239\n",
      "(Iteration 71401 / 76500) loss: 2.303257\n",
      "(Iteration 71501 / 76500) loss: 2.303329\n",
      "(Iteration 71601 / 76500) loss: 2.303198\n",
      "(Iteration 71701 / 76500) loss: 2.303279\n",
      "(Iteration 71801 / 76500) loss: 2.303248\n",
      "(Iteration 71901 / 76500) loss: 2.303260\n",
      "(Epoch 94 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 72001 / 76500) loss: 2.303340\n",
      "(Iteration 72101 / 76500) loss: 2.303341\n",
      "(Iteration 72201 / 76500) loss: 2.303228\n",
      "(Iteration 72301 / 76500) loss: 2.303293\n",
      "(Iteration 72401 / 76500) loss: 2.303372\n",
      "(Iteration 72501 / 76500) loss: 2.303394\n",
      "(Iteration 72601 / 76500) loss: 2.303255\n",
      "(Epoch 95 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 72701 / 76500) loss: 2.303282\n",
      "(Iteration 72801 / 76500) loss: 2.303267\n",
      "(Iteration 72901 / 76500) loss: 2.303279\n",
      "(Iteration 73001 / 76500) loss: 2.303259\n",
      "(Iteration 73101 / 76500) loss: 2.303217\n",
      "(Iteration 73201 / 76500) loss: 2.303346\n",
      "(Iteration 73301 / 76500) loss: 2.303234\n",
      "(Iteration 73401 / 76500) loss: 2.303199\n",
      "(Epoch 96 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 73501 / 76500) loss: 2.303181\n",
      "(Iteration 73601 / 76500) loss: 2.303342\n",
      "(Iteration 73701 / 76500) loss: 2.303264\n",
      "(Iteration 73801 / 76500) loss: 2.303371\n",
      "(Iteration 73901 / 76500) loss: 2.303297\n",
      "(Iteration 74001 / 76500) loss: 2.303289\n",
      "(Iteration 74101 / 76500) loss: 2.303268\n",
      "(Iteration 74201 / 76500) loss: 2.303234\n",
      "(Epoch 97 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 74301 / 76500) loss: 2.303207\n",
      "(Iteration 74401 / 76500) loss: 2.303369\n",
      "(Iteration 74501 / 76500) loss: 2.303360\n",
      "(Iteration 74601 / 76500) loss: 2.303226\n",
      "(Iteration 74701 / 76500) loss: 2.303340\n",
      "(Iteration 74801 / 76500) loss: 2.303200\n",
      "(Iteration 74901 / 76500) loss: 2.303276\n",
      "(Epoch 98 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 75001 / 76500) loss: 2.303228\n",
      "(Iteration 75101 / 76500) loss: 2.303277\n",
      "(Iteration 75201 / 76500) loss: 2.303261\n",
      "(Iteration 75301 / 76500) loss: 2.303282\n",
      "(Iteration 75401 / 76500) loss: 2.303258\n",
      "(Iteration 75501 / 76500) loss: 2.303293\n",
      "(Iteration 75601 / 76500) loss: 2.303224\n",
      "(Iteration 75701 / 76500) loss: 2.303241\n",
      "(Epoch 99 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 75801 / 76500) loss: 2.303351\n",
      "(Iteration 75901 / 76500) loss: 2.303330\n",
      "(Iteration 76001 / 76500) loss: 2.303192\n",
      "(Iteration 76101 / 76500) loss: 2.303202\n",
      "(Iteration 76201 / 76500) loss: 2.303281\n",
      "(Iteration 76301 / 76500) loss: 2.303222\n",
      "(Iteration 76401 / 76500) loss: 2.303318\n",
      "(Epoch 100 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "Training with parameters: {'hidden_size': 100, 'learning_rate': 0.0001, 'num_epochs': 100, 'reg': 0.7, 'lr_decay': 0.95, 'batch_size': 128}\n",
      "(Iteration 1 / 38200) loss: 2.308301\n",
      "(Epoch 0 / 100) train acc: 0.097000; val_acc: 0.102000\n",
      "(Iteration 101 / 38200) loss: 2.308208\n",
      "(Iteration 201 / 38200) loss: 2.308140\n",
      "(Iteration 301 / 38200) loss: 2.308062\n",
      "(Epoch 1 / 100) train acc: 0.091000; val_acc: 0.112000\n",
      "(Iteration 401 / 38200) loss: 2.307994\n",
      "(Iteration 501 / 38200) loss: 2.307922\n",
      "(Iteration 601 / 38200) loss: 2.307834\n",
      "(Iteration 701 / 38200) loss: 2.307790\n",
      "(Epoch 2 / 100) train acc: 0.096000; val_acc: 0.097000\n",
      "(Iteration 801 / 38200) loss: 2.307711\n",
      "(Iteration 901 / 38200) loss: 2.307652\n",
      "(Iteration 1001 / 38200) loss: 2.307577\n",
      "(Iteration 1101 / 38200) loss: 2.307525\n",
      "(Epoch 3 / 100) train acc: 0.133000; val_acc: 0.100000\n",
      "(Iteration 1201 / 38200) loss: 2.307450\n",
      "(Iteration 1301 / 38200) loss: 2.307394\n",
      "(Iteration 1401 / 38200) loss: 2.307350\n",
      "(Iteration 1501 / 38200) loss: 2.307280\n",
      "(Epoch 4 / 100) train acc: 0.123000; val_acc: 0.098000\n",
      "(Iteration 1601 / 38200) loss: 2.307222\n",
      "(Iteration 1701 / 38200) loss: 2.307182\n",
      "(Iteration 1801 / 38200) loss: 2.307126\n",
      "(Iteration 1901 / 38200) loss: 2.307093\n",
      "(Epoch 5 / 100) train acc: 0.106000; val_acc: 0.098000\n",
      "(Iteration 2001 / 38200) loss: 2.307026\n",
      "(Iteration 2101 / 38200) loss: 2.306974\n",
      "(Iteration 2201 / 38200) loss: 2.306931\n",
      "(Epoch 6 / 100) train acc: 0.093000; val_acc: 0.096000\n",
      "(Iteration 2301 / 38200) loss: 2.306879\n",
      "(Iteration 2401 / 38200) loss: 2.306827\n",
      "(Iteration 2501 / 38200) loss: 2.306783\n",
      "(Iteration 2601 / 38200) loss: 2.306760\n",
      "(Epoch 7 / 100) train acc: 0.103000; val_acc: 0.096000\n",
      "(Iteration 2701 / 38200) loss: 2.306704\n",
      "(Iteration 2801 / 38200) loss: 2.306658\n",
      "(Iteration 2901 / 38200) loss: 2.306664\n",
      "(Iteration 3001 / 38200) loss: 2.306570\n",
      "(Epoch 8 / 100) train acc: 0.112000; val_acc: 0.097000\n",
      "(Iteration 3101 / 38200) loss: 2.306544\n",
      "(Iteration 3201 / 38200) loss: 2.306533\n",
      "(Iteration 3301 / 38200) loss: 2.306459\n",
      "(Iteration 3401 / 38200) loss: 2.306467\n",
      "(Epoch 9 / 100) train acc: 0.097000; val_acc: 0.098000\n",
      "(Iteration 3501 / 38200) loss: 2.306442\n",
      "(Iteration 3601 / 38200) loss: 2.306388\n",
      "(Iteration 3701 / 38200) loss: 2.306330\n",
      "(Iteration 3801 / 38200) loss: 2.306341\n",
      "(Epoch 10 / 100) train acc: 0.097000; val_acc: 0.095000\n",
      "(Iteration 3901 / 38200) loss: 2.306329\n",
      "(Iteration 4001 / 38200) loss: 2.306226\n",
      "(Iteration 4101 / 38200) loss: 2.306206\n",
      "(Iteration 4201 / 38200) loss: 2.306195\n",
      "(Epoch 11 / 100) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 4301 / 38200) loss: 2.306138\n",
      "(Iteration 4401 / 38200) loss: 2.306114\n",
      "(Iteration 4501 / 38200) loss: 2.306112\n",
      "(Epoch 12 / 100) train acc: 0.093000; val_acc: 0.094000\n",
      "(Iteration 4601 / 38200) loss: 2.306081\n",
      "(Iteration 4701 / 38200) loss: 2.306048\n",
      "(Iteration 4801 / 38200) loss: 2.306016\n",
      "(Iteration 4901 / 38200) loss: 2.305964\n",
      "(Epoch 13 / 100) train acc: 0.118000; val_acc: 0.083000\n",
      "(Iteration 5001 / 38200) loss: 2.305945\n",
      "(Iteration 5101 / 38200) loss: 2.305938\n",
      "(Iteration 5201 / 38200) loss: 2.305942\n",
      "(Iteration 5301 / 38200) loss: 2.305898\n",
      "(Epoch 14 / 100) train acc: 0.101000; val_acc: 0.081000\n",
      "(Iteration 5401 / 38200) loss: 2.305832\n",
      "(Iteration 5501 / 38200) loss: 2.305867\n",
      "(Iteration 5601 / 38200) loss: 2.305821\n",
      "(Iteration 5701 / 38200) loss: 2.305832\n",
      "(Epoch 15 / 100) train acc: 0.102000; val_acc: 0.084000\n",
      "(Iteration 5801 / 38200) loss: 2.305799\n",
      "(Iteration 5901 / 38200) loss: 2.305750\n",
      "(Iteration 6001 / 38200) loss: 2.305698\n",
      "(Iteration 6101 / 38200) loss: 2.305733\n",
      "(Epoch 16 / 100) train acc: 0.094000; val_acc: 0.086000\n",
      "(Iteration 6201 / 38200) loss: 2.305701\n",
      "(Iteration 6301 / 38200) loss: 2.305680\n",
      "(Iteration 6401 / 38200) loss: 2.305646\n",
      "(Epoch 17 / 100) train acc: 0.125000; val_acc: 0.084000\n",
      "(Iteration 6501 / 38200) loss: 2.305651\n",
      "(Iteration 6601 / 38200) loss: 2.305626\n",
      "(Iteration 6701 / 38200) loss: 2.305602\n",
      "(Iteration 6801 / 38200) loss: 2.305571\n",
      "(Epoch 18 / 100) train acc: 0.112000; val_acc: 0.080000\n",
      "(Iteration 6901 / 38200) loss: 2.305619\n",
      "(Iteration 7001 / 38200) loss: 2.305558\n",
      "(Iteration 7101 / 38200) loss: 2.305515\n",
      "(Iteration 7201 / 38200) loss: 2.305528\n",
      "(Epoch 19 / 100) train acc: 0.115000; val_acc: 0.079000\n",
      "(Iteration 7301 / 38200) loss: 2.305495\n",
      "(Iteration 7401 / 38200) loss: 2.305476\n",
      "(Iteration 7501 / 38200) loss: 2.305479\n",
      "(Iteration 7601 / 38200) loss: 2.305474\n",
      "(Epoch 20 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 7701 / 38200) loss: 2.305447\n",
      "(Iteration 7801 / 38200) loss: 2.305437\n",
      "(Iteration 7901 / 38200) loss: 2.305364\n",
      "(Iteration 8001 / 38200) loss: 2.305397\n",
      "(Epoch 21 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 8101 / 38200) loss: 2.305400\n",
      "(Iteration 8201 / 38200) loss: 2.305406\n",
      "(Iteration 8301 / 38200) loss: 2.305350\n",
      "(Iteration 8401 / 38200) loss: 2.305366\n",
      "(Epoch 22 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 8501 / 38200) loss: 2.305326\n",
      "(Iteration 8601 / 38200) loss: 2.305320\n",
      "(Iteration 8701 / 38200) loss: 2.305272\n",
      "(Epoch 23 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 8801 / 38200) loss: 2.305296\n",
      "(Iteration 8901 / 38200) loss: 2.305267\n",
      "(Iteration 9001 / 38200) loss: 2.305283\n",
      "(Iteration 9101 / 38200) loss: 2.305274\n",
      "(Epoch 24 / 100) train acc: 0.112000; val_acc: 0.078000\n",
      "(Iteration 9201 / 38200) loss: 2.305281\n",
      "(Iteration 9301 / 38200) loss: 2.305224\n",
      "(Iteration 9401 / 38200) loss: 2.305242\n",
      "(Iteration 9501 / 38200) loss: 2.305258\n",
      "(Epoch 25 / 100) train acc: 0.087000; val_acc: 0.078000\n",
      "(Iteration 9601 / 38200) loss: 2.305187\n",
      "(Iteration 9701 / 38200) loss: 2.305178\n",
      "(Iteration 9801 / 38200) loss: 2.305151\n",
      "(Iteration 9901 / 38200) loss: 2.305133\n",
      "(Epoch 26 / 100) train acc: 0.112000; val_acc: 0.078000\n",
      "(Iteration 10001 / 38200) loss: 2.305153\n",
      "(Iteration 10101 / 38200) loss: 2.305192\n",
      "(Iteration 10201 / 38200) loss: 2.305175\n",
      "(Iteration 10301 / 38200) loss: 2.305140\n",
      "(Epoch 27 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 10401 / 38200) loss: 2.305127\n",
      "(Iteration 10501 / 38200) loss: 2.305138\n",
      "(Iteration 10601 / 38200) loss: 2.305115\n",
      "(Epoch 28 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 10701 / 38200) loss: 2.305140\n",
      "(Iteration 10801 / 38200) loss: 2.305094\n",
      "(Iteration 10901 / 38200) loss: 2.305108\n",
      "(Iteration 11001 / 38200) loss: 2.305043\n",
      "(Epoch 29 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 11101 / 38200) loss: 2.305056\n",
      "(Iteration 11201 / 38200) loss: 2.305059\n",
      "(Iteration 11301 / 38200) loss: 2.305054\n",
      "(Iteration 11401 / 38200) loss: 2.305071\n",
      "(Epoch 30 / 100) train acc: 0.086000; val_acc: 0.078000\n",
      "(Iteration 11501 / 38200) loss: 2.305018\n",
      "(Iteration 11601 / 38200) loss: 2.305002\n",
      "(Iteration 11701 / 38200) loss: 2.305017\n",
      "(Iteration 11801 / 38200) loss: 2.304987\n",
      "(Epoch 31 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 11901 / 38200) loss: 2.305035\n",
      "(Iteration 12001 / 38200) loss: 2.304979\n",
      "(Iteration 12101 / 38200) loss: 2.304970\n",
      "(Iteration 12201 / 38200) loss: 2.305017\n",
      "(Epoch 32 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 12301 / 38200) loss: 2.304990\n",
      "(Iteration 12401 / 38200) loss: 2.304986\n",
      "(Iteration 12501 / 38200) loss: 2.304976\n",
      "(Iteration 12601 / 38200) loss: 2.304936\n",
      "(Epoch 33 / 100) train acc: 0.097000; val_acc: 0.078000\n",
      "(Iteration 12701 / 38200) loss: 2.304969\n",
      "(Iteration 12801 / 38200) loss: 2.304963\n",
      "(Iteration 12901 / 38200) loss: 2.304907\n",
      "(Epoch 34 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 13001 / 38200) loss: 2.304990\n",
      "(Iteration 13101 / 38200) loss: 2.304944\n",
      "(Iteration 13201 / 38200) loss: 2.304924\n",
      "(Iteration 13301 / 38200) loss: 2.304934\n",
      "(Epoch 35 / 100) train acc: 0.104000; val_acc: 0.078000\n",
      "(Iteration 13401 / 38200) loss: 2.304932\n",
      "(Iteration 13501 / 38200) loss: 2.304903\n",
      "(Iteration 13601 / 38200) loss: 2.304938\n",
      "(Iteration 13701 / 38200) loss: 2.304919\n",
      "(Epoch 36 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 13801 / 38200) loss: 2.304926\n",
      "(Iteration 13901 / 38200) loss: 2.304913\n",
      "(Iteration 14001 / 38200) loss: 2.304857\n",
      "(Iteration 14101 / 38200) loss: 2.304856\n",
      "(Epoch 37 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 14201 / 38200) loss: 2.304902\n",
      "(Iteration 14301 / 38200) loss: 2.304854\n",
      "(Iteration 14401 / 38200) loss: 2.304866\n",
      "(Iteration 14501 / 38200) loss: 2.304880\n",
      "(Epoch 38 / 100) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 14601 / 38200) loss: 2.304874\n",
      "(Iteration 14701 / 38200) loss: 2.304873\n",
      "(Iteration 14801 / 38200) loss: 2.304868\n",
      "(Epoch 39 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 14901 / 38200) loss: 2.304823\n",
      "(Iteration 15001 / 38200) loss: 2.304882\n",
      "(Iteration 15101 / 38200) loss: 2.304846\n",
      "(Iteration 15201 / 38200) loss: 2.304842\n",
      "(Epoch 40 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 15301 / 38200) loss: 2.304829\n",
      "(Iteration 15401 / 38200) loss: 2.304855\n",
      "(Iteration 15501 / 38200) loss: 2.304837\n",
      "(Iteration 15601 / 38200) loss: 2.304801\n",
      "(Epoch 41 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 15701 / 38200) loss: 2.304786\n",
      "(Iteration 15801 / 38200) loss: 2.304770\n",
      "(Iteration 15901 / 38200) loss: 2.304756\n",
      "(Iteration 16001 / 38200) loss: 2.304802\n",
      "(Epoch 42 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 16101 / 38200) loss: 2.304823\n",
      "(Iteration 16201 / 38200) loss: 2.304815\n",
      "(Iteration 16301 / 38200) loss: 2.304776\n",
      "(Iteration 16401 / 38200) loss: 2.304808\n",
      "(Epoch 43 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 16501 / 38200) loss: 2.304719\n",
      "(Iteration 16601 / 38200) loss: 2.304775\n",
      "(Iteration 16701 / 38200) loss: 2.304800\n",
      "(Iteration 16801 / 38200) loss: 2.304759\n",
      "(Epoch 44 / 100) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 16901 / 38200) loss: 2.304806\n",
      "(Iteration 17001 / 38200) loss: 2.304765\n",
      "(Iteration 17101 / 38200) loss: 2.304774\n",
      "(Epoch 45 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 17201 / 38200) loss: 2.304753\n",
      "(Iteration 17301 / 38200) loss: 2.304762\n",
      "(Iteration 17401 / 38200) loss: 2.304754\n",
      "(Iteration 17501 / 38200) loss: 2.304725\n",
      "(Epoch 46 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 17601 / 38200) loss: 2.304763\n",
      "(Iteration 17701 / 38200) loss: 2.304713\n",
      "(Iteration 17801 / 38200) loss: 2.304747\n",
      "(Iteration 17901 / 38200) loss: 2.304714\n",
      "(Epoch 47 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 18001 / 38200) loss: 2.304794\n",
      "(Iteration 18101 / 38200) loss: 2.304732\n",
      "(Iteration 18201 / 38200) loss: 2.304730\n",
      "(Iteration 18301 / 38200) loss: 2.304738\n",
      "(Epoch 48 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 18401 / 38200) loss: 2.304711\n",
      "(Iteration 18501 / 38200) loss: 2.304689\n",
      "(Iteration 18601 / 38200) loss: 2.304714\n",
      "(Iteration 18701 / 38200) loss: 2.304668\n",
      "(Epoch 49 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 18801 / 38200) loss: 2.304737\n",
      "(Iteration 18901 / 38200) loss: 2.304711\n",
      "(Iteration 19001 / 38200) loss: 2.304721\n",
      "(Epoch 50 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 19101 / 38200) loss: 2.304717\n",
      "(Iteration 19201 / 38200) loss: 2.304655\n",
      "(Iteration 19301 / 38200) loss: 2.304724\n",
      "(Iteration 19401 / 38200) loss: 2.304639\n",
      "(Epoch 51 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 19501 / 38200) loss: 2.304720\n",
      "(Iteration 19601 / 38200) loss: 2.304702\n",
      "(Iteration 19701 / 38200) loss: 2.304696\n",
      "(Iteration 19801 / 38200) loss: 2.304679\n",
      "(Epoch 52 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 19901 / 38200) loss: 2.304683\n",
      "(Iteration 20001 / 38200) loss: 2.304701\n",
      "(Iteration 20101 / 38200) loss: 2.304669\n",
      "(Iteration 20201 / 38200) loss: 2.304669\n",
      "(Epoch 53 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 20301 / 38200) loss: 2.304664\n",
      "(Iteration 20401 / 38200) loss: 2.304633\n",
      "(Iteration 20501 / 38200) loss: 2.304655\n",
      "(Iteration 20601 / 38200) loss: 2.304677\n",
      "(Epoch 54 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 20701 / 38200) loss: 2.304684\n",
      "(Iteration 20801 / 38200) loss: 2.304620\n",
      "(Iteration 20901 / 38200) loss: 2.304663\n",
      "(Iteration 21001 / 38200) loss: 2.304664\n",
      "(Epoch 55 / 100) train acc: 0.109000; val_acc: 0.078000\n",
      "(Iteration 21101 / 38200) loss: 2.304658\n",
      "(Iteration 21201 / 38200) loss: 2.304642\n",
      "(Iteration 21301 / 38200) loss: 2.304654\n",
      "(Epoch 56 / 100) train acc: 0.083000; val_acc: 0.078000\n",
      "(Iteration 21401 / 38200) loss: 2.304682\n",
      "(Iteration 21501 / 38200) loss: 2.304697\n",
      "(Iteration 21601 / 38200) loss: 2.304658\n",
      "(Iteration 21701 / 38200) loss: 2.304658\n",
      "(Epoch 57 / 100) train acc: 0.093000; val_acc: 0.078000\n",
      "(Iteration 21801 / 38200) loss: 2.304680\n",
      "(Iteration 21901 / 38200) loss: 2.304643\n",
      "(Iteration 22001 / 38200) loss: 2.304666\n",
      "(Iteration 22101 / 38200) loss: 2.304650\n",
      "(Epoch 58 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 22201 / 38200) loss: 2.304611\n",
      "(Iteration 22301 / 38200) loss: 2.304609\n",
      "(Iteration 22401 / 38200) loss: 2.304643\n",
      "(Iteration 22501 / 38200) loss: 2.304646\n",
      "(Epoch 59 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 22601 / 38200) loss: 2.304594\n",
      "(Iteration 22701 / 38200) loss: 2.304656\n",
      "(Iteration 22801 / 38200) loss: 2.304609\n",
      "(Iteration 22901 / 38200) loss: 2.304591\n",
      "(Epoch 60 / 100) train acc: 0.137000; val_acc: 0.078000\n",
      "(Iteration 23001 / 38200) loss: 2.304652\n",
      "(Iteration 23101 / 38200) loss: 2.304639\n",
      "(Iteration 23201 / 38200) loss: 2.304625\n",
      "(Iteration 23301 / 38200) loss: 2.304603\n",
      "(Epoch 61 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 23401 / 38200) loss: 2.304672\n",
      "(Iteration 23501 / 38200) loss: 2.304652\n",
      "(Iteration 23601 / 38200) loss: 2.304628\n",
      "(Epoch 62 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 23701 / 38200) loss: 2.304648\n",
      "(Iteration 23801 / 38200) loss: 2.304603\n",
      "(Iteration 23901 / 38200) loss: 2.304608\n",
      "(Iteration 24001 / 38200) loss: 2.304649\n",
      "(Epoch 63 / 100) train acc: 0.086000; val_acc: 0.078000\n",
      "(Iteration 24101 / 38200) loss: 2.304622\n",
      "(Iteration 24201 / 38200) loss: 2.304584\n",
      "(Iteration 24301 / 38200) loss: 2.304596\n",
      "(Iteration 24401 / 38200) loss: 2.304594\n",
      "(Epoch 64 / 100) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 24501 / 38200) loss: 2.304655\n",
      "(Iteration 24601 / 38200) loss: 2.304634\n",
      "(Iteration 24701 / 38200) loss: 2.304646\n",
      "(Iteration 24801 / 38200) loss: 2.304639\n",
      "(Epoch 65 / 100) train acc: 0.082000; val_acc: 0.078000\n",
      "(Iteration 24901 / 38200) loss: 2.304549\n",
      "(Iteration 25001 / 38200) loss: 2.304581\n",
      "(Iteration 25101 / 38200) loss: 2.304618\n",
      "(Iteration 25201 / 38200) loss: 2.304648\n",
      "(Epoch 66 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 25301 / 38200) loss: 2.304590\n",
      "(Iteration 25401 / 38200) loss: 2.304572\n",
      "(Iteration 25501 / 38200) loss: 2.304559\n",
      "(Epoch 67 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 25601 / 38200) loss: 2.304636\n",
      "(Iteration 25701 / 38200) loss: 2.304591\n",
      "(Iteration 25801 / 38200) loss: 2.304664\n",
      "(Iteration 25901 / 38200) loss: 2.304614\n",
      "(Epoch 68 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 26001 / 38200) loss: 2.304604\n",
      "(Iteration 26101 / 38200) loss: 2.304613\n",
      "(Iteration 26201 / 38200) loss: 2.304588\n",
      "(Iteration 26301 / 38200) loss: 2.304614\n",
      "(Epoch 69 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 26401 / 38200) loss: 2.304583\n",
      "(Iteration 26501 / 38200) loss: 2.304613\n",
      "(Iteration 26601 / 38200) loss: 2.304563\n",
      "(Iteration 26701 / 38200) loss: 2.304623\n",
      "(Epoch 70 / 100) train acc: 0.103000; val_acc: 0.078000\n",
      "(Iteration 26801 / 38200) loss: 2.304599\n",
      "(Iteration 26901 / 38200) loss: 2.304605\n",
      "(Iteration 27001 / 38200) loss: 2.304557\n",
      "(Iteration 27101 / 38200) loss: 2.304601\n",
      "(Epoch 71 / 100) train acc: 0.123000; val_acc: 0.078000\n",
      "(Iteration 27201 / 38200) loss: 2.304603\n",
      "(Iteration 27301 / 38200) loss: 2.304573\n",
      "(Iteration 27401 / 38200) loss: 2.304565\n",
      "(Iteration 27501 / 38200) loss: 2.304541\n",
      "(Epoch 72 / 100) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 27601 / 38200) loss: 2.304580\n",
      "(Iteration 27701 / 38200) loss: 2.304584\n",
      "(Iteration 27801 / 38200) loss: 2.304584\n",
      "(Epoch 73 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 27901 / 38200) loss: 2.304560\n",
      "(Iteration 28001 / 38200) loss: 2.304597\n",
      "(Iteration 28101 / 38200) loss: 2.304532\n",
      "(Iteration 28201 / 38200) loss: 2.304561\n",
      "(Epoch 74 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 28301 / 38200) loss: 2.304568\n",
      "(Iteration 28401 / 38200) loss: 2.304648\n",
      "(Iteration 28501 / 38200) loss: 2.304590\n",
      "(Iteration 28601 / 38200) loss: 2.304597\n",
      "(Epoch 75 / 100) train acc: 0.079000; val_acc: 0.078000\n",
      "(Iteration 28701 / 38200) loss: 2.304616\n",
      "(Iteration 28801 / 38200) loss: 2.304622\n",
      "(Iteration 28901 / 38200) loss: 2.304573\n",
      "(Iteration 29001 / 38200) loss: 2.304581\n",
      "(Epoch 76 / 100) train acc: 0.094000; val_acc: 0.078000\n",
      "(Iteration 29101 / 38200) loss: 2.304547\n",
      "(Iteration 29201 / 38200) loss: 2.304600\n",
      "(Iteration 29301 / 38200) loss: 2.304655\n",
      "(Iteration 29401 / 38200) loss: 2.304567\n",
      "(Epoch 77 / 100) train acc: 0.088000; val_acc: 0.078000\n",
      "(Iteration 29501 / 38200) loss: 2.304552\n",
      "(Iteration 29601 / 38200) loss: 2.304563\n",
      "(Iteration 29701 / 38200) loss: 2.304556\n",
      "(Epoch 78 / 100) train acc: 0.107000; val_acc: 0.078000\n",
      "(Iteration 29801 / 38200) loss: 2.304631\n",
      "(Iteration 29901 / 38200) loss: 2.304561\n",
      "(Iteration 30001 / 38200) loss: 2.304544\n",
      "(Iteration 30101 / 38200) loss: 2.304566\n",
      "(Epoch 79 / 100) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 30201 / 38200) loss: 2.304612\n",
      "(Iteration 30301 / 38200) loss: 2.304545\n",
      "(Iteration 30401 / 38200) loss: 2.304591\n",
      "(Iteration 30501 / 38200) loss: 2.304614\n",
      "(Epoch 80 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 30601 / 38200) loss: 2.304551\n",
      "(Iteration 30701 / 38200) loss: 2.304573\n",
      "(Iteration 30801 / 38200) loss: 2.304543\n",
      "(Iteration 30901 / 38200) loss: 2.304582\n",
      "(Epoch 81 / 100) train acc: 0.101000; val_acc: 0.078000\n",
      "(Iteration 31001 / 38200) loss: 2.304583\n",
      "(Iteration 31101 / 38200) loss: 2.304521\n",
      "(Iteration 31201 / 38200) loss: 2.304559\n",
      "(Iteration 31301 / 38200) loss: 2.304577\n",
      "(Epoch 82 / 100) train acc: 0.096000; val_acc: 0.078000\n",
      "(Iteration 31401 / 38200) loss: 2.304545\n",
      "(Iteration 31501 / 38200) loss: 2.304587\n",
      "(Iteration 31601 / 38200) loss: 2.304565\n",
      "(Iteration 31701 / 38200) loss: 2.304548\n",
      "(Epoch 83 / 100) train acc: 0.098000; val_acc: 0.078000\n",
      "(Iteration 31801 / 38200) loss: 2.304556\n",
      "(Iteration 31901 / 38200) loss: 2.304575\n",
      "(Iteration 32001 / 38200) loss: 2.304544\n",
      "(Epoch 84 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 32101 / 38200) loss: 2.304515\n",
      "(Iteration 32201 / 38200) loss: 2.304579\n",
      "(Iteration 32301 / 38200) loss: 2.304577\n",
      "(Iteration 32401 / 38200) loss: 2.304549\n",
      "(Epoch 85 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 32501 / 38200) loss: 2.304572\n",
      "(Iteration 32601 / 38200) loss: 2.304562\n",
      "(Iteration 32701 / 38200) loss: 2.304555\n",
      "(Iteration 32801 / 38200) loss: 2.304537\n",
      "(Epoch 86 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "(Iteration 32901 / 38200) loss: 2.304554\n",
      "(Iteration 33001 / 38200) loss: 2.304563\n",
      "(Iteration 33101 / 38200) loss: 2.304532\n",
      "(Iteration 33201 / 38200) loss: 2.304571\n",
      "(Epoch 87 / 100) train acc: 0.089000; val_acc: 0.078000\n",
      "(Iteration 33301 / 38200) loss: 2.304564\n",
      "(Iteration 33401 / 38200) loss: 2.304577\n",
      "(Iteration 33501 / 38200) loss: 2.304515\n",
      "(Iteration 33601 / 38200) loss: 2.304529\n",
      "(Epoch 88 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 33701 / 38200) loss: 2.304533\n",
      "(Iteration 33801 / 38200) loss: 2.304553\n",
      "(Iteration 33901 / 38200) loss: 2.304588\n",
      "(Epoch 89 / 100) train acc: 0.091000; val_acc: 0.078000\n",
      "(Iteration 34001 / 38200) loss: 2.304588\n",
      "(Iteration 34101 / 38200) loss: 2.304629\n",
      "(Iteration 34201 / 38200) loss: 2.304569\n",
      "(Iteration 34301 / 38200) loss: 2.304575\n",
      "(Epoch 90 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 34401 / 38200) loss: 2.304546\n",
      "(Iteration 34501 / 38200) loss: 2.304568\n",
      "(Iteration 34601 / 38200) loss: 2.304557\n",
      "(Iteration 34701 / 38200) loss: 2.304550\n",
      "(Epoch 91 / 100) train acc: 0.100000; val_acc: 0.078000\n",
      "(Iteration 34801 / 38200) loss: 2.304587\n",
      "(Iteration 34901 / 38200) loss: 2.304591\n",
      "(Iteration 35001 / 38200) loss: 2.304540\n",
      "(Iteration 35101 / 38200) loss: 2.304567\n",
      "(Epoch 92 / 100) train acc: 0.112000; val_acc: 0.078000\n",
      "(Iteration 35201 / 38200) loss: 2.304549\n",
      "(Iteration 35301 / 38200) loss: 2.304544\n",
      "(Iteration 35401 / 38200) loss: 2.304555\n",
      "(Iteration 35501 / 38200) loss: 2.304529\n",
      "(Epoch 93 / 100) train acc: 0.114000; val_acc: 0.078000\n",
      "(Iteration 35601 / 38200) loss: 2.304561\n",
      "(Iteration 35701 / 38200) loss: 2.304569\n",
      "(Iteration 35801 / 38200) loss: 2.304577\n",
      "(Iteration 35901 / 38200) loss: 2.304597\n",
      "(Epoch 94 / 100) train acc: 0.110000; val_acc: 0.078000\n",
      "(Iteration 36001 / 38200) loss: 2.304531\n",
      "(Iteration 36101 / 38200) loss: 2.304531\n",
      "(Iteration 36201 / 38200) loss: 2.304534\n",
      "(Epoch 95 / 100) train acc: 0.118000; val_acc: 0.078000\n",
      "(Iteration 36301 / 38200) loss: 2.304543\n",
      "(Iteration 36401 / 38200) loss: 2.304530\n",
      "(Iteration 36501 / 38200) loss: 2.304596\n",
      "(Iteration 36601 / 38200) loss: 2.304550\n",
      "(Epoch 96 / 100) train acc: 0.095000; val_acc: 0.078000\n",
      "(Iteration 36701 / 38200) loss: 2.304605\n",
      "(Iteration 36801 / 38200) loss: 2.304527\n",
      "(Iteration 36901 / 38200) loss: 2.304558\n",
      "(Iteration 37001 / 38200) loss: 2.304556\n",
      "(Epoch 97 / 100) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 37101 / 38200) loss: 2.304561\n",
      "(Iteration 37201 / 38200) loss: 2.304554\n",
      "(Iteration 37301 / 38200) loss: 2.304572\n",
      "(Iteration 37401 / 38200) loss: 2.304583\n",
      "(Epoch 98 / 100) train acc: 0.111000; val_acc: 0.078000\n",
      "(Iteration 37501 / 38200) loss: 2.304513\n",
      "(Iteration 37601 / 38200) loss: 2.304594\n",
      "(Iteration 37701 / 38200) loss: 2.304541\n",
      "(Iteration 37801 / 38200) loss: 2.304564\n",
      "(Epoch 99 / 100) train acc: 0.106000; val_acc: 0.078000\n",
      "(Iteration 37901 / 38200) loss: 2.304553\n",
      "(Iteration 38001 / 38200) loss: 2.304568\n",
      "(Iteration 38101 / 38200) loss: 2.304540\n",
      "(Epoch 100 / 100) train acc: 0.102000; val_acc: 0.078000\n",
      "Best validation accuracy: 0.1509\n",
      "Best hyperparameters: {'hidden_size': 50, 'learning_rate': 1e-07, 'num_epochs': 80, 'reg': 0.5, 'lr_decay': 0.95, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "from cs231n.classifiers.fc_net import TwoLayerNet\n",
    "from cs231n.solver import Solver\n",
    "import itertools\n",
    "\n",
    "data = {\n",
    "    'X_train': X_train_feats, \n",
    "    'y_train': y_train, \n",
    "    'X_val': X_val_feats, \n",
    "    'y_val': y_val, \n",
    "    'X_test': X_test_feats, \n",
    "    'y_test': y_test, \n",
    "}\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TODO: Train a two-layer neural network on image features. You may want to    #\n",
    "# cross-validate various parameters as in previous sections. Store your best   #\n",
    "# model in the best_net variable.                                              #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "#################################################################################\n",
    "### FUNCTION THAT PERFORMS THE TRAINING BASED ON PARAMETERS\n",
    "\n",
    "def train_ann(data, hidden_size, learning_rate, lr_decay, num_epochs, reg, batch_size):\n",
    "\n",
    "    input_size = data['X_train'].shape[1]\n",
    "    num_classes = 10\n",
    "    \n",
    "    model = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "    model.reg = reg\n",
    "\n",
    "    solver = Solver(model, \n",
    "                    data,\n",
    "                    update_rule='sgd',\n",
    "                    optim_config={\n",
    "                        'learning_rate': learning_rate,\n",
    "                    },\n",
    "                    lr_decay=lr_decay,\n",
    "                    num_epochs=num_epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    print_every=100)\n",
    "\n",
    "    solver.train()   \n",
    "\n",
    "    return solver, model\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "best_val_accuracy = -1\n",
    "results = {}\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_size': [ 50, 100],  \n",
    "    'learning_rate': [1e-7, 1e-4],  \n",
    "    'num_epochs': [80, 100],  \n",
    "    'reg': [0.5, 0.7],  \n",
    "    'lr_decay': [0.9, 0.95],  \n",
    "    'batch_size': [64, 128]  \n",
    "}\n",
    "\n",
    "# Exhaustive param_grid\n",
    "\n",
    "param_grid_exh = {\n",
    "    'hidden_size': [50, 100, 200, 300],  \n",
    "    'learning_rate': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2],  \n",
    "    'num_epochs': [10, 30, 50, 80, 100],  \n",
    "    'reg': [ 0.2, 0.3, 0.5, 0.7],  \n",
    "    'lr_decay': [0.9, 0.95, 0.99],  \n",
    "    'batch_size': [64, 128, 256, 512]  \n",
    "}\n",
    "\n",
    "# Grid search with itertools.product\n",
    "keys, values = zip(*param_grid.items())\n",
    "\n",
    "for v in itertools.product(*values):\n",
    "    params = dict(zip(keys, v))\n",
    "\n",
    "    print(f\"Training with parameters: {params}\")\n",
    "\n",
    "    solver, model = train_ann(data, **params)\n",
    "\n",
    "    # Update dictionary (train, val)\n",
    "    results[tuple(params.items())] = (np.mean(solver.train_acc_history), np.mean(solver.val_acc_history))\n",
    "\n",
    "    # Track the best model\n",
    "    if np.mean(solver.val_acc_history) > best_val_accuracy:\n",
    "        best_val_accuracy = np.mean(solver.val_acc_history)\n",
    "        best_model = model\n",
    "        best_solver = solver\n",
    "        best_params = params  # Save the best hyperparameters\n",
    "        print(f\"New best model found with validation accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "# Summary of best results\n",
    "print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "441fd5c2",
   "metadata": {
    "test": "nn_test_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.122\n"
     ]
    }
   ],
   "source": [
    "# Run your best neural net classifier on the test set. You should be able\n",
    "# to get more than 55% accuracy.\n",
    "\n",
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "test_acc = (y_test_pred == data['y_test']).mean()\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151c2ba",
   "metadata": {},
   "source": [
    "Best iteration on the small set of parameters got a test accuracy of: 0.163.\n",
    "The more exhaustive param_grid took too much time to train."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
